module.exports={"all_articles":{"total":454,"items":[{"calculated_slug":"/products/atlas/building-e-commerce-content-catalog-atlas-search","content":"Search is now a fundamental part of applications across all industries—but especially so in the world of retail and e-commerce. If your customers can’t find what they’re looking for, they’ll go to another website and buy it there instead. The best way to provide your customers with a great shopping experience is to provide a great search experience. As far as searching goes, Atlas Search, part of  MongoDB Atlas, is the easiest way to build rich, fast, and relevance-based search directly into your applications. In this tutorial, we’ll make a website that has a simple text search and use Atlas Search to integrate full-text search capabilities, add autocomplete to our search box, and even promote some of our products on sale. \n\n## Pre-requisites\n\nYou can find the complete source code for this application on [Github](https://github.com/joellord/content-catalog). The application is built using the [MERN stack](https://www.mongodb.com/mern-stack). It has a Node.js back end running the express framework, a MongoDB Atlas database, and a React front end.\n\n## Getting started\n\nFirst, start by cloning the repository that contains the starting source code.\n\n```bash\ngit clone https://github.com/mongodb-developer/content-catalog\ncd content-catalog\n```\n\nIn this repository, you will see three sub-folders:\n\n* `mdbstore`: contains the front end\n* `backend`: has the Node.js back end\n* `data`: includes a dataset that you can use with this e-commerce application\n\n### Create a database and import the dataset\n\nFirst, start by creating a free MongoDB Atlas cluster by following the [instructions from the docs](https://www.mongodb.com/docs/atlas/tutorial/deploy-free-tier-cluster/). Once you have a cluster up and running, find your [connection string](https://www.mongodb.com/docs/guides/atlas/connection-string/). You will use this connection string with `mongorestore` to import the provided dataset into your cluster.\n\n>You can find the installation instructions and usage information for `mongorestore` from the [MongoDB documentation](https://www.mongodb.com/docs/database-tools/mongorestore/). \n\nUse your connection string without the database name at the end. It should look like `mongodb+srv://user:password@cluster0.xxxxx.mongodb.net`\n\n```bash\ncd data\nmongorestore <CONNECTION_STRING>\n```\n\nThis tool will automatically locate the BSON file from the dump folder and import these documents into the `items` collection inside the `grocery` database.\n\nYou now have a dataset of about 20,000 items to use and explore.\n\n### Start the Node.js backend API\n\nThe Node.js back end will act as an API that your front end can use. It will be connecting to your database by using a connection string provided in a `.env` file. Start by creating that file.\n\n```bash\ncd backend\ntouch .env\n```\n\nOpen your favourite code editor, and enter the following in the `.env` file. Change <CONNECTION_STRING> to your current connection string from MongoDB Atlas.\n\n```\nPORT=5050\nMONGODB_URI=<CONNECTION_STRING>\n```\n\nNow, start your server. You can use the `node` executable to start your server, but it’s easier to use `nodemon` while in development. This tool will automatically reload your server when it detects a change to the source code. You can find out more about installing the tool from [the official website](https://nodemon.io/).\n\n```bash\nnodemon .\n```\n\nThis command will start the server. You should see a message in your console confirming that the server is running and the database is connected.\n\n### Start the React frontend application\n\nIt’s now time to start the front end of your application. In a new terminal window, go to the `mdbstore` folder, install all the dependencies for this project, and start the project using `npm`.\n\n```bash\ncd ../mdbstore\nnpm install\nnpm start\n```\n\nOnce this is completed, a browser tab will open, and you will see your fully functioning store. The front end is a React application. Everything in the front end is already connected to the backend API, so we won’t be making any changes here. Feel free to explore the source code to learn more about using React with a Node.js back end.\n\n### Explore the application\n\nYour storefront is now up and running. A single page lets you search for and list all products. Try searching for `chicken`. Well, you probably don’t have a lot of results. As a matter of fact, you won't find any result. Now try `Boneless Chicken Thighs`. There’s a match! But that’s not very convenient. Your users don’t know the exact name of your products. Never mind possible typos or mistakes. This e-commerce offers a very poor experience to its customers and risks losing some business. In this tutorial, you will see how to leverage Atlas Search to provide a seamless experience to your users.\n\n## Add full-text search capabilities\n\nThe first thing we’ll do for our users is to add full-text search capabilities to this e-commerce application. By adding a [search index](https://www.mongodb.com/docs/atlas/atlas-search/create-index/), we will have the ability to search through all the text fields from our documents. So, instead of searching only for a product name, we can search through the name, category, tags, and so on.\n\nStart by creating a search index on your collection. Find your collection in the MongoDB Atlas UI and click on Search in the top navigation bar. This will bring you to the Atlas Search Index creation screen. Click on Create Index.\n\n![The Create Search Index UI in Atlas](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/create_search_index_ui_be0dc64467.png)\n\nFrom this screen, click Next to use the visual editor. Then, choose the newly imported data—‘grocery/items’, on the database and collection screen. Accept all the defaults and create that index.\n\nWhile you’re there, you can also create the index that will be used later for autocomplete. Click Create Index again, and click Next to use the visual editor. Give this new index the name `autocomplete`, select ‘grocery/items’ again, and then click Next.\n\nOn the following screen, click the Refine Index button to add the autocomplete capabilities to the index. Click on the Add Field button to add a new field that will support autocomplete searches. Choose the `name` field in the dropdown. Then toggle off the `Enable Dynamic Mapping` option. Finally, click Add data type, and from the dropdown, pick autocomplete. You can save these settings and click on the Create Search Index button. You can find the detailed instructions to set up the index in this [tutorial](https://www.mongodb.com/docs/atlas/atlas-search/tutorial/autocomplete-tutorial/).\n\n![The UI with the necessary settings](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ui_with_necessary_settings_f9a0c5c7c5.png)\n\nOnce your index is created, you will be able to use the $search stage in an aggregation pipeline. The $search stage enables you to perform a full-text search in your collections. You can experiment by going to the Aggregations tab once you’ve selected your collection or [using Compass](https://www.mongodb.com/docs/atlas/compass-connection/), the MongoDB GUI.\n\nThe first aggregation pipeline we will create is for the search results. Rather than returning only results that have an exact match, we will use Altas Search to return all similar results or close to the user search intent. \n\nIn the Aggregation Builder screen, create a new pipeline by adding a first $search stage.\n\n![The Aggregation Builder in Compass](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/aggregation_builder_compass_47cc8fe897.png)\n\nYou use the following JSON for the first stage of your pipeline.\n\n```javascript\n{\n  index: 'default',\n  text: {\n    query: \"chicken\",\n    path: [\"name\"]\n  }\n}\n```\n\nAnd voilà! You already have much better search results. You could also add other [stages](https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/) here to limit the number of results or sort them in a specific order. For this application, this is all we need for now. Let’s try to import this into the API used for this project.\n\nIn the file _backend/index.js_, look for the route that listens for GET requests on `/search/:query`. Here, replace the code between the comments with the code you used for your aggregation pipeline. This time, rather than using the hard-coded value, use `req.params.query` to use the query string sent to the server.\n\n```javascript\n    /** TODO: Update this to use Atlas Search */\n    results = await itemCollection.aggregate([\n      { $search: {\n          index: 'default',\n          text: {\n            query: req.params.query,\n            path: [\"name\"]\n          }\n        }\n      }\n    ]).toArray();\n    /** End */\n```\n\n\nThe old code used the `find()` method to find an exact match. This new code uses the newly created Search index to return any records that would contain, in part or in full, the search term that we’ve passed to it.\n\nIf you try the application again with the word “Chicken,” you will get much more results this time. In addition to that, you might also notice that your searches are also case insensitive. But we can do even better. Sometimes, your users might be searching for more generic terms, such as one of the tags that describe the products or the brand name. Let’s add more fields to this search to return more relevant records. \n\nIn the `$search` stage that you added in the previous code snippet, change the value of the path field to contain all the fields you want to search.\n\n```javascript\n    /** TODO: Update this to use Atlas Search */\n    results = await itemCollection.aggregate([\n      { $search: {\n          index: 'default',\n          text: {\n            query: req.params.query,\n            path: [\"name\", \"brand\", \"category\", \"tags\"]\n          }\n        }\n      }\n    ]).toArray();\n    /** End */\n```\n\nExperiment with your new application again. Try out some brand names that you know to see if you can find the product you are looking for. \n\nYour search capabilities are now much better, and the user experience of your website is already improved, but let’s see if we can make this even better.\n\n## Add autocomplete to your search box\n\nA common feature of most modern search engines is an autocomplete dropdown that shows suggestions as you type. In fact, this is expected behaviour from users. They don’t want to scroll through an infinite list of possible matches; they’d rather find the right one quickly. \n\nIn this section, you will use the Atlas Search autocomplete capabilities to enable this in your search box. The UI already has this feature implemented, and you already created the required indexes, but it doesn’t show up because the API is sending back no results. \n\nOpen up the aggregation builder again to build a new pipeline. Start with a $search stage again, and use the following. Note how this $search stage uses the `autocomplete` stage that was created earlier.\n\n```javascript\n{\n  'index': 'autocomplete', \n  'autocomplete': {\n    'query': \"chic\", \n    'path': 'name'\n  }, \n  'highlight': {\n    'path': [\n      'name'\n    ]\n  }\n}\n```\n\nIn the preview panel, you should see some results containing the string “chic” in their name. That’s a lot of potential matches. For our application, we won’t want to return all possible matches. Instead, we’ll only take the first five. To do so, a $limit stage is used to limit the results to five. Click on Add Stage, select $limit from the dropdown, and replace `number` with the value `5`.\n\n![The autocomplete aggregation pipeline in Compass](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/autocomplete_aggregation_pipeline_compass_6cc4aa24fe.png)\n\nExcellent! Now we only have five results. Since this request will be executed on each keypress, we want it to be as fast as possible and limit the required bandwidth as much as possible. A $project stage can be added to help with this—we will return only the ‘name’ field instead of the full documents. Click Add Stage again, select $project from the dropdown, and use the following JSON.\n\n```javascript\n{\n  'name': 1, \n  'highlights': {\n    '$meta': 'searchHighlights'\n  }\n}\n```\n\nNote that we also added a new field named `highlights`. This field returns the metadata provided to us by Atlas Search. You can find much information in this metadata, such as each item's score. This can be useful to sort the data, for example.\n\nNow that you have a working aggregation pipeline, you can use it in your application.\n\nIn the file _backend/index.js_, look for the route that listens for GET requests on `/autocomplete/:query`. After the `TODO` comment, add the following code to execute your aggregation pipeline. Don’t forget to replace the hard-coded query with `req.params.query`. You can [export the pipeline](https://www.mongodb.com/docs/compass/current/export-query-to-language/) directly from Compass or use the following code snippet.\n\n```javascript\n    // TODO: Insert the autocomplete functionality here\n    results = await itemCollection.aggregate([\n      {\n        '$search': {\n          'index': 'autocomplete', \n          'autocomplete': {\n            'query': req.params.query, \n            'path': 'name'\n          }, \n          'highlight': {\n            'path': [\n              'name'\n            ]\n          }\n        }\n      }, {\n        '$limit': 5\n      }, {\n        '$project': {\n          'name': 1, \n          'highlights': {\n            '$meta': 'searchHighlights'\n          }\n        }\n      }\n    ]).toArray();\n    /** End */\n```\n\nGo back to your application, and test it out to see the new autocomplete functionality. \n\n![The final application in action](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/final_application_in_action_79650c4cc7.png)\n\nAnd look at that! Your site now offers a much better experience to your developers with very little additional code. \n\n## Add custom scoring to adjust search results\n\nWhen delivering results to your users, you might want to push some products forward. Altas Search can help you promote specific results by giving you the power to change and tweak the relevance score of the results. A typical example is to put the currently on sale items at the top of the search results. Let’s do that right away.\n\nIn the _backend/index.js_ file, replace the database query for the `/search/:query` route again to use the following aggregation pipeline.\n\n```javascript\n    /** TODO: Update this to use Atlas Search */\n    results = await itemCollection.aggregate([\n      { $search: {\n          index: 'default',\n          compound: {\n            must: [\n              {text: {\n                query: req.params.query,\n                path: [\"name\", \"brand\", \"category\", \"tags\"]\n              }},\n              {exists: {\n                path: \"price_special\",\n                score: {\n                  boost: {\n                    value: 3\n                  }\n                }\n              }}\n            ]\n          }\n        }\n      }\n    ]).toArray();\n    /** End */\n```\n\nThis might seem like a lot; let’s look at it in more detail. \n\n```javascript\n      { $search: {\n          index: 'default',\n          compound: {\n            must: [\n              {...},\n              {...}\n            ]\n          }\n        }\n      }\n```\n\nFirst, we added a `compound` object to the `$search` operator. This lets us use two or more operators to search on. Then we use the `must` operator, which is the equivalent of a logical `AND` operator. In this new array, we added two search operations. The first one is the same `text` as we had before. Let’s focus on that second one.\n\n```javascript\n{\nexists: {\n  path: \"price_special\",\n  score: {\n    boost: {\n      value: 3\n    }\n  }\n}\n```\n\nHere, we tell Atlas Search to boost the current relevance score by three if the field `price_special` exists in the document. By doing so, any document that is on sale will have a much higher relevance score and be at the top of the search results. If you try your application again, you should notice that all the first results have a sale price.\n\n## Add fuzzy matching\n\nAnother common feature in product catalog search nowadays is fuzzy matching. Implementing a fuzzy matching feature can be somewhat complex, but Atlas Search makes it simpler. In a `text` search, you can add the `fuzzy` field to specify that you want to add this capability to your search results. You can tweak this functionality using [multiple options](https://www.mongodb.com/docs/atlas/atlas-search/text/#fuzzy-examples), but we’ll stick to the defaults for this application.\n\nOnce again, in the _backend/index.js_ file, change the `search/:query` route to the following.\n\n```javascript\n    /** TODO: Update this to use Atlas Search */\n    results = await itemCollection.aggregate([\n      { $search: {\n          index: 'default',\n          compound: {\n            must: [\n              {text: {\n                query: req.params.query,\n                path: [\"name\", \"brand\", \"category\", \"tags\"],\n                fuzzy: {}\n              }},\n              {exists: {\n                path: \"price_special\",\n                score: {\n                  boost: {\n                    value: 3\n                  }\n                }\n              }}\n            ]\n          }\n        }\n      }\n    ]).toArray();\n    /** End */\n```\n\nYou’ll notice that the difference is very subtle. A single line was added.\n\n```javascript\nfuzzy: {}\n```\n\nThis enables fuzzy matching for this `$search` operation. This means that the search engine will be looking for matching keywords, as well as matches that could differ slightly. Try out your application again, and this time, try searching for `chickn`. You should still be able to see some results.\n\nA fuzzy search is a process that locates web pages that are likely to be relevant to a search argument even when the argument does not exactly correspond to the desired information.\n\n## Summary\n\nTo ensure that your website is successful, you need to make it easy for your users to find what they are looking for. In addition to that, there might be some products that you want to push forward. Atlas Search offers all the necessary tooling to enable you to quickly add those features to your application, all by using the same MongoDB Query API you are already familiar with. In addition to that, there’s no need to maintain a second server and synchronize with a search engine. \n\nAll of these features are available right now on [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/register). If you haven’t already, why not give it a try right now on our free-to-use clusters?","description":"In this tutorial, we’ll make a website that has a simple text search and use Atlas Search to promote some of our products on sale.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt9be92f512ddc42e5/644c4ae6b92bcd10d4787eb8/build-ecommerce-content-catalog.png?branch=prod","description":null}}]},"slug":"/building-e-commerce-content-catalog-atlas-search","title":"*Building an E-commerce Content Catalog with Atlas Search","original_publish_date":"2022-05-31T19:00:00.000Z","strapi_updated_at":"2022-06-27T14:52:45.000Z","expiry_date":null,"authorsConnection":{"edges":[{"node":{"title":"*Joel Lord","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt528a91a6d9322a8b/644c4543ebc12af6f65d7ca1/profile-no-bg_1200x1200.png?branch=prod"}}]},"bio":"Joel Lord is passionate about the web and technology in general. He likes to learn new things, but most of all, he wants to share his discoveries. He does so by travelling at various conferences all across the globe.\nHe graduated from college in computer programming in the last millennium. Apart from a little break to get his BSc in computational astrophysics, he was always in the industry.\nIn his daily job, Joel is a developer advocate with MongoDB, where he connects with software engineers to help them make the web better by using best practices in web development.\nDuring his free time, he can be found stargazing on a campground somewhere or brewing a fresh batch of beer in his garage.\n","calculated_slug":"/author/joel-lord","twitter":"https://twitter.com/joel__lord "}}]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":"","l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Search","calculated_slug":"/products/atlas/search"}}]},"livesite_url":"","programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-06-02T03:47:29.355Z","publish_details":{"time":"2023-06-02T03:48:28.634Z"}}},{"calculated_slug":"/products/realm/realm-graphql-demo-custom-resolvers","content":"## Motivation\n\nRealm offers several simple ways to provide third-party access to our data. One of them can be through the so-called [Webhooks](https://docs.mongodb.com/realm/services/configure/service-webhooks/). Through these, we could offer in a controlled way different endpoints to access our services and therefore our data.\n\nA real example would be to provide such webhooks to customers or partners so that they can access certain information necessary for the business logic existing within that particular use case.\n\nHowever, this way of solving this problem has certain disadvantages. The most important ones include:\n\n1. **Fixed-size for the response payload**: The size of the response cannot exceed **4 MB**. This limitation would prevent us from being able to query aggregated data that would require a response larger than 4 MB.\n2. **Creation of different endpoints for each business logic requirement**. This would not only pose a management problem, but also a maintenance problem: what if we make changes in the future? How do we manage the different versions? This model becomes more complex if we want to send parameters in the request to tailor the response to the requirements.\n\n***So what could be a more efficient solution?***\n\nRealm offers the possibility to use its GraphQL API to cover this and other needs. The advantages of using GraphQL for this use case could be listed as:\n\n1. An easier and simpler method to execute accurate endpoint calls.\n2. Easier to choose what we need in the response, alleviating the size of the response by not requiring unnecessary fields.\n3. Simpler maintenance.\n4. Avoid versioning.\n\n## What are we going to build?\n\nMaking use of the [sample available datasets in MongoDB](https://docs.atlas.mongodb.com/sample-data/available-sample-datasets/). We will build a filter to be able to return those movies that meet a set of requirements. These requirements are:\n\n1. Having a given IMDB rating.\n2. Belonging to a set of genres.\n3. Being a certain rate.\n4. Being in several available languages.\n\nThe filtering parameters will be dynamic so that we can return those that best fit our criteria.\n\nFor this, we will use a **GraphQL Custom Resolver** and an external client application to execute our queries.\n\n## Prerequisites\n\nThis tutorial will provide a step-by-step guide to run the demo. To do this we must follow these prerequisites:\n\n1. Have a Cloud MongoDB account.\n2. Create a Free Tier Cluster.\n3. Configure `realm-cli` .\n4. Add an API-Key to be able to access using `realm-cli`.\n5. Load initial data into the Cluster.\n\n### Create an Atlas Account\n\nTo begin, you’ll need a MongoDB Atlas account. If you’ve already got an existing MongoDB Atlas Account, you can skip this step and jump to [**Install the Realm CLI section**](#install-the-realm-cli). If you don’t have an Atlas account, follow the steps below to create one:\n\n1. Navigate to the [MongoDB Atlas login page](https://account.mongodb.com/account/register).\n2. Click Login.\n3. Either enter a new set of user credentials or click the Sign Up with Google button.\n4. Click Sign Up to create your account.\n5. Follow the prompts to create an organization and project in your Atlas account. You can use the default suggested names or enter your own.\n\n![Account startup screenshot](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_A_Du4xd_Ay_MD_Ak_N3_B6_37497a6abc)\n\nWhen you finish creating your organization and project, you should end up on a screen that prompts you to create an Atlas cluster:\n\n![Free Tier Cluster screenshot](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_tt_Wrkz45j_WO_Xag7_af1456d227)\n\n## Create a Free Atlas Cluster\n\nNext, you’ll need a MongoDB Atlas cluster running MongoDB 4.4 or higher. If you’ve already created a free cluster in your Atlas project running a version of MongoDB other than 4.4, you can [create a new project in Atlas](https://docs.atlas.mongodb.com/tutorial/manage-projects/#procedure) and then create a new cluster running MongoDB 4.4 in that project using the instructions below. If you haven’t created any clusters yet, follow the instructions below to create your first free cluster:\n\n1. Log into your MongoDB Atlas account at [cloud.mongodb.com](https://cloud.mongodb.com/).\n2. Once you’re logged into your account, Atlas should prompt you to create your first cluster. In the Shared Clusters category, click Create a Cluster. Alternatively, you can click Build a Cluster from the project view in your Atlas account.\n3. Under Cloud Provider & Region, select AWS and N. Virginia (us-east-1).\n4. Under Additional Settings, select MongoDB 4.4 from the Select a Version dropdown.\n5. Under Cluster Name, enter the name Cluster0 for your new cluster.\n6. Click the Create Cluster button at the bottom of the page.\n\nAfter creating your cluster, Atlas should launch the project view for your Atlas account. In this view, you’ll see Atlas’s progress as it initializes your new cluster:\n\n![Screenshot of a recently created cluster](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_f9_B_Zh_Cw7_V7_XVR_8d_O_06a5e01c6c)\n\n## Install the Realm CLI\n\nNow that you’ve created a cluster to use as the data source for your Realm app, we need some way to create the app itself. In most cases, you’d use the Realm UI, which you can access through the Atlas UI. However, for this tutorial, we’re going to use the [Realm Command Line Interface](https://docs.mongodb.com/realm/deploy/realm-cli-reference/#std-label-realm-cli), also known as `realm-cli`.\n\nWe’re using the Realm CLI because it allows you to manage your Realm apps programmatically using JSON configuration files instead of the Realm UI.\n\nThis lets you get started with a pre-prepared app configuration faster. Follow the instructions below to install the Realm CLI in your development environment using either a package manager or the `realm-cli` binary:\n\nRealm CLI is available on npm. To install it on your system, ensure that you have [Node.js](https://nodejs.org/en/download/) installed and then run the following command in your shell:\n\n```\nnpm install -g mongodb-realm-cli@beta\n```\n\nAfter installing the `realm-cli`, you can run the following command to confirm that your installation was successful:\n\n```\nrealm-cli --version\n```\n\nIf you see output containing a version number such as `2.0.0-beta.4`, your `realm-cli` installation was successful.\n\n## Add an API Key to Your Atlas Project & Log into the Realm CLI\n\nNow that you’ve got `realm-cli`installed to your development environment, you’ll need a way to authenticate using `realm-cli`. For security reasons, `realm-cli`only allows login using a programmatic API key, so we’ll begin by creating a programmatic API Key that you can use to administrate your new Atlas project:\n\n* Click **Access Manager** at the top of the Atlas UI. Select the **Project Access** option from the dropdown.\n* Navigate to the **API Keys** tab.\n* Click the **Create API** Key button.\n* In the **Description** text box, enter “API Key for the MongoDB Realm CLI”.\n\n![Create API key screenshot in Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_GTFJ_7z_FF_2q_D_c_Zpd_935dddcc53)\n\n* In the **Project Permissions** dropdown, select “Project Owner” and deselect “Project Read Only”.\n\n![Project Permissions screenshot](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_vr1_F4_D5e_Vhq_3we_efc619b9df)\n\n* Copy your Public API Key and save it somewhere.\n* Click **Next**.\n* Copy your Private API Key and save it somewhere; after leaving this page, you will no longer be able to view it via the Realm UI.\n* Click the **Add Access List Entry** button.\n* Click **Use Current IP Address**.\n* Click **Save**.\n* When you have safely recorded your private API key, click **Done** to navigate back to the Project Access Manager page.\n* Use the following command in your terminal to authenticate with the Realm CLI:\n\n``` bash\nrealm-cli login --api-key <public API key> --private-api-key <private API key>\n```\n\nIf `realm-cli` produces output like the following, you have successfully authenticated:\n\n```\nyou have successfully logged in as <public API key>\n```\n\n## Load sample dataset to the Cluster\n\nOnce we have deployed the Cluster, we can make use of the sample collections that MongoDB provides. To do this, we must click on “…” and “Load Sample Dataset”.\n\n![Load Sample Dataset screenshot](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_7_T7_Ifl_Cl_Qr_BLYFDQ_bac2090539)\n\nThis process may take a few minutes due to the size of the sample collections. They are approximate \\~**350 MB**. Once it has finished, we can verify, by clicking on collections, that the [`sample_mflix.movies`](https://docs.atlas.mongodb.com/sample-data/sample-mflix/) the collection has been loaded successfully.\n\n![One document of Movie collection screenshot](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_Eg4_Iaz_Vr_ELU_9p_CL_8_22cc39b57e)\n\nAnother way to check that the data has been loaded into our Cluster and interact with it can be by installing[ MongoDB Compass](https://www.mongodb.com/products/compass) or accessing it through a terminal and [Mongo shell](https://docs.mongodb.com/manual/mongo/).\n\n## Adding Rules to our collections\n\nTo use GraphQL, we need to configure rules for the collections that we are going to use, to do so we go to **DATA ACCESS** and **RULES** in Realm UI.\n\nWe then need to select the newest imported `sample_mflix.movies`collection and click on “**Configure Collection**”. We can configure it without selecting any Template (in the future we can make changes to it).\n\n![Rules section in Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_ky_Pqx_V0v_Ltossbn_R_a471fd8610)\n\nOnce the collection is selected and after clicking “**Configure Collection**”, we must select the “**All Additional Fields**” option and check the “**Read**” box. At this point, this will allow any authenticated user to read the data in this collection. This will be necessary to be able to later make requests through external clients such as Postman.\n\n![Rules section in Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_Hbqfhn_Rf_I_Ut_Kboco_311418b5df)\n\nAfter this step, we can configure the **Schema**. To do this and once in the “**Schema**” tab, we can use the documents already loaded in the collection for Realm to generate a schema from them.\n\n![Generate Schema in Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_u_Iqel7_Yf_V40_LY_5_V_b8e8601a29)\n\nAfter waiting a few minutes, we will be able to consult the Schema created on this same screen.\n\n![Schema generated for the movies collection](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_O_Dpul_PC_Rs_Oe_Rip6l_b9d9631bef)\n\n## Add an authentication provider\n\nAll the requests that we are going to make through our GraphQL client must be authenticated. For this, we can activate any of the authentication providers available in Realm.\n\nFor this example, we will use **API Keys**, this way we could create an API Key for each of our clients/partners and disable access in the future if needed.\n\nOnce the authentication provider has been activated, we generate a new key and copy the value provided by the interface.\n\n![API Key Authentication Provider](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_nl_S_Jvk_Cak0_Lz_Zc_H_ee0c35e7ba)\n\n## Testing GraphQL with GraphiQL\n\nRealm provides an embedded GraphiQL interface to test requests directly.\n\nThese requests do not need to be authenticated and allow you to test the requests before making them in a real scenario.\n\nTo check that everything is configured correctly, we can perform the following Query.\n\n```\nquery {\n  movie {\n    _id\n  }\n}\n```\n\nThis request is a simple Query to the movies collection where we are requesting to return just the `_id`. From a MongoDB point of view, this request could be equated to the following `mongoshell` method:\n\n``` javascript\ndb.movies.findOne({},{\"_id\" : 1})\n```\n\nOne of the advantages of using the GraphQL API in Realm is that it generates the Schema automatically for the configured collections. We can check this by navigating to the “**Schema**” tab in GraphQL and verifying that indeed the schema for the “**movies**” collection is generated. At this point, we can download the Schema for later use in a third-party GraphQL client such as Postman.\n\n## Create a custom resolver\n\nInthe description of our problem, we talk about how we need to detect in our dataset movies that correspond to a set of predefined filters. Up to this point, we could get all the movies from the dataset and perform some processing in a client application, but fortunately, we can make use of the *aggregation* *pipeline* in MongoDB to perform this transformation on the server through a custom resolver.\n\nIf we want to test the syntax needed to get all movies that match our filter we must perform this *aggregation*:\n\n``` javascript\n[{\"$match\": {\n  \"imdb.rating\": { \"$gte\": 7 },\n  \"genres\": { \"$nin\": [ \"Crime\", \"Horror\" ] } ,\n  \"rated\": { \"$in\": [\"PG\", \"G\" ] },\n  \"languages\": { \"$all\": [ \"English\", \"Japanese\" ] }\n  }\n}]\n```\n\nThe first step to perform is to “**Add a Custom Resolver**” in GraphQL.\n\n![Screenshot of Add Custom Resolver Screen in Realm](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_CB_Gypxk_Nh_Hr_U_Ev63_4045da240d)\n\nThe fields to be filled in are the following:\n\n* **GraphQL Field Name**: This corresponds to the name we want to use when we refer to this custom resolver in our queries.\n* **Parent Type**: Type of custom resolver we are creating, in our case, being a read request, we will select Query.\n* **Function**: Here we create the function that will be called every time a call to our custom resolver is executed. You can link an existing function or create it here.\n\nThe code of the function would be the following:\n\n``` javascript\nexports = async function() {\n  const request = context.services.get('mongodb-atlas').db('sample_mflix').collection(\"movies\");\n\n  const pipeline = [\n  {\n    \"$match\": {\n      \"imdb.rating\": { \"$gte\": 7 },\n      \"genres\": { \"$nin\": [ \"Crime\", \"Horror\" ] } ,\n      \"rated\": { \"$in\": [\"PG\", \"G\" ] },\n      \"languages\": { \"$all\": [ \"English\", \"Japanese\" ] }\n    }\n  }];\n\n  return await request.aggregate(pipeline).toArray()\n  .then(data => {\n    console.log(data.length);\n    return data;\n  })\n  .catch(err => {\n    console.log(err.toString());\n    return err.toString();\n  });\n};\n```\n\n![Screenshot of the Custom Resolver editor in Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_o_C_Hp9rkfhqd_Uj_Y_dc7a616c22)\n\n* **Input Type (Recommended)**: At the moment we will leave it at **None**. Later we will explain what we can do with it and what it is used for.\n* **Payload Type (Recommended)**: Type of object of the response. In our case, our *aggregate* will return a set of movies, therefore we choose **Existing Type (List)** and type **[Movie]**.\n\nAt this point, we can test our new **custom resolver** directly in Realm. To do this, in the same GraphiQL section we can write our query as follows:\n\n```\nquery {\n  oneTitleMovies {\n    title\n  }\n}\n```\n\nAfter clicking the Play button we should see the results of our query.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_2_Og_Ewm_Vo_T_Mw_GR_8_YE_3a6eca2987)\n\n## Testing our newest created **Custom Resolver** in Postman\n\nTo test our query in an external client we will use Postman. There is a [tutorial](https://learning.postman.com/docs/sending-requests/supported-api-frameworks/graphql/) to make queries in Postman with an API/Schema. In this example, we will make a simple query without a schema and therefore we will not use it (but it is worth a look).\n\nTo test this query in Postman, we will create a new POST request where the URL is provided by Realm (GraphQL Endpoint). In the body of the request, we will select GraphQL and write:\n\n```\nquery {\n  oneTitleMovies {\n    title\n    cast\n  }\n}\n```\n\nWhen working with Realm and an external GraphQL client, we need to add some kind of **authentication.** In a previous step, we created an API Key as an authentication provider, although we can use any of them.\n\nIn the request headers, we should add:\n\n``` javascript\n{\"apiKey\",\"{{api_key}}\"}\n```\n\nWhere we replace “*{{api\\_key}}*” with the value obtained previously.\n\nA general idea of what Postman’s cURL for this request would look like is:\n\n``` bash\ncurl --location --request POST '[your_graphql_endpoint]' \\\n --header 'apiKey: [your_api_key]' \\\n --header 'Content-Type: application/json' \\\n --data-raw '{\"query\": \"query {\"query oneTitleMovies {\"title\" {\"cast\", \"variables\":{}}'\n```\n\n![Postman request for a GraphQL query](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_Mu_P4o_Gnc1g660_Z1_F_33761d1369)\n\n## Add an **Input Type** to our **Custom Resolver**\n\nWhen we created our custom resolver, we associated it to a function. This function had some preset parameters:\n\n1. “imdb.rating”: { $gte: 7 }\n2. genres: { $nin: [ “Crime”, “Horror” ] } ,\n3. rated: { $in: [ “PG”, “G” ] },\n4. languages: { $all: [ “English”, “Japanese” ] } }\n\nWe can create an **Input Type** of type **Custom Type** so that those values that are fixed for the moment can be sent by parameters. **Custom Types** must be defined as a Schema in a JSON object. For our use case, our Schema will be the following:\n\n``` json\n{\n  \"bsonType\": \"object\",\n  \"title\": \"Filter\",\n  \"required\" : [\n      \"imdbRating\",\n      \"genres\",\n      \"rated\"],\n  \"properties\": {\n    \"imdbRating\": {\n      \"bsonType\": \"int\"\n    },\n    \"genres\": {\n      \"bsonType\": \"array\",\n      \"items\": {\n        \"bsonType\": \"string\"\n      }\n    },\n    \"rated\": {\n      \"bsonType\": \"array\",\n      \"items\": {\n        \"bsonType\": \"string\"\n      }\n    },\n    \"languages\": {\n      \"bsonType\": \"array\",\n      \"items\": {\n        \"bsonType\": \"string\"\n      }\n    }\n  }\n}\n```\n\nWe will make the “IMDB.rating”, “genres” and “rated” fields mandatory so that they have to be always sent and the “languages” field will be optional.\n\n![Input Type of Custom Type in Custom Resolver](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/0_R7_PGJ_98l_L4_C2x_Nlx_68abb12e92)\n\nTo be able to use the data sent by our new **Input Type**,we must modify the linked function so that we can receive them by parameters.\n\n``` javascript\nexports = async function({imdbRating, genres, rated, languages}) {\n\n  const request = context.services.get('mongodb-atlas').db('sample_mflix').collection('movies');\n\n  const lang = languages === undefined ? [\"English\", \"Japanese\"] : languages;\n\n  const pipeline = [\n  {\n    $match: {\n      \"imdb.rating\": { \"$gte\": imdbRating },\n      \"genres\": { \"$nin\": genres } ,\n      \"rated\": { \"$in\": rated },\n      \"languages\": { \"$all\": lang }\n    }\n  }];\n\n  return await request.aggregate(pipeline).toArray()\n  .then(data => {\n    console.log(data.length);\n    return data;\n  })\n  .catch(err => {\n    console.log(err.toString());\n    return err.toString();\n  });\n};\n```\n\nSince we know that “imdbRating”, “genres” and “rated” are mandatory, we can assume that they will always come as parameters and therefore we assign them directly to our *aggregation*. For the “languages” field as it is optional, we will have to verify that there is indeed an associated value and if not we will send default values.\n\nNow we can test this query in our external GraphQL client. The query would look like this (to get the same results as at the beginning):\n\n```\nquery {\n  oneTitleMovies(input: {\n      imdbRating: 7\n      genres: [\n          \"Crime\"\n          \"Horror\"\n      ]\n      rated: [\n          \"PG\"\n          \"G\"\n      ]\n      languages: [\n          \"English\"\n          \"Japanese\"\n      ]\n  }) {\n      title\n  }\n}\n```\n\nFrom here we can play with the different fields of our input to filter our results. One of the advantages of using GraphQL as a replacement for a Rest API is that the fields or response values can be selected in advance. In our example above, we are only returning the “title” field, but we could return a subset of all the fields in the “Movies” collection.\n\n## Wrapping up\n\nRealm GraphQL is a powerful tool to create serverless applications that can easily cover all your basic and complex use cases. Using Realm as a BaaS can help you build and deploy applications faster than ever.\n\nIn this tutorial, we have learned how to create a custom resolver linked to a function to resolve an aggregation pipeline. You can simply adapt this example to your own complex use case.\n\nQuestions? Comments? We'd love to connect with you. Join the conversation on the [MongoDB Community Forums.](https://developer.mongodb.com/community/forums/c/realm/9)\n\n## Download example code from GitHub\n\nYou can download the sample code from [here](https://github.com/josmanperez/realmGraphQLCustomResolverDemo) and [import](https://docs.mongodb.com/realm/deploy/realm-cli-reference/) it into your Realm application with\n\n``` bash\nrealm-cli import \\\n  --app-id=myapp-abcde \\\n  --path=./path/to/app/dir \\\n  --strategy=merge \\\n  --include-hosting \\\n  --include-dependencies\n```","description":"The aim of this tutorial is to learn how to use Custom Resolvers for complex use cases.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt1929eac8bb9f9073/644c463847e9ccb5b3cd0921/realm-logo.jpeg?branch=prod","description":null}}]},"slug":"/realm-graphql-demo-custom-resolvers","title":"*Realm GraphQL Demo: Custom Resolvers","original_publish_date":"2021-05-25T14:26:19.000Z","strapi_updated_at":"2022-07-19T16:19:21.000Z","expiry_date":null,"authorsConnection":{"edges":[{"node":{"title":"*Megha Arora","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt1dfda2d2df56320a/644c459a2bc49439e685eddb/aJ9O4Heo_400x400.jpg?branch=prod"}}]},"bio":"","calculated_slug":"/author/megha-arora","twitter":""}}]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":"","l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":"","programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*GraphQL","calculated_slug":"/technologies/graphql"}}]}},"seo":{"canonical_url":"","meta_description":"The aim of this tutorial is to learn how to use Custom Resolvers for complex use cases.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf8c1d8d29ac7df3a/644c4639d46f521a109d4a06/og-realm-logo.jpeg?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-05-30T04:39:19.748Z","publish_details":{"time":"2023-05-30T04:39:32.607Z"}}},{"calculated_slug":"/languages/swift/authentication-ios-apps-atlas-app-services","content":"Authentication is one of the most important features for any app these days, and there will be a point when your users might want to reset their password for different reasons.\n\nAtlas App Services can help implement this functionality in a clear and simple way. In this tutorial, we’ll develop a simple app that you can follow along with and incorporate into your apps.\n\nIf you also want to follow along and check the code that I’ll be explaining in this article, you can find it in the [Github repository](https://github.com/mongodb-developer/Reset-Password-Swift). \n\n## Context\n\nThe application consists of a login flow where the user will be able to create their own account by using a username/password. It will also allow them to reset the password by implementing the use of Atlas App Services for it and [Universal Links](https://developer.apple.com/ios/universal-links/).\n\nThere are different options in order to implement this functionality.\n\n* You can configure an email provider to [send a password reset email</u>](https://www.mongodb.com/docs/atlas/app-services/authentication/email-password/#send-a-password-reset-email)<span class=\"colour\" style=\"color:rgb(0, 0, 0)\">. This option will send an email to the user with the MongoDB logo and a URL that contains the necessary parameters that will be needed in order to reset the password.\n* App Services can automatically run a password reset function. You can implement it guided by our [password reset documentation](https://www.mongodb.com/docs/atlas/app-services/authentication/email-password/#run-a-password-reset-function). App Services passes this function unique confirmation tokens and data about the user. Use these values to define custom logic to reset a user's password.\n* If you decide to use a custom password reset email from a specific domain by using an external service, when the email for the reset password is received, you will get a URL that will be valid for 30 minutes, and you will need to implement [Universal Links](https://developer.apple.com/documentation/xcode/allowing-apps-and-websites-to-link-to-your-content?language=objc) for it so your app can detect the URL when the user taps on it and extract the tokens from it.\n* You can define a function for App Services to run when you callResetPasswordFunction() in the SDK. App Services passes this function with unique confirmation tokens.\n\nFor this tutorial, we are going to use the first option. When it gets triggered, it will send the user an email and a valid URL for 30 minutes. But please be aware that we do not recommend using this option in production. Confirmation emails are not currently customizable beyond the base URL and subject line. In particular, they always come from a mongodb.com email address. For production apps, we recommend using a confirmation function. You can check [how to run a confirmation function in our MongoDB documentation](https://www.mongodb.com/docs/atlas/app-services/authentication/email-password/#run-a-confirmation-function).\n\n## Configuring authentication\n\nFirst, you’ll need to create your Atlas App Services App. I recommend [following our documentation](https://www.mongodb.com/docs/atlas/app-services/manage-apps/create/create-with-ui/#create-an-app-with-the-app-services-ui) and this will provide you with the base to start configuring your app.\n\nAfter creating your app, go to the **Atlas App Services** tab, click on your app, and go to **Data Access → Authentication** on the sidebar.\n\nIn the Authentication Providers section, enable the provider **Email/Password**. In the configuration window that will get displayed after, we will focus on the **Password Reset Method** part.\n\n![Authentication section - Atlas App Services](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/authentication_section_atlas_app_services_fdfd671e78.png)\n\nFor this example, the user confirmation will be done automatically. But make sure that the **Send a password reset email** option is enabled.\n\n![Authentication Configuration - Atlas App Services](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/authentication_configuration_atlas_app_services_ad44f6abff.png)\n\nOne important thing to note is that **you won’t be able to save and deploy these changes unless the URL section is completed**. Therefore, we’ll use a temporary URL and we’ll change it later to the final one.\n\n![password reset in authentication section ](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/password_reset_url_925424c75b.png)\n\nClick on the Save Draft button and your changes will be deployed.\n\n### Implementing the reset password functionality\n\nBefore starting to write the related code, please make sure that you have followed this [quick start guide](https://www.mongodb.com/docs/realm/sdk/swift/quick-start/#quick-start---swift-sdk) to make sure that you can use our Swift SDK.\n\nThe logic of implementing reset password will be implemented in the `MainViewController.swift` file. In it, we have an IBAction called `resetPasswordButtonTapped`, and inside we are going to write the following code:\n\n``` swift\n \n    @IBAction func resetPasswordButtonTapped(_ sender: Any) {\n        let email = app.currentUser?.profile.email ?? \"\"\n        let client = app.emailPasswordAuth\n \n        client.sendResetPasswordEmail(email) { (error) in\n            DispatchQueue.main.async {\n                guard error == nil else {\n                    print(\"Reset password email not sent: \\(error!.localizedDescription)\")\n                    return\n                }\n \n                print(\"Password reset email sent to the following address: \\(email)\")\n \n\tlet alert = UIAlertController(title: \"Reset Password\", message: \"Please check your inbox to continue the process\", preferredStyle: UIAlertController.Style.alert)\n                alert.addAction(UIAlertAction(title: \"OK\", style: UIAlertAction.Style.default, handler: nil))\n                self.present(alert, animated: true, completion: nil)\n \n            }\n        }\n    }\n```\n\nBy making a call to `client.sendResetPasswordEmail` with the user's email, App Services sends an email to the user that contains a unique URL. The user must visit this URL within 30 minutes to confirm the reset.\n\nNow we have the first part of the functionality implemented. But if we try to tap on the button, it won’t work as expected. We must go back to our Atlas App Services App, to the Authentication configuration.\n\n![password reset in authentication section ](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/password_reset_url_925424c75b.png)\n\nThe URL that we define here will be the one that will be sent in the email to the user. You can use your own from your own website hosted on a different server but if you don’t, don’t worry! Atlas App Services provides [Static Hosting](https://www.mongodb.com/docs/atlas/app-services/hosting/#static-hosting). You can use hosting to store individual pieces of content or to upload and serve your entire client application, but please note that in order to enable static hosting, **you must have a paid tier** (i.e M2 or higher).\n\n## Configuring hosting\n\nGo to the Hosting section of your Atlas App Services app and click on the Enable Hosting button. App Services will begin provisioning hosting for your application. It may take a few minutes for App Services to finish provisioning hosting for your application once you've enabled it.\n\n![hosting section in Atlas App Services](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/hosting_section_atlas_app_services_fa7ca5fcf0.png)\n\nThe resource path that you see in the screenshot above is the URL that will be used to redirect the user to our website so they can continue the process of resetting their password.\n\nNow we have to go back to the Authentication section in your Atlas App Services app and tap on the Edit button for Email/Password. We will focus our attention on the lower area of the window.\n\n![authentication configuration, password reset](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/resetting_password_3b9333daa7.png)\n\nIn the Password Reset URL we are going to add our hosted URL. This will create the link between your back end and the email that gets sent to the user. \n\nThe base of the URL is included in every password reset email. App Services appends a unique `token` and `tokenId` to this URL. These serve as query parameters to create a unique link for every password reset. To reset the user's password, extract these query parameters from the user's unique URL.\n\nIn order to extract these query parameters and use them in our client application, we can use Universal Links.\n\n## Universal links\n\nAccording to Apple, when adding [universal links](https://developer.apple.com/library/archive/documentation/General/Conceptual/AppSearch/UniversalLinks.html) support to your app, your users can tap a link to your website and get seamlessly redirected to your installed app without going through Safari. But if the app isn’t installed, then tapping a link to your website will open it in Safari. \n\n**Note**: Be aware that in order to add the universal links entitlement to your Xcode project, you need to have an Apple Developer subscription. \n\n#1 Add the **Associated Domains** entitlement to the **Signing & Capabilities** section of your project on Xcode and add to the domains the URL from your hosted website following the syntax: `>applinks:<url>`\n\n![Associated domains, Xcode project](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/associated_domains_xcode_project_94c5960cce.png)\n\n#2 You now need to create an `apple-app-site-association` file that contains JSON data about the URL that the app will handle. In my case, this is the structure of my file. The value of the `appID` key is the team ID or app ID prefix, followed by the bundle ID.\n\n``` json\n{\n    \"applinks\": {\n        \"apps\": [],\n        \"details\": [\n            {\n                \"appID\": \"QX5CR2FTN2.io.realm.marcabrera.aries\",\n                \"paths\": [ \"*\" ]\n            }\n        ]\n    }\n}\n```\n\n#3 Upload the file to your HTTPS web server. In my case, I’ll update it to my Atlas App Services hosted website. Therefore, now I have two files including `index.html`.\n\n![hosting section, Atlas App Services](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/hosting_section_2_5feddbca85.png)\n\n### Code\n\nYou need to implement the code that will handle the functionality when your user taps on the link from the received email.\n\nGo to the `SceneDelegate.swift` file of your Xcode project, and on the continue() delegate method, add the following code:\n\n``` swift\n   func scene(_ scene: UIScene, continue userActivity: NSUserActivity) {\n \n        if let url = userActivity.webpageURL {\n            handleUniversalLinks(url)\n        }\n    }\n```\n\n``` swift\n func scene(_ scene: UIScene, willConnectTo session: UISceneSession, options connectionOptions: UIScene.ConnectionOptions)  {\n \n        guard let _ = (scene as? UIWindowScene) else { return }\n \n        // UNIVERSAL LINKS HANDLING\n \n        guard let userActivity = connectionOptions.userActivities.first, userActivity.activityType == NSUserActivityTypeBrowsingWeb,\n                let incomingURL = userActivity.webpageURL else {\n            // If we don't get a link (meaning it's not handling the reset password flow then we have to check if user is logged in)\n            if let _ = app.currentUser {\n                // We make sure that the session is being kept active for users that have previously logged in\n                let storyboard = UIStoryboard(name: \"Main\", bundle: nil)\n                let tabBarController = storyboard.instantiateViewController(identifier: \"TabBarController\")\n                let navigationController = UINavigationController(rootViewController: tabBarController)\n \n            }\n            return\n        }\n \n        handleUniversalLinks(incomingURL)\n    }\n```\n\n``` swift\n   private func handleUniversalLinks(_ url: URL) {\n        // We get the token and tokenId URL parameters, they're necessary in order to reset password\n        let token = url.valueOf(\"token\")\n        let tokenId = url.valueOf(\"tokenId\")\n \n        let storyboard = UIStoryboard(name: \"Main\", bundle: nil)\n        let resetPasswordViewController = storyboard.instantiateViewController(identifier: \"ResetPasswordViewController\") as! ResetPasswordViewController\n \n        resetPasswordViewController.token = token\n        resetPasswordViewController.tokenId = tokenId\n \n    }\n```\n\nThe `handleUniversalLinks()` private method will extract the `token` and `tokenId` parameters that we need to use in order to reset the password. We will store them as properties on the `ResetPassword` view controller.\n\nAlso note that we use the function `url.valueOf(“token”)`, which is an extension that I have created in order to extract the query parameters that match the string that we pass as an argument and store its value in the `token` variable.\n\n``` swift\nextension URL {\n    // Function that returns a specific query parameter from the URL\n    func valueOf(_ queryParameterName: String) -> String? {\n        guard let url = URLComponents(string: self.absoluteString) else { return nil }\n \n        return url.queryItems?.first(where: {$0.name == queryParameterName})?.value\n    }\n}\n```\n\n**Note**: This functionality won’t work if the user decides to terminate the app and it’s not in the foreground. For that, we need to implement similar functionality on the `willConnectTo()` delegate method.\n\n``` swift\n    func scene(_ scene: UIScene, willConnectTo session: UISceneSession, options connectionOptions: UIScene.ConnectionOptions) {\n        // Use this method to optionally configure and attach the UIWindow `window` to the provided UIWindowScene `scene`.\n        // If using a storyboard, the `window` property will automatically be initialized and attached to the scene.\n        // This delegate does not imply the connecting scene or session are new (see `application:configurationForConnectingSceneSession` instead).\n \n        guard let _ = (scene as? UIWindowScene) else { return }\n \n        // UNIVERSAL LINKS HANDLING\n \n        guard let userActivity = connectionOptions.userActivities.first, userActivity.activityType == NSUserActivityTypeBrowsingWeb,\n                let incomingURL = userActivity.webpageURL else {\n            // If we don't get a link (meaning it's not handling the reset password flow then we have to check if user is logged in)\n            if let _ = app.currentUser {\n                // We make sure that the session is being kept active for users that have previously logged in\n\t    let storyboard = UIStoryboard(name: \"Main\", bundle: nil)\n                let mainVC = storyboard.instantiateViewController(identifier: \"MainViewController\")\n \n                window?.rootViewController = mainVC\n                window?.makeKeyAndVisible()\n            }\n            return\n        }\n \n        handleUniversalLinks(incomingURL)\n    }\n```\n\n## Reset password\n\nThis view controller contains a text field that will capture the new password that the user wants to set up, and when the Reset Password button is tapped, the `resetPassword` function will get triggered and it will make a call to the Client SDK’s resetPassword() function. If there are no errors, a success alert will be displayed on the app. Otherwise, an error message will be displayed.\n\n``` swift\n    private func resetPassword() {\n \n        let password = confirmPasswordTextField.text ?? \"\"\n \n        app.emailPasswordAuth.resetPassword(to: password, token: token ?? \"\", tokenId: tokenId ?? \"\") { (error) in\n            DispatchQueue.main.async {\n                self.confirmButton.hideLoading()\n                guard error == nil else {\n                    print(\"Failed to reset password: \\(error!.localizedDescription)\")\n                    self.presentErrorAlert(message: \"There was an error resetting the password\")\n                    return\n                }\n                print(\"Successfully reset password\")\n                self.presentSuccessAlert()\n            }\n        }\n    }\n```\n\n## Repository\n\nThe code for this project can be found in the [Github repository](https://github.com/mongodb-developer/Reset-Password-Swift). \n\nI hope you found this tutorial useful and that it will solve any doubts you may have! I encourage you to explore our [Realm Swift SDK documentation](https://www.mongodb.com/docs/realm/sdk/swift/) so you can check all the features and advantages that Realm can offer you while developing your iOS apps. We also have a lot of resources for you to dive in and learn how to implement them.","description":"Learn how to easily implement reset password functionality thanks to Atlas App Services on your iOS apps.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blta10088103f8aed16/644c4b368bfaaa8c49424ade/reset-password-authentication.jpg?branch=prod","description":null}}]},"slug":"/authentication-ios-apps-atlas-app-services","title":"*Authentication for Your iOS Apps with Atlas App Services","original_publish_date":"2022-10-28T20:57:08.000Z","strapi_updated_at":"2022-10-28T20:57:08.000Z","expiry_date":"2023-10-28T20:06:00.000Z","authorsConnection":{"edges":[{"node":{"title":"*Megha Arora","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt1dfda2d2df56320a/644c459a2bc49439e685eddb/aJ9O4Heo_400x400.jpg?branch=prod"}}]},"bio":"","calculated_slug":"/author/megha-arora","twitter":""}},{"node":{"title":"*Megan Grant","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt6f9ca862e66879dd/644c459919a211e5f757d84a/website-headshot.png?branch=prod"}}]},"bio":"","calculated_slug":"/author/megan-grant","twitter":""}},{"node":{"title":"*Otso Virtanen","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt4e89825e99fb315f/644c4597558dd8ef0e96a649/otso.jpeg?branch=prod"}}]},"bio":"","calculated_slug":"/author/otso-virtanen","twitter":""}}]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[]},"github_url":"","l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":"","programming_languagesConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}},{"node":{"title":"*Bash","calculated_slug":"/languages/bash"}},{"node":{"title":"*TypeScript","calculated_slug":"/languages/typescript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*iOS","calculated_slug":"/technologies/ios"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to easily implement reset password functionality thanks to Atlas App Services on your iOS apps.","og_description":"Learn how to easily implement reset password functionality thanks to Atlas App Services on your iOS apps.","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blta10088103f8aed16/644c4b368bfaaa8c49424ade/reset-password-authentication.jpg?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-05-15T18:27:53.197Z","publish_details":{"time":"2023-05-15T18:28:42.990Z"}}},{"calculated_slug":"/products/mongodb/upgrade-fearlessly-stable-api","content":"Do you hesitate to upgrade MongoDB, for fear the new database will be incompatible with your existing code?\n\nOnce you've written and deployed your MongoDB application, you want to be able to upgrade your MongoDB database at will, without worrying that a behavior change will break your application. In the past, we've tried our best to ensure each database release is backward-compatible, while also adding new features. But sometimes we've had to break compatibility, because there was no other way to fix an issue or improve behavior. Besides, we didn't have a single definition of backward compatibility.\n\nSolving this problem is more important now: We're releasing new versions [four times a year instead of one](https://www.mongodb.com/blog/post/new-quarterly-releases-starting-with-mongodb-5-0), and we plan to go faster in the future. We want to help you upgrade frequently and take advantage of new features, but first you must feel confident you can upgrade safely. Ideally, you could immediately upgrade all your applications to the latest MongoDB whenever we release.\n\nThe [MongoDB Stable API](https://docs.mongodb.com/manual/reference/stable-api/) is how we will make this possible. The Stable API encompasses the subset of MongoDB commands that applications commonly use to read and write data, create collections and indexes, and so on. We commit to keeping these commands backward-compatible in new MongoDB versions. We can add new features (such as new command parameters, new aggregation operators, new commands, etc.) to the Stable API, but only in backward-compatible ways.\n\nWe follow this principle:\n\n> For any API version V, if an application declares API version V and uses only behaviors in V, and it is deployed along with a specific version of an official driver, then it will experience no semantically significant behavior changes resulting from database upgrades so long as the new database supports V.\n\n(What's a semantically **insignificant** behavior change? Examples include the text of some error message, the order of a query result if you **don't** explicitly sort it, or the performance of a particular query. Behaviors like these, which are not documented and don't affect correctness, may change from version to version.)\n\nTo use the Stable API, upgrade to the latest driver and create your application's MongoClient like this:\n\n```js\nclient = MongoClient(\n  \"mongodb://host/\",\n  api={\"version\": \"1\", \"strict\": True})\n  ```\n\nFor now, \"1\" is the only API version. Passing \"strict\": True means the database will reject all commands that aren't in the Stable API. For example, if you call replSetGetStatus, which isn't in the Stable API, you'll receive an error:\n\n```js\n{\n  \"ok\" : 0,\n  \"errmsg\" : \"Provided apiStrict:true, but replSetGetStatus is not in API Version 1\",\n  \"code\" : 323,\n  \"codeName\" : \"APIStrictError\"\n}\n```\n\nRun your application's test suite with the new MongoClient options, see what commands and features you're using that are outside the Stable API, and migrate to versioned alternatives. For example, \"mapreduce\" is not in the Stable API but \"aggregate\" is. Once your application uses only the Stable API, you can redeploy it with the new MongoClient options, and be confident that future database upgrades won't affect your application.\n\nThe mongosh shell now supports the Stable API too:\n\n```bash\nmongosh --apiVersion 1 --apiStrict\n```\n\nYou may need to use unversioned features in some part of your application, perhaps temporarily while you are migrating to the Stable API, perhaps permanently. The **escape hatch** is to create a non-strict MongoClient and use it just for using unversioned features:\n\n```PYTHON\n# Non-strict client.\nclient = MongoClient(\n  \"mongodb://host/\",\n  api={\"version\": \"1\", \"strict\": False})\n\nclient.admin.command({\"replSetGetStatus\": 1})\n```\n\nThe \"strict\" option is false by default, I'm just being explicit here. Use this non-strict client for the few unversioned commands your application needs. Be aware that we occasionally make backwards-incompatible changes in these commands.\n\nThe only API version that exists today is \"1\", but in the future we'll release new API versions. This is exciting for us: MongoDB has a few warts that we had to keep for compatibility's sake, but the Stable API gives us a safe way to remove them. Consider the following:\n\n```PYTHON\nclient = MongoClient(\"mongodb://host\")\nclient.test.collection.insertOne({\"a\": [1]})\n\n# Strangely, this matches the document above.\nresult = client.test.collection.findOne(\n  {\"a.b\": {\"$ne\": null}})\n  ```\n\nIt's clearly wrong that `{\"a\": [1]}` matches the query `{\"a.b\": {\"$ne\": null}}`, but we can't fix this behavior, for fear that user's applications rely on it. The Stable API gives us a way to safely fix this. We can provide cleaner query semantics in Version 2:\n\n```PYTHON\n# Explicitly opt in to new behavior.\nclient = MongoClient(\n  \"mongodb://host/\",\n  api={\"version\": \"2\", \"strict\": True})\n\nclient.test.collection.insertOne({\"a\": [1]})\n\n# New behavior: doesn't match document above.\nresult = client.test.collection.findOne(\n  {\"a.b\": {\"$ne\": null}})\n  ```\n  \nFuture versions of MongoDB will support **both** Version 1 and 2, and we'll maintain Version 1 for many years. Applications requesting the old or new versions can run concurrently against the same database. The default behavior will be Version 1 (for compatibility with old applications that don't request a specific version), but new applications can be written for Version 2 and get the new, obviously more sensible behavior.\n\nOver time we'll deprecate some Version 1 features. That's a signal that when we introduce Version 2, those features won't be included. (Future MongoDB releases will support both Version 1 with deprecated features, and Version 2 without them.) When the time comes for you to migrate an existing application from Version 1 to 2, your first step will be to find all the deprecated features it uses:\n\n```PYTHON\n# Catch uses of features deprecated in Version 1.\nclient = MongoClient(\n  \"mongodb://host/\",\n  api={\"version\": \"1\",\n       \"strict\": True,\n       \"deprecationErrors\": True})\n```       \n\nThe database will return an APIDeprecationError whenever your code tries to use a deprecated feature. Once you've run your tests and fixed all the errors, you'll be ready to test your application with Version 2.\n\nVersion 2 might be a long way off, though. Until then, we're continuing to add features and make improvements in Version 1. We'll introduce new commands, new options, new aggregation operators, and so on. Each change to Version 1 will be an **extension** of the existing API, and it will never affect existing application code. With quarterly releases, we can improve MongoDB faster than ever before. Once you've upgraded to 5.0 and migrated your app to the Stable API, you can always use the latest release fearlessly.\n\nYou can try out the Stable API with the MongoDB 5.0 Release Candidate, which is available now from our [Download Center](https://www.mongodb.com/try/download/community). \n\n## Appendix\n\nHere's a list of commands included in API Version 1 in MongoDB 5.0. You can call these commands with version \"1\" and strict: true. (But of course, you can also call them without configuring your MongoClient's API version at all, just like before.) We won't make backwards-incompatible changes to any of these commands. In future releases, we may add features to these commands, and we may add new commands to Version 1.\n\n* [abortTransaction](https://docs.mongodb.com/manual/reference/command/abortTransaction/)\n* [aggregate](https://docs.mongodb.com/manual/reference/command/aggregate/#mongodb-dbcommand-dbcmd.aggregate)\n* [authenticate](https://docs.mongodb.com/manual/reference/command/authenticate/#mongodb-dbcommand-dbcmd.authenticate)\n* [collMod](https://docs.mongodb.com/manual/reference/command/collMod/#mongodb-dbcommand-dbcmd.collMod)\n* [commitTransaction](https://docs.mongodb.com/manual/reference/command/commitTransaction/#mongodb-dbcommand-dbcmd.commitTransaction)\n* [create](https://docs.mongodb.com/manual/reference/command/create/#mongodb-dbcommand-dbcmd.create)\n* [createIndexes](https://docs.mongodb.com/manual/reference/command/createIndexes/#mongodb-dbcommand-dbcmd.createIndexes)\n* [delete](https://docs.mongodb.com/manual/reference/command/delete/#mongodb-dbcommand-dbcmd.delete)\n* [drop](https://docs.mongodb.com/manual/reference/command/drop/#mongodb-dbcommand-dbcmd.drop)\n* [dropDatabase](https://docs.mongodb.com/manual/reference/command/dropDatabase/#mongodb-dbcommand-dbcmd.dropDatabase)\n* [dropIndexes](https://docs.mongodb.com/manual/reference/command/dropIndexes/#mongodb-dbcommand-dbcmd.dropIndexes)\n* [endSessions](https://docs.mongodb.com/manual/reference/command/endSessions/#mongodb-dbcommand-dbcmd.endSessions)\n* [explain](https://docs.mongodb.com/manual/reference/command/explain/#mongodb-dbcommand-dbcmd.explain) (we won't make incompatible changes to this command's input parameters, although its output format may change arbitrarily)\n* [find](https://docs.mongodb.com/manual/reference/command/find/#mongodb-dbcommand-dbcmd.find)\n* [findAndModify](https://docs.mongodb.com/manual/reference/command/findAndModify/#mongodb-dbcommand-dbcmd.findAndModify)\n* [getMore](https://docs.mongodb.com/manual/reference/command/getMore/#mongodb-dbcommand-dbcmd.getMore)\n* [hello](https://docs.mongodb.com/manual/reference/command/hello/#mongodb-dbcommand-dbcmd.hello)\n* [insert](https://docs.mongodb.com/manual/reference/command/insert/#mongodb-dbcommand-dbcmd.insert)\n* [killCursors](https://docs.mongodb.com/manual/reference/command/killCursors/#mongodb-dbcommand-dbcmd.killCursors)\n* [listCollections](https://docs.mongodb.com/manual/reference/command/listCollections/#mongodb-dbcommand-dbcmd.listCollections)\n* [listDatabases](https://docs.mongodb.com/manual/reference/command/listDatabases/#mongodb-dbcommand-dbcmd.listDatabases)\n* [listIndexes](https://docs.mongodb.com/manual/reference/command/listIndexes/#mongodb-dbcommand-dbcmd.listIndexes)\n* [ping](https://docs.mongodb.com/manual/reference/command/ping/#mongodb-dbcommand-dbcmd.ping)\n* [refreshSessions](https://docs.mongodb.com/manual/reference/command/refreshSessions/#mongodb-dbcommand-dbcmd.refreshSessions)\n* saslContinue\n* saslStart\n* [update](https://docs.mongodb.com/manual/reference/command/update/)\n\n## Safe Harbor\n\nThe development, release, and timing of any features or functionality described for our products remains at our sole discretion. This information is merely intended to outline our general product direction and it should not be relied on in making a purchasing decision nor is this a commitment, promise or legal obligation to deliver any material, code, or functionality.\n","description":"With the Stable API, you can upgrade to the latest MongoDB releases without introducing backward-breaking app changes. Learn what it is and how to use it.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt158a9627678007ed/644c463c47e9cc6ef7cd0925/twitter-mdb-developer.png?branch=prod","description":null}}]},"slug":"/upgrade-fearlessly-stable-api","title":"*Upgrade Fearlessly with the MongoDB Stable API","original_publish_date":"2021-06-01T18:23:56.609Z","strapi_updated_at":"2022-05-16T18:28:37.833Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Python","calculated_slug":"/languages/python"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:36.734Z","publish_details":{"time":"2023-04-28T22:52:50.385Z"}}},{"calculated_slug":"/products/mongodb/scaling-gaming-mongodb-square-enix-gaspard-petit","content":"[Square Enix](https://squareenix.com) is one of the most popular gaming brands in the world. They're known for such franchise games as Tomb Raider, Final Fantasy, Dragon Quest, and more. In this article, we provide a transcript of the MongoDB Podcast episode in which Michael and Nic sit down with Gaspard Petit, software architect at Square Enix, to talk about how they're leveraging MongoDB, and his own personal experience with MongoDB as a data platform.\n\nYou can learn more about Square Enix on [their website](https://www.square-enix.com/). You can find Gaspard on [LinkedIn](https://www.linkedin.com/in/gaspardpetit/).\n\nJoin us in the [forums](https://developer.mongodb.com/community/forums/) to chat about this episode, about gaming, or about anything related to MongoDB and Software Development. \n\nGaspard Petit (00:00):\nHi everybody, this is Gaspard Petit. I'm from Square Enix. Welcome to this MongoDB Podcast.\n\nGaspard Petit (00:09):\nMongoDB was perfect for processes, there wasn't any columns predefined, any schema, we could just add fields. And why this is important as designers is that we don't know ahead of time what the final game will look like. This is something that evolves, we do a prototype of it, you like it, you don't like it, you undo something, you redo something, you go back to something you did previously, and it keeps changing as the game evolves. It's very rare that I've seen a game production go straight from point A to Z without twirling a little bit and going back and forth. So that back and forth process is cumbersome. For the back-end, where the requirements are set in stone, you have to deliver it so the game team can experience it, and then they'll iterate on it. And if you're set in stone on your database, and each time you change something you have to migrate your data, you're wasting an awful lot of time.\n\nMichael Lynn (00:50):\nWelcome to the show. On today's episode, we're talking with Gaspard Petit of the Square Enix, maker of some of the best-known, best-loved games in the gaming industry. Today, we're talking about how they're leveraging MongoDB and a little bit about Gaspard's journey as a software architect. Hope you enjoy this episode.\n\nAutomated (01:07):\nYou're listening to the MongoDB podcast, exploring the world of software development, data, and all things MongoDB. And now your hosts, Michael Lynn and Nic Raboy.\n\nMichael Lynn (01:26):\nHey, Nic. How you doing today?\n\nNic Raboy (01:27):\nI'm doing great, Mike. I'm really looking forward to this episode. I've been looking forward to it for what is it? More than a month now because it's really one of the things that hits home to me, and that's gaming. It's one of the reasons why I got into software development. So this is going to be awesome stuff. What do you think, Mike?\n\nMichael Lynn (01:43):\nFantastic. I'm looking forward to it as well. And we have a special guest, Gaspard Petit, from Square Enix. Welcome to the podcast, it's great to have you on the show.\n\nGaspard Petit (01:51):\nHi, it's good to be here.\n\nMichael Lynn (01:52):\nFantastic. Maybe if you could introduce yourself to the folks and let folks know what you do at Square Enix.\n\nGaspard Petit (01:58):\nSure. So I'm software online architect at Square Enix. I've been into gaming pretty much my whole life. And when I was a kid that was drawing game levels on piece of papers with my friends, went to university as a software engineer, worked in a few companies, some were gaming, some were around gaming. For example, with Autodesk or Softimage. And then got into gaming, first game was a multiplayer game. And it led me slowly into multiplayer games. First company was at Behaviour and then to Eidos working on the reboot of Tomb Raider on the multiplayer side. Took a short break, went back into actually a company called Datamine, where I learned about the back-end how to work. It wasn't on the Azure Cloud at the time. And I learned a lot about how to do these processes on the cloud, which turned out to be fascinating how you can converge a lot of requests, a lot of users into a distributed environment, and process this data efficiently.\n\nGaspard Petit (03:03):\nAnd then came back to Square Enix as a lead at the time for the internally, we call it our team, the online suite, which is a team in charge of many of the Square Enix's game back-ends. And I've been there for a couple of years now. Six years, I think, and now became online architect. So my role is making sure we're developing in the right direction using the right services, that our solutions will scale, that they're appropriate for the needs of the game team. That we're giving them good online services basically, and that they're also reliable for the users.\n\nNic Raboy (03:44):\nSo the Tomb Raider reboot, was that your first big moment in the professional game industry, or did you have prior big moments before that?\n\nGaspard Petit (03:54):\nI have to say it was probably one of the ones I'm most proud of. To be honest, I worked on a previous game, it was called Naughty Bear. It wasn't a great success from the public's point of view, the meta critics weren't great. But the team I worked on was an amazing team, and everyone on that team was dedicated. It was a small team, the challenges were huge. So from my point of view, that game was a huge success. It didn't make it, the public didn't see it that way. But the challenges, it was a multiplayer game. We had the requirements fairly last-minute to make this a multiplayer game. So we had to turn in single player into multiplayer, do the replication. A lot of complicated things in a short amount of time. But with the right team, with the right people motivated. To me, that was my first gaming achievement.\n\nMichael Lynn (04:49):\nYou said the game is called Naughty Bear?\n\nGaspard Petit (04:51):\nNaughty Bear, yes.\n\nMichael Lynn (04:52):\nWhat type of game is that? Because I'm not familiar with that.\n\nGaspard Petit (04:55):\nNo, not many people are. It's a game where you play a teddy bear waking up on an island. And you realize that there's a party and you're not invited to that party. So you just go postal and kill all the bears on the island pretty much. But there's AI involved, there's different ways of killing, there's different ways of interacting with those teddy bears. And of course, there's no blood, right? So it's not violence. It's just plain fun, right? So it's playing a little bit on that side, on the-\n\nMichael Lynn (05:23):\nAbsolutely.\n\nGaspard Petit (05:26):\nBut it's on a small island, so it's very limited. But the fun is about the AI and playing with friends. So you can play as the bears that are trying to hide or as the bear that's trying to carnage the island.\n\nGaspard Petit (05:41):\nThis is pretty much what introduced me to leaderboards, multiplayer replication. We didn't have any saved game. It was over 10 years ago, so the cloud was just building up. But you'd still have add matchmaking features, these kind of features that brought me into the online environment.\n\nNic Raboy (05:59):\nAwesome. In regards to your Naughty Bear game, before we get into the scoring and stuff, what did you use to develop it?\n\nGaspard Petit (06:05):\nIt was all C++, a little bit of Lua back then. Like I said, on the back-end side, there wasn't much to do. We used the first party API's which were C++ connected to their server. The rest was a black box. To me at the time, I didn't know how matchmaking worked or how all these leaderboards worked, I just remember that it felt a bit frustrating that I remember posting scores, for example, to leaderboards. And sometimes it would take a couple of seconds for the rank to be updated. And I remember feeling frustration about that. Why isn't this updated right away? I've just posted my score and can take a minute or two before my rank is updated. And now that I'm working back-end, I totally get it. I understand the volume of scores getting posted, the ranking, the sorting out, all the challenges on the back-end. But to me back then it was still a black box.\n\nMichael Lynn (06:57):\nSo was that game leveraging MongoDB as part of the back-end?\n\nGaspard Petit (07:01):\nNo, no, no. Like I said, it wasn't really on the cloud. It was just first party API. I couldn't tell you what Microsoft, Sony is using. But from our point of view, we were not using any in-house database. So that was a different company, it was at Behaviour.\n\nMichael Lynn (07:19):\nAnd I'm curious as an early developer in your career, what things did you learn about game development that you still take with you today?\n\nGaspard Petit (07:28):\nI think a lot of people are interested in game development for the same reasons I am. It is very left and right brain, you have a lot of creativity, you have to find ways to make things work. Sometimes you're early on in a project and you get a chance to do things right. So you architect things, you do the proper design, you even sometimes draw UML and organize your objects so that it's all clean, and you feel like you're doing theoretical and academic almost work, and then the project evolves. And as you get closer to the release date, this is not something that will live forever, it's not a product that you will recycle, and needs to be maintained for the next 10 years. This is something you're going to ship and it has to work on ideally on the day you ship it.\n\nGaspard Petit (08:13):\nSo you start shifting your focus saying, \"This has to work no matter what. I have to find a solution. There's something here that doesn't work.\" And I don't have time to find a proper design to refactor this, I just have to make it work. And you shift your way of working completely into ship it, make it work, find a solution. And you get into a different kind of creativity as a programmer. Which I love, which is also scary some time because you put this duct tape in your code and it works. And you'rE wondering, \"Should I feel right about shipping this?\" And actually, nobody's going to notice and it's going to hold and the game will be fun. And it doesn't matter that you have this duct tape somewhere. I think this is part of the fun of shaping the game, making it work at the end no matter what. And it doesn't have to be perfectly clean, it has to be fun at the end.\n\nGaspard Petit (09:08):\nThis is definitely one aspect of it. The other aspect is the real-time, you want to hit 30fps or 60fps or more. I'm sure PC people are now demanding more. But you want this frame rate, and at the same time you want the AI, and you want the audio, and you want the physics and you want everything in that FPS. And you somehow have to make it all work. And you have to find whatever trick you can. If you can pre-process things on their hard drive assets, you do it. Whatever needs you can optimize, you get a chance to optimize it.\n\nGaspard Petit (09:37):\nAnd there's very few places in the industry where you still get that chance to optimize things and say, \"If I can remove this one millisecond somewhere, it will have actually an impact on something.\" Back-end has that in a way. MongoDB, I'm sure if you can remove one second in one place, you get that feeling of I can now perform this amount of more queries per second. But the game also has this aspect of, I'll be able to process a little bit more, I'll be able to load more assets, more triangles, render more things or hit more bounding boxes. So the performance is definitely an interesting aspect of the game.\n\nNic Raboy (10:12):\nYou spent a lot of time doing the actual game development being the creative side, being the performance engineer, things like that. How was the transition to becoming an online architect? I assume, at least you're no longer actually making what people see, but what people experience in the back-end, right? What's that like?\n\nGaspard Petit (10:34):\nThat's right. It wasn't an easy transition. And I was the lead on the team for a couple of years. So I got that from a few candidates joining the team, you could tell they wish they were doing gameplay or graphics, and they got into the back-end team. And it feels like you're, \"Okay, I'll do that for a couple of years and then I'll see.\" But it ended up that I really loved it. You get a global view of the players what they're doing, not just on a single console, you also get to experience the game as it is live, which I didn't get to experience when I was programming the game, you program the game, it goes to a disk or a digital format, it's shipped and this is where Julian, you take your vacation after when a game has shipped.\n\nGaspard Petit (11:20):\nThe exhilaration of living the moment where the game is out, monitoring it, seeing the player while something disconnect, or having some problems, monitoring the metrics, seeing that the game is performing as expected or not. And then you get into other interesting things you can do on the back-end, which I couldn't do on the game is fixing the game after it has shipped. So for example, you discovered that the balancing is off. Something on the game doesn't work as expected. But you have a way of somehow figuring out from the back-end how you can fix it.\n\nGaspard Petit (11:54):\nOf course, ideally, you would fix in the game. But nowadays, it's not always easy to repackage the game on each platform and deliver it on time. It can take a couple of weeks to fix it to fix the game from the code. So whatever we can fix from the back-end, we do. So we need to have the proper tools for monitoring this humongous amount of data coming our way. And then we have this creativity kicking in saying, \"Okay, I've got this data, how can I act on it to make the game better?\" So I still get those feelings from the back-end.\n\nMichael Lynn (12:25):\nAnd I feel like the line between back-end and front-end is really blurring lately. Anytime I get online to play a game, I'm forced to go through the update process for many of the games that I play. To what degree do you have flexibility? I'll ask the question this way. How frequently Are you making changes to games that have already shipped?\n\nGaspard Petit (12:46):\nIt's not that frequent. It's not rare, either. It's somewhere in between. Ideally, we would not have to make any changes after the game is out. But in practice, the games are becoming so complex, they no longer fit on a small 32 megabyte cartridge. So there's a lot of things going on in the game. They're they're huge. It's almost impossible to get them perfectly right, and deliver them within a couple of years.\n\nGaspard Petit (13:16):\nAnd there's also a limitation to what you can test internally. Even with a huge team of QA, you will discover things only when players are experiencing the game. Like I said the flow of fixing the game is long. You hear about the report on Reddit or on Twitter, and then you try to reproduce it internally right there. It might take a couple of days to get the same bug the player has reported. And then after that, you have to figure out in the code how you can fix it, make sure you don't break anything else. So it can take literally weeks before you fix something very trivial.\n\nGaspard Petit (13:55):\nOn the back-end, if we can try it out, we can segment a specific fix for a single player, make sure for that player it works. Do some blue-green introduction of that test or do it only on staging first, making sure it works, doing it on production. And within a couple of sometimes I would say, a fix has come out in a couple of hours in some case where we noticed it on production, went to staging and to production within the same day with something that would fix the game.\n\nGaspard Petit (14:25):\nSo ideally, you would put as much as you can on the back-end because you have so much agility from the back-end. I know players are something called about this idea of using back-ends for game because they see it as a threat. I don't think they realize how much they can benefit from fixes we do on the back-end.\n\nNic Raboy (14:45):\nSo in regards to the back-end that you're heavily a part of, what typically goes in to the back-end? I assume that you're using quite a few tools, frameworks, programming languages, maybe you could shed some light onto that.\n\nGaspard Petit (14:57):\nOh yes, sure. So typically, in almost every project, there is some telemetry that is useful for us to monitor that the game is working like I said, as expected. We want to know if the game is crashing, we want to know if players are stuck on the level and they can't go past through it. If there's an achievement that doesn't lock or something that shouldn't be happening and doesn't happen. So we want to make sure that we're monitoring these things.\n\nGaspard Petit (15:23):\nThere's, depending on the project, we have community features. For example, comparing what you did in the life experience series to what the community did, and sometime it will be engagements or creating challenges that will change on a weekly basis. In some cases recently for outriders for example, we have the whole save game saved online, which means two things, right? We can get an idea of the state of each player, but we can also fix things. So it really depends on the project. It goes from simple telemetry, just so we know that things are going okay, or we can act on it to adding some game logic on the back-end getting executed on the back-end.\n\nMichael Lynn (16:09):\nAnd what are the frameworks and development tools that you leverage?\n\nGaspard Petit (16:12):\nYes, sorry. So the back-ends, we write are written in Java. We have different tools, we use outside of the back-end. We deploy on Kubernetes. Almost everything is Docker images at this point. We use MongoDB as the main storage. Redis as ephemeral storage. We also use Kafka for the telemetry pipeline to make sure we don't lose them and can process them asynchronously. Jenkins for building. So this is pretty much our environment.\n\nGaspard Petit (16:45):\nWe also work on the game integration, this is in C++ and C#. So our team provides and actually does some C++ development where we try to make a HTTP client, C++ clients, that is cross platform and as efficient as possible. So at least impacting the frame rate. Even sometimes it means downloading things a little bit slower or are not ticking as many ticks. But we customize our HTTP client to make sure that the online impact is minimal on the gameplay. So our team is in charge of both this client integration into the game and the back-end development.\n\nMichael Lynn (17:24):\nSo those HTTP clients, are those custom SDKs that you're providing your own internal developers for using?\n\nGaspard Petit (17:31):\nExactly, so it's our own library that we maintain. It makes sure that what we provide can authenticate correctly with the back-end as a right way to communicate with it, the right retries, the right queuing. So we don't have to enforce through policies to each game themes, how to connect to the back-end. We can bundle these policies within the SDK that we provide to them.\n\nMichael Lynn (17:57):\nSo what advice would you have for someone that's just getting into developing games? Maybe some advice for where to focus on their journey as a game developer?\n\nGaspard Petit (18:08):\nThat's a great question. The advice I would give is, it starts of course, being passionate about it. You have to because there's a lot of work in the gaming, it's true that we do a lot of hours. If we did not enjoy the work that we did, we would probably go somewhere else. But it is fun. If you're passionate about it, you won't mind as much because the success and the feeling you get on each release compensates the effort that you put into those projects. So first, you need to be passionate about it, you need to be wanting to get those projects and be proud of them.\n\nGaspard Petit (18:46):\nAnd then I would say not to focus too much on one aspect of gaming because at first, I did several things, right? My studies were on the image processing, I wanted to do 3D rendering. At first, that was my initial goal as a teenager. And this is definitely not what I ended up doing. I did almost everything. I did a little bit of rendering, but almost none. I ended up in the back-end. And I learned that almost every aspect of the game development has something interesting and challenging.\n\nGaspard Petit (19:18):\nSo I would say not too much to focus on doing the physics or the rendering, sometime you might end up doing the audio and that is still something fascinating. How you can place your audio within the scene and make it sound like it comes from one place, and hit the walls. And then in each aspect, you can dig and do something interesting. And the games now at least within Square Enix they're too big for one person to do it all. So it's generally, you will be part of a team anyway. And within that team, there will be something challenging to do.\n\nGaspard Petit (19:49):\nAnd even the back-end, I know not so many people consider back-end as their first choice. But I think that's something that's actually a mistake. There is a lot of interesting things to do with the back-end, especially now that there is some gameplay happening on back-ends, and increasingly more logic happening on the back-end. I don't want to say that one is better than the other, of course, but I would personally not go back, and I never expected to love it so much. So be open-minded and be passionate. I think that's my general advice.\n\nMichael Lynn (20:26):\nSo speaking of back-end, can we talk a little bit about how Square Enix is leveraging MongoDB today?\n\nGaspard Petit (20:32):\nSo we've been using MongoDB for quite some time. When I joined the team, it was already been used. We were on, I think version 2.4. MongoDB had just implemented authentication on collections, I think. So quite a while ago, and I saw it evolve over time. If I can share this, I remember my first day on the team hitting MongoDB. And I was coming from a SQL-like world, and I was thinking, \"What is this? What is this query language and JSON?\" And of course, I couldn't query anything at first because it all seemed the syntax was completely strange to me. And I didn't understand anything about sharding, anything about chunking, anything about how the database works. So it actually took me a couple of months, I would say before I started appreciating what Mongo did, and why it had been picked.\n\nGaspard Petit (21:27):\nSo it has been recommended, if I remember, I don't want to say incorrect things. But I think it had been recommended before my time. It was a consulting team that had recommended MongoDB for the gaming. I wouldn't be able to tell you exactly why. So over time, what I realized is that MongoDB was perfect for our processes because there wasn't any columns predefine, any schema, we could just add fields. If the fields were missing, it wasn't a big deal, we could encode in the back-end, and we could just set them to default values.\n\nGaspard Petit (22:03):\nAnd why this is important is because the game team generally doesn't know. I don't want to say the game team actually, the designers or the producer, they don't know ahead of time, what the final game will look like, this is something that evolves. You play, you do a prototype of it, you like it, you don't like it, you undo something, you redo something, you go back to something you did previously, and it keeps changing as the game evolves. It's very rare that I've seen a game production go straight from point A to Z without twirling a little bit and going back and forth.\n\nGaspard Petit (22:30):\nSo that back and forth process is cumbersome for the back-end. You're asked to implement something before the requirements are set in stone, you have to deliver it so the game team can experience it and then we'll iterate on it. And if you're set in stone on your database, and each time that you change something, you have to migrate your data, you're wasting an awful lot of time. And after, like I said, after a couple of months that become obvious that MongoDB was a perfect fit for that because the game team would ask us, \"Hey, I need now to store this thing, or can you change this type for that type?\" And it was seamless, we would change a string for an integer or a string, we would add a field to a document and that was it. No migration. If we needed, the back-end would catch the cases where a default value was missing. But that was it.\n\nGaspard Petit (23:19):\nAnd we were able to progress with the game team as they evolved their design, we were able to follow them quite rapidly with our non-schema database. So now I wouldn't switch back. I've got used to the JSON query language, I think human being get used to anything. And once you're familiar with something, you don't want to learn something else. And I ended up learning the SQL Mongo syntax, and now I'm actually very comfortable with it. I do aggregation on the command line, these kinds of things. So it's just something you have to be patient off if you haven't used MongoDB before. At first, it looks a little bit weird, but it quickly becomes quite obvious why it is designed in a way. It's actually very intuitive to use.\n\nNic Raboy (24:07):\nIn regards to game development in general, who is determining what the data should look like? Is that the people actually creating the local installable copy of the game? Or is that the back-end team deciding what the model looks like in general?\n\nGaspard Petit (24:23):\nIt's a mix of both. Our team acts as an expert team, so we don't dictate where the back-end should be. But since we've been on multiple projects, we have some experience on the good and bad patterns. And in MongoDB it's not always easy, right? We've been hit pretty hard with anti-patterns in the past. So we would now jump right away if the game team asks us to store something in a way that we knew would not perform well when scaling up. So we're cautious about it, but it in general, the requirements come from the game team, and we translate that into a database schema, which says in a few cases, the game team knows exactly what they want. And in those cases, we generally just store their data as a raw string on MongoDB. And then we can process it back, whether it's JSON or whatever other format they want. We give them a field saying, \"This belongs to you, and use whatever schema you want inside of it.\"\n\nGaspard Petit (25:28):\nBut of course, then they won't be able to insert any query into that data. It's more of a storage than anything else. If they need to perform operations, and we're definitely involved because we want to make sure that they will be hitting the right indexes, that the sharding will be done properly. So it's a combination of both sides.\n\nMichael Lynn (25:47):\nOkay, so we've got MongoDB in the stack. And I'm imagining that as a developer, I'm going to get a development environment. And tell me about the way that as a developer, I'm interacting with MongoDB. And then how does that transition into the production environment?\n\nGaspard Petit (26:04):\nSure. So every developer has a local MongoDB, we use that for development. So we have our own. Right now is docker-compose image. And it has a full virtual environment. It has all the other components I mentioned earlier, it has Kafka, it even LDAP, it has a bunch of things running virtually including MongoDB. And it is even configured as a sharded cluster. So we have a local sharded cluster on each of our machine to make sure that our queries will work fine on the actual sharded cluster. So it's actually very close to production, even though it's on our local PC. And we start with that, we develop in Java and write our unit test to make sure we cover what we write and don't have regression. And those unit tests will run against a local MongoDB instance.\n\nGaspard Petit (26:54):\nAt some point, we are about to release something on production especially when there's a lot of changes, we want to make sure we do load testing. For our load testing, we have something else and I am not sure that that's a very well known feature from MongoDB, but it's extremely useful for us. It's the MongoDB Operator, which is an operator within Kubernetes. And it allows spinning up clusters based on the simple YAML. So you can say, \"I want a sharded cluster with three deep, five shards,\" and it will spin it up for you, it will take a couple of seconds a couple of minutes depending on what you have in your YAML. And then you have it. You have your cluster configured in your Kubernetes cluster. And then we run our tests on this. It's a new cluster, fresh. Run the full test, simulate millions of requests of users, destroy it. And then if we're wondering you know what? Does our back-end scale with the number of shards? And then we just spin up a new shard cluster with twice the number of shards, expect twice the performance, run the same test. Again, if we don't have one. Generally, we won't get that exactly twice the performance, right? But it will get an idea of, this operation would scale with the number of shards, and this one wouldn't.\n\nGaspard Petit (28:13):\nSo that Operator is very useful for us because it'll allow us to simulate these scenarios very easily. There's very little work involved in spinning up these Kubernetes cluster.\n\nGaspard Petit (28:23):\nAnd then when we're satisfied with that, we go to Atlas, which provides us the deployment of the CloudReady clusters. So this is not me personally who does it, we have an ops team who handle this, but they will prepare for us through Atlas, they will prepare the final database that we want to use. We work together to find the number of shards, the type of instance we want to deploy. And then Atlas takes care of it. We benefit from disk auto-scaling on Atlas. We generally start with lower instance, to set up the database when the big approaches for the game release, we scale up instance type again, through Atlas.\n\nGaspard Petit (29:10):\nIn some cases, we've realized that the number of shards was insufficient after testing, and Atlas allows us to make these changes quite close to the launch date. So what that means is that we can have a good estimate a couple of weeks before the launch of our requirements in terms of infrastructure, but if we're wrong, it doesn't take that long to adjust and say, \"Okay, you know what? We don't need five shards, we need 10 shards.\" And especially if you're before the launch, you don't have that much data. It just takes a couple of minutes, a couple of hours for Atlas to redeploy these things and get the database ready for us. So it goes in those three stages of going local for unit testing with our own image of Mongo. We have a Kubernetes cluster for load testing which use the Mongo Operator, and then we use Atlas in the end for the actual cloud deployment.\n\nGaspard Petit (30:08):\nWe actually go one step further when the game is getting old and load is predictable on it. And it's not as high as it used to be, we move this database in-house. So we have our own data centers. And we will actually share Mongo instances for multiple games. So we co-host multiple games on a single cluster, not single database, of course, but a single Mongo cluster. And that becomes very, very cost effective. We get to see, for example, if there's a sales on one game, while the other games are less active, it takes a bit more load. But next week, something else is on sales, and they kind of average out on that cluster. So older games, I'm talking like four or five years old games tend to be moved back to on-premises for cost effectiveness.\n\nNic Raboy (31:00):\nSo it's great to know that you can have that choice to bring games back in when they become old, and you need to scale them down. Maybe you can talk about some of the other benefits that come with that.\n\nGaspard Petit (31:12):\nYeas. And while it also ties in to the other aspects I mentioned of. We don't feel locked with MongoDB, we have options. So we have the Atlas option, which is extremely useful when we launch a game. And it's high risk, right? If an incident happened on the first week of a game launch, you want all hands on deck and as much support as you can. After a couple of years, we know the kind of errors we can get, we know what can go wrong with the back-end. And generally the volume is not as high, so we don't necessarily need that kind of support anymore. And there's also a lot of overhead on running things on the cloud, if you're on the small volume. There's not just the Mongo itself, there's the pods themselves that need to run on a compute environment, there's the traffic that is counting.\n\nGaspard Petit (32:05):\nSo we have that data center. We actually have multiple data centers, we're lucky to be big enough to have those. But it gives us this extra option of saying, \"We're not locked to the cloud, it's an option to be on the cloud with MongoDB.\" We can run it locally on a Docker, we can run it on the cloud, where we can control where we go. And this has been a key element in the architecture of our back-ends from the start actually, making sure that every component we use can be virtualized, brought back on-premises so that we can control locally. For example, we can run tests and have everything controlled, not depending on the cloud. But we also get the opportunity of getting an external team looking at the project with us on the critical moments. So I think we're quite happy to have those options of running it wherever we want.\n\nMichael Lynn (32:56):\nYeah, that's clearly a benefit. Talk to me a little bit about the scale. I know you probably can't mention numbers and transactions per second and things like that. But this is clearly one of the challenges in the gaming space, you're going to face massive scale. Do you want to talk a little bit about some of the challenges that you're facing, with the level of scale that you're achieving today?\n\nGaspard Petit (33:17):\nYes, sure. That's actually one of the challenging aspects of the back-end, making sure that you won't hit a ceiling at some point or an unexpected ceiling. And there's always one, you just don't always know which one it is. When we prepare for a game launch, regardless of its success, we have to prepare for the worst, the best success. I don't know how to phrase that. But the best success might be the worst case for us. But we want to make sure that we will support whatever number of players comes our way. And we have to be prepared for that.\n\nGaspard Petit (33:48):\nAnd depending on the scenarios, it can be extremely costly to be prepared for the worst/best. Because it might be that you have to over scale right away, and make sure that your ceiling is very high. Ideally, you want to hit something somewhere in the middle where you're comfortable that if you were to go beyond that, you would be able to adjust quickly. So you sort of compromise between the cost of your launch with the risk and getting to a point where you feel comfortable saying, \"If I were to hit that and it took 30 minutes to recover, that would be fine.\" Nobody would mind because it's such a success that everyone would understand at that point. That ceiling has to be pretty high in the gaming industry. We're talking millions of concurrent users that are connecting within the same minute, are making queries at the same time on their data. It's a huge number. It's difficult, I think, even for the human mind to comprehend these numbers when we're talking millions.\n\nGaspard Petit (34:50):\nIt is a lot of requests per second. So it has to be distributed in a way that will scale, and that was also one of the things that I realized Mongo did very well with the mongos and the mongod split to a sharded cluster, where you pretty much have as many databases you want, you can split the workload on as many database as you want with the mongos, routing it to the right place. So if you're hitting your ceiling with two shards, and you had two more shards, in theory, you can get twice the volume of queries. For that to work, you have to be careful, you have to shard appropriately. So this is where you want to have some experience and you want to make sure that your shard keys is well picked. This is something we've tuned over the years that we've had different experience with different shard keys.\n\nGaspard Petit (35:41):\nFor us, I don't know if everyone in the gaming is doing it this way, but what seems to be the most intuitive and most convenient shard key is the user ID, and we hash it. This way it goes to... Every user profile goes to a random shard, and we can scale Mongo within pretty much the number of users we have, which is generally what tends to go up and down in our case.\n\nGaspard Petit (36:05):\nSo we've had a couple of projects, we've had smaller clusters on one, two. We pretty much never have one shard, but two shards, three shards. And we've been up to 30 plus shards in some cases, and it's never really been an issue. The size, Mongo wise, I would say. There's been issues, but it wasn't really with the architecture itself, it was more of the query pattern, or in some cases, we would pull too much data in the cache. And the cache wasn't used efficiently. But there was always a workaround. And it was never really a limitation on the database. So the sharding model works very well for us.\n\nMichael Lynn (36:45):\nSo I'm curious how you test in that type of scale. I imagine you can duplicate the load patterns, but the number of transactions per second must be difficult to approximate in a development environment. Are you leveraging Atlas for your production load testing?\n\nGaspard Petit (37:04):\nNo. Well, yes and no. The initial tests are done on Kubernetes using the Mongo Operator. So this is where we will simulate. For one operation, we will test will it scale with instance type? So adding more CPU, more RAM, will it scale with number of shards? So we do this grid on each operation that the players might be using ahead of time. At some point, we're comfortable that everything looks right. But testing each operation individually doesn't mean that they will all work fine, they will all play fine when they're mixed together. So the final mix goes through either the production database, if it's not being used yet, or a copy is something that it would look like the production database in Atlas.\n\nGaspard Petit (37:52):\nSo we spin up a Atlas database, similar to the one we expect to use in production. And we run the final load test on that one, just to get clear number with their real components, what will it look like. So it's not necessarily the final cluster we will use, sometimes it's a copy of it. Depending if it's available, sometimes there's already certification ongoing, or QA is already testing on production. So we can't hit the production database for that, so we just spin a different instance of it.\n\nNic Raboy (38:22):\nSo this episode has been fantastic so far, I wanted to leave it open for you giving us or the listeners I should say, any kind of last minute words of wisdom or any anything that we might have missed that you think would be valuable for them to walk away with.\n\nGaspard Petit (38:38):\nSure. So maybe I can share something about why I think we're efficient at what we do and why we're still enjoying the work we're doing. And it has to do a little bit with how we're organized within Square Enix with the different teams. I mentioned earlier that with our interaction with the game team was not so much to dictate how the back-end should be for them, but rather to act as experts. And this is something I think we're lucky to have within Square Enix, where our operation team and our development team are not necessarily acting purely as service providers. And this touches Mongo as well, the way we integrate Mongo in our ecosystem is not so much at... It is in part, \"Please give us database, please make sure they're healthy and working and give us support when we need it.\" But it's also about tapping into different teams as experts.\n\nGaspard Petit (39:31):\nSo Mongo for us is a source of experts where if we need recommendations about shards, query patterns, even know how to use a Java driver. We get a chance to ask MongoDB experts and get accurate feedback on how we should be doing things. And this translate on every level of our processes. We have the ops team that will of course be monitoring and making sure things are healthy, but they're also acting as experts to tell us how the development should be ongoing or what are the best practices?\n\nGaspard Petit (40:03):\nThe back-end dev team does the same thing with the game dev team, where we will bring them our recommendations of how the game should use, consume the services of the back-end, even how they should design some features so that it will scale efficiently or tell them, \"This won't work because the back-end won't scale.\" But act as experts, and I think that's been key for our success is making sure that each team is not just a service provider, but is also bringing expertise on the table so that each other team can be guided in the right direction.\n\nGaspard Petit (40:37):\nSo that's definitely one of the thing that I appreciate over my years. And it's been pushed down from management down to every developers where we have this mentality of acting as experts to others. So we have that as embedded engineers model, where we have some of our folks within our team dedicated to the game teams. And same thing with the ops team, they have the dedicated embedded engineers from their team dedicated to our team, making sure that we're not in silos. So that's definitely a recommendation I would give to anyone in this industry, making sure that the silos are broken and that each team is teaching other teams about their best practices.\n\nMichael Lynn (41:21):\nFantastic. And we love that customers are willing to partner in that way and leverage the teams that have those best practices. So Gaspard, I want to thank you for spending so much time with us. It's been wonderful to chat with you and to learn more about how Square Enix is using MongoDB and everything in the game space.\n\nGaspard Petit (41:40):\nWell, thank you very much. It was a pleasure.\n\nAutomated (41:44):\nThanks for listening. If you enjoyed this episode, please like and subscribe. Have a question or a suggestion for the show? Visit us in the MongoDB community forums at community.mongodb.com.","description":"Join Michael Lynn and Nic Raboy as they chat with Gaspard Petit of Square Enix to learn how one of the largest and best-loved gaming brands in the world is using MongoDB to scale and grow.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt6d5f01206a0d14a7/644c463ea24aa149e119d491/mongodb-square-enix-1.png?branch=prod","description":null}}]},"slug":"/scaling-gaming-mongodb-square-enix-gaspard-petit","title":"*Scaling the Gaming Industry with Gaspard Petit of Square Enix","original_publish_date":"2022-05-09T19:29:17.884Z","strapi_updated_at":"2023-03-22T00:56:15.049Z","expiry_date":"2022-05-17T16:30:47.473Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Podcast","calculated_slug":"/podcasts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Sharding","calculated_slug":"/products/mongodb/sharding"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Kubernetes","calculated_slug":"/technologies/kubernetes"}},{"node":{"title":"*Docker","calculated_slug":"/technologies/docker"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:36.260Z","publish_details":{"time":"2023-04-28T22:52:50.417Z"}}},{"calculated_slug":"/products/mongodb/enhancing-diabetes-data-visibility-with-tidepool-and-mongodb","content":"The data behind diabetes management can be overwhelming — understanding it all is empowering. Tidepool turns diabetes data points into accessible, actionable, and meaningful insights using an open source tech stack that incorporates MongoDB. Tidepool is a nonprofit organization founded by people with diabetes, caregivers, and leading healthcare providers committed to helping all people with dependent diabetes safely achieve great outcomes through more accessible, actionable, and meaningful diabetes data.\n\nThey are committed to empowering the next generation of innovations in diabetes management. We harness the power of technology to provide intuitive software products that help people with diabetes.\n\nIn this episode of the MongoDB Podcast, Michael and Nic sit down with Tapani Otala, V.P. of Engineering at Tidepool, to talk about their platform, how it was built, and how it uses MongoDB to provide unparalleled flexibility and visibility into the critical data that patients use to manage their condition. \n\n:youtube[]{vid=Ocf6ZJiq7ys}\n\n### MongoDB Podcast - Tidepool with Tapani Otala and Christopher Snyder\n### \n\n**Tapani: [00:00:00]** Hi, my name is Tapani Otala. I'm the VP of engineering at [Tidepool](https://tidepool.org). We are a nonprofit organization whose mission is to make diabetes data more accessible, meaningful, and actionable. The software we develop is designed to integrate [00:01:00] data from various diabetes devices like insulin pumps, continuous glucose monitors, and blood glucose meters into a single intuitive interface that allows people with diabetes and their care team to make sense of that data.\nAnd we're using Mongo DB to power all this. Stay tuned for more.\n\n**Chris: [00:00:47]**\nMy name is Christopher Snyder. I've been living with type one diabetes since 2002. I'm also Tidepool's community and clinic success manager. Having this data available to me just gives me the opportunity to make sense of everything that's happening. Prior to using Tidepool, if I wanted to look at my data, I either had to write everything down and keep track of all those notes.\nOr I do use proprietary software for each of my devices and then potentially print things out and hold them up to the light to align events and data points and things like that. Because Tidepool brings everything together in one place, I am biased. I think it looks real pretty. It makes it a lot easier for me to identify trends, make meaningful changes in my diabetes management habits, and hopefully lead a healthier life.\n\n**Mike: [00:01:28]** So we're talking today about Tidepool and maybe you could give us a quick description of what Tidepool is and who it may appeal to \n**Tapani: [00:01:38] **We're a nonprofit organization. And we're developing software that helps people with diabetes manage that condition. We enable people to upload data from their devices, different types of devices, like glucose monitors, meters, insulin pumps, and so on into a single place where you can view that data in one place.\nAnd you can share it with your care team members like doctors, clinicians, or [00:02:00] your family members. They can view that data in real time as well.\n \n**Mike: [00:02:03]** Are there many companies that are doing this type of thing today?\n\n**Tapani: [00:02:06]** There are a \nfew companies, as far as I'm aware, the only non-profit in this space though. Everything else is for profit.\nAnd there are a lot of companies that look at it from diabetes, from different perspective. They might work with type two diabetes or type one. We work with any kind. There's no difference.  \n\n**Nic: [00:02:24]** In regards to Tidepool, are you building hardware as well as software? Or are you just looking at data? Can you shed some more light into that?\n\n**Tapani: [00:02:33]** Sure. We're a hundred percent software company. We don't make any other hardware. We do work with lots of great manufacturers of those devices in the space and medical space in general, but in particular diabetes that make those devices. And so we collaborate with them.\n\n**Mike: [00:02:48]** So what stage is Tidepool in today? Are you live? \n\n**Tapani: [00:02:50]** Yeah, we've been live since 2013 and we we've grown since a fair bit. And we're now at 33 or so people, but still, I guess you could consider as a [00:03:00] startup, substance. So \n\n**Nic: [00:03:01]** I'd actually like to dig deeper into the software that Tidepool produces.\nSo you said that there are many great hardware manufacturers working in this space. How are you obtaining that data? Are you like a mobile application connecting to the hardware? Are you some kind of IoT or are they sending you that information and you're working with it at that point?\n\n**Tapani: [00:03:22]** So it really depends on the device and the integration that we have. For most devices, we talk directly to the device. So these are devices that you would use at your home and you connect them to a PC over Bluetooth or USB or your phone for that matter. And we have software that can read the data directly from the device and upload it to our backend service that's using Mongo DB to store that data. \n\n**Mike: [00:03:43]** Is there a common format that is required in order to send data to Tidepool? \n\n**Tapani: [00:03:49]** We wish. That would make our life a whole lot simpler. No, actually a good chunk of the work that's involved in here is writing software that knows how to talk to each individual device.\nAnd there's some [00:04:00] families of devices that, that use similar protocols and so on, but no, there's no really universal protocol that talk to the devices or for the format of the data that comes from the devices for that matter. So a lot of the work goes into normalizing that data so that when it is stored in in our backend, it's then visible and viewable by people.\n\n**Nic: [00:04:21]** So we'll get to this in a second. It does sound like a perfect case for a kind of a document database, but in regards to supporting all of these other devices, so I imagine that any single device over its lifetime might experience different kind of data output through the versions.\nWhat kind of compatibility is Tidepool having on these devices? Do you use, do say support like the latest version or? Maybe you can shed some light on that, how many devices in general you're supporting. \nTapani: [00:04:50] Right now, we support over 50 different devices. And then by extension anything that Apple Health supports.\nSo if you have a device that stores data in apple [00:05:00] health kit, we can read that as well. But 50 devices directly. You can actually go to type bullet org slash devices, and you can see the list full list there. You can filter it by different types of devices and manufacturers and so on. And that those devices are some of them are actually obsolete at this point.\nThey're end of life. You can't buy them anymore. So we support devices even long past the point when there've been sold. We try to keep up with the latest devices, but that's not always feasible.\n\n**Mike: [00:05:26]** This is it's like a health oriented IOT application right? \n\n**Tapani: [00:05:30]** Yeah. In a way that that's certainly true.\nThe only difference here maybe is that those devices don't directly usually connect to the net. So they need an intermediary. Like in our case, we have a mobile application. We have a desktop application that talks to the device that's in your possession, but you can't reach the device directly over internet.\n\n**Mike:** And just so we can understand the scale, how many devices are reporting into Tidepool today?\n\n**Tapani:** I don't actually know exactly how many devices there are. Those are discreet different types of devices. [00:06:00] What I can say is our main database production database, we're storing something it's approaching to 6 billion documents at this point\nin terms of the amount of data across across and hundreds of thousands of users. \n\n**Nic: [00:06:11]**  Just for clarity, because I want to get to, because the diabetes space is not something I'm personally too familiar in. And the different hardware that exists. So say I'm a user of the hardware and it's reporting to Tidepool.\nIs Tidepool gonna alert you if there's some kind of low blood sugar level or does it serve a different purpose? \n\n**Tapani: [00:06:32]** Both. And this is actually a picture that's changing. So right now what we have out there in terms of the products, they're backward looking. So what happened in the past, but you might might be using these devices and you might upload data, a few times a day.\nBut if you're using some of the more, more newer devices like continuous glucose monitors, those record data every five minutes. So the opposite frequency, it could be much higher, but that's going to change going [00:07:00] forward as more and more people start using this continuous glucose monitors that are actually doing that. For the older devices might be, this is classic fingerprint what glucose meter or you poke your finger, or you draw some little bit of blood and you measure it and you might do that five to 10 times a day.\nVersus 288 times, if you have a glucose monitor, continuous glucose monitor that sends data every five minutes. So it varies from device to device.  \n\n**Mike: [00:07:24]**  This is a fascinating space. I test myself on a regular basis as part of my diet not necessarily for diabetes, but for for ketosis and that's an interesting concept to me. The continuous monitoring devices, though,\nthat's something that you attach to your body, right? \n\n**Tapani: [00:07:39]**  Yeah. These are little devices about the size of a stack of quarters that sits somewhere on your skin, on an arm or leg or somewhere on your body. There's a little filament that goes onto your skin, that does the actual measurements, but it's basically a little full. \n\n**Mike: [00:07:54]**  So thinking about the application itself and how you're leveraging MongoDB, do you want to talk a little bit about how the [00:08:00] application comes together and what the stack looks like?\n\n**Tapani: [00:08:01]**  Sure. So we're hosted in AWS, first of all. We have about 20 or so microservices in there. And as part of those microservices, they all communicate to all MongoDB Atlas.\nThat's implemented with the sort of best practices of suppose security in mind because security and privacy are critically important for us. So we're using the busy gearing from our microservices to MongoDB Atlas. And we're using a three node replica set in MongoDB Atlas, so that there's no chance of losing any of that data.\n\n**Mike: [00:08:32]**  And in terms of the application itself, is it largely an API? I'm sure that there's a user interface or your application set, but what does the backend or the API look like in terms of the technology? \n\n**Tapani: [00:08:43]**  So, what people see in front of them as a, either a desktop application or mobile application, that's the visible manifestation of it.\nBoth of those communicate to our backend through a set of rest APIs for authentication authorization, data upload, data retrieval, and so on. Those APIs then take that data and they store it in our MongoDB production cluster. So the API is very from give me our user profile to upload this pile of continuous glucose monitor samples.\n\n**Mike: [00:09:13]**  What is the API written in? What technologies are you using?\n\n**Tapani: [00:09:16]**  It's a mix of Node JS and Golang. I would say 80% Golang and 20% Node JS. \n\n**Nic: [00:09:23]** I'm interested in why Golang for this type of application. I wouldn't have thought it as a typical use case. So are you able to shed any light on that? \n\n**Tapani: [00:09:32]** The decision to switch to Golang? And so this actually the growing set of services. That happened before my time. I would say it's pretty well suited for this particular application. This, the backend service is fundamentally, it's a set of APIs that have no real user visible manifestation themselves.\nWe do have a web service, a web front end to all this as well, and that's written in React and so on,  but the Golang is proven to be a very good language for developing this, services specifically that respond to API requests because really all they do is they're taking a bunch of inputs from the, on the caller and translating, applying business policy and so on, and then storing the data in Mongo.\nSo it's a good way to do it. \n\n**Nic: [00:10:16]** Awesome. So we know that you're using Go and Node for your APIs, and we know that you're using a MongaDB as your data layer. What features in particular using with MongoDB specifically? \n\n**Tapani: [00:10:26]** So right now, and I mentioned we were running a three node replica set.\nWe don't yet use sharding, but that's actually the next big thing that we'll be tackling in the near future because that set of data that we have is growing fairly fast and it will be growing very fast, even faster in the future with a new product coming out. But sharding will be next one.\nWe do a lot of aggregate queries across several different collections. So some fairly complicated queries. And as I mentioned, that largest collection is fairly large. So performance, that becomes critical. Having the right indices in place and being able to look for all the right data is critical.\n\n**Nic: [00:11:07]** You mentioned aggregations across numerous collections at a high level. Are you able to talk us through what exactly you're aggregating to give us an idea of a use case. \n\n**Tapani: [00:11:16]** Yeah. Sure. In fact, the one thing I should've mentioned earlier perhaps is besides being non-profit, we're also open source.\nSo everything we do is actually visible on GitHub in our open-source repo. So if anybody's interested in the details, they're welcome to take a look in there. But in the sort of broader sense, we have a user collection where all the user accounts profiles are stored. We have a data collection or device data collection, rather.\nThat's where all the data from diabetes devices goes. There's other collections for things like messages that we sent to the users, emails, basically invitations to join this account or so on and confirmations of those and so different collections for different use cases. Broadly speaking is it's, there's one collection for each use case like user profiles or messages, notifications, device data.\n\n**Mike: [00:12:03]** And I'm thinking about the schema and the aggregations across multiple collections. Can you share what that schema looks like? And maybe even just the number of collections that you're storing. \n\n**Tapani: [00:12:12]** Sure. Number of collections is actually relatively small. It's only a half a dozen or so, but the schema is pretty straightforward for most of them.\nThey like the user profiles. There's only so many things you store in a user profile, but that device data collection is perhaps the most complex because it stores data from all the devices, regardless of type. So the data that comes out of a continuous glucose monitor is different than the data that comes from an insulin pump.\nFor instance, for example. So there's different fields. There are different units that we're dealing with and so on. \n\n**Mike: [00:12:44]** Okay, so Tapani, what other features within the Atlas platform are you leveraging today? And have you possibly look at automated scalability as a solution moving forward?\n\n**Tapani: [00:12:55]** So our use of MongoDB Atlas right now is pretty straightforward and intensive. So a  lot of data in the different collections, indices and aggregate queries that are used to manage that data and so on. The things that we're looking forward in the future are things like sharding because of the scale of data that's growing.\nOther things are a data lake, for instance, archiving some of the data. Currently our production database stores all the data from 2013 onwards. And really the value of that data beyond the past few months to a few years is not that important. So we'd want to archive it. We can't lose it because it's important data, but we don't want to archive it and move it someplace else.\nSo that, and bucketizing the data in the more effective ways. And so it's faster to access by different stakeholders in the company.\n\n**Mike: [00:13:43]** So some really compelling features that are available today around online archiving. I think we can definitely help out there. And coming down the pike, we've got some really exciting stuff happening in the time series space.\nSo stay tuned for that. We'll be talking more about that at our .live conference in July. So stay tuned for that. \n\n**Nic: [00:14:04]** Hey Mike, how about you to give a plug about that conference right now?\n\n**Mike: [00:14:06]** Yeah, sure. It's our biggest user conference of the year. And we get together, thousands of developers join us and we present all of the feature updates.\nWe're going to be talking about MongoDB 5.0, which is the latest upcoming release and some really super exciting announcements there. There's a lot of breaks and brain breaking activities and just a great way to get plugged into the MongoDB community. You can get more information at mongodb.com/live.\nSo Tapani, thanks so much for sharing the details of how you're leveraging Mongo DB. As we touched on earlier, this is an application that users are going to be sharing very sensitive details about their health. Do you want to talk a little bit about the security?\n\n**Tapani: [00:14:49]** Sure. Yeah, it's actually, it's a critically important piece for us. So first of all of those APS that we talked about earlier, those are all the traffic is encrypted in transit. There's no unauthorized or unauthenticated access to any other data or API. In MongoDB Atlas, what we're obviously leveraging is we use the encryption at rest.\nSo all the data that's stored by MongoDB is encrypted. We're using VPC peering between our services and MongoDB Atlas, to make sure that traffic is even more secure. And yeah, privacy and security of the data is key thing for us, because this is all what what the health and human services calls, protected health information or PHI. That's the sort of highest level of private information you could possibly have.\n\n**Nic: [00:15:30]** So in regards to the information being sent, we know that the information is being encrypted at rest. Are you collecting data that could be sensitive, like social security numbers and things like that that might need to be encrypted at a field level to prevent prying eyes of DBAs and similar?\n\n**Tapani: [00:15:45]** We do not collect any social security information or anything like that. That's purely healthcare data. Um, diabetes device data, and so on. No credit cards. No SSNs.\n\n**Nic: [00:15:56]** Got it. So nothing that could technically tie the information back to an individual or be used in a malicious way?\n\n**Tapani: [00:16:02]**  Not in that way now. I mean, I think it's fair to say that this is obviously people's healthcare information, so  that is sensitive regardless of whether it could be used maliciously or not. \n\n**Mike: [00:16:13]** Makes sense. Okay. So I'm wondering if you want to talk a little bit about what's next for Tidepool. You did make a brief mention of another application that you'll be launching.\nMaybe talk a little bit about the roadmap. \n\n**Tapani: [00:16:25]** Sure. We're working on, besides the existing products we're working on a new product that's called Tidepool Loop and that's an effort to build an automatic insulin dosing system. This takes a more proactive role in the treatment of diabetes.\nExisting products show data that you already have. This is actually helping you administer insulin. And so it's a smartphone application that's currently under FDA review. We are working with a couple of great partners and the medical device space to launch that with them, with their products. \n\n**Mike: [00:16:55]** Well, I love the open nature of Tidepool.\nIt seems like everything you're doing is kind of out in the open. From open source to full disclosure on the architecture stack. That's something that that I can really appreciate as a developer. I love the ability to kind of dig a little deeper and see how things work.\nIs there anything else that you'd like to cover from an organizational perspective? Any other details you wanna share? \n\n**Tapani: [00:17:16]** Sure. I mean, you mentioned the transparency and openness. We practice what some people might call radical transparency. Not only is our software open source. It's in GitHub.\nAnybody can take a look at it. Our JIRA boards for bugs and so on. They're also open, visible to anybody. Our interactions with the FDA, our meeting minutes, filings, and so on. We also make those available. Our employee handbook is open. We actually forked another company's employee handbook, committed ours opened as well.\nAnd in the hopes that people can benefit from that. Ultimately, why we do this is we hope that we can help improve public health by making everything as, as much as possible we can do make it publicly. And as far as the open source projects go, we have a, several people out there who are making open source contributions or pull requests and so on. Now, because we do operate in the healthcare space,\nwe have to review those submissions pretty carefully before we integrate them into the product. But yeah, we do take to take full requests from people we've gotten community submissions, for instance, translations to Spanish and German and French products. But we'd have to verify those before we can roll them up.\n\n**Mike: [00:18:25]** Well, this has been a great discussion. Is there anything else that you'd like to share with the audience before we begin to wrap up? \n\n**Tapani: [00:18:29]** Oh, a couple of things it's closing. So I was, I guess it would be one is first of all we're a hundred percent remote first and globally distributed organization.\nWe have people in five countries in 14 states within the US right now. We're always hiring in some form or another. So if anybody's interested in, they're welcome to take a look at our job postings tidepool.org/jobs. The other thing is as a nonprofit, we tend suddenly gracefully accept donations as well.\nSo there's another link there that will donate. And if anybody's interested in the technical details of how we actually built this all, there's a couple of links that I can throw out there. One is tidepool.org/pubsecc, that'll be secc, that's a R a security white paper, basically whole lot of information about the architecture and infrastructure and security and so on.\nWe also publish a series of blood postings, at tidepool.org/blog, where the engineering team has put out a couple of things in there about our infrastructure. We went through some pretty significant upgrades over the past couple of years, and then finally github.com/tidepool is where are all our sources.\n\n**Nic: [00:19:30]** Awesome. And you mentioned that you're a remote company and that you were looking for candidates. Were these candidates global, strictly to the US, does it matter?\n\n**Tapani: [00:19:39]**  So we hire anywhere people are, and they work from wherever they are. We don't require relocation. We don't require a visa in that sense that you'd have to come to the US, for instance, to work. We have people in five countries, us, Canada, UK, Bulgaria, and Croatia right now.\n\n**Mike: [00:19:55]** Well, Tapani I want to thank you so much for joining us today. I really enjoyed the conversation. \n\n**Tapani: [00:19:58]** Thanks as well. Really enjoyed it.","description":"Tapani Otala is the VP of Engineering at Tidepool, an open source, not-for-profit company focused on liberating data from diabetes devices, supporting researchers, and providing great, free software to people with diabetes and their care teams. He joins us today to share details of the Tidepool solution, how it enables enhanced visibility into Diabetes data and enables people living with this disease to better manage their condition.  Visit https://tidepool.org for more information.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt8c5c02e71ea24459/644c463f8bfaaa889b424a56/tidepool-and-mongodb.png?branch=prod","description":null}}]},"slug":"/enhancing-diabetes-data-visibility-with-tidepool-and-mongodb","title":"*Making Diabetes Data More Accessible and Meaningful with Tidepool and MongoDB","original_publish_date":"2021-06-01T11:54:50.599Z","strapi_updated_at":"2022-05-16T18:47:14.935Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Podcast","calculated_slug":"/podcasts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:35.861Z","publish_details":{"time":"2023-04-28T22:52:50.454Z"}}},{"calculated_slug":"/languages/php/exploring-php-driver-jeremy-mikola","content":"[Jeremy Mikola](https://www.linkedin.com/in/jmikola/) is a Staff Engineer at MongoDB and helps maintain the MongoDB PHP Driver and Extension. In this episode of the podcast, [Jesse Hall](https://www.linkedin.com/in/codestackr/) and [Michael Lynn](https://www.linkedin.com/in/mlynn/) sit down with Jeremy to talk about the PHP Driver and some of the history of PHP and Mongodb.\n\n:youtube[]{vid=qOuGM6dNDm8}\n\nMichael: [00:00:00] Hey, Jesse, how are you doing today? \n\nJesse: [00:00:02] Good. How are you?\n\nMichael: [00:00:02] Fantastic. It's good to have you back on the podcast. Hey, what's your experience with PHP? \n\nJesse: I've done a little bit of PHP in the past. Mostly JavaScript though, so not too much, but today we do have a special guest. Jeremy Mikola is a staff engineer with Mongo DB, and he knows all about the PHP driver. Why don't you give us a little bit of background on how long have you been with MongoDB?\n\nJeremy: [00:00:26] Hi, nice to be here. So I joined MongoDB  just over nine years. So in the middle of May was my nine-year anniversary. And the entire time of year, a lot of employees been here that long. They tend to shuffle around in different departments and get new experiences. I've been on the drivers team the entire time.\nSo when I find a place that you're comfortable with, you stick there. So when I came on board team was maybe 10 or 12 people, maybe one or two people per language. We didn't have nearly as many officially supported languages as we do today. But the PHP driver was one of the first ones.\n\nIt was developed actually by some of the server engineers. Christina, she was one of the early employees, no longer at MongoDB now, but. So yeah, back then it was PHP, Python, Ruby, C# Java, and I think Node. And we've kind of grown out since then. \n\nMichael: [00:01:05] Fantastic. And what's your personal experience with PHP? How did you get involved in PHP? \n\nJeremy: [00:01:11] So I picked up PHP as a hobby in high school. Date myself here in high school graduation was around 2001. It's kind of the mid nineties getting home from school, load up Napster work on a personal, had a personal SimCity website. We started off around this time of PHP. Nuke was one of the early CMS frameworks back then.\n\nAnd a lot of it was just tinkering, copy/pasting and finding out how stuff works, kind of self-taught until you get to college and then actually have real computer science classes and you understand there's math behind programming and all these other things, concepts. So it's definitely, it was a hobby through most of college.\n\nMy college curriculum was not PHP at all. And then afterwards I was able to, ended up getting a full-time job I working on, and that was with a Symfony 1.0 at the time around like 2007 and followed a couple of companies in the role after that. Ended up being the Symfony 2.0 framework, I was just coming out and that was around the time that PHP really started maturing with like package managers and much more object oriented, kind of shedding the some of the old\n\nbad publicity had had of the early years. And from there, that was also the that second PHP job was where I got started with MongoDB. So we were actually across the street from MongoDB's office in Midtown, New York on the flat iron district and customer support back then used to be go downstairs, go across the street and go up to Elliot's desk and the ShopWiki offices and the Mongo old 10gen offices.\nAnd you'd go ask your question. That kind of works when you only have a few official customers. \n\nMichael: [00:02:36] Talking about Elliot Horowitz. \n\nJeremy: [00:02:37] Yes, as Elliot Horowitz, the co-founder was much more accessible then when the company was a lot smaller. And from that role ended up jumping to a second PHP company kind of the same framework, also using MongoDB.\nIt was the same tech stack. And after that role, I was approached by an old coworker from the first company that used MongoDB. He had ended up at the drivers team, Steve Franzia. He was one of the first engineering managers, the drivers team help build the initial, a lot of the employees that are still on the drivers team\n\nnow, a lot of the folks leading the teams were hired by him or came around the same time. So the early developers of the Python, the Java driver and so he, we had a interview came back, wasn't allowed to recruit me out of the first job whatever paperwork you signed, you can't recruit your old coworkers.\n\nBut after I spent some time somewhere else, he was happy to bring me on. I learned about the opportunity to come on the drivers team. And I was really excited to go from working on applications, to going and developing libraries suited for other developers instead of like a customer facing product. And so that's kind of been the story since then, just really enjoyed working on APIs as well as it was working on the ODM library at the time, which we can talk about a little bit later. So kind of was already involved in a lot of MongoDB PHP ecosystem.\n\nJesse: [00:03:46] Cool. So let's, let's talk more about that, that PHP driver. So, what is it, why is it useful to our listeners? How does it work?\n\nJeremy: [00:03:54] okay. Yep. So level set for the basic explanation. So every language since MongoDB to be doesn't expose a ... it's. Not like some databases, that might have a REST protocol or you just have a web client accessing it. So you do need a driver to speak the wire protocol language, and the MongoDB drivers are particularly different from some other database drivers.\n\nWe do a lot of the monitoring of servers and kind of a lot more heavy than you might find in an equivalent like SQL driver especially like the PHP SQL drivers. So the drivers much more intensive library with a lot of the network programming. We're also responsible for converting, how MongoDB stores documents in a it's binary JSON format BSON converting that to whatever the language's\nnatural representation is. So I can Java that may be just be mapping at the Java classes with PHP. The original driver would turn everything into associative arrays. Make sure that a MongoDB string becomes a PHP string, vice versa. And so the original PHP driver you had familiar concepts across all drivers.\n\nYou have your client object that you connect to the database with, and then you have your database, your collection. And the goal is to make whatever language to users. Running their application, make the drivers as idiomatic as possible. And this kind of bit us early on because the drivers may be too idiomatic and they're inconsistent with each other, which becomes a struggle with someone that's writing a MongoDB application, in say C# and PHP.\n\nThere might be two very different experiences over Python and NodeJS. And the one thing that we hadn't since then was writing specifications to kind of codify what are the areas that we want to be idiomatic, but we also want to have consistent APIs. And this has also been a boon to our support team because if the drivers can behave predictably, both kind of have a familiar API in the outside that our users develop with.\nAnd then also internally, how do they behave when they connect to MongoDO, so kind of being able to enforce that and having internal tests that are shared across all the different drivers has been a huge plus to our support team.\nMichael: [00:05:38] So talk, talk a little bit about that, the balance between a standards-based approach and the idiomatic approach, how does that come together? \n\nJeremy: [00:05:48] Right. So this has definitely been a learning process from the, some of the early specifications. One of the first specifications we had was for the CRUD API which stands acronym for create, read, update, delete. And that was one of the, that's an essential component of every API. Like how do you insert data into MongoDB and read it back? And having that API let's us standardize on a this is a fine method. What are the options that should take how does this map to the servers? And the MongoDB shell API as well.\nThat was another project that exists outside of the driver's team's control. But from our customer standpoint, the Mongo shell is also something that they're common to use. So we try to enforce some consistency with that as well. And the specifications we want to, at a functional level provide a consistent experience.\n\nBut in terms of honoring that every language should be idiomatic. We're going to make allowances that say in C# you have special types to represent time units of time. Whereas other languages like C or Python, you might just use integers or numeric types. So having the specifications say if you're going to express\n\nlike the query time or a time limit on the query will allow like C# driver will say, if you have a time object, you can certainly make use of that type. And another language or students providing guidance and also consistent naming. So we'll say this method should be called find or findOne in your language, if you use camel case or you use snake case like Python with underscores, we're going to let you use that variation.\nAnd that'll keep things idiomatic, so that a user using a Python library doesn't expect to see Pascal style method names in their application. They're going to want it to blend in with other libraries in that languages ecosystem. But the behaviors should be predictable. And there should be a common sense of what functionality is supported across all the different the drivers. \n\nMichael: [00:07:26] Is that supported through synonyms in the language itself? So for, you mentioned, find and find one and maybe some people are used to other, other words to that stand for the, the read functionality in CRUD. \n\nJeremy: [00:07:41] So, this is, that's a point where we do need to be opinionated about, because this overlaps with also the MongoDB documentation. So if you go to the MongoDB server manual that the driver's team doesn't maintain you'll find language examples in there. An initiative we started a few years ago and that's code that we keep in the driver project that the docs team will then parse out and be able to embed in them are going to be manual.\n\nSo the benefit of a, we get to test it in C.I. Environments. And then the MongoDB manual you're browsing. You can say, I use this language and then all the code examples, instead of the MongoDB shell might be in your, in C# or Java or PHP. And so having consistent having, being able to enforce the actual names, we have to be opinionated that we want a method that reads the database instead of calling it query or select.\n\nWe want that to be called find. So we want that to be consistently named and we'll just leave flexibility in terms of the, the casing or if you need prefixing or something like that, but there's certain common or certain core words. We want users to think, oh, this is a find, this is a find operation.\n\nIt also maps to the find command in the database. That's the same thing with inserts and updates. One of the other changes with the old drivers. We would have an update method and in MongoDB different ways that you work with documents, you can update them in place, or you can replace the document.\n\nAnd both of those in the server's perspective happened to be called an update command. So you had original drivers that would just have an update method with a bunch of options. And depending what options you pass in, they could do myriad different behaviors. You might be overwriting the entire document.\n\nYou might be incrementing a value inside of it. So one of the things that CRUD API implemented was saying, we're going to kind of, it's a kind of a poor design pattern to have an overloaded method name that changes behavior wildly based on the arguments. So let's create an updateOne method I replaced one method and updateMany method.\n\n> For more information about the PHP Driver's implementation of CRUD, refer to the [PHP Quickstart series](https://developer.mongodb.com/quickstart/php-setup/).\n\nSo now that when the users write their applications instead of having to infer, what are the options that I'm passing into this method? The method name itself leads to more by self-documenting code in that user's application.\n\nJesse: [00:09:31] awesome. So how do users get started using the driver?\n\nJeremy: [00:09:35] Yeah, so I think a lot of users some, maybe their first interaction might be through the online education courses that we have through MongoDB university. Not every driver, I don't believe there's a PHP class for that. There's definitely a Python Java node, a few others and just kind of a priority list of limited resources to produce that content.\n\nBut a lot of users are introduced, I would say through MongoDB University. Probably also through going back nine years early on in the company. MongoDB had a huge presence at like college hackathons going to conferences and doing booths, try out MongoDB and that definitely more appropriate when we were sa maller company, less people had heard about MongoDB now where it's kind of a different approach to capturing the developers.\n\nI think in this case, a lot of developers already heard about MongoDB and maybe it's less of a. Maybe the focus has shifted towards find out how this database works to changing maybe misconceptions they might have about it, or getting them to learn about some new features that we're implementing. I think another way that users pick up using databases sometimes through projects that have MongoDB integrations. \nSo at the first company where I was using MongoDB and Symfony to in both of them were, it was like a really early time to be using both of those technologies much less together. There was the concept of ORM libraries for PHP, which would kind of map your PHP classes to relational database.\n\nAnd at the time I don't know who made this decision, but early startup, the worst thing you can possibly do is use two very new technologies that are changing constantly and are arguably unproven. Someone higher up than me decided let's use MongoDB with this new web framework. It was still being actively developed and not formally released yet.\n\nAnd we need an ORM library for MongoDB cause we don't want to just write raw database queries back and forth. And so we developed a ODM library, object document mapper instead of object relational mapper. And that was based on the same common interfaces as the corresponding ORM library. So that was the doctrine ODM.\n\nAnd so this was really early time to be writing that. But it integrated so well. It was into the framework and from a such an early point that a lot of users when picking up the Symphony two framework, they realized, oh, we have this ORM library that's integrated in an ODM library. They both have\n\nbasically the same kind of support for all the common features, both in terms of integrating with the web forms all the bundles for like storing user accounts and user sessions and things like that. So in all those fleet or functionalities is kind of a drop-in replacement. And maybe those users said, oh MongoDB's new.\n\nI want to try this out. And so that being able to. Have a very low barrier of entry to switch into it. Probably drove some users to to certainly try it out and stick with it. We definitely that's. The second company was at was kind of using it in the same vein. It was available as a drop-in replacement and they were excited about the not being bound to a relational schema.\nSo definitely had its use as a first company. It was an e-commerce product. So it definitely made use of storing like flexible the flexible schema design for storing like product information and stuff. And then the, we actually used SQL database side by side there just to do all the order, transactional stuff.\n\nBecause certainly at the time MongoDB did not have the same kind of level of transactions and stuff that it does today. So that was, I credit that experience of using the right tool for the job and the different part of the company like using MongoDB to represent products and using the relational database to do the order processing and transactions with time.\n\nDefinitely left me with a positive experience of using MongoDB versus like trying to shoehorn everything into the database at the time and realizing, oh, it doesn't work for, for this use case. I'm gonna write an angry blog post about it. \n\nMichael: [00:12:53] Yeah, I can relate. So if listeners are looking to get started today, you mentioned the ODM, you mentioned the driver what's the best way to get started today?\n\nJeremy: [00:13:04] So I definitely would suggest users not jump right in with an ODM library. Because while that's going to help you ramp up and quickly develop an application, it's also going to extract a lot of the components of the database away from you. So you're not going to get an understanding of how the query language works completely, or maybe how to interact with aggregation pipelines, which are some of the richer features of MongoDB.\n\nThat said there's going to be some users that like, when you need to you're rapidly developing something, you don't want to think about that. Like you're deciding like uncomfortable and maybe I want to use Atlas and use all the infrastructure behind it with the scaling and being able to easily set up backups and all that functionality.\n\nAnd so I just want to get down and start writing my application, crank out these model classes and just tap them, store to MongoDB. So different use cases, I would say, but if you really want to learn MongoDB, install the PHP driver comes in two parts. There's the PHP extension, which is implemented in C.\n\nSo that's gonna be the first thing you're gonna install. And that's published as a pickle package, like a lot of third-party PHP extensions. So you will install that and that's going to provide a very basic API on top of that. We have a higher level package written in PHP code itself. And that's kind of the offload, like what is the essential heavy lifting code that we have to do in C and what is the high level API that we can implement in PHP? It's more maintainable for us. And then also users can read the code or easily contribute to it if they wish. And so those two components collectively form what we call it, the PHP driver. And so using once those are both installed getting familiar with the API in terms of our documentation for that high-level library kind of goes through all the methods.\n\nWe don't, I would say where there's never nearly enough tutorials, but there's a bunch of tutorials in there to introduce the CRUD methods. Kind of explain the basics of inserting and reading and writing documents. MongoDB writing queries at patient pipelines iterating cursors. When you do a query, you get this cursor object back, how you read your results back.\n\nSo that would hopefully give users enough of a kind of a launchpad to get started. And I was certainly biased from having been exposed to MongoDB so long, but I think the driver APIs are mostly intuitive. And that's been, certainly been the goal with a lot of the specifications we write. And I'll say this, this does fall apart\n\nwhen we get into things like say client-side encryption, these advanced features we're even being a long-term employee. Some of these features don't make complete sense to me because I'm not writing applications with them the same way our users are. We would kind of, a driver engineer, we might have a portion of the, the average team work on a, on a new feature, a new specification for it.\nSo not every driver engineer has the same benefit of being, having the same holistic experience of the database platform as is was easy to do so not years ago where going to say oh, I came in, I was familiar with all these aspects of MongoDB, and now there's like components of MongoDB that I've never interacted with.\n\nLike some of the authentication mechanisms. Some of that, like the Atlas, a full text search features there's just like way too much for us to wrap our heads around.\n\nJesse: [00:15:49] Awesome. Yeah. And if the users want to get started, be sure to check the show notes. We'll include links to everything there. Let's talk about the development process. So, how does that work? And is there any community participation there?\n\nJeremy: [00:16:02] Yep. So the drivers spec process That's something that's definitely that's changed over the time is that I mentioned the specifications. So all the work that I mean kind of divide the drivers workload into two different things. We have the downstream work that comes from the server or other teams like Atlas has a new feature.\n\nThe server has a new feature, something like client side encryption or the full text search. And so the, for that to be used by our community, we need support for that in the driver. Right? So we're going to have downstream tickets be created and a driver engineer or two, a small team is going to spec out what the driver API for that feature should be.\n\nAnd that's going to come on our plate for the next so if you consider like MongoDB 5.0, I was coming out soon. Or so if we look at them, MongoDB 5.0, which should be out within the summer that's going to have a bunch of new features that need to end up in the driver API. And we're in the process of designing those and writing our tests for those.\n\nAnd then there's going to be another handful of features that are maybe fully contained within the driver, or maybe a single language as a new feature we want to write, let's give you an example, a PHP, we have a desire to improve the API is around mapping these on to PHB classes and back and forth.\n\nSo that's something that tied back to the doctorate ODM library. That was something that was. The heavy lifting and that was done. That doctor did entirely at PHB there's ways that we can use the C extension to do that. And it's a matter of writing enough C code to get the job done that said doctrine can fully rely on it instead of having to do a lot of it still on its own.\n\n So the two of us working on the PHP driver now, myself and Andres Broan we both have a history of working on Doctrine, ODM project. So we know what the needs of that library are.\n \nAnd we're a good position to spec out the kind of features. And more importantly, in this case, it involves a lot of prototyping to find out the right balance of how much code we want to write. And what's the performance improvement that we'll be able to give the third, the higher level libraries that can use the driver.\n\nThat's something that we're going to be. Another example for other drivers is implementing a client side operations timeout. So that's, this is an example of a cross driver project that is basically entirely on the language drivers. And this is to give users a better API. Then so right now MongoDB\n\nhas a whole bunch of options. If you want to use socket timeout. So we can say run this operation X amount of time, but in terms of what we want to give our users and the driver is just think about a logical amount of time that you want something to complete in and not have to set five different timeout options at various low levels.\n\nAnd so this is something that's being developed inside. We're specing out a common driver API to provide this and this feature really kind of depends entirely on the drivers and money and it's not really a server feature or an Atlas feature. So those are two examples of the tickets that aren't downstream changes at all.\n\nWe are the originators of that feature. And so you've got, we have a mix of both, and it's always a lack of, not enough people to get all the work done. And so what do we prioritize? What gets punted? And fortunately, it's usually the organic drivers projects that have to take a back seat to the downstream stuff coming from other departments, because there's a, we have to think in terms of the global MongoDB ecosystem.\n\nAnd so if an Atlas team is going to develop a new feature and folks can't use that from drivers, no one's going to be writing their application with the MongoDB shell directly. So if we need, there are certain things we need to have and drivers, and then we've just kind of solved this by finding enough resources and staff to get the job done. \n\nMichael: [00:19:12] I'm curious about the community involvement, are there a lot of developers contributing code? \n\nJeremy: [00:19:19] So I can say definitely on the PHP driver, there's looking at the extension side and see there's a high barrier of entry in terms of like, when I joined the company, I didn't know how to write C extensions and see, it's not just a matter of even knowing C. It's knowing all the macros that PHP itself uses.\n\nWe've definitely had a few smaller contributions for the library that's written in PHP. But I would say even then it's not the same as if we compare it to like the Symfony project or other web frameworks, like Laravel where there's a lot of community involvement. Like people aren't running an application, they want a particular feature.\n\nOr there's a huge list of bugs. That there's not enough time for the core developers to work on. And so users pick up the low-hanging fruit and or the bigger projects, depending on what time. And they make a contribution back to the framework and that's what I was doing. And that for the first company, when you use Symphone and Mongo. But I'd say in terms of the drivers speaking for PHP, there's not a lot of community involvement in terms of us.\n\nDefinitely for, we get issues reported, but in terms of submitting patches or requesting new features, I don't kind of see that same activity. And I don't remember that. I'd say what the PHP driver, I don't see the same kind of user contribution activity that you'd see in popular web frameworks and things.\n\nI don't know if that's a factor of the driver does what it needs to do or people are just kind of considered a black box. It's this is the API I'm going to do its functionally here and not try and add in new features. Every now and then we do get feature requests, but I don't think they materialize in, into code contributions.\n\nIt might be like someone wants this functionality. They're not sure how we would design it. Or they're not sure, like what, what internal refactorings or what, what is it? What is the full scope of work required to get this feature done? But they've voiced to us that oh, it'd be nice if maybe going like MongoDB's date type was more usable with, with time zones or something like that.\nSo can you provide us with a better way to this is identifiable identify a pain point for us, and that will point us to say, develop some resources into thinking it through. And maybe that becomes a general drivers spec. Maybe that just becomes a project for the PHP driver. Could say a little bit of both.\n\nI do want to point out with community participation in drivers versus existing drivers. We definitely have a lot of community developed drivers, so that MongoDB as a company limited staffing. We have maybe a dozen or so languages that we actively support with drivers. There's many more than that in terms of community developed drivers.\n\nAnd so that's one of the benefits of us publishing specifications to develop our drivers kind of like open sourcing our development process. Is also a boon for community drivers, whether they have the resources to follow along with every feature or not, they might decide some of these features like the more enterprise features, maybe a community driver doesn't doesn't care about that.\nBut if we're updating the CRUD API or one of the more essential and generally useful features, they can follow along the development processes and see what changes are coming for new server versions and implement that into the community driver. And so that's kind of in the most efficient way that we've come up with to both support them without having the resources to actually contribute on all those community projects.\n\nCause I think if we could, it would be great to have MongoDB employees working on a driver for every possible language just isn't feasible. So it's the second best thing we can do. And maybe in lieu of throwing venture capital money at them and sponsoring the work, which we've done in the past with some drivers at different degrees.\n\nBut is this open sourcing the design process, keeping that as much the, not just the finished product, but also the communication, the review process and keeping that in it to give up yards as much as possible so people can follow the design rationale that goes into the specifications and keep up to date with the driver changes. \n\nMichael: [00:22:46] I'm curious about the about the decline in the PHP community, there's been obviously a number of factors around that, right? The advent of Node JS and the popularity of frameworks around JavaScript, it's probably contributing to it. But I'm curious as someone who works in the PHP space, what are your thoughts around the, the general decline\nof, or also I say the decrease in the number of new programmers leveraging PHP, do you see that continuing or do you think that maybe PHP has some life left?\n\nJeremy: [00:23:24] so I think the it's hard for me to truly identify this cause I've been disconnected from developing PHP applications for a long time. But in my time at MongoDB, I'd say maybe with the first seven or eight years of my time here, COVID kind of disrupted everything, but I was reasonably active in attending conferences in the community and watching the changes in the PHP ecosystem with the frameworks like Symfony and Laravel I think Laravel particularly. And some of these are kind of focused on region where you might say so Symfony is definitely like more active in, in Europe. Laravel I think if you look at like USB HP users and they may be there versus if they didn't catch on in the US quite the same way that Laravel did, I'm like, excuse me where the Symfony community, maybe didn't develop in\n\nat the same pace that laravel did it in the United States. The, if you go to these conferences, you'll see there's huge amounts of people excited about the language and actively people still giving testimonies that they like taught themselves programming, wrote their first application in one of these frameworks and our supporting their families, or transitioned from a non-tech job into those.\nSo you definitely still have people learning PHP. I'd say it doesn't have the same story that we get from thinking about Node JS where there's like these bootcamps that exists. I don't think you kind of have that same experience for PHP. But there's definitely still a lot of people learning PHP and then making careers out of it.\n\nAnd even in the shift of, in terms of the language maturity, you could say. Maybe it's a bit of a stereotype that you'd say PHP is a relic of the early nineties. And when people think about the older CMS platforms and maybe projects like a WordPress or Drupal which if we focused on the numbers are still in like using an incredible numbers in terms of the number of websites they power.\n\nBut it's also, I don't think people necessarily, they look at WordPress deployments and things like, oh, this is the they might look at that as a more data platform and that's a WordPress. It was more of a software that you deploy as well as a web framework. But like in terms of them supporting older PHP installations and things, and then looking at the newer frameworks where they can do cutting edge, like we're only going to support PHP.\n\nThe latest three-year releases of PHP, which is not a luxury that an established platform like WordPress or Drupal might have. But even if we consider Drupal or in the last, in the time I've been at MongoDB, they went from being a kind of a roll their own framework to redeveloping themselves on top of the Symphony framework and kind of modernizing their innards.\n\nAnd that brought a lot of that. We could say the siloed communities where someone might identify as a Drupal developer and just only work in the Drupal ecosystem. And then having that framework change now be developed upon a Symfony and had more interoperability with other web frameworks and PHP packages.\n\nSome of those only triple developers transitioned to becoming a kind of a Jack of all trades, PHP developer and more of a, kind of a well-balanced software engineer in that respect. And I think you'll find people in both camps, like you could certainly be incredibly successful, writing WordPress plugins.\n\nSo you could be incredibly successful writing pumping out websites for clients on web frameworks. The same way that you can join a full-time company that signs this entire platform is going to be built on a particular web framework.\n\nJesse: [00:26:28] Yeah, that's kind of a loaded question there. I don't think that PHP is a, is going to go anywhere. I think JavaScript gets a lot of publicity. But PHP has a strong foothold in the community and that's where I have some experience there with WordPress. That's kind of where I got introduced to PHP as well.\n\nBut PHP is, yeah, it's not going to go anywhere.\n\nJeremy: [00:26:49] I think from our perspective on the drivers, it's also, we get to look longingly at a lot of the new PHP versions that come out. So like right now they're working on kind of an API for async support, a lot of the new we have typing a lot more strictly type systems, which as a software engineer, you appreciate, you realize in terms of the flexibility of a scripting language, you don't want typing, but depending which way you're approaching it, as it says, it.\n\nWorking on the MongoDB driver, there's a lot of new features we want to use. And we're kind of limited in terms of we have customers that are still on earlier versions of PHP seven are definitely still some customers maybe on PHP five. So we have to do the dance in terms of when did we cut off support for older PHP versions or even older MongoDB versions?\n\nSo it's not quite as not quite the same struggle that maybe WordPress has to do with being able to be deployed everywhere. But I think when you're developing a project for your own company and you have full control of the tech stack, you can use the latest new features and like some new technology comes off.\n\nYou want to integrate it, you control your full tech stack. When you're writing a library, you kind of have to walk the balance of what is the lowest common denominator reasonably that we're going to support? Because we still have a user base. And so that's where the driver's team we make use of our, our product managers to kind of help us do that research.\n\nWe collect stats on Atlas users to find out what PHP versions they're using, what MongoDB versions are using as well. And so that gives us some kind of intelligence to say should we still be supporting this old PHP version while we have one, one or 2% of users? Is that, is that worth the amount of time or the sacrifice of features?\n\nThat we're not being able to take advantage of.\n\nJesse: [00:28:18] Sure. So I think you talked a little bit about this already, but what's on the roadmap? What's coming up?\n\nJeremy: [00:28:24] So Andreas and I are definitely looking forward when we have time to focus on just the PHP project development revisiting some of the BSON integration on coming up with better API is to not just benefit doctrine, but I'd say any library that integrates step provides Object mapper on top of the driver.\n\nFind something generally useful. There's also framework integrations that so I mentioned I alluded to Laravel previously. So for Laravel, as a framework is kind of a PDA around there or on that ships with the framework is based around relational databases. And so there is a MongoDB integration for laravel that's Kind of community developed and that kind of deals with the least common denominator problem over well, we can't take advantage of all the MongoDB features because we have to provide a consistent API with the relational ORM that ships with Laravel. And this is a similar challenge when in the past, within the drivers team or people outside, the other departments in MongoDB have said, oh, why don't we get WordPress working on them? MongoDB around, we get Drupal running on MongoDB, and it's not as easy as it seems, because if the entire platform assumes because... same thing has come up before with a very long time ago with the Django Python framework. It was like, oh, let's get Django running on MongoDB. And this was like 10 years ago. And I think it's certainly a challenge when the framework itself has you can't fight the inertia of the opinionated decisions of the framework.\n\nSo in Laravel's case they have this community supported MongoDB integration and it struggles with implementing a lot of MongoDB features that just kind of can't be shoehorned into that. And so that's a project that is no longer in the original developers' hands. It kind of as a team behind it, of people in the community that have varying levels of amount of time to focus on these features.\nSo that project is now in the hands of a team, not the original maintainer. And there, I think, I mean, they all have jobs. They all have other things that they're doing this in their spare time offer free. So is this something that we can provide some guidance on in the past, like we've chipped in on code reviews and try to answer some difficult questions about MongoDB.\n\nI think the direction they're going now is kind of, they want to remove features for next future version and kind of simplify things and get what they have really stable. But if that's something when, if we can build up our staff here and devote more time to, because. We look at our internal stats.\n\nWe definitely have a lot of MongoDB customers happen to be using Laravel with PHP or the Symfony framework. So I think a lot of our, given how many PHP users use things like Drupal and WordPress, we're not seeing them on MongoDB the same way that people using the raw frameworks and developing applications themselves might choose that in that case, they're in full control of what they deploy on.\nAnd when they choose to use MongoDB, we want to make sure that they have as. It may not be the first class because it's that can't be the same experience as the aura that ships with the framework. But I think it's definitely there's if we strategize and think about what are the features that we can support.\n\nBut that, and that's definitely gonna require us familiarizing ourselves with the framework, because I'd say that the longer we spend at MongoDB working on the driver directly. We become more disconnected from the time when we were application developers. And so we can approach this two ways.\n\nWe can devote our time to spending time running example applications and finding those pain points for ourselves. We can try and hire someone familiar with the library, which is like the benefit of when I was hired or Andreas was hired coming out of a PHP application job. And then you get to bring that experience and then it's a matter of time before they become disconnected over the next 10 years.\n\nEither, yeah. Either recruiting someone with the experience or spending time to experiment the framework and find out the pain points or interview users is another thing that our product managers do. And that'll give us some direction in terms of one of the things we want to focus on time permitting and where can we have the most impact to give our users a better experience? \n\nMichael: [00:31:59] Folks listening that want to give feedback. What's the best way to do that? Are are you involved in the forums, the community.MongoDB.com forums? \n\nJeremy: [00:32:09] so we do monitor those. I'd say a lot of the support questions there because the drivers team itself is just a few people for language versus the entirety of our community support team and the technical services department. So I'm not certainly not going on there every day to check for a new user questions.\n\nAnd to give credit to our community support team. Like they're able to answer a lot of the language questions themselves. That's something, and then they come, they escalate stuff to us. If there's, if there's a bigger question, just like our paids commercial support team does they feel so many things ourselves.\n\nAnd then maybe once or twice a month, we'll get like a language question come to us. And it's just we're, we're kind of stumped here. Can you explain what the driver's doing here? Tell us that this is a bug. But I would say the community forums is the best way to if you're posting there. The information will definitely reach us because certainly our product managers the people that are kind of a full-time focus to dealing with the community are going to see that first in terms of, for the drivers we're definitely active on JIRA and our various GitHub projects.\nAnd I think those are best used for reporting actual bugs instead of general support inquiries. I know like some other open source projects, they'll use GitHub to track like ideas and then the whole not just bug reports and things like that. In our case, we kind of, for our make best use of our time, we kind of silo okay, we want to keep JIRA and GitHub for bugs and the customer support issues.\n\nIf there is open discussion to have, we have these community forums and that helps us efficiently kind of keep the information in the, in the best forum, no pun intended to discuss it. \n\nMichael: [00:33:32] Yeah, this has been a great discussion. Thank you so much for sharing all of the details about the PHP driver and the extension. Is there anything we missed? Anything you want to make sure that the listeners know about the about PHP and Mongo DB? \n\nJeremy: [00:33:46] I guess an encouragement to share a feedback and if there are, if there are pain points, we definitely like we're I definitely say like other like over languages have more vocal people. And so it's always unsure. It's do we just have not had people talking to us or is it a matter of or the users don't think that they should be raising the concerns, so just reiterate and encourage people to share the feedback?\n\nJesse: [00:34:10] Or there's no concerns.\nJeremy: [00:34:12] Yeah. Or maybe they're actually in terms of our, our bug reports are like very, we have very few bug reports relatively compared to some other drivers. \n\nMichael: [00:34:19] That's a good thing. Yeah.\n\nAwesome. Jeremy, thank you so much once again, truly appreciate your time, Jesse. Thanks for for helping out with the interview. \n\nJesse: [00:34:28] Thanks for having me.\n\nJeremy: [00:34:29] Great talking to you guys. Thanks.\n\nWe hope you enjoyed this podcast episode. If you're interested in learning more about the PHP Driver, please visit our [documentation page](https://docs.mongodb.com/drivers/php/), and the [GitHub Repository](https://github.com/mongodb/mongo-php-driver). I would also encourage you to visit our forums, where we have a [category specifically for PHP](https://developer.mongodb.com/community/forums/tags/c/drivers-odms/7/php).\n\nI would also encourage you to check out the [PHP Quickstart articles](https://developer.mongodb.com/quickstart/php-setup/) I wrote recently on our Developer Hub. Feedback is always welcome!","description":"Jeremy Mikola is a Senior Software Engineer at MongoDB and helps maintain the MongoDB PHP Driver and Extension. In this episode of the podcast, Jesse Hall and Michael Lynn sit down with Jeremy to talk about the PHP Driver and some of the history of PHP and MongoDB.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltdf2f9a2cb68ac969/644c4640b4288c7b7dfb395e/php.png?branch=prod","description":null}}]},"slug":"/exploring-php-driver-jeremy-mikola","title":"*Exploring the PHP Driver with Jeremy Mikola - Podcast Episode","original_publish_date":"2021-06-09T20:11:14.722Z","strapi_updated_at":"2022-05-16T18:47:14.935Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*PHP","calculated_slug":"/languages/php"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Podcast","calculated_slug":"/podcasts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*PHP","calculated_slug":"/languages/php"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:35.417Z","publish_details":{"time":"2023-04-28T22:52:50.497Z"}}},{"calculated_slug":"/products/realm/realm-meetup-jwt-authentication","content":"Didn't get a chance to attend the Easy Realm JWT Authentication with CosyncJWT Meetup? Don't worry, we recorded the session and you can now watch it at your leisure to get you caught up.\n\n:youtube[]{vid=k5ZcrOW-leY}\n\nIn this meetup, Richard Krueger, CEO Cosync, will focus on the benefits of JWT authentication and how to easily implement CosyncJWT within a Realm application.  CosyncJWT is a JWT Authentication service specifically designed for MongoDB Realm application. It supports RSA public/private key third party email authentication and a number of features for onboard users to a Realm application. These features include signup and invite email confirmation, two-factor verification through the Google authenticator and SMS through Twilio, and configurable meta-data through the JWT standard. CosyncJWT offers both a cloud implementation where Cosync hosts the application/user authentication data, and will soon be releasing a self-hosted version of the service, where developers can save their user data to their own MongoDB Atlas cluster. \n\nIn this 60-minute recording, Richard spends about 40 minutes presenting an overview of Cosync, and then dives straight into a live coding demo. After this, we have about 20 minutes of live Q&A with our Community. For those of you who prefer to read, below we have a full transcript of the meetup too. As this is verbatim, please excuse any typos or punctuation errors!\n\nThroughout 2021, our Realm Global User Group will be planning many more online events to help developers experience how Realm makes data stunningly easy to work with. So you don't miss out in the future, join our Realm Global Community and you can keep updated with everything we have going on with events, hackathons, office hours, and (virtual) meetups. Stay tuned to find out more in the coming weeks and months.\n\nTo learn more, ask questions, leave feedback, or simply connect with other Realm developers, visit our [community forums](https://developer.mongodb.com/community/forums/c/realm-sdks/58). Come to learn. Stay to connect.\n\n### Transcript \n\nShane:\nSo, you're very, very welcome. We have a great guest here speaker today, Richard Krueger's joined us, which is brilliant to have. But just before Richard get started into the main event, I just wanted to do introductions and a bit of housekeeping and a bit of information about our upcoming events too. My name is Shane McAllister. I look after developer advocacy for Realm, for MongoDB. And we have been doing these meetups, I suppose, steadily since the beginning of this year, this is our fifth meetup and we're delighted that you can all attend. We're delighted to get an audience on board our platform. And as we know in COVID, our events and conferences are few and far between and everything has moved online. And while that is still the case, this is going to be a main channel for our developer community that we're trying to build up here in Realm at MongoDB.\n\nWe are going to do these regularly. We are featuring talkers and speakers from both the Realm team, our SDK leads, our advocacy team, number of them who are joining us here today as well too, our users and also our partners. And that's where Richard comes in as well too. So I do want to share with you a couple of future meetups that we have coming as well to show you what we have in store. We have a lot coming on the horizon very, very soon. So just next week we have Klaus talking about Realm Kotlin Multiplatform, followed a week or so later by Jason who's done these meetups before. Jason is our lead for our Coco team, our Swift team, and he's on June 2nd. He's talking about SwiftUI testing and Realm with projections. And then June 10th, a week later again, we have Kræn, who's talking about Realm JS for react native applications.\n\nBut that's not the end. June 17th, we have Igor from Amazon Web Services talking about building a serverless event driven application with MongoDB in Realm. And that will also be done with Andrew Morgan who's one of our developer advocates. We've built, and you can see that on our developer hub, we've built a very, very neat application integrating with Slack. And then Jason, a glutton for punishment is back at the end of June and joining us again for a key path filtering and auto open. We really are pushing forward with Swift and SwiftUI with Realm. And we see great uptake within our community. On top of all of that in July is mongodb.live. This is our key MongoDB event. It's on July 13th and 14th, fully online. And we do hope that if you're not registered already, you will sign up, just search for mongodb.live, sign up and register. It's free. And over the two days, we will have a number of talks, a number of sessions, a number of live coding sessions, a number tutorials and an interactive elements as well too. So, it's where we're announcing our new products, our roadmap for the year, and engage in across everything MongoDB, including Realm. We have a number of Realm's specific sessions there as well too. So, just a little bit of housekeeping. We're using this bevy platform, for those of you familiar with Zoom, and who've been here before to meet ups, you're very familiar. We have the chat. Thank you so much on the right-hand side, we have the chats. Thank you for joining there, letting us know where you're all from. We've got people tuning in from India, Sweden, Spain, Germany. So that's brilliant. It's great to see a global audience and I hope this time zone suits all of you.\n\nWe're going to take probably about, I think roughly, maybe 40 minutes for both the presentation and Richard's brave enough to do some live coding as well too. So we very much look forward to that. We will be having a Q&A at the end. So, by all means, please ask any questions in the chat during Richard's presentation. We have some people, Kurt and others here, and who'll be able to answer some questions on Cosync. We also have some of our advocates, Diego and Mohit who joined in and answer any questions that you have on Realm as well too. So, we can have the chat in the sidebar. But what happens in this, what happened before at other meetups is that if you have some questions at the end and you're very comfortable, we can open up your mic and your video and allow you to join in in this meetup.\n\nIt is a meetup after all, and the more the merrier. So, if you're comfortable, let me know, make a note or a DM in the chats, and you can ask your question directly to Richard or myself at the end as well too. The other thing then really with regard to the housekeeping is, do get connected. This is our meetup, this is our forums. This is our channels. And that we're on as well too. So, developer.mongodb.com is our forums and our developer hub. We're creating articles there weekly and very in-depth tutorials, demos, links to repos, et cetera. That's where our advocates hang out and create content there. And around global community, you're obviously familiar with that because you've ended up here, right? But do spread the word. We're trying to get more and more people joining that community.\n\nThe reason being is that you will be first to know about the future events that we're hosting in our Realm global community if you're signed up and a member there. As soon as we add them, you'll automatically get an email, simple button inside the email to RSVP and to join future events as well too. And as always, we're really active on Twitter. We really like to engage with our mobile community there on Twitter. So, please follow us, DM us and get in touch there as well too. And if you do, and especially for this event now, I'm hoping that you will ... We have some prizes, you can win some swag.\n\nIt's not for everybody, but please post comments and your thoughts during the presentation or later on today, and we'll pick somebody at random and we send them a bunch of nice swag, as you can see, happily models there by our Realm SDK engineers, and indeed by Richard and myself as well too. So, I won't keep you much longer, essentially, we should get started now. So I would like to introduce Richard Krueger who's the CEO of Cosync. I'm going to stop sharing my screen. Richard, you can swap over to your screen. I'll still be here. I'll be moderating the chat. I'm going to jump back in at the end as well too. So, Richard, really looking forward to today. Thank you so much. We're really happy to have you here.\n\nRichard:\nSounds good. Okay. I'm Richard Krueger, I'm the CEO of Cosync, and I'm going to be presenting a JWT authentication system, which we've built and as we're adding more features to it as we speak. So let me go ahead and share my screen here. And I'm going to share the screen right. Okay. Do you guys see my screen?\nShane:\nWe see double of your screen at the moment there.\n\nRichard:\nOh, okay. Let me take this away. Okay, there you go.\n\nShane:\nWe can see that, if you make that full screen, we should be good and happier. I'd say, are you going to move between windows because you're doing-\n\nRichard:\nYeah, I will. There we go. Let me just ... I could make this full screen right now. I might toggle between full screen and non-full screen. So, what is a little bit about myself, I've been a Realm programmer for now almost six years. I was a very early adopter of the very first object database which I used for ... I've been doing kind of cloud synchronization programs. So my previous employer, Needley we used that extensively, that was before there was even a cloud version of Realm. So, in order to build collaborative apps, one, back in the day would have to use something like Parse and Realm or Firebase and Realm. And it was kind of hybrid systems. And then about 2017, Realm came out with its own cloud version, the Realm Cloud and I was a very early adopter and enthusiast for that system.\n\nI was so enthusiastic. I started a company that would build some add on tools for it. The way I see Realm is as kind of a seminole technology for doing full collaborative computing, I don't think there's any technology out there. The closest would be Firebase but that is still very server centric. What I love about Realm is that it kind of grew out of the client first and then kind of synchronizes client-side database with a mirrored copy on a server automatically. So, what Realm gives you is kind of an offline first capability and that's just absolutely huge. So you could be using your local app and you could be in a non-synced environment or non-connected environment. Then later when you connect everything automatically synchronizes to a server, copy all the updates.\n\nAnd I think it scales well. And I think this is really seminal to develop collaborative computing apps. So one of the things we decided to do was, and this was about a year ago was build an authentication system. We first did it on the old Realm cloud system. And then in June of last year, Mongo, actually two years ago, Mongo acquired Realm and then merged the Atlas infrastructure with the Realm front end. And that new product was released last June and called MongoDB Realm. And which I actually think is a major improvement even on Realm sync, which I was very happy with, but I think the Apple infrastructures is significantly more featured than the Realm cloud infrastructure was. And they did a number of additional support capabilities on the authentication side.\n\nSo, what we did is we retargeted, co-synced JWT as an authentication system for the new MongoDB Realm. So, what is JWT? That stands for Java Script Web Tokens. So it's essentially a mechanism by which a third party can authenticate users for an app and verify their identity. And it's secure because the technology that's used, that underlies JWT's public private key encryption, it's the same technology that's behind Bitcoin. So you have a private key that encrypts the token or signs it, and then a public key that can verify the signature that can verify that a trusted party actually authenticated the user. And so why would you want to separate these two? Well, because very often you may want to do additional processing on your users. And a lot of the authentication systems that are right now with MongoDB Realm, you have anonymous authentication, or you have email password, but you may want to get more sophisticated than that.\n\nYou may want to attach metadata. You may want to have a single user that authenticates the same way across multiple apps. And so it was to kind of deal with these more complex issues in a MongoDB Realm environment that we developed this product. Currently, this product is a SaaS system. So, we actually host the authentication server, but the summer we're going to release a self hosted version. So you, the developer can host your own users on your own MongoDB Atlas cluster, and you run a NodeJS module called CosyncJWT server, and you will basically provide your own rest API to your own application. The only thing Cosync portal will do will be to manage that for you to administrate it.\n\nSo let me move on to the next slide here. Realm allows you to build better apps faster. So the big thing about Realm is that it works in an offline mode first. And that to me is absolutely huge because if anybody has ever developed synchronized software, often you require people to be connected or just doesn't work at all. Systems like Slack come to mind or most chat programs. But with Realm you can work completely offline. And then when you come back online, your local Realm automatically syncs up to your background Realm. So what we're going to do here is kind of show you how easy it is to implement a JWT server for a MongoDB Realm app. And so what I'm going to go ahead and do is we're going to kind of create an app from scratch and we're going to first create the MongoDB Realm app.\n\nAnd so what I'm going to go here, I've already created this Atlas cluster. I'm going to go ahead and create an app called, let's call it CosyncJWT test. And this is I'm inside the MongoDB Realm portal right now. And I'm just going to go ahead and create this app. And then I'm going to set up its sync parameters, all of the MongoDB Realm developers are familiar with this. And so we're going to go to is we'll give it a partition key called partition, and we will go ahead and give it a database called CosyncJWT TestDB. And then we will turn our development mode on. Wait, what happened here?\n\nWhat is the problem there? Okay. Review and deploy. Okay. Let me go ahead and deploy this. Okay. So, now this is a complete Realm app. It's got nothing on it whatsoever. And if I look at its authentication providers, all I have is anonymous login. I don't have JWT set at all. And so what we're going to do is show you how easy it is to configure a JWT token. But the very first thing we need to do is create what I call an API key, and an API key enables a third party program to manipulate programmatically your MongoDB Realm app. And so for that, what we'll do is go into the access manager and for this project, we'll go ahead and create an API key. So let me go ahead and create an API key. And I'm going to call this CosyncJWT test API key, and let's give it some permissions.\n\nI'll be the project owner and let's go ahead and create it. Okay. So that will create both a public key and a private cake. So the very first thing you need to do when you do this is you need to save all of your keys to a file, which your private key, you have to be very careful because the minute somebody has this, go in and programmatically monkey with your stuff. So, save this away securely, not the way I'm doing it now, but write it down or save it to a zip drive. So let me copy the private key here. For the purpose of this demo and let me copy the public key.\n\nOkay. Let me turn that. Not bold. Okay. Now the other thing we need is the project ID, and that's very easy to get, you just hit this little menu here and you go to project settings and you'll have your project ID here. So I'm going to, also, I'll need that as well. And lastly, what we need is the Realm app ID. So, let's go back to Realm here and go into the Realm tab there, and you can always get your app ID here. That's so unique, that uniquely identifies your app to Realm and you'll need that both the cursing portal level and at your app level. Okay, so now we've retrieved all of our data there. So what we're going to go ahead and do now is we're going to go into our Cosync portal and we're going to go ahead and create a Cosync app that mirrors this.\n\nSo I'm going to say create new app and I'll say Cosync. And by the way, to get to the Cosync portal, just quick note, to get to the Cosync portal, all you have to do is go to our Cosync website, which is here and then click on sign in, and then you're in your Cosync. I've already signed in. So, you can register yourself with Cosync. So we're going to go ahead and create a new app called Cosync JWT test and I'm going to go ahead and create it here. And close this. And it's initializing there, just takes a minute to create it on our server. Okay. Right. Something's just going wrong here. You go back in here.\n\nShane:\nSuch is the world of live demos!\n\nRichard:\nThat's just the world of live demos. It always goes wrong the very second. Okay, here we go. It's created.\n\nShane:\nThere you go.\n\nRichard:\nYeah. Okay. So, now let me explain here. We have a bunch of tabs and this is basically a development app. We either provide free development apps up to 50 users. And after that they become commercial apps and we charge a dollar for 1,000 users per month. So, if you have an app with 10,000 users, that would cost you $10 per month. And let me go, and then there's Realm tab to initialize your Realm. And we'll go into that in a minute. And then there's a JWT tab that kind of has all of the parameters that regulate JWT. So, one of the things I want to do is talk about metadata and for this demo, we can attach some metadata to the JWT token.\n\nSo the metadata we're going to attach as a first name and a last name, just to show you how that works. So, I'm going to make this a required field. And I'll say we're going to have a first name, this actually gets attached to the user object. So this will be its path, user data dot name dot first. And then this is the field name that gets attached to the user object. And there'll be first name and let's set another field, which is user data dot name dot last. And that will be last name. Okay. And so we have our metadata defined, let's go ahead and save it. There's also some invite metadata. So, if you want to do an invitation, you could attach a coupon to an invitation. So these are various onboarding techniques.\n\nWe support two types of onboarding, which is either invitation or sign up. You could have a system of the invitation only where a user would ... the free masons or something where somebody would have to know you, and then you could only get in if you were invited. Okay. So, now what we're going to go ahead and do is initialize our instance. So that's pretty easy. Let's go take our Realm app ID here, and we paste that in and let's go ahead and initialize our Kosik JWT, our token expiration will be 24 hours. So let's go ahead and initialize this. I'll put in my project ID.\n\nAll right. My project ID here, and then I will put in my public key, and I will put in my private key here. Okay. Let's go ahead and do this. Okay. And it's successfully initialized it, and we can kind of see that it did. If we go back over here to authentication, we're going to actually see that now we have cosynced JWT authentication. If we go in, it'll actually have set the signing algorithm to RS256, intellectually, have set the public key. So the Cosync, I mean, the MongoDB Realm app will hold onto the public key so that it knows that only this provider which holds onto the private key has the ability to sign. And then it also is defined metadata fields, which are first name, last name and email. Okay. So, anytime you sign up, those metadata fields will be kind of cemented into your user object.\n\nAnd we also provide APIs to be able to change the metadata at runtime. So if you need to change it, you can. But it's important to realize that this metadata doesn't reside in Realm, it resides with the provider itself. And that's kind of the big difference there. So you could have another database that only had your user data. That was not part of your MongoDB Realm database, and you could mine that database for just your user stuff. So, that's the idea there. So the next step, what we're going to do is we're going to go ahead and run this kind of sample app. So the sample, we provide a number of sample apps. If you go to our docs here and you go down to sample application, we provide a good hub project called Cosync samples, which has samples for both our Cosync storage product, which we're not talking about here today, and our CosyncJWT project.\n\nCosync storage basically maps Amazon as three assets onto a MongoDB Realm app. So CosyncJWT has different directories. So, we have a Swift directory, a Kotlin directory and a ReactNative. Today I'm primarily just showing the Swift, but we also have ReactNative binding as well that works fine with this example. Okay. So what happens is you go ahead and clone this. You would go ahead and clone this, Github project here and install it. And then once you've installed it, let me bring it up here, here we go, this is what you would get. We have a sample app called CosyncJWT iOS. Now, that has three packages that depends on. One is a package called CosyncJWT Swift, which wrappers around our arrest API that uses NSURL.\n\nAnd then we depend on the Realm packages. And so this little sample app will do nothing, but allow you to sign up a user to CosyncJWT, and logging in. And it'll also do things like two factor verification. We support both phones two factor verification if you have a Twilio account and we support the Google two-factor authentication, which is free, and even more secure than a phone. So, that gives you an added level of security, and I'll just show you how easy it is too. So, in order to kind of customize this, you need to set two constants. You need to set your Realm app ID and your wrap token. So, that's very easy to do. I can go ahead, and let me just copy this Realm app ID, which I copied from the Realm portal.\n\nAnd I'll stick that here. Let me go ahead and get the app token, which itself is a JWT token because the Cosync, this token enables your client side app to use the CosyncJWT rust API and identify you as the client is belonging to the sound. And so if we actually looked at that token, we could go to utilities that have used JWT. You always use jwt.io, and you can paste any JWT token in the world into this little thing. And you'll see that this is this app token is in fact itself, a JWT token, and it's signed with CosyncJWT, and that will enable your client side to use the rest API.\n\nSo, let's go ahead and paste that in here, and now we're ready to go. So, at this point, if I just run this app, it should connect to the MongoDB Realm instance that we just previously created, and it should be able to connect to the CosyncJWT service for authentication. There are no users by the way in the system yet. So, let me go ahead and build and run this app here, and comes up, [inaudible 00:29:18] an iPhone 8+ simulator. And what we'll do is we'll sign up a user. So if we actually go to the JWT users, you'll see we have no users in our system at all. So, what we're going to go ahead and do is sign up a user. It'll just come up in a second.\n\nShane:\nSimulators are always slow, Richard, especially-\n\nRichard:\nI know.\nShane:\n... when you try to enable them. There you go.\n\nRichard:\nRight. There we go. Okay. So I would log in here. This is just simple SwiftUI. The design is Apple, generic Apple stuff. So, this was our signup. Now, if I actually look at the code here, I have a logged out view, and this is the actual calls here. I would have a sign up where I would scrape the email, the password, and then some metadata. So what I'm going to go ahead and do is I'm going to go ahead and put a break point right there and let's go ahead and sign myself up as richard@cosync.io, give it a password and let's go ahead and let's say Richard Krueger. So, at this point, we're right here. So, if we look at ... Let me just make this a little bit bigger.\n\nShane:\nYeah. If you could a little bit, because some of this obviously bevy adjusts itself by your connection and sometimes-\n\nRichard:\nRight away.\n\nShane:\n... excavated in code. Thank you.\n\nRichard:\nYeah. Okay. So if we look at the ... We have an email here, which is, I think we might be able to see it. I'm not sure. Okay, wait. Self.email. So, for some reason it's coming out empty there, but I'm pretty sure it's not empty. It's just the debugger is not showing the right stuff, but that's the call. I would just make a call to CosyncJWT sign up. I pass in an email, I pass in a password, pass in the metadata and it'll basically come back with it signed in. So, if I just run it here, it came back and then should not be ... there's no error. And it's now going to ask me to verify my code. So, the next step after that will be ... So, at this point I should get an email here. Let's run. So, it's not going to be prompting me for a code. So I just got this email, which says let me give it a code. And I'll make another call, Russ call to verify the code. And this should let me in.\n\nYeah. Which it did log me in. So, the call to verify the code. We also have things where you can just click on a link. So, by the way, let me close this. How your signup flow, you can either have code, link or none. So, you might have an app that doesn't need purification. So then you would just turn it on to none. If you don't want to enter a code, you would have them click on a link and all of these things themselves can be configured. So, the emails that go out like this particular email looks very generic. But I can customize the HTML of that email with these email templates. So, the email verification, the password reset email, all of these emails can be customized to 50 branding of the client itself.\n\nSo, you wouldn't have the words cosync in there. Anyways, so that kind of shows you. So now let me go ahead and log out and I can go ahead and log back in if I wanted to. Let me go ahead and the show you where the log in is. So, this is going to call user manager, which will have a log in here. And that we'll call Realm manage ... Wait a minute, log out, log in this right here. So, let's go put a break point on log in and I'm going to go ahead and say Richard@krueger@cosync.io. I'm going to go ahead and log in here. And I just make a call to CosyncJWT rest. And again, I should be able to just come right back.\n\nAnd there I am. Often, by the way, you'll see this dispatch main async a lot of times when you make Rest calls, you come back on a different thread. The thing to remember, I wrote an article on Medium about this, but the thing to remember about Realm and threads is this, what happens on a thread? It's the Vegas rule. What happens on a thread must stay on a thread. So with Realm does support multithreading very, very well except for the one rule. If you open a Realm on a thread, you have to write it on the same thread and read it from the same thread. If you try and open a Realm on one thread and then try and read it from another thread, you'll cause an exception. So, often what I do a lot is force it back on the main thread.\n\nAnd that's what this dispatch queue main async is. So, this went ahead and there's no error and it should just go ahead and log me in. So, what this is doing here, by the way, let me step into this. You'll see that that's going to go ahead and now issue a Realm log in. So that's an actual Realm call app.login.credentials, and then I pass it the JWT token that was returned to me by CosyncJWT. So by the way, if you don't want to force your user to go through the whole authentication procedure, every time he takes this app out of process, you can go ahead and save that JWT token to your key chain, and then just redo this this way.\n\nSo you could bypass that whole step, but this is a demo app, so I'd put it in there. So this will go ahead and log me in and it should transition, let me see. Yeah, and it did. Okay. So, that kind of shows you that. We also have capabilities for example, if you wanted to change your password, I could. So, I could change my password. Let me give my existing password and then I'll change it to a new password and let me change my password. And it did that. So, that itself is a function called change password.\n\nIt's right here, Cosync change password, is passing your new password, your old password, and that's another Rest call. We also have forgotten password, the same kind of thing. And we have two factor phone verification, which I'm not going to go into just because of time right now, or on two factor Google authentication. So, this was kind of what we're working on. It's a system that you can use today as a SaaS system. I think it's going to get very interesting this summer, once we release the self hosted version, because then, we're very big believers in open source, all of the code that you have here result released under the Apache open source license. And so anything that you guys get as developers you can modify and it's the same way that Realm has recently developed, Andrew Morgan recently developed a great chat app for Realm, and it's all equally under the Apache license.\n\nSo, if you need to implement chat functionality, I highly recommend to go download that app. And they show you very easily how to build a chat app using the new Swift combine nomenclature which was absolutely phenomenal in terms of opaque ... I mean, in terms of terseness. I actually wrote a chat program recently called Tinychat and I'd say MongoDB Realm app, and it's a cloud hosted chat app that is no more than 70 lines of code. Just to give you an idea how powerful the MongoDB Realm stuff and I'm going to try and get a JWT version of that posted in the next few days. And without it, yes, we probably should take some questions because we're coming up at quarter to the hour here. Shane.\n\nShane:\nExcellent. No, thank you, Richard. Definitely, there's been some questions in the sidebar. Kurt has been answering some of them there, probably no harm to revisit a couple of them. So, Gigan, I hope I'm pronouncing that correctly as well too, was asking about changing the metadata at the beginning, when you were showing first name, last name, can you change that in future? Can you modify it?\n\nRichard:\nYeah. So, if I want to add to the metadata, so what I could do is if I want to go ahead and add another field, so let's go ahead and add another field a year called user data coupon, and I'll just call this guy coupon. I can go ahead and add that. Now if I add something that's required, that could be a problem if I already have users without a required piece of metadata. So, we may actually have to come up with some migration techniques there. You don't want to delete metadata, but yeah, you could go ahead and add things.\n\nShane:\nAnd is there any limits to how much metadata? I mean, obviously you don't want-\n\nRichard:\nNot really.\n\nShane:\n... fields for users to fill in, but is there any strict limit at all?\n\nRichard:\nI mean, I don't think you want to store image data even if it's 64 encoded. If you were to store an avatar as metadata I'd store the link to the image somewhere, you might store that avatar on Amazon, that's free, and then you would store the link to it in the metadata. So, it's got normally JWT tokens pretty sparse. It's something supposed to be a 10 HighQ object, but the metadata I find is one of the powers of this thing because ... and all of this metadata gets rolled into the user objects. So, if you get the Realm user object, you can get access to all the metadata once you log in.\n\nShane:\nI mean, the metadata can reside with the provider. That's obviously really important for, look, we see data breaches and I break, so you can essentially have that metadata elsewhere as well too.\n\nRichard:\nRight.\n\nShane:\nIt's very important for the likes of say publications and things like that.\n\nRichard:\nRight. Yeah, exactly. And by the way, this was a big feature MongoDB Realm added, because metadata was not part of the JWT support in the old Realm cloud. So, it was actually a woman on the forum. So MongoDB employee that tuned me into this about a year ago. And I think it was Shakuri I think is her name. And that's why it was after some discussion on the forums. By the way, these forums are fantastic. If you have any, you meet people there, you have great discussions. If you have a problem, you can just post it. If I know an issue, I try to answer it. I would say there it's much better than flashed off. And then it's the best place to get Realm questions answered okay much better than Stack Overflow. So, [inaudible 00:44:20]. Right?\n\nShane:\nI know in our community, especially for Realm are slightly scattered all rights as well too. Our advocates look at questions on Stack Overflow, also get help comments and in our forum as well too. And I know you're an active member there, which is great. Just on another question then that came up was the CosyncJWT. You mentioned it was with Swift and ReactNative by way of examples. Have you plans for other languages?\n\nRichard:\nWe have, I don't think we've published it yet, but we have a Kotlin example. I've just got to dig that up. I mean, if we like to hear more, I think Swift and Kotlin and React Native are the big ones. And I've noticed what's going on is it seems that people feel compelled to have a Native iOS, just because that's the cache operating system. And then what they do is they'll do an iOS version and then they'll do a ReactNative version to cover desktop and Android. And I haven't bumped into that many people that are pure Android, purest or the iOS people tend to be more purest than the Android people. I know...\n\nShane:\n... partly down to Apple's review process with apps as well too can be incredibly stringent. And so you want to by the letter of the law, essentially try and put two things as natively as possible. Or as we know, obviously with Google, it's much more open, it's much freer to use whatever frameworks you want. Right?\n\nRichard:\nRight. I would recommend though, if you're an iOS developer, definitely go with SwiftUI for a number ... Apple is putting a huge amount of effort into that. And I have the impression that if you don't go there, you'll be locked out of a lot of features. And then more importantly, it's like Jason Flax who's a MongoDB employee has done a phenomenal job on getting these MongoDB Realm combined primitives working that make it just super easy to develop a SwiftUI app. I mean, it's gotten to the point where one of our developer advocate, Kurt Libby, is telling me that his 12 year old could \nJason flax's stuff. That was like normally two years ago to use something like Realm required a master's degree, but it's gone from a master's degree to a twelve-year-old. It just in simplification right now.\n\nShane:\nYeah. We're really impressed with what we've seen in SwiftUI. It's one of the areas we see a lot of innovation, a huge amount of traction, I suppose. Realm, historically, was seen as a leader in the Swift space as well too. Not only did we have Realm compatible with Swift, but we talked about swift a lot outside of, we led one of the largest Swift meetup groups in San Francisco at the time. And we see the same happening again with SwiftUI. Some people, look, dyed in the wool, developers are saying, \"Oh, it's not ready for real time commercial apps,\" but it's 95% there. I think you can build an app wholly with SwiftUI. There's a couple of things that you might want to do, and kind of using UI kit and other things as well too, it's all right, but that's going to change quickly. Let's see what's in store at DC as well for us coming up.\n\nRichard:\nYeah, exactly.\n\nShane:\nRight. Excellent. I know, does anybody, I said at the beginning, we can open up the mic and the cameras to anybody who'd like to come on and ask a question directly of Richard or myself. If you want to do that, please make a comment in the chat. And I can certainly do that, if not just ask the questions in the chat there as well too. While we're waiting for that, you spoke about Google two factor and also Twilio. Your example there was with the code with the Google email, how much more work is involved in the two factor side of things either\n\nRichard:\nSo, the two factor stuff, what you have to do, when you go here, you can turn on two factor verification. So, if you select Google you would have to put in your ... Let me just see what my ... You would have to put in the name of your Google app. And then if you did phone ... Yes, change it, you'd have to put your Twilio account SI, your off the token from Twilio and your Twilio phone number. Now, Twilio, it looks cheap. It's just like a penny a message. It adds up pretty fast.\n\nRichard:\nMy previous company I worked with, Needley, we had crypto wallet for EOS and we released it and we had 15,000 users within two weeks. And then our Twilio bill was $4,000 within the week. It just added up very quickly. So it's the kind of thing that ... it doesn't cost much, but if you start sending out machine gunning out these SMS messages, it can start adding up. But if you're a banking app, you don't really care. You're more interested in providing the security for your ... Anyways, I guess that would answer that question. Are there any other questions here?\n\nShane:\nThere's been a bit of, I think it was a comment that was funny while you were doing the demo there, Richard, with regards to working on the main thread. And you were saying that there was issues. Now, look, Realm, we have frozen objects as well too, if you need to pass objects rights, but they are frozen. So maybe you might want to just maybe clarify your thoughts on that a little bit there. There was one or two comments in the sidebar.\n\nRichard:\nWell, with threading in Realm, this is what I tend to do. If you have a background, one of the problems you bump into is the way threading in SwiftUI works is you have your main thread that's a little bit like you're Sergeant major. And then you have all your secondary threads that are more like your privates. And the Sergeant major says, \"Go do this, go clean the latrine, or go peel some potatoes.\" And he doesn't really care which private goes off and doesn't, just the system in the background will go assign some private to go clean the little train. But when Realm, you have to be careful because if you do an async open on a particular thread, particular worker thread, then all the other subsequent things, all the writes and the reads should be done on that same thread.\n\nRichard:\nSo, what I found is I go ahead and create a worker thread at the beginning that will kind of handle requests. And then I make sure I can get back there and to that particular thread. There was an article I wrote on Medium about how to do this, because you obviously you don't want to burden your main thread with all your Realm rights. You don't want to do that because it will start eating ... I mean, your main threads should be for SwiftUI and nothing more. And you want to then have a secondary thread that can process that, and having just one secondary thread that's working in the background is sufficient. And then that guy handles the Realm request in a sense. That was the strategy seemed to work best I found.\n\nRichard:\nBut you could open a Realm on your primary thread. You can also open the same Realm on a background thread. You just have to be careful when you're doing the read better beyond the Realm that was opened on the thread that it was opened on that the read is taking place from. Otherwise, you just got an exception. That's what I've found. But I can't say that I'm a complete expert at it, but in general, with most of my programming, I've always had to eventually revert to kind of multi-threading just to get the performance up because otherwise you'll just be sitting there just waiting and waiting and waiting sometimes.\n\nShane:\nYeah, no, that's good. And I think everybody has a certain few points on this. Sebastian asked the question originally, I know both Mohit and Andrew who are developer advocates here at Realm have chimed in on that as well too. And it is right by best practices and finding the effect on what might happen depending on where you are trying to read and write.\n\nRichard:\nRight. Well, this particular example, I was just forcing it back on the main thread, because I think that's where I had to do the Rest calls from. There was an article I wrote, I think it was about three months ago, Multithreading and MongoDB Realm, because I was messing around with it for some imaging out that there was writing and we needed to get the performance out of it. And so anyways, that was ... But yeah, I hope that answers that question.\n\nShane:\nYeah, yeah. Look, we could probably do a whole session on this as well. That's the reality of it. And maybe we might do that. I'm conscious of everybody's time. It'd be mindful of that. And didn't see anything else pop up in the questions. Andrew's linked your Medium articles there as well too. We've published them on Realm, also writes on Medium. We publish a lot of the content, we create on dev up to Medium, but we do and we are looking for others who are writing about Realm that who may be writing Medium to also contribute. So if you are, please reach out to us on Medium there to add to that or ping us on the forums or at Realm. I look after a lot of our Twitter content on that Realm as we [crosstalk 00:56:12] there. I've noticed during this, that nobody wants T-shirts and face masks, nobody's tweeted yet at Realm. Please do. We'll keep that open towards the end of the day as well. If there's no other questions, I first of all want to say thank you very much, Richard.\n\nRichard:\nWell, thank you for having me.\n\nShane:\nNo, we're delighted. I think this is a thing that we want to do ongoing. Yes, we are running our own meetups with our own advocates and engineers, but we also want, at least perhaps once a month, maybe more if we could fit it in to invite guests along to share their experience of using MongoDB Realm as well too. So, this is the first one of those. As we saw at the beginning, we do have Igor in AWS during the presentation in June as well too. But really appreciate the attendance here today. Do keep an eye. We are very busy. You saw it's pretty much once week for the next four or five weeks,  these meetups. Please share amongst your team as well too.\n\nShane:\nAnd above all, join us. As you said, Richard, look, I know you're a contributor in our forums and we do appreciate that. We have a lot of active participants in our forums. We like to, I suppose, let the community answer some of those questions themselves before the engineers and the advocates dive in. It's a slow growth obviously, but we're seeing that happen as well too, so we do appreciate it. So communicate with us via forums, via @realm and go to our dev hub, consume those articles. The articles Richard mentioned about the chat app is on our dev hub by Andrew. If you go look there and select actually the product category, you can select just mobile and see all our mobile articles. Since certainly November of last year, I think there's 24, 25 articles there now. So, they are relatively recent and relatively current. So, I don't know, Richard, have you any parting words? I mean, where do people ... you said up to 50 users it's free, right? And all that.\n\nRichard:\nRight. So, up to 50 users it's free. And then after that you would be charged a dollar for 1,000 users per month.\n\nShane:\nThat's good.\n\nRichard:\nWell, what we're going to try and do is push once we get the self hosted version. We're actually going to try and push developers into that option, we don't know the price of it yet, but it will be equally as affordable. And then you basically host your own authentication server on your own servers and you'll save all your users to your own Atlas cluster. Because one of the things we have bumped into is people go, \"Well, I don't really know if I want to have all my user data hosted by you,\" and which is a valid point. It's very sensitive data.\n\nShane:\nSure.\n\nRichard:\nAnd so that was why we wanted to build an option so your government agency, you can't share your user data, then you would host, we would just provide the software for you to do that and nothing more. And so that's where the self hosted version of CosyncJWT would do them.\n\nShane:\nExcellent. It sounds great. And look, you mentioned then your storage framework that you're building at the moment as well too. So hopefully, Richard, we can have you back in a couple of months when that's ready.\n\nRichard:\nGreat. Okay. Sounds good.\n\nShane:\nExcellent.\n\nRichard:\nThanks, Shane.\n\nShane:\nNo problem at all. Well, look, thank you everybody for tuning in. This is recorded. So, it will end up on YouTube as well too and we'll send that link to the group once that's ready. We'll also end up on the developer hub where we've got a transcript of the content that Richard's presented here as well. That'd be perfect. Richard, you have some pieces in your presentation too that we can share in our community as well too later?\n\nRichard:\nYeah, yeah. That's fine. Go ahead and share.\n\nShane:\nExcellent. We'll certainly do that.\n\nRichard:\nYeah.\n\nShane:\nSo, thank you very much everybody for joining, and look forward to seeing you at the future meetups, as I said, five of them over the next six weeks or so. Very, very [inaudible 01:00:34] time for us. And thank you so much, Richard. Really entertaining, really informative and great to see the demo of the live coding.\n\nRichard:\nOkay. Thanks Shane. Excellent one guys.\n\nShane:\nTake care, everybody. Bye.\n\nRichard:\nBye.","description":"This meetup talk will focus on the benefits of JWT authentication and how to easily implement CosyncJWT within a Realm application.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blte140995ddbc9a8e3/644c4642a2d3bc3eeae53e5b/speaker-grant.png?branch=prod","description":null}}]},"slug":"/realm-meetup-jwt-authentication","title":"*Easy Realm JWT Authentication with CosyncJWT","original_publish_date":"2021-06-03T00:28:35.565Z","strapi_updated_at":"2022-05-23T14:54:19.390Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:34.960Z","publish_details":{"time":"2023-04-28T22:52:50.546Z"}}},{"calculated_slug":"/products/realm/build-ci-cd-pipelines-realm-apps-github-actions","content":"> As of June 2022, the functionality previously known as MongoDB Realm is now named [Atlas App Services](https://www.mongodb.com/docs/atlas/app-services/). Atlas App Services refers to the cloud services that simplify building applications with Atlas – Atlas Data API, Atlas GraphQL API, Atlas Triggers, and Atlas Device Sync. [Realm](https://www.mongodb.com/docs/realm/) will continue to be used to refer to the client-side database and SDKs.\n\nBuilding [Continuous Integration/Continuous Deployment (CI/CD)](https://en.wikipedia.org/wiki/CI/CD) pipelines can be challenging. You have to map your team's ideal pipeline, identify and fix any gaps in your team's test automation, and then actually build the pipeline. Once you put in the work to craft a pipeline, you'll reap a variety of benefits like...\n\n* Faster releases, which means you can get value to your end users quicker)\n* Smaller releases, which can you help you find bugs faster\n* Fewer manual tasks, which can reduce manual errors in things like testing and deployment.\n\nAs Tom Haverford from the incredible TV show Parks and Recreation wisely said, \"Sometimes you gotta **work a little**, so you can **ball a lot**.\" (View the entire scene [here](https://www.youtube.com/watch?v=BcJaSl2UZcw&t=194s). But don't get too sucked into the silliness that you forget to return to this article 😉.)\n\n<div style=\"text-align: center\">\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ballalot_1bbe08b37d.gif\" width=\"400\" alt=\"Screen recording of user adding an item to the inventory in the Inventory App\" align=\"center\"/>\n</div>\n\n\nIn this article, I'll walk you through how I crafted a CI/CD pipeline for a mobile app built with MongoDB Realm. I'll provide strategies as well as code you can reuse and modify, so you can put in just **a little bit of work** to craft a pipeline for your app and **ball a lot**.\n\nThis article covers the following topics:\n\n- [All About the Inventory App](#all-about-the-inventory-app)\n   - [What the App Does](#what-the-app-does)\n   - [The System Architecture](#the-system-architecture)\n- [All About the Pipeline](#all-about-the-pipeline)\n    - [Pipeline Implementation Using GitHub Actions](#pipeline-implementation-using-github-actions)\n    - [MongoDB Atlas Project Configuration](#mongodb-atlas-project-configuration)\n    - [What Happens in Each Stage of the Pipeline](#what-happens-in-each-stage-of-the-pipeline)\n- [Building Your Pipeline](#building-your-pipeline)\n   - [Map Your Pipeline](#map-your-pipeline)\n   - [Implement Your Pipeline](#implement-your-pipeline)\n- [Summary](#summary) \n\n> More of a video person? No worries. Check out the recording below of a talk I gave at MongoDB.live 2021 that covers the exact same content this article does. :youtube[]{vid=-JcEa1snwVQ}\n\n## All About the Inventory App\n\nI recently created a CI/CD pipeline for an iOS app that manages stores' inventories. In this section, I'll walk you through what the app does and how it was architected. This information will help you understand why I built my CI/CD pipeline the way that I did.\n\n### What the App Does\n\nThe Inventory App is a fairly simple iOS app that allows users to manage the online record of their physical stores' inventories. The app allows users to take the following actions:\n\n* Create an account\n* Login and logout\n* Add an item to the inventory\n* Adjust item quantities and prices\n\n<div style=\"text-align: center\">\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/completedemo_small_fee3ae5aa6.gif\" width=\"400\" alt=\"Screen recording of a user interacting with the Inventory App to login, add an item named 'Concealer' to the inventory, adjust item quantities, and logout\" style=\"text-align: center;\"/>\n</div>\n\nIf you'd like to try the app for yourself, you can get a copy of the code in the GitHub repo: [mongodb-developer/realm-demos](https://github.com/mongodb-developer/realm-demos).\n\n### The System Architecture\n\nThe system has three major components:\n\n* <img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/general_objects_mobile_4x_6fe67c6c8c.png\" width=\"30\" alt=\"Mobile device icon\" style=\"float: left; padding-right: 5px;\"/> **The Inventory App** is the iOS app that will be installed on the mobile device. The local Realm database is embedded in the Inventory App and stores a local copy of the inventory data.\n* <img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Realm_Icon_2x_9bef9956b3.png\" width=\"30\" alt=\"Realm icon\" style=\"float: left; padding-right: 5px;\"/>**The Realm App** is the central MongoDB Realm backend instance of the mobile application. In this case, the Realm App utilizes Realm features like [authentication](https://docs.mongodb.com/realm/authentication/), [rules](https://docs.mongodb.com/realm/mongodb/), [schema](https://docs.mongodb.com/realm/mongodb/document-schemas/), [GraphQL API](https://docs.mongodb.com/realm/graphql/), and [Sync](https://docs.mongodb.com/realm/sync/). The Inventory App is connected to the Realm App. **Note**: The Inventory App and the Realm App are NOT the same thing; they have two different code bases.\n* <img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/mongodb_atlas_cluster_4x_ce6d233ecb.png\" width=\"30\" alt=\"Atlas database icon\" style=\"float: left; padding-right: 5px;\"/> **The Atlas Database** stores the inventory data. [Atlas](https://cloud.mongodb.com/) is MongoDB's fully managed Database-as-a-Service. Realm Sync handles keeping the data synced between Atlas and the mobile apps.\n\n![Diagram showing how the Mobile App syncs with the Atlas database using Realm Sync](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_06_14_at_7_50_18_AM_09bb66451d.png)\n\nAs you're building a CI/CD pipeline for a mobile app with an associated Realm App and Atlas database, you'll need to take into consideration how you're going to build and deploy both the mobile app and the Realm App. You'll also need to figure out how you're going to indicate which database the Realm App should be syncing to. Don't worry, I'll share strategies for how to do all of this in the sections below.\n\n<div style=\"text-align: center\">\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/bored_5c6b8e36c1.gif\" width=\"400\" alt=\"I've never been more bored in my entire life\"/>\n</div>\n\nOkay, that's enough boring stuff. Let's get to my favorite part: the CI/CD pipeline!\n\n## All About the Pipeline\n\nNow that you know what the Inventory App does and how it was architected, let's dive into the details of the CI/CD pipeline for this app. You can use this pipeline as a basis for your pipeline and tweak it to fit your team's process.\n\nMy pipeline has three main stages:\n\n* **Development**: In the Development Stage, developers do their development work like creating new features and fixing bugs.\n* **Staging**: In the Staging Stage, the team simulates the production environment to make sure everything works together as intended. The Staging Stage could also be known as QA (Quality Assurance), Testing, or Pre-Production.\n* **Production**: The Production Stage is the final stage where the end users have access to your apps.\n\n![Diagram showing the three stages of the pipeline: Development, Staging, and Production](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_06_14_at_7_53_34_AM_5062cea6d6.png)\n\n### Pipeline Implementation Using GitHub Actions\n\nA variety of tools exist to help teams implement CI/CD pipelines. I chose to use [GitHub Actions](https://github.com/features/actions), because it works well with GitHub (which is where my code is already) and it has a free plan for public repositories (and I like free things!). GitHub Actions allows you to automate [workflows](https://docs.github.com/en/actions/learn-github-actions/introduction-to-github-actions#workflows). As you'll see in later sections, I implemented my CI/CD pipeline using a workflow. Each workflow can contain one or more [jobs](https://docs.github.com/en/actions/learn-github-actions/introduction-to-github-actions#jobs), and each job contains one or more [steps](https://docs.github.com/en/actions/learn-github-actions/introduction-to-github-actions#steps).\n\nThe complete workflow is available in [build.yml](https://github.com/mongodb-developer/realm-demos/blob/main/.github/workflows/build.yml) in the [Inventory App's GitHub repository](https://github.com/mongodb-developer/realm-demos).\n\n### MongoDB Atlas Project Configuration\n\nThroughout the pipeline, the workflow will deploy to new or existing Realm Apps that are associated with new or existing databases based on the pipeline stage. I decided to create four Atlas projects to support my pipeline:\n* **Inventory Demo - Feature Development.** This project contains the Realm Apps associated with every new feature. Each Realm App syncs with a database that has a custom name based on the feature (for example, a feature branch named `beta6-improvements` would have a database named `InventoryDemo-beta6-improvements`). All of the databases for feature branches are stored in this project's Atlas cluster. The Realm Apps and databases for feature branches are deleted after the feature work is completed.\n* **Inventory Demo - Pull Requests.** This project contains the Realm Apps that are created for every pull request. Each Realm App syncs with a database that has a custom name based on the time the workflow runs (for example, `InventoryDemo-2021-06-07_1623089424`). All of the databases associated with pull requests are stored in this project's Atlas cluster.  \n\n   As part of my pipeline, I chose to delete the Realm App and associated database at the end of the workflow that was triggered by the pull request. Another option would be to skip deleting the Realm App and associated database when the tests in the workflow fail, so that a developer could manually investigate the source of the failure.\n* **Inventory Demo - Staging.** This project contains the Realm App for Staging. The Realm App syncs with a database used only for Staging. The Staging database is the only database in this project's cluster. The Realm App and database are never deleted, so the team can always look in the same consistent locations for the Staging app and its data.\n* **Inventory Demo - Production.** This project contains the Realm App for Production. The Realm App syncs with a database used only for Production. The Production database is the only database in this project's cluster. The Realm App and database are never deleted.\n\nNote: This app requires only a single database. If your app uses more than one database, the principles described above would still hold true.\n\n### What Happens in Each Stage of the Pipeline\n\nI've been assigned a ticket to change the color of the **Log In** button in the iOS app from blue to pink. In the following sections, I'll walk you through what happens in each stage of the pipeline and how my code change is moved from one stage to the next.\n\nAll of the stages and transitions below use the same GitHub Actions workflow. The workflow has conditions that modify which steps are taken. I'll walk you through what steps are run in each workflow execution in the sections below. The workflow uses [environment variables](https://docs.github.com/en/actions/reference/environment-variables) and [secrets](https://docs.github.com/en/actions/reference/encrypted-secrets) to store values. Visit the [realm-demos GitHub repo](https://github.com/mongodb-developer/realm-demos/blob/main/.github/workflows/build.yml) to see the complete workflow source code.\n\n#### Development\n\nThe Development stage is where I'll do my work to update the button color. In the subsections below, I'll walk you through how I do my work and trigger a workflow.\n\n##### Updating the Inventory App\n\nSince I want to update my iOS app code, I'll begin by opening a copy of my app's code in [Xcode](https://developer.apple.com/xcode/). I'll change the color of the **Log In** button there. I'm a good developer 😉, so I'll run the automated tests to make sure I didn't break anything. The Inventory App has automated unit and UI tests that were implemented using [XCTest](https://developer.apple.com/documentation/xctest). I'll also kick off a simulator, so I can manually test that the new button color looks fabulous.\n\n![Updating the button color in Xcode](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_06_07_at_1_52_51_PM_231b26e4c2.png \"Updating the button color in Xcode\")\n\n##### Updating the Realm App\n\nIf I wanted to make an update to the Realm App code, I could either:\n\n* work in the cloud in the Realm web interface or\n* work locally in a code editor like Visual Studio Code.\n\nIf I choose to work in the Realm web interface, I can make changes and deploy them. The Realm web interface was recently updated to allow developers to commit changes they make there to their GitHub repositories. This means changes made in the web interface won't get lost when changes are deployed through other methods (like through the Realm Command Line Interface or automated GitHub deployments).\n\n![Updating the Realm App code in the Realm web interface](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_06_07_at_2_00_42_PM_e78f5d2efc.png \"Updating the Realm App code in the Realm web interface\")\n\nIf I choose to work with my Realm App code locally, I could make my code changes and then run unit tests. If I want to run integration tests or do some manual testing, I need to deploy the Realm App. One option is to use the [Realm Command Line Interface](https://docs.mongodb.com/realm/deploy/realm-cli-reference/#realm-cli) (Realm CLI) to deploy with a command like [`realm-cli push`](https://docs.mongodb.com/realm/deploy/deploy-cli/#deploy-the-updated-app). Another option is to automate the deployment using a GitHub Actions workflow.\n\n![Updating the Realm App code locally in Visual Studio Code](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_06_07_at_1_52_32_PM_d331a77e6e.png \"Updating the Realm App code locally in Visual Studio Code\")\n\nI've chosen to automate the deployment using a GitHub Actions workflow, which I'll describe in the following section.\n\n##### Kicking Off the Workflow\n\nAs I am working locally to make changes to both the Inventory App and the Realm App, I can commit the changes to a new [feature branch](https://martinfowler.com/bliki/FeatureBranch.html) in my GitHub repository.\n\nWhen I am ready to deploy my Realm App and run all of my automated tests, I will push the commits to my repository. The push will trigger the workflow. \n\n![Screenshot of the GitHub Actions web interface after a push to a feature branch triggers a workflow](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_06_07_at_1_55_38_PM_9499d4483d.png \"The workflow is in progress after a push to a feature branch\")\n\nThe workflow runs the [`build`](https://github.com/mongodb-developer/realm-demos/blob/5c3a1d3554527fe9fb2602dcbc84e2cc4d39acac/.github/workflows/build.yml#L6) job, which runs the following steps:\n\n1. **Set up job.** This step is created by GitHub Actions to prepare the workflow.\n2. **Run actions/checkout@v2.** Uses the [Checkout V2 Action](https://github.com/actions/checkout) to check out the repository so the workflow can access the code.\n3. <a name=\"store-current-time\">**Store current time in variable.** Stores the current time in an environment variable named `CURRENT_TIME`. This variable is used later in the workflow.</a>\n\n    ```\n    echo \"CURRENT_TIME=$(date +'%Y-%m-%d_%s')\" >> $GITHUB_ENV\n    ```\n\n4. **Is this a push to a feature branch?** If this is a push to a feature branch (which it is), do the following:\n    * Create a new environment variable to store the name of the feature branch.\n       ```\n       ref=$(echo ${{ github.ref }})\n       branch=$(echo \"${ref##*/}\")\n       echo \"FEATURE_BRANCH=$branch\" >> $GITHUB_ENV\n       ```\n    * Check the `GitHubActionsMetadata` Atlas database to see if a Realm App already exists for this feature branch. If a Realm App exists, store the Realm App ID in an environment variable. Note: Accessing the Atlas database requires the IP address of the GitHub Actions virtual machine to be in the [Atlas IP Access List](https://docs.atlas.mongodb.com/security/ip-access-list/).\n       ```\n       output=$(mongo \"mongodb+srv://${{ secrets.ATLAS_URI_FEATURE_BRANCHES }}/GitHubActionsMetadata\" --username ${{ secrets.ATLAS_USERNAME_FEATURE_BRANCHES }} --password ${{ secrets.ATLAS_PASSWORD_FEATURE_BRANCHES }} --eval \"db.metadata.findOne({'branch': '$branch'})\")\n        \n       if [[ $output == *null ]]; then\n           echo \"No Realm App found for this branch. A new app will be pushed later in this workflow\"\n       else\n           echo \"A Realm App was found for this branch. Updates will be pushed to the existing app later in this workflow\"\n           app_id=$(echo $output | sed 's/^.*realm_app_id\" : \"\\([^\"]*\\).*/\\1/')\n           echo \"REALM_APP_ID=$app_id\" >> $GITHUB_ENV\n       fi\n       ```\n\n    * Update the `databaseName` in the `development.json` [environment file](https://docs.mongodb.com/realm/config/environments/). Set the database name to contain the branch name to ensure it's unique.\n\n      ```\n      cd inventory/export/sync/environments\n      printf '{\\n     \"values\": {\"databaseName\": \"InventoryDemo-%s\"}\\n}' \"$branch\" > development.json \n       ```\n    * Indicate that the Realm App should use the `development` [environment](https://docs.mongodb.com/realm/values-and-secrets/define-environment-values/) by updating `realm_config.json`.\n       ```\n       cd ..\n       sed -i txt 's/{/{ \"environment\": \"development\",/' realm_config.json\n       ```\n\n5. <a name=\"install-realm-cli\">**Install the Realm CLI and authenticate.**</a>  This step installs the Realm CLI and authenticates using the API keys that are stored as GitHub secrets.\n   ```\n   npm install -g mongodb-realm-cli@beta\n   realm-cli login --api-key=\"${{ secrets.REALM_API_PUBLIC_KEY }}\" --private-api-key=\"${{ secrets.REALM_API_PRIVATE_KEY }}\" --realm-url https://realm.mongodb.com --atlas-url https://cloud.mongodb.com\n   ```\n6. **Create a new Realm App for feature branches where the Realm App does not yet exist.** This step has three primary pieces:\n   * Push the Realm App to the Atlas project specifically for feature branches.\n      ```\n      cd inventory/export/sync\n      realm-cli push -y --project 609ea554944fe545460529a1\n       ```\n   * Retrieve and store the Realm App ID from the output of `realm-cli app describe`.\n      ```\n      output=$(realm-cli app describe)\n      app_id=$(echo $output | sed 's/^.*client_app_id\": \"\\([^\"]*\\).*/\\1/')\n      echo \"REALM_APP_ID=$app_id\" >> $GITHUB_ENV\n      ```\n   * Store the Realm App ID in the GitHubActionsMetadata database. Note: Accessing the Atlas database requires the IP address of the GitHub Actions virtual machine to be in the [Atlas IP Access List](https://docs.atlas.mongodb.com/security/ip-access-list/).\n      ```\n      mongo \"mongodb+srv://${{ secrets.ATLAS_URI_FEATURE_BRANCHES }}/GitHubActionsMetadata\" --username ${{ secrets.ATLAS_USERNAME_FEATURE_BRANCHES }} --password ${{ secrets.ATLAS_PASSWORD_FEATURE_BRANCHES }} --eval \"db.metadata.insertOne({'branch': '${{ env.FEATURE_BRANCH}}', 'realm_app_id': '$app_id'})\"\n      ```\n7. <a name=\"create-realm-app-id-txt\">**Create `realm-app-id.txt` that stores the Realm App ID.** This file will be stored in the mobile app code. The sole purpose of this file is to tell the mobile app to which Realm App it should connect.</a>\n   ```\n   echo \"${{ env.REALM_APP_ID }}\" > $PWD/inventory/clients/ios-swiftui/InventoryDemo/realm-app-id.txt\n   ```\n8. <a name=\"build-mobile-app\">**Build mobile app and run tests.** This step builds the mobile app for testing and then runs the tests using a variety of simulators. If you have integration tests, you could also choose to checkout previous releases of the mobile app and run the integration tests against the current version of the Realm App to ensure backwards compatibility.</a>\n   * Navigate to the mobile app's directory.\n      ```\n      cd inventory/clients/ios-swiftui/InventoryDemo\n      ```\n   * Build the mobile app for testing.\n      ```\n      xcodebuild -project InventoryDemo.xcodeproj -scheme \"ci\" -sdk iphonesimulator -destination 'platform=iOS Simulator,name=iPhone 12 Pro Max,OS=14.4' -derivedDataPath './output' build-for-testing\n      ```\n   * Define the simulators that will be used for testing.\n      ```\n      iPhone12Pro='platform=iOS Simulator,name=iPhone 12 Pro Max,OS=14.4'\n      iPhone12='platform=iOS Simulator,name=iPhone 12,OS=14.4'\n      iPadPro4='platform=iOS Simulator,name=iPad Pro (12.9-inch) (4th generation)'\n      ```   \n   * Run the tests on a variety of simulators. Optionally, you could put these in separate jobs to run in parallel.\n      ```\n      xcodebuild -project InventoryDemo.xcodeproj -scheme \"ci\" -sdk iphonesimulator -destination \"$iPhone12Pro\" -derivedDataPath './output' test-without-building\n      xcodebuild -project InventoryDemo.xcodeproj -scheme \"ci\" -sdk iphonesimulator -destination \"$iPhone12\" -derivedDataPath './output' test-without-building     \n      xcodebuild -project InventoryDemo.xcodeproj -scheme \"ci\" -sdk iphonesimulator -destination \"$iPadPro4\" -derivedDataPath './output' test-without-building\n        ```\n9. **Post Run actions/checkout@v2.** This cleanup step runs automatically when you use the [Checkout V2 Action](https://github.com/actions/checkout).\n10. **Complete job.** This step is created by GitHub Actions to complete the workflow.\n\nThe nice thing here is that simply by pushing my code changes to my feature branch, my Realm App is deployed and the tests are run. When I am finished making updates to the code, I can feel confident that a Staging build will be successful.\n\n#### Moving from Development to Staging\n\nNow that I'm done working on my code changes, I'm ready to move to Staging. I can kick off this process by creating a [GitHub pull request](https://docs.github.com/en/github/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests). In the pull request, I'll request to merge my code from my feature branch to the `staging` branch. When I submit the pull request, GitHub will automatically kick off another workflow for me. \n\n![Screenshot of a pull request](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/pull_request_dbe365f655.png \"The pull request shows that a workflow is in progress\")\n\nThe workflow runs the following steps.\n\n1. **Set up job.** This step is created by GitHub Actions to prepare the workflow.\n2. **Run actions/checkout@v2.** Uses the [Checkout V2 Action](https://github.com/actions/checkout) to check out the repository so the workflow can access the code.\n3. **Store current time in variable.** See the [section above](#store-current-time) for more information on this step.\n4. **Set environment variables for all other runs.** This step sets the necessary environment variables for pull requests where a new Realm App and database will be created for *each* pull request. This step has three primary pieces.\n    * Create a new environment variable named `IS_DYNAMICALLY_GENERATED_APP` to indicate this is a dynamically generated app that should be deleted later in this workflow.\n       ```\n       echo \"IS_DYNAMICALLY_GENERATED_APP=true\" >> $GITHUB_ENV\n       ```\n    * Update the `databaseName` in the `testing.json` [environment file](https://docs.mongodb.com/realm/config/environments/). Set the database name to contain the current time to ensure it's unique.\n       ```\n       cd inventory/export/sync/environments\n       printf '{\\n     \"values\": {\"databaseName\": \"InventoryDemo-%s\"}\\n}' \"${{ env.CURRENT_TIME }}\" > testing.json  \n       ```\n   * Indicate that the Realm App should use the `testing` [environment](https://docs.mongodb.com/realm/values-and-secrets/define-environment-values/) by updating `realm_config.json`.\n      ```\n      cd ..\n      sed -i txt 's/{/{ \"environment\": \"testing\",/' realm_config.json \n      ```\n5. **Install the Realm CLI and authenticate.** See the [section above](#install-realm-cli) for more information on this step.\n6. **Create a new Realm App for pull requests.** Since this is a pull request, the workflow creates a new Realm App just for this workflow. The Realm App will be deleted at the end of the workflow.\n    * Push to the Atlas project specifically for pull requests.\n       ```\n       cd inventory/export/sync\n       realm-cli push -y --project 609ea554944fe545460529a1\n        ```\n    * Retrieve and store the Realm App ID from the output of `realm-cli app describe`.\n       ```\n       output=$(realm-cli app describe)\n       app_id=$(echo $output | sed 's/^.*client_app_id\": \"\\([^\"]*\\).*/\\1/')\n       echo \"REALM_APP_ID=$app_id\" >> $GITHUB_ENV\n        ```\n    * Store the Realm App ID in the `GitHubActionsMetadata` database. Note: Accessing the Atlas database requires the IP address of the GitHub Actions virtual machine to be in the [Atlas IP Access List](https://docs.atlas.mongodb.com/security/ip-access-list/).\n       ```\n       mongo \"mongodb+srv://${{ secrets.ATLAS_URI_FEATURE_BRANCHES }}/GitHubActionsMetadata\" --username ${{ secrets.ATLAS_USERNAME_FEATURE_BRANCHES }} --password ${{ secrets.ATLAS_PASSWORD_FEATURE_BRANCHES }} --eval \"db.metadata.insertOne({'branch': '${{ env.FEATURE_BRANCH}}', 'realm_app_id': '$app_id'})\"\n       ```\n7. **Create `realm-app-id.txt` that stores the Realm App ID.** See the [section above](#create-realm-app-id-txt) for more information on this step.\n8. **Build mobile app and run tests.** See the [section above](#build-mobile-app) for more information on this step.\n9. **Delete dynamically generated Realm App.** The workflow created a Realm App just for this pull request in an earlier step. This step deletes that Realm App.\n   ```\n   realm-cli app delete --app ${{ env.REALM_APP_ID }}\n   ```\n10. **Delete dynamically generated database.** The workflow also created a database just for this pull request in an earlier step. This step deletes that database.\n    ```\n    mongo \"mongodb+srv://${{ secrets.ATLAS_URI_PULL_REQUESTS }}/InventoryDemo-${{ env.CURRENT_TIME }}\" --username ${{ secrets.ATLAS_USERNAME_PULL_REQUESTS }} --password ${{ secrets.ATLAS_PASSWORD_PULL_REQUESTS }} --eval \"db.dropDatabase()\"\n    ```\n11. **Post Run actions/checkout@v2.** This cleanup step runs automatically when you use the [Checkout V2 Action](https://github.com/actions/checkout).\n12. **Complete job.** This step is created by GitHub Actions to complete the workflow.\n\nThe results of the workflow are included in the pull request.\n\n![\"Screenshot of the workflow results in the pull request](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/prcheckspassed_a67dde1eaa.png \"The pull request shows that all workflows completed successfully\")\n\nMy teammate will review the pull request. They will likely review the code and double check that the workflow passed. We might go back and forth with suggestions and updates until we both agree the code is ready to be merged into the `staging` branch.\n\nWhen the code is ready, my teammate will approve the pull request and then click the button to squash and merge the commits. My teammate may also choose to delete the branch as it is no longer needed. \n\n![Screenshot of the option to delete the branch](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/deletebranch_ff52d9ee1c.png \"My teammate can delete the branch with the click of a button in the pull request\")\n\nDeleting the branch triggers the `delete-feature-branch-artifacts` workflow. This workflow is different from all of the workflows I will discuss in this article. This workflow's job is to delete the artifacts that were associated with the branch. \n\n![Screenshot of the GitHub Actions web interface after deleting a branch triggers a workflow](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/deletebuild_474562b75d.png \"The workflow to delete the feature branch artifacts is in progress\")\n\nThe [`delete-feature-branch-artifacts`](https://github.com/mongodb-developer/realm-demos/blob/main/.github/workflows/delete-feature-branch-artifacts.yml) workflow runs the following steps.\n\n1. **Set up job.** This step is created by GitHub Actions to prepare the workflow.\n2. **Install the Realm CLI and authenticate.** See the [section above](#install-realm-cli) for more information on this step.\n3. **Store the name of the branch.** This step retrieves the name of the branch that was just deleted and stores it in an environment variable named `FEATURE_BRANCH`.\n   ```\n   ref=$(echo ${{ github.event.ref }})\n   branch=$(echo \"${ref##*/}\")\n   echo \"FEATURE_BRANCH=$branch\" >> $GITHUB_ENV\n   ```\n\n4. **Delete the Realm App associated with the branch.** This step queries the `GitHubActionsMetadata` database for the ID of the Realm App associated with this branch. Then it deletes the Realm App, and deletes the information in the `GitHubActionsMetadata` database. Note: Accessing the Atlas database requires the IP address of the GitHub Actions virtual machine to be in the [Atlas IP Access List](https://docs.atlas.mongodb.com/security/ip-access-list/).\n\n   ```\n   # Get the Realm App associated with this branch\n   output=$(mongo \"mongodb+srv://${{ secrets.ATLAS_URI_FEATURE_BRANCHES }}/GitHubActionsMetadata\" --username ${{ secrets.ATLAS_USERNAME_FEATURE_BRANCHES }} --password ${{ secrets.ATLAS_PASSWORD_FEATURE_BRANCHES }} --eval \"db.metadata.findOne({'branch': '${{ env.FEATURE_BRANCH }}'})\")\n   \n   if [[ $output == *null ]]; then\n       echo \"No Realm App found for this branch\"\n   else\n       # Parse the output to retrieve the realm_app_id\n       app_id=$(echo $output | sed 's/^.*realm_app_id\" : \"\\([^\"]*\\).*/\\1/')\n              \n       # Delete the Realm App\n       echo \"A Realm App was found for this branch: $app_id. It will now be deleted\"\n       realm-cli app delete --app $app_id\n        \n       # Delete the record in the GitHubActionsMetadata database\n       output=$(mongo \"mongodb+srv://${{ secrets.ATLAS_URI_FEATURE_BRANCHES }}/GitHubActionsMetadata\" --username ${{ secrets.ATLAS_USERNAME_FEATURE_BRANCHES }} --password ${{ secrets.ATLAS_PASSWORD_FEATURE_BRANCHES }} --eval \"db.metadata.deleteOne({'branch': '${{ env.FEATURE_BRANCH }}'})\")\n   fi\n   ```\n\n5. **Delete the database associated with the branch.** This step deletes the database associated with the branch that was just deleted.\n\n   ```\n   mongo \"mongodb+srv://${{ secrets.ATLAS_URI_FEATURE_BRANCHES }}/InventoryDemo-${{ env.FEATURE_BRANCH }}\" --username ${{ secrets.ATLAS_USERNAME_FEATURE_BRANCHES }} --password ${{ secrets.ATLAS_PASSWORD_FEATURE_BRANCHES }} --eval \"db.dropDatabase()\"\n   ```\n\n6. **Complete job.** This step is created by GitHub Actions to complete the workflow.\n\n#### Staging\n\nAs part of the pull request process, my teammate merged my code change into the `staging` branch. I call this stage \"Staging,\" but teams have a variety of names for this stage. They might call it \"QA (Quality Assurance),\" \"Testing,\" \"Pre-Production,\" or something else entirely. This is the stage where teams simulate the production environment and make sure everything works together as intended.\n\nWhen my teammate merged my code change into the `staging` branch, GitHub kicked off another workflow. The purpose of this workflow is to deploy the code changes to the Staging environment and ensure everything continues to work as expected. \n\n![Screenshot of the GitHub Actions web interface after a push to the 'staging' branch triggers a workflow](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/stagingbuildinprogress_1e69bbb434.png \"The workflow that was kicked off when code was merged into the 'staging' branch is in progress\")\n\nThe [workflow](https://github.com/mongodb-developer/realm-demos/blob/5c3a1d3554527fe9fb2602dcbc84e2cc4d39acac/.github/workflows/build.yml) runs the following steps.\n\n1. **Set up job.** This step is created by GitHub Actions to prepare the workflow.\n2. **Run actions/checkout@v2.** Uses the [Checkout V2 Action](https://github.com/actions/checkout) to check out the repository so the workflow can access the code.\n3. **Store current time in variable.** See the [section above](#store-current-time) for more information on this step.\n4. **Is this a push to the Staging branch?** This step checks if the workflow was triggered by a push to the `staging` branch. If so, it stores the ID of the Staging Realm App in the `REALM_APP_ID` environment variable.\n   ```\n   echo \"REALM_APP_ID=inventorydemo-staging-zahjj\" >> $GITHUB_ENV\n   ```\n\n5. **Install the Realm CLI and authenticate.** See the [section above](#install-realm-cli) for more information on this step.\n6. <a name=\"push-updated-copy\">**Push updated copy of the Realm App for existing apps (Main, Staging, or Feature branches).** This step pushes an updated copy of the Realm App (stored in `inventory/export/sync`) for cases when the Realm App already exists.</a>\n   ```\n   cd inventory/export/sync\n   realm-cli push --remote=\"${{ env.REALM_APP_ID }}\" -y\n   ```\n7. **Create `realm-app-id.txt` that stores the Realm App ID.** See the [section above](#create-realm-app-id-txt) for more information on this step.\n8. **Build mobile app and run tests.** See the [section above](#build-mobile-app) for more information on this step.\n9. **Post Run actions/checkout@v2.** This cleanup step runs automatically when you use the [Checkout V2 Action](https://github.com/actions/checkout).\n10. **Complete job.** This step is created by GitHub Actions to complete the workflow.\n\nRealm has a new feature releasing soon that will allow you to roll back deployments. When this feature releases, I plan to add a step to the workflow above to automatically roll back the deployment to the previous one in the event of test failures.\n\n#### Moving from Staging to Production\n\nAt this point, some teams may choose to have their pipeline automation stop before automatically moving to production. They may want to run manual tests. Or they may want to intentionally limit their number of releases.\n\nI've chosen to move forward with continuous deployment in my pipeline. So, if the tests in Staging pass, the workflow above continues on to the [`pushToMainBranch`](https://github.com/mongodb-developer/realm-demos/blob/5c3a1d3554527fe9fb2602dcbc84e2cc4d39acac/.github/workflows/build.yml#L238) job that automatically pushes the latest commits to the `main` branch. The job runs the following steps:\n\n1. **Set up job.** This step is created by GitHub Actions to prepare the workflow.\n2. **Run actions/checkout@v2.** Uses the [Checkout V2 Action](https://github.com/actions/checkout) to check out all branches in the repository, so the workflow can access both the `main` and `staging` branches.\n3. **Push to the Main branch.** Merges the code from `staging` into `main`.\n   ```\n   git merge origin/staging\n   git push\n    ```\n4. **Post Run actions/checkout@v2.** This cleanup step runs automatically when you use the [Checkout V2 Action](https://github.com/actions/checkout).\n5. **Complete job.** This step is created by GitHub Actions to complete the workflow.\n\n#### Production\n\nNow my code is in the final stage: production. Production is where the end users get access to the application.\n\nWhen the previous workflow merged the code changes from the `staging` branch into the `main` branch, another workflow began. \n\n![Screenshot of the GitHub Actions web interface after a push to the 'main' branch triggers a workflow](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/prod_build_5eac510122.png \"The workflow that was kicked off when code was merged into the 'main' branch is in progress\")\n\nThe workflow runs the following steps.\n\n1. **Set up job.** This step is created by GitHub Actions to prepare the workflow.\n2. **Run actions/checkout@v2.** Uses the [Checkout V2 Action](https://github.com/actions/checkout) to check out the repository so the workflow can access the code.\n3. **Store current time in variable.** See the [section above](#store-current-time) for more information on this step.\n4. **Is this a push to the Main branch?** This step checks if the workflow was triggered by a push to the `main` branch. If so, it stores the ID of the Production Realm App in the `REALM_APP_ID` environment variable.\n   ```\n   echo \"REALM_APP_ID=inventorysync-ctnnu\" >> $GITHUB_ENV\n   ```\n5. **Install the Realm CLI and authenticate.** See the [section above](#install-realm-cli) for more information on this step.\n6. **Push updated copy of the Realm App for existing apps (Main, Staging, or Feature branches).** See the [section above](#push-updated-copy) for more information on this step.\n7. **Create `realm-app-id.txt` that stores the Realm App ID.** See the [section above](#create-realm-app-id-txt) for more information on this step.\n8. **Build mobile app and run tests.** See the [section above](#build-mobile-app) for more information on this step.\n9. **Install the Apple certificate and provisioning profile (so we can create the archive).** When the workflow is in the production stage, it does something that is unique to all of the other workflows: This workflow creates the [mobile app archive file](https://en.wikipedia.org/wiki/.ipa) (the `.ipa` file). In order to create the archive file, the Apple certificate and provisioning profile need to be installed. For more information on how the Apple certificate and provisioning profile are installed, see the [GitHub documentation](https://docs.github.com/en/actions/guides/installing-an-apple-certificate-on-macos-runners-for-xcode-development).\n10. **Archive the mobile app.** This step creates the mobile app archive file (the `.ipa` file).\n    ```\n    cd inventory/clients/ios-swiftui/InventoryDemo\n    xcodebuild -workspace InventoryDemo.xcodeproj/project.xcworkspace/ -scheme ci archive -archivePath $PWD/build/ci.xcarchive -allowProvisioningUpdates\n    xcodebuild -exportArchive -archivePath $PWD/build/ci.xcarchive -exportPath $PWD/build -exportOptionsPlist $PWD/build/ci.xcarchive/Info.plist\n    ```\n11. **Store the Archive in a GitHub Release.** This step uses the [gh-release action](https://github.com/softprops/action-gh-release) to store the mobile app archive in a [GitHub Release](https://github.com/mongodb-developer/realm-demos/releases) as shown in the screenshot below. ![Screenshot of a GitHub release that contains a mobile app archive](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/releases_d967ea181b.png)\n12. **Post Run actions/checkout@v2.** This cleanup step runs automatically when you use the [Checkout V2 Action](https://github.com/actions/checkout).\n13. **Complete job.** This step is created by GitHub Actions to complete the workflow.\n\nAs I described above, my pipeline creates a GitHub release and stores the `.ipa` file in the release. Another option would be to push the `.ipa` file to [TestFlight](https://developer.apple.com/testflight/) so you could send it to your users for beta testing. Or you could automatically upload the `.ipa` to the [App Store](https://developer.apple.com/ios/submit/) for Apple to review and approve for publication. You have the ability to customize your worfklow based on your team's process.\n\nThe nice thing about automating the deployment to production is that no one has to build the mobile app archive locally. You don't have to worry about that one person who knows how to build the archive going on vacation or leaving the company—everything is automated, so you can keep delivering new features to your users without the panic of what to do if a key person is out of the office.\n\n## Building Your Pipeline\n\nAs I wrap up this article, I want to help you get started building your pipeline.\n\n### Map Your Pipeline\n\nI encourage you to begin by working with key stakeholders to map your ideal pipeline. Ask questions like the following:\n\n* **What stages will be in the pipeline?** Do you have more stages than just Development, Staging, and Production?\n* **What automated tests should be run in the various stages of your pipeline?** Consider if you need to create more automated tests so that you feel confident in your releases.\n* **What should be the final output of your pipeline?** Is the result a fully automated pipeline that pushes changes automatically to the App Store? Or do you want to do some steps manually?\n\n### Implement Your Pipeline\n\nOnce you've mapped out your pipeline and figured out what your steps should be, it's time to start implementing your pipeline. Starting from scratch can be challenging... but you don't have to start from scratch. Here are some resources you can use:\n\n1. The [**mongodb-developer/realm-demos GitHub repo**](https://github.com/mongodb-developer/realm-demos) contains the code I discussed today.\n    * The repo has example mobile app and sync code, so you can see how the app itself was implemented. Check out the [ios-swiftui](https://github.com/mongodb-developer/realm-demos/tree/main/inventory/clients/ios-swiftui) directory.\n    * The repo also has automated tests in it, so you can take a peak at those and see how my team wrote those. Check out the [InventoryDemoTests](https://github.com/mongodb-developer/realm-demos/tree/main/inventory/clients/ios-swiftui/InventoryDemo/InventoryDemoTests) and the [InventoryDemoUITests](https://github.com/mongodb-developer/realm-demos/tree/main/inventory/clients/ios-swiftui/InventoryDemo/InventoryDemoUITests) directories.\n    * The part I'm most excited about is the GitHub Actions Workflow: [build.yml](https://github.com/mongodb-developer/realm-demos/blob/main/.github/workflows/build.yml). This is where you can find all of the code for my pipeline automation. Even if you're not going to use GitHub Actions to implement your pipeline, this file can be helpful in showing how to execute the various steps from the command line. You can take those commands and use them in other CI/CD tools.\n    * The [delete-feature-branch-artifacts.yml](https://github.com/mongodb-developer/realm-demos/blob/main/.github/workflows/delete-feature-branch-artifacts.yml) workflow shows how to clean up artifacts whenever a feature branch is deleted.\n2. The [**MongoDB Realm documentation**](https://docs.mongodb.com/realm/) has a ton of great information and is really helpful in figuring out what you can do with the Realm CLI.\n3. The [**MongoDB Community**](https://community.mongodb.com) is the best place to ask questions as you are implementing your pipeline. If you want to show off your pipeline and share your knowledge, we'd love to hear that as well. I hope to see you there!\n\n## Summary\n\nYou've learned a lot about how to craft your own CI/CD pipeline in this article. Creating a CI/CD pipeline can seem like a daunting task. \n\n<div style=\"text-align: center\">\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/itshard_360adab29b.gif\" width=\"400\" alt=\"But it's so hard!\"/>\n</div>\n\nWith the resources I've given you in this article, you can create a CI/CD pipeline that is customized to your team's process.  \n\nAs Tom Haverford wisely said, \"Sometimes you gotta work a little so you can ball a lot.\" Once you put in the work of building a pipeline that works for you and your team, your app development can really fly, and you can feel confident in your releases. And that's a really big deal.\n\n<div style=\"text-align: center\">\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/bigdeal_fc038aaa03.gif\" width=\"400\" alt=\"That's a really big deal\"/>\n</div>\n","description":"Learn how to build CI/CD pipelines in GitHub Actions for apps built using MongoDB Realm.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt1929eac8bb9f9073/644c463847e9ccb5b3cd0921/realm-logo.jpeg?branch=prod","description":null}}]},"slug":"/build-ci-cd-pipelines-realm-apps-github-actions","title":"*How to Build CI/CD Pipelines for MongoDB Realm Apps Using GitHub Actions","original_publish_date":"2021-07-08T18:45:36.603Z","strapi_updated_at":"2022-08-26T18:40:12.562Z","expiry_date":"2022-06-03T12:08:47.153Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*GitHub Actions","calculated_slug":"/technologies/github-actions"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to build CI/CD pipelines in GitHub Actions for apps built using MongoDB Realm.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf8c1d8d29ac7df3a/644c4639d46f521a109d4a06/og-realm-logo.jpeg?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:34.550Z","publish_details":{"time":"2023-04-28T22:52:50.647Z"}}},{"calculated_slug":"/products/realm/realm-data-types","content":"## Introduction\n\nA key feature of Realm is you don’t have to think about converting data to/from JSON, or using ORMs. Just create your objects using the data types your language natively supports. We’re adding new supported types to all our SDKs, here is a refresher and a taste of the new supported types.\n\n## Swift: Already supported types\n\nThe complete reference of supported data types for iOS can be found [here](https://docs.mongodb.com/realm/sdk/ios/data-types/supported-property-types/).\n\n| Type Name | Code Sample |\n| --------- | ----------- |\n| [Bool](https://developer.apple.com/documentation/swift/bool)<br>A value type whose instances are either true or false. | `// Declaring as Required`<br>`@objc dynamic var value = false`<br><br>`// Declaring as Optional`<br>`let value = RealmProperty()` |\n| [Int](https://developer.apple.com/documentation/swift/int), Int8, Int16, Int32, Int64<br>A signed integer value type. | `// Declaring as Required`<br>`@objc dynamic var value = 0`<br><br>`// Declaring as Optional`<br>`let value = RealmProperty()` |\n| [Float](https://developer.apple.com/documentation/swift/float)<br>A single-precision, floating-point value type. | `// Declaring as Required` <br>`@objc dynamic var value: Float = 0.0`<br><br>`// Declaring as Optional` `let value = RealmProperty()` |\n| [Double](https://developer.apple.com/documentation/swift/double)<br>A double-precision, floating-point value type. | `// Declaring as Required`<br>`@objc dynamic var value: Double = 0.0`<br><br>`// Declaring as Optional`<br>`let value = RealmProperty()` |\n| [String](https://developer.apple.com/documentation/swift/string)<br>A Unicode string value that is a collection of characters. | `// Declaring as Required`<br>`@objc dynamic var value = \"\"`<br><br>`// Declaring as Optional`<br>`@objc dynamic var value: String? = nil` |\n| [Data](https://developer.apple.com/documentation/foundation/data)<br>A byte buffer in memory. | `// Declaring as Required`<br>`@objc dynamic var value = Data()`<br><br>`// Declaring as Optional`<br>`@objc dynamic var value: Data? = nil` |\n| [Date](https://developer.apple.com/documentation/foundation/date)<br>A specific point in time, independent of any calendar or time zone. | `// Declaring as Required`<br>`@objc dynamic var value = Date()`<br><br>`// Declaring as Optional`<br>`@objc dynamic var value: Date? = nil` |\n| [Decimal128](https://developer.apple.com/documentation/foundation/decimal)<br>A structure representing a base-10 number. | `// Declaring as Required`<br>`@objc dynamic var decimal: Decimal128 = 0`<br><br>`// Declaring as Optional`<br>`@objc dynamic var decimal: Decimal128? = nil` |\n| [List](https://docs.mongodb.com/realm-sdks/swift/latest/Classes/List.html)<br>List is the container type in Realm used to define to-many relationships. | `let value = List()` |\n| [ObjectId](https://docs.mongodb.com/realm-sdks/swift/latest/Classes/ObjectId.html)<br>A 12-byte (probably) unique object identifier. Compatible with the ObjectId type used in the MongoDB database. | `// Declaring as Required`<br>`@objc dynamic var objectId = ObjectId.generate()`<br><br>`// Declaring as Optional`<br>`@objc dynamic var objectId: ObjectId? = nil` |\n| User-defined Object Your own classes. | `// Declaring as Optional`<br>`@objc dynamic var value: MyClass? = nil` |\n\n<br/><br/>\n\n## Swift: New Realm Supported Data Types\n\nStarting with **Realm iOS 10.8.0**\n\n| Type Name | Code Sample |\n| --------- | ----------- |\n| [Maps](https://docs.mongodb.com/realm/sdk/ios/data-types/collections/#map) <br>Store data in arbitrary key-value pairs. They’re used when a developer wants to add flexibility to data models that may evolve over time, or handle unstructured data from a remote endpoint. | `class Player: Object {` <br>&emsp;`@objc dynamic var name = String?`<br>&emsp;`@objc dynamic var email: String?`<br>&emsp;`@objc dynamic var playerHandle: String?`<br>&emsp;`let gameplayStats = Map()`<br>&emsp;`let competitionStats = Map()`<br>`}`<br>`try! realm.write {`<br>&emsp;`let player = Player()`<br>&emsp;`player.name = \"iDubs\"`<br><br>&emsp;`// get the RealmDictionary field from the object we just created and add stats`<br>&emsp;`let statsDictionary = player.gameplayStats`<br>&emsp;`statsDictioanry[\"mostCommonRole\"] = \"Medic\"`<br>&emsp;`statsDictioanry[\"clan\"] = \"Realmers\"`<br>&emsp;`statsDictioanry[\"favoriteMap\"] = \"Scorpian bay\"`<br>&emsp;`statsDictioanry[\"tagLine\"] = \"Always Be Healin\"`<br>&emsp;`statsDictioanry[\"nemesisHandle\"] = \"snakeCase4Life\"`<br>&emsp;`let competitionStats = player.comeptitionStats`<br><br>&emsp;`competitionStats[\"EastCoastInvitational\"] = \"2nd Place\"`<br>&emsp;`competitionStats[\"TransAtlanticOpen\"] = \"4th Place\"`<br>`}` |\n| [MutableSet](https://docs.mongodb.com/realm-sdks/swift/10.8.0-beta.0/Classes/MutableSet.html) <br>MutableSet is the container type in Realm used to define to-many relationships with distinct values as objects. | `// MutableSet declaring as required` <br>`let value = MutableSet()`<br><br>`// Declaring as Optional`<br>`let value: MutableSet? = nil `|\n| [AnyRealmValue](https://docs.mongodb.com/realm/sdk/ios/data-types/supported-property-types/#mixed-data-type) <br>AnyRealmValue is a Realm property type that can hold different data types. | `// Declaring as Required` <br>`let value = RealmProperty()`<br><br>`// Declaring as Optional`<br>`let value: RealmProperty? = nil` |\n| [UUID](https://docs.mongodb.com/realm/sdk/ios/data-types/supported-property-types/#unique-identifiers) <br>UUID is a 16-byte globally-unique value. | `// Declaring as Required` <br>`@objc dynamic var uuid = UUID()`<br><br>`// Declaring as Optional`<br>`@objc dynamic var uuidOpt: UUID? = nil` |\n\n<br/><br/>\n\n## Android/Kotlin: Already supported types\n\nYou can use these types in your RealmObject subclasses. The complete reference of supported data types for Kotlin can be found [here](https://docs.mongodb.com/realm/sdk/android/data-types/field-types/).\n\n| Type Name | Code Sample |\n| --------- | ----------- |\n| [Boolean](https://kotlinlang.org/docs/basic-types.html#booleans) or boolean<br>Represents boolean objects that can have two values: true and false. | `// Declaring as Required` <br>`var visited = false`<br><br>`// Declaring as Optional`<br>`var visited = false` |\n| [Integer](https://kotlinlang.org/docs/basic-types.html#integer-types) or int<br>A 32-bit signed number. | `// Declaring as Required` <br>`var number: Int = 0`<br><br>`// Declaring as Optional`<br>`var number: Int? = 0` |\n| [Short](https://kotlinlang.org/docs/basic-types.html#integer-types) or short<br>A 16-bit signed number. | `// Declaring as Required` <br>`var number: Short = 0`<br><br>`// Declaring as Optional`<br>`var number: Short? = 0` |\n| [Long](https://kotlinlang.org/docs/basic-types.html#integer-types) or long<br>A 64-bit signed number. | `// Declaring as Required` <br>`var number: Long = 0`<br><br>`// Declaring as Optional`<br>`var number: Long? = 0` |\n| [Byte](https://kotlinlang.org/docs/basic-types.html#integer-types) or byte[]<br>A 8-bit signed number. | `// Declaring as Required` <br>`var number: Byte = 0`<br><br>`// Declaring as Optional`<br>`var number: Byte? = 0` |\n| [Double](https://kotlinlang.org/docs/basic-types.html#floating-point-types) or double<br>Floating point number(IEEE 754 double precision) | `// Declaring as Required` <br>`var number: Double = 0`<br><br>`// Declaring as Optional`<br>`var number: Double? = 0.0` |\n| [Float](https://kotlinlang.org/docs/basic-types.html#floating-point-types) or float<br>Floating point number(IEEE 754 single precision) | `// Declaring as Required` <br>`var number: Float = 0`<br><br>`// Declaring as Optional`<br>`var number: Float? = 0.0` |\n| [String](https://kotlinlang.org/docs/basic-types.html#strings) | `// Declaring as Required` <br>`var sdkName: String = \"Realm\"`<br><br>`// Declaring as Optional`<br>`var sdkName: String? = \"Realm\"` |\n| [Date](https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.js/-date/) | `// Declaring as Required` <br>`var visited: Date = Date()`<br><br>`// Declaring as Optional`<br>`var visited: Date? = null` |\n| [Decimal128](https://mongodb.github.io/mongo-java-driver/3.9/javadoc/org/bson/types/Decimal128.html) from org.bson.types<br>A binary integer decimal representation of a 128-bit decimal value | `var number: Decimal128 = Decimal128.POSITIVE_INFINITY` |\n| [ObjectId](http://mongodb.github.io/mongo-java-driver/3.4/javadoc/org/bson/types/ObjectId.html) from org.bson.types<br>A globally unique identifier for objects. | `var oId = ObjectId()` |\n| Any RealmObject subclass | `// Define an embedded object` <br>`@RealmClass(embedded = true)`<br>`open class Address(`<br>&emsp;`var street: String? = null,`<br>&emsp;`var city: String? = null,`<br>&emsp;`var country: String? = null,`<br>&emsp;`var postalCode: String? = null`<br>`): RealmObject() {}`<br>`// Define an object containing one embedded object`<br>`open class Contact(_name: String = \"\", _address: Address? = null) : RealmObject() {`<br>&emsp;`@PrimaryKey var _id: ObjectId = ObjectId()`<br>&emsp;`var name: String = _name`<br><br>&emsp;`// Embed a single object.`<br>&emsp;`// Embedded object properties must be marked optional`<br>&emsp;`var address: Address? = _address`<br>`}` |\n| [RealmList](https://docs.mongodb.com/realm/sdk/android/data-types/collections/) <br>RealmList is used to model one-to-many relationships in a RealmObject. | `var favoriteColors : RealmList? = null` |\n\n<br/><br/>\n\n## Android/Kotlin: New Realm Supported Data Types\nStarting with **Realm Android 10.6.0**\n\n| Type Name | Code Sample |\n| --------- | ----------- |\n| [RealmDictionary](https://docs.mongodb.com/realm/sdk/android/data-types/realmdictionary/) <br>Manages a collection of unique String keys paired with values. | `import io.realm.RealmDictionary` <br>`import io.realm.RealmObject`<br><br>`open class Frog: RealmObject() {`<br>&emsp;`var name: String? = null`<br>&emsp;`var nicknamesToFriends: RealmDictionary = RealmDictionary()`<br>`}` |\n| [RealmSet](https://docs.mongodb.com/realm/sdk/android/data-types/realmset/) <br>You can use the RealmSet data type to manage a collection of unique keys. | `import io.realm.RealmObject` <br>`import io.realm.RealmSet`<br><br>`open class Frog: RealmObject() {`<br>&emsp;`var name: String = \"\"`<br>&emsp;`var favoriteSnacks: RealmSet = RealmSet();`<br>`}` |\n| Mixed<br>[RealmAny](https://docs.mongodb.com/realm/sdk/android/data-types/realmany/)<br>You can use the RealmAny data type to create Realm object fields that can contain any of several underlying types. | `import io.realm.RealmAny` <br>`import io.realm.RealmObject`<br><br>`open class Frog(var bestFriend: RealmAny? = RealmAny.nullValue()) : RealmObject() {`<br>&emsp;`var name: String? = null`<br>&emsp;`open fun bestFriendToString(): String {`<br>&emsp;&emsp;`if (bestFriend == null) {`<br>&emsp;&emsp;&emsp;`return \"null\"`<br>&emsp;&emsp;`}`<br>&emsp;&emsp;`return when (bestFriend!!.type) {`<br>&emsp;&emsp;&emsp;`RealmAny.Type.NULL -> {`<br>&emsp;&emsp;&emsp;&emsp;`\"no best friend\"`<br>&emsp;&emsp;&emsp;`}`<br>&emsp;&emsp;&emsp;`RealmAny.Type.STRING -> {`<br>&emsp;&emsp;&emsp;&emsp;`bestFriend!!.asString()`<br>&emsp;&emsp;&emsp;`}`<br>&emsp;&emsp;&emsp;`RealmAny.Type.OBJECT -> {`<br>&emsp;&emsp;&emsp;&emsp;`if (bestFriend!!.valueClass == Person::class.java) {`<br>&emsp;&emsp;&emsp;&emsp;&emsp;`val person = bestFriend!!.asRealmModel(Person::class.java)`<br>&emsp;&emsp;&emsp;&emsp;&emsp;`person.name`<br>&emsp;&emsp;&emsp;&emsp;`}`<br>&emsp;&emsp;&emsp;&emsp;`\"unknown type\"`<br>&emsp;&emsp;&emsp;`}`<br>&emsp;&emsp;&emsp;`else -> {`<br>&emsp;&emsp;&emsp;&emsp;`\"unknown type\"`<br>&emsp;&emsp;&emsp;`}`<br>&emsp;&emsp;`}`<br>&emsp;`}`<br>`}` |\n| [UUID](https://docs.oracle.com/javase/8/docs/api/java/util/class-use/UUID.html) from java.util.UUID | `var id = UUID.randomUUID()` |\n\n<br/><br/>\n\n## JavaScript - React Native SDK: : Already supported types\n\nThe complete reference of supported data types for JavaScript Node.js can be found [here](https://docs.mongodb.com/realm/sdk/node/data-types/field-types/).\n\n| Type Name | Code Sample |\n| --------- | ----------- |\n| `bool` maps to the JavaScript [Boolean](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Boolean) type | `var x = new Boolean(false);` |\n| `int` maps to the JavaScript [Number](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number) type. Internally, Realm Database stores int with 64 bits. | `Number('123')` |\n| `float` maps to the JavaScript [Number](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number) type. Internally, Realm Database stores float with 32 bits. | `Number('123.0')` |\n| `double` maps to the JavaScript [Number](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number) type. Internally, Realm Database stores double with 64 bits. | `Number('123.0')` |\n| `string` `maps` to the JavaScript [String](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Boolean) type. | `const string1 = \"A string primitive\";` |\n| `decimal128` for high precision numbers. |  |\n| `objectId` maps to BSON [`ObjectId`](https://docs.mongodb.com/manual/reference/method/ObjectId/) type. | `ObjectId(\"507f1f77bcf86cd799439011\")` |\n| `data` maps to the JavaScript [ArrayBuffer](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/ArrayBuffer) type. | `const buffer = new ArrayBuffer(8);` |\n| `date` maps to the JavaScript [Date](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date) type. | `new Date()` |\n| `list` maps to the JavaScript [Array](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array) type. You can also specify that a field contains a list of primitive value type by appending [] to the type name. | `let fruits = ['Apple', 'Banana']` |\n| `linkingObjects` is a special type used to define an inverse relationship. |  |\n\n<br/><br/>\n\n## JavaScript - React Native SDK: New Realm supported types\n\nStarting with __Realm JS 10.5.0__\n\n| Type Name | Code Sample |\n| --------- | ----------- |\n| [dictionary](https://docs.mongodb.com/realm/sdk/react-native/data-types/dictionaries/) used to manage a collection of unique String keys paired with values. | `let johnDoe;` <br>`let janeSmith;`<br>`realm.write(() => {`<br>&emsp;`johnDoe = realm.create(\"Person\", {`<br>&emsp;&emsp;`name: \"John Doe\",`<br>&emsp;&emsp;`home: {`<br>&emsp;&emsp;&emsp;`windows: 5,`<br>&emsp;&emsp;&emsp;`doors: 3,`<br>&emsp;&emsp;&emsp;`color: \"red\",`<br>&emsp;&emsp;&emsp;`address: \"Summerhill St.\",`<br>&emsp;&emsp;&emsp;`price: 400123,`<br>&emsp;&emsp;`},`<br>&emsp;`});`<br>&emsp;`janeSmith = realm.create(\"Person\", {`<br>&emsp;&emsp;`name: \"Jane Smith\",`<br>&emsp;&emsp;`home: {`<br>&emsp;&emsp;&emsp;`address: \"100 northroad st.\",`<br>&emsp;&emsp;&emsp;`yearBuilt: 1990,`<br>&emsp;&emsp;`},`<br>&emsp;`});`<br>`});` |\n| [set](https://docs.mongodb.com/realm/sdk/react-native/data-types/sets/) is based on the JavaScript Set type.<br>A Realm Set is a special object that allows you to store a collection of unique values. Realm Sets are based on JavaScript sets, but can only contain values of a single type and can only be modified within a write transaction. | `let characterOne, characterTwo;` <br>`realm.write(() => {`<br>&emsp;`characterOne = realm.create(\"Character\", {`<br>&emsp;&emsp;`_id: new BSON.ObjectId(),`<br>&emsp;&emsp;`name: \"CharacterOne\",`<br>&emsp;&emsp;`inventory: [\"elixir\", \"compass\", \"glowing shield\"],`<br>&emsp;&emsp;`levelsCompleted: [4, 9],`<br>&emsp;`});`<br>`characterTwo = realm.create(\"Character\", {`<br>&emsp;&emsp;`_id: new BSON.ObjectId(),`<br>&emsp;`name: \"CharacterTwo\",`<br>&emsp;&emsp;`inventory: [\"estus flask\", \"gloves\", \"rune\"],`<br>&emsp;&emsp;`levelsCompleted: [1, 2, 5, 24],`<br>&emsp;`});`<br>`});` |\n| [mixed](https://docs.mongodb.com/realm/sdk/react-native/data-types/mixed/) is a property type that can hold different data types.<br>The mixed data type is a realm property type that can hold any valid Realm data type except a collection. You can create collections (lists, sets, and dictionaries) of type mixed, but a mixed itself cannot be a collection. Properties using the mixed data type can also hold null values. | `realm.write(() => {` <br>&emsp;`// create a Dog with a birthDate value of type string`<br>&emsp;`realm.create(\"Dog\", { name: \"Euler\", birthDate: \"December 25th, 2017\" });`<br>&emsp;`// create a Dog with a birthDate value of type date`<br>`realm.create(\"Dog\", {`<br>&emsp;&emsp;`name: \"Blaise\",`<br>&emsp;&emsp;`birthDate: new Date(\"August 17, 2020\"),`<br>&emsp;`});`<br>&emsp;`// create a Dog with a birthDate value of type int`<br>&emsp;`realm.create(\"Dog\", {`<br>&emsp;&emsp;`name: \"Euclid\",`<br>&emsp;&emsp;`birthDate: 10152021,`<br>&emsp;`});`<br>&emsp;`// create a Dog with a birthDate value of type null`<br>&emsp;&emsp;`realm.create(\"Dog\", {`<br>&emsp;&emsp;`name: \"Pythagoras\",`<br>&emsp;&emsp;`birthDate: null,`<br>&emsp;`});`<br>`});` |\n| [uuid](https://docs.mongodb.com/realm/sdk/react-native/data-types/uuid/) is a universally unique identifier from Realm.BSON.<br>UUID (Universal Unique Identifier) is a 16-byte unique value. You can use UUID as an identifier for objects. UUID is indexable and you can use it as a primary key. | `const { UUID } = Realm.BSON;` <br>`const ProfileSchema = {`<br>&emsp;`name: \"Profile\",`<br>&emsp;`primaryKey: \"_id\",`<br>&emsp;`properties: {`<br>&emsp;&emsp;`_id: \"uuid\",`<br>&emsp;&emsp;`name: \"string\",`<br>&emsp;`},`<br>`};`<br>`const realm = await Realm.open({`<br>&emsp;`schema: [ProfileSchema],`<br>`});`<br>`realm.write(() => {`<br>&emsp;`realm.create(\"Profile\", {`<br>&emsp;&emsp;`name: \"John Doe.\",`<br>&emsp;&emsp;`_id: new UUID(), // create a _id with a randomly generated UUID`<br>&emsp;`});`<br>`realm.create(\"Profile\", {`<br>&emsp;&emsp;`name: \"Tim Doe.\",`<br>&emsp;&emsp;`_id: new UUID(\"882dd631-bc6e-4e0e-a9e8-f07b685fec8c\"), // create a _id with a specific UUID value`<br>&emsp;`});`<br>`});` |\n\n<br/><br/>\n\n## .NET Field Types \n The complete reference of supported data types for .Net/C# can be found [here](https://docs.mongodb.com/realm/sdk/dotnet/data-types/field-types/).\n\n| Type Name | Code Sample |\n| --- | --- |\n| Realm Database supports the following .NET data types and their nullable counterparts:<br>bool<br>byte<br>short<br>int<br>long<br>float<br>double<br>decimal<br>char<br>string<br>byte[]<br>DateTimeOffset<br>Guid<br>IList, where T is any of the supported data types | Regular C# code, nothing special to see here! |\n| ObjectId maps to [BSON](https://docs.mongodb.com/manual/reference/bson-types/) [`ObjectId`](https://docs.mongodb.com/manual/reference/method/ObjectId/) type. |  |\n\n<br/><br/>\n\n## .Net Field Types: New supported types\n\nStarting with __.NET SDK 10.2.0__\n\n| Type Name | Code Sample |\n| --------- | ----------- |\n| [Dictionary](https://docs.mongodb.com/realm/sdk/dotnet/data-types/dictionaries/) <br>A Realm dictionary is an implementation of IDictionary that has keys of type String and supports values of any Realm type except collections. To define a dictionary, use a getter-only IDictionary property, where TValue is any of the supported types. | `public class Inventory : RealmObject` <br>`{`<br>&emsp;`// The key must be of type string; the value can be`<br>&emsp;`// of any Realm-supported type, including objects`<br>&emsp;`// that inherit from RealmObject or EmbeddedObject`<br>&emsp;`public IDictionary PlantDict { get; }`<br>&emsp;`public IDictionary BooleansDict { get; }`<br>&emsp;`// Nullable types are supported in local-only`<br>&emsp;`// Realms, but not with Sync`<br>&emsp;`public IDictionary NullableIntDict { get; }`<br>&emsp;`// For C# types that are implicitly nullable, you can`<br>&emsp;`// use the [Required] attribute to prevent storing null values`<br>&emsp;`[Required]`<br>&emsp;`public IDictionary RequiredStringsDict { get; }`<br>`}` |\n| [Sets](https://docs.mongodb.com/realm/sdk/dotnet/data-types/sets/) <br>A Realm set, like the C# [HashSet<>](https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.hashset-1?view=net-5.0), is an implementation of [ICollection<>](https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.icollection-1?view=net-5.0) and [IEnumerable<>](https://docs.microsoft.com/en-us/dotnet/api/system.collections.generic.ienumerable-1?view=net-5.0). It supports values of any [Realm type](https://docs.mongodb.com/realm/sdk/dotnet/fundamentals/object-models-and-schemas/#std-label-dotnet-objects) except collections. To define a set, use a getter-only ISet property, where TValue is any of the supported types. | `public class Inventory : RealmObject` <br>`{`<br>&emsp;`// A Set can contain any Realm-supported type, including`<br>&emsp;`// objects that inherit from RealmObject or EmbeddedObject`<br>&emsp;`public ISet PlantSet { get; }<br>public ISet DoubleSet { get; }`<br>&emsp;`// Nullable types are supported in local-only`<br>&emsp;`// Realms, but not with Sync`<br>&emsp;`public ISet NullableIntsSet { get; }`<br>&emsp;`// For C# types that are implicitly nullable, you can`<br>&emsp;`// use the [Required] attribute to prevent storing null values`<br>&emsp;`[Required]`<br>&emsp;`public ISet RequiredStrings { get; }`<br>`}` |\n| [RealmValue](https://docs.mongodb.com/realm/sdk/dotnet/data-types/field-types/#realmvalue--beta-) <br>The RealmValue data type is a mixed data type, and can represent any other valid Realm data type except a collection. You can create collections (lists, sets and dictionaries) of type RealmValue, but a RealmValue itself cannot be a collection. | `public class MyRealmValueObject : RealmObject` <br>`{`<br>&emsp;`[PrimaryKey]`<br>&emsp;`[MapTo(\"_id\")]`<br>&emsp;`public Guid Id { get; set; }`<br>&emsp;`public RealmValue MyValue { get; set; }`<br>&emsp;`// A nullable RealmValue preoprtrty is *not supported*`<br>&emsp;`// public RealmValue? NullableRealmValueNotAllowed { get; set; }`<br>`}`<br>`private void TestRealmValue()`<br>`{`<br>&emsp;`var obj = new MyRealmValueObject();`<br>&emsp;`// set the value to null:`<br>&emsp;`obj.MyValue = RealmValue.Null;`<br>&emsp;`// or an int...`<br>&emsp;`obj.MyValue = 1;`<br>&emsp;`// or a string...`<br>&emsp;`obj.MyValue = \"abc\";`<br>&emsp;`// Use RealmValueType to check the type:`<br>&emsp;`if (obj.MyValue.Type == RealmValueType.String)`<br>&emsp;`{`<br>&emsp;&emsp;`var myString = obj.MyValue.AsString();`<br>&emsp;`}`<br>`}` |\n| [Guid and ObjectId Properties](https://docs.mongodb.com/realm/sdk/dotnet/data-types/field-types/#guid-and-objectid-properties) <br>MongoDB.Bson.ObjectId is a MongoDB-specific 12-byte unique value, while the built-in .NET type Guid is a 16-byte universally-unique value. Both types are indexable, and either can be used as a [Primary Key](https://docs.mongodb.com/realm/sdk/dotnet/fundamentals/object-models-and-schemas/#primary-key). |  |","description":"Review of existing and supported Realm Data Types for the different SDKs.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt788d5b71223935b3/644c4643bc3f0a0307815104/realm-logo.jpg?branch=prod","description":null}}]},"slug":"/realm-data-types","title":"*Realm Data Types","original_publish_date":"2021-06-14T12:03:49.608Z","strapi_updated_at":"2022-05-09T19:20:03.778Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*SDK","calculated_slug":"/products/realm/sdk"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:34.162Z","publish_details":{"time":"2023-04-28T22:52:50.769Z"}}},{"calculated_slug":"/products/atlas/how-to-use-custom-archival-rules-and-partitioning-on-mongodb-atlas-online-archive","content":">As of June 2022, the functionality previously known as Atlas Data Lake is now named Atlas Data Federation. Atlas Data Federation’s functionality is unchanged and you can learn more about it [here](http://mongodb.com/atlas/data-federation). Atlas Data Lake will remain in the Atlas Platform, with newly introduced functionality that you can learn about [here](http://mongodb.com/atlas/data-lake).\n\nOkay, so you've set up a simple MongoDB Atlas Online Archive, and now you might be wondering, \"What's next?\" In this post, we will cover some more advanced Online Archive use cases, including setting up custom archival rules and how to improve query performance through partitioning.\n\n## Prerequisites\n\n-   The Online Archive feature is available on [M10](https://docs.atlas.mongodb.com/cluster-tier/) and greater Atlas clusters that run MongoDB 3.6 or later. So for this demo, you will need to create a [M10 cluster](https://docs.atlas.mongodb.com/cluster-tier/) in MongoDB Atlas. [Click here for information on setting up a new MongoDB Atlas cluster](https://docs.atlas.mongodb.com/getting-started/) or check out [How to Manage Data at Scale With MongoDB Atlas Online Archive](https://developer.mongodb.com/how-to/manage-data-at-scale-with-online-archive/).\n\n-   Ensure that each database has been seeded by [loading sample data into our Atlas cluster](https://docs.atlas.mongodb.com/sample-data/). I will be using the `sample_analytics.customers` dataset for this demo.\n\n## Creating a Custom Archival Rule\n\nCreating an Online Archive rule based on the date makes sense for a lot of archiving situations, such as automatically archiving documents that are over X years old, or that were last updated Y months ago. But what if you want to have more control over what gets archived? Some examples of data that might be eligible to be archived are:\n\n-   Data that has been flagged for archival by an administrator.\n-   Discontinued products on your eCommerce site.\n-   User data from users that have closed their accounts on your platform (unless they are European citizens).\n-   Employee data from employees that no longer work at your company.\n\nThere are lots of reasons why you might want to set up custom rules for archiving your cold data. Let's dig into how you can achieve this using custom archive rules with MongoDB Atlas Online Archive. For this demo, we will be setting up an automatic archive of all users in the `sample_analytics.customers` collection that have the 'active' field set to `false`.\n\nIn order to configure our Online Archive, first navigate to the Cluster page for your project, click on the name of the cluster you want to configure Online Archive for, and click on the **Online Archive** tab.\n\nNext, click the Configure Online Archive button the first time and the **Add Archive** button subsequently to start configuring Online Archive for your collection. Then, you will need to create an Archiving Rule by specifying the collection namespace, which will be `sample_analytics.customers`.\n\nYou will also need to specify your custom criteria for archiving documents. You can specify the documents you would like to filter for archival with a MongoDB query, in JSON, the same way as you would write filters in MongoDB Atlas.\n\n> Note: You can use any valid MongoDB Query Language (MQL) query, however, you cannot use the empty document argument ({}) to return all documents.\n\nTo retrieve the documents staged for archival, we will use the following find command. This will retrieve all documents that have the \\`active\\` field set to \\`false\\` or do not have an \\`active\\` key at all.\n\n```\n{ $or: [\n    { active: false }, \n    { active: null }\n] }\n```\nContinue setting up your archive, and then you should be done!\n\n> Note: It's always a good idea to run your custom queries in the [mongo shell](https://docs.mongodb.com/mongodb-shell/install/) first to ensure that you are archiving the correct documents.\n\n> Note: Once you initiate an archive and a MongoDB document is queued for archiving, you can no longer edit the document.\n\n## Improving Query Performance Through Partitioning\n\nOne of the reasons we archive data is to access and query it in the future, if for some reason we still need to use it. In fact, you might be accessing this data quite frequently! That's why it's useful to be able to partition your archived data and speed up query times. With Atlas Online Archive, you can specify the two most frequently queried fields in your collection to create partitions in your online archive.\n\nFields with a moderate to high cardinality (or the number of elements in a set or grouping) are good choices to be used as a partition. Queries that don't contain these fields will require a full collection scan of all archived documents, which will take longer and increase your costs. However, it's a bit of a bit of a balancing act. \n\nFor example, fields with low cardinality wont partition the data well and therefore wont improve performance greatly. However, this may be OK for range queries or collection scans,  but will result in fast archival performance.\n\nFields with mid to high cardinality will partition the data better leading to better general query performance, but maybe slightly slower archival performance.\n\nFields with extremely high cardinality like `_id` will lead to poor query performance for everything but \"point queries\" that query on _id, and will lead to terrible archival performance due to writing many partitions.\n\n> Note: Online Archive is powered by MongoDB Atlas Data Lake. To learn more about how partitions improve your query performance in Data Lake, see [Data Structure in S3](https://docs.mongodb.com/datalake/admin/optimize-query-performance#data-structure-in-s3).\n\nThe specified fields are used to partition your archived data for optimal query performance. Partitions are similar to folders. You can move whichever field to the first position of the partition if you frequently query by that field.\n\nThe order of fields listed in the path is important in the same way as it is in [Compound Indexes](https://docs.mongodb.com/manual/core/index-compound/). Data in the specified path is partitioned first by the value of the first field, and then by the value of the next field, and so on. Atlas supports queries on the specified fields using the partitions.\n\nYou can specify the two most frequently queried fields in your collection and order them from the most frequently queried in the first position to the least queried field in the second position. For example, suppose you are configuring the online archive for your `customers` collection in the `sample_analytics` database. If your archived field is set to the custom archival rule in our example above, your first queried field is `username`, and your second queried field is `email`, your partition will look like the following:\n\n```\n/username/email\n```\n\nAtlas creates partitions first for the `username` field, followed by the `email`. Atlas uses the partitions for queries on the following fields:\n\n-   the `username` field\n-   the ` username` field and the `email` field\n\n> Note: The value of a partition field can be up to a maximum of 700 characters. Documents with values exceeding 700 characters are not archived.\n\n![Figure showing how Atlas Online Archive partitions data and how it uses it to query documents quickly.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Online_Archive_Partition_3466b335d6.png)\n\nFor more information on how to partition data in your Online Archive, please [refer to the documentation](https://docs.atlas.mongodb.com/online-archive/configure-online-archive/).\n\n## Summary\n\nIn this post, we covered some advanced use cases for Online Archive to help you take advantage of this MongoDB Atlas feature. We initialized a demo project to show you how to set up custom archival rules with Atlas Online Archive, as well as improve query performance through partitioning your archived data.\n\nIf you have questions, please head to our [developer community website](https://community.mongodb.com/) where the MongoDB engineers and the MongoDB community will help you build your next big idea with MongoDB.\n","description":"So you've set up a simple MongoDB Atlas Online Archive, and now you might be wondering, \"What's next?\" In this post, we will cover some more advanced Online Archive use cases, including setting up custom archival rules and how to improve query performance through partitioning.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt735a59f515c12ed3/644c464406be8e1e78f54a61/data-streaming.png?branch=prod","description":null}}]},"slug":"/how-to-use-custom-archival-rules-and-partitioning-on-mongodb-atlas-online-archive","title":"*How to Use Custom Archival Rules and Partitioning on MongoDB Atlas Online Archive","original_publish_date":"2021-06-08T20:40:57.165Z","strapi_updated_at":"2022-06-07T14:42:26.299Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Online Archive","calculated_slug":"/products/atlas/online-archive"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"n this post, we will cover custom archival rules and how to improve query performance.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltc418aead338c78e1/644c4645bebcd967cbab7039/og-green-pattern.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@joekarlsson1"},"system":{"updated_at":"2023-04-28T22:52:33.764Z","publish_details":{"time":"2023-04-28T22:52:50.808Z"}}},{"calculated_slug":"/products/atlas/mongodb-data-parquet","content":"For those of you not familiar with [Parquet](https://parquet.apache.org/), it’s an amazing file format that does a lot of the heavy lifting to ensure blazing fast query performance on data stored in files. This is a popular file format in the Data Warehouse and Data Lake space as well as for a variety of machine learning tasks.\n\nOne thing we frequently see users struggle with is getting NoSQL data into Parquet as it is a columnar format. Historically, you would have to write some custom code to get the data out of the database, transform it into an appropriate structure, and then probably utilize a third-party library to write it to Parquet. Fortunately, with MongoDB Atlas Data Federation's $out to S3, you can now convert MongoDB Data into Parquet with little effort.\n\nIn this blog post, I’m going to walk you through the steps necessary to write data from your Atlas Cluster directly to S3 in the Parquet format and then finish up by reviewing some things to keep in mind when using Parquet with NoSQL data. I’m going to use a sample data set that contains taxi ride data from New York City.\n\n## Prerequisites\n\nIn order to follow along with this tutorial yourself, you will need the following:\nAn Atlas cluster with some data in it. (It can be the sample data.)\nAn AWS account with privileges to create IAM Roles and S3 Buckets (to give us access to write data to your S3 bucket).\n\n## Create a Federated Database Instance and Connect to S3\n\nThe first thing you'll need to do is navigate to the \"Data Federation\" tab on the left hand side of your Atlas Dashboard and then click \"Create Federated Database Instance\" or \"Configure a New Federated Database Instance.\"\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screenshot_2022_12_19_124638_f1f05379d5.jpg)\n\nThen, you need to connect your S3 bucket to your Federated Database Instance. This is where we will write the Parquet files. The setup wizard should guide you through this pretty quickly but you will need access to your credentials for AWS. (Be sure to give Atlas Data Federation “Read and Write” access to the bucket so it can write the Parquet files there.)\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screenshot_2022_12_19_125037_440fb93c33.jpg)\n\nOnce you’ve connected your S3 bucket, we’re going to create a simple data source to query the data in S3 so we can verify we’ve written the data to S3 at the end of this tutorial. Our new setup tool makes it easier than ever to configure your Federated Database Instance to take advantage of the partitioning of data in S3. Partitioning allows us to only select the relevant data to process in order to satisfy your query. (I’ve put a sample file in there for this test that will fit how we’re going to partition the data by \\_cab\\_type).\n\n``` bash\nmongoimport --uri mongodb+srv://<USERNAME>:<PASSWORD>@<MONGODB_URI>/<DATABASE> --collection <COLLECTION> --type json --file <FILENAME>\n```\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screenshot_2022_12_20_164216_22544c3908.jpg)\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screenshot_2022_12_20_163429_6d7a53327c.jpg)\n\n## Connect Your Federated Database Instance to an Atlas Cluster\n\nNow we’re going to connect our Atlas cluster, so we can write data from it into the Parquet files. This involves picking the cluster from a list of clusters in your Atlas project and then selecting the databases and collections you’d like to create Data Sources from and dragging them into your Federated Database Instance.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screenshot_2022_12_20_164727_7f084fa42e.jpg)\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screenshot_2022_12_20_164926_350b9a7a74.jpg)\n\n## $out to S3 in Parquet\n\nNow we’re going to connect to our Federated Database Instance using the mongo shell and execute the following command. This is going to do quite a few things, so I’m going to explain the important ones.\n- First, you can use the ‘filename’ field of the $out stage to have your Federated Database Instance partition files by “_cab_type”, so all the green cabs will go in one set of files and all the yellow cabs will go in another.\n- Then in the format, we’re going to specify parquet and determine a maxFileSize and maxRowGroupSize.\n   -- maxFileSize is going to determine the maximum size each partition will be.\n   -- maxRowGroupSize is going to determine how records are grouped inside of the Parquet file in “row groups” which will impact performance querying your Parquet files, similarly to file size.\n- Lastly, we’re using a special Atlas Data Federation aggregation “background: true” which simply tells the Federated Database Instance to keep executing the query even if the client disconnects. (This is handy for long running queries or environments where your network connection is not stable.)\n\n``` js\ndb.getSiblingDB(\"clusterData\").getCollection(\"trips\").aggregate([\n    {\n        \"$out\" : {\n            \"s3\" : {\n                \"bucket\" : \"ben.flast\",\n                \"region\" : \"us-east-1\",\n                \"filename\" : {\n                    \"$concat\" : [\n                        \"taxi-trips/\",\n                        \"$_cab_type\",\n                        \"/\"\n                    ]\n                },\n                \"format\" : {\n                    \"name\" : \"parquet\",\n                    \"maxFileSize\" : \"10GB\",\n                    \"maxRowGroupSize\" : \"100MB\"\n                }\n            }\n        }\n    }\n], {\n    background: true\n})\n```\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/aws_s3_interface_f21ce56fa9.png)\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/aws_s3_yellow_cab_folder_55e21a7e89.png)\n\n## Blazing Fast Queries on Parquet Files\n\nNow, to give you some idea of the potential performance improvements for Object Store Data you can see, I’ve written three sets of data, each with 10 million documents: one in Parquet, one in uncompressed JSON, and another in compressed JSON. And I ran a count command on each of them with the following results.\n\n*db.trips.count()*\n10,000,000\n\n| Type | Data Size (GB) | Count Command Latency (Seconds) |\n| ---- | -------------- | ------------------------------- |\n| JSON (Uncompressed) | \\~16.1 | 297.182 |\n| JSON (Compressed) | \\~1.1 | 78.070 |\n| Parquet | \\~1.02 | 1.596 |\n\n## In Review\n\nSo, what have we done and what have we learned?\n- We saw how quickly and easily you can create a Federated Database Instance in MongoDB Atlas.\n- We connected an Atlas cluster to our Federated Database Instance.\n- We used our Federated Database Instance to write Atlas cluster data to S3 in Parquet format.\n- We demonstrated how fast and space-efficient Parquet is when compared to JSON.\n\n## A Couple of Things to Remember About Atlas Data Federation\n\n- Parquet is a super fast columnar format that can be read and written with Atlas Data Federation.\n- Atlas Data Federation takes advantage of various pieces of metadata contained in Parquet files, not just the maxRowGroupSize. For instance, if your first stage in an aggregation pipeline was $project: {fieldA: 1, filedB: 1}, we would only read the two columns from the Parquet file which results in faster performance and lower costs as we are scanning less data.\n- Atlas Data Federation writes Parquet files flexibly so if you have polymorphic data, we will create union columns so you can have ‘Column A - String’ and ‘Column A - Int’. Atlas Data Federation will read union columns back in as one field but other tools may not handle union types. So if you’re going to be using these Parquet files with other tools, you should transform your data before the $out stage to ensure no union columns.\n- Atlas Data Federation will also write files with different schemas if it encounters data with varying schemas throughout the aggregation. It can handle different schemas across files in one collection, but other tools may require a consistent schema across files. So if you’re going to be using these Parquet files with other tools, you should do a $project with $convert’s before the $out stage to ensure a consistent schema across generated files.\n- Parquet is a great format for your MongoDB data when you need to use columnar oriented tools like Tableau for visualizations or machine learning frameworks that use data frames. Parquet can be quickly and easily converted into Pandas data frames in Python.","description":"Learn how to transform MongoDB data to Parquet with Atlas Data Federation.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltd84b4dcad491c048/644c464927dc2b62ddcc9125/Screen_Shot_2021-05-17_at_10.39.26_AM.png?branch=prod","description":null}}]},"slug":"/mongodb-data-parquet","title":"*How to Get MongoDB Data into Parquet in 10 Seconds or Less","original_publish_date":"2021-06-11T16:40:21.893Z","strapi_updated_at":"2023-01-31T17:35:06.179Z","expiry_date":"2022-06-09T17:29:32.695Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Data Federation","calculated_slug":"/products/atlas/data-federation"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Parquet","calculated_slug":"/technologies/parquet"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to transform MongoDB data to Parquet with Atlas Data Federation.","og_description":"Learn how to transform MongoDB data to Parquet with Atlas Data Federation.","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:33.350Z","publish_details":{"time":"2023-04-28T22:52:50.837Z"}}},{"calculated_slug":"/products/mongodb/schema-suggestions-julia-oppenheim","content":"Today, we are joined by Julia Oppenheim, Associate Product Manager at MongoDB. Julia chats with us and shares details of a set of features within MongoDB Atlas designed to help developers improve the design of their schemas to avoid common anti-patterns. \n\nThe notion that MongoDB is schema-less is a bit of a misnomer. Traditional relational databases use a separate entity in the database that defines the schema - the structure of the tables/rows/columns and acceptable values that get stored in the database. MongoDB takes a slightly different approach. The schema does exist in MongoDB, but to see what that schema is - you typically look at the documents previously written to the database. With this in mind, you, as a developer have the power to make decisions about the structure of the documents you store in your database... and as they say with great power, comes great responsibility. \n\nMongoDB has created a set of features built into Atlas that enable you to see when your assumptions about the structure of your documents turn out to be less than optimal. These features come under the umbrella of Schema Suggestions and on today's podcast episode, Julia Oppenheim joins Nic Raboy and I to talk about how Schema Suggestions can help you maintain and improve the performance of your applications by exposing anti-patterns in your schema.\n\n:youtube[]{vid=zAvQMPu7XWk}\n\n**Julia: [00:00:00]** My name is Julia Oppenheim and welcome to the Mongo DB podcast. Stay tuned to learn more about how to improve your schema and alleviate schema anti-patterns with schema suggestions and Mongo DB Atlas.\n\n**Michael: [00:00:12]** And today we're talking with Julia Oppenheim. Welcome to the show, Julia, it's great to have you on the podcast. Thanks. It's great to be here. So why don't you introduce yourself to the audience? Let folks know who you are and what you do at Mongo DB. \n\n**Julia: [00:00:26]** Yeah. Sure. So  hi, I'm Julia. I actually joined Mongo DB about nine months ago as a product manager on Rez's team.\nSo yeah, I actually did know that you had spoken to him before. And if you listened to those episodes  Rez probably touched on what our team does, which is. Ensure that the customer's journey or the user's journey with Mongo DB runs smoothly and that their deployments are performance.   Making sure that, you know, developers can focus on what's truly exciting and interesting to them like pushing out new features and they don't have the stress of is my deployment is my database.\n You know, going to have any problems. We try to make that process as smooth as possible. \n \n**Michael: [00:01:10]** Fantastic. And today we're going to be focusing on schemas, right. Schema suggestions, and eliminating schema. Anti-patterns  so hold the phone, Mike. Yeah, yeah, go ahead, Nick. \n\n**Nic: [00:01:22]** I thought  I thought I'm going to be people call this the schema-less database.\n\n**Michael: [00:01:28]** Yeah, I guess that is, that is true.  With the document database, it's not necessary to plan your schema ahead of time. So maybe Julia, do you want to shed some light on why we need schema suggestions in the Mongo DB \n\n**Julia: [00:01:41]** Yeah, no, I think that's a really good point and definitely a common misconception.\nSo I think one of the draws of Mongo DB is that schema can be pretty flexible.  And it's not rigid in the sense that other more relational databases  you know, they have a strict set of rules and how you can access the data.  I'm going to be as definitely more lenient in that regard, but at the end of the day, you still.\n\nNeed certain fields value types and things like that dependent on the needs of your application. So  one of the first things that any developer will do is kind of map out what their use cases for their applications are and figure out how they should store the data to make sure that those use cases can be carried out.\n\n I think that you can kind of get a little stuck with schema in MongoDB, is that.  The needs of your application changed throughout the development cycle. So a schema that may work on day one when you're  you know, user base is relatively small, your feature set is pretty limited. May not work.  As your app, you get Cisco, you may need to refactor a little bit, and  it may not always be immediately obvious how to do that.\n \nAnd, you know, we don't expect users to be experts in MongoDB and schema design with Mongo DB  which is why I think. Highlighting schema anti-patterns is very useful. \n\n**Michael: [00:03:03]** Fantastic. So do you want to talk a little bit about how the product works?  How schema suggestions work in Mongo DB. Atlas? \n\n**Julia: [00:03:12]** Yeah. So there are two places where  you as a user can see  schema anti-patterns they're in.\nThe performance advisor tab a, which Rez definitely touched on if he talked about autopilot and  index suggestions, and you can also see schema anti-patterns in the  in our data Explorer. So the collections tab, and we can talk about  you know, in a little bit why we have them in two separate places, but in general  what you, as the user will see is the same.\n\nSo we. Flag schema anti-patterns  we give kind of like a brief explanation as to why we flagged them. We'll show, which  collections are impacted by  you know, this anti-pattern that we've identified and we'll also kind of give a call to action on how to address them. So we actually have custom docs on the six schema anti-patterns that we.\n\nLook for  at this stage of the products, you know, life cycle, and we give kind of steps on how to solve it, what our recommendation would be, and also kind of explain, you know, why it's a problem and how it can really  you know, come back to hurt you later on. \n\n**Nic: [00:04:29]** So you've thrown out the keyword schema.\n\nAnti-patterns a few times now, do you want to go over what you said? There are six of them, right? We want to go what  each of those six are. \n\n**Julia: [00:04:39]** Yeah, sure. So there are, like you said, there are six. So I think that we look for use of  Our dollar lookup operations. So this means that where it's very, very similar to  joining in the relational world where you would be accessing data across different collections.\nAnd this is not always ideal because you're reading and performing, you know, different logic on more than one collection. So in general, it just takes a lot of time  a little more resource intensive and. You know, when we see this, we're kind of thinking, oh, this person might come from a more relational background.\n\n That's not to say that this is always a problem. It could make sense to do this in certain cases.  Which is where things get a little dicier, but that's the first one that we look for.  The, another one is looking for unbounded arrays. So if you just keep. Embedding information and have no limit on that.\n \nThe size of your documents can get really, really big.  This, we actually have a limit in place and this is one of our third  anti-patterns where if you keep you'll hit our 16 megabyte per document limit  which kind of means that. Your hottest documents are the working set, takes up too much space on RAM.\n\nSo now we're going to disk to fulfill your request, which is, you know, generally  again, we'll take some time it's more resource  you know, consumptive, things like that. \n\n**Nic: [00:06:15]** This might be out of scope, but  how do you prevent an unbounded array in Mongo DB? Like. I get the concept, but I've never, I've never heard of it done in a database before, so this would be new to me.\n\n**Julia: [00:06:27]** So this is going to be a little contradictory to the lookup anti-pattern that I just mentioned, and I think that we can talk about this more. Cause I know that when I was first learning about anti-patterns and  they did seem very contradictory to me and I got of stressed.  So we'll talk about that in a little bit, but the way you would avoid.\n\nThe unbounded array would probably be to  reference other documents. So that's essentially doing the look of that. I just said was an anti-pattern, but  one way to think of it is say you have, okay, so you have a developer collection and you have different information about the developer, like their team at Mongo DB.\n\nYou know how long they've been here and maybe you have all of their get commits and like they get commit. It could be an embedded document. It could have like the date of the commit and what project it was on and things like that.  A developer can have, you know, infinitely many commits, like maybe they just commit a lot and there was no bound on that.\n\nSo  you know, it's a one to many relationship and. If that were in an array, I think we all see that that would grow  probably would hit that 16 megabyte limit. What we would instead maybe want to consider doing is creating like a commit collection where we would then tie it back to the developer who made the commit and reference it from the original developer document.\n\n I don't know if that analogy was helpful, but that's, that's kind of how you would handle that. \n \n**Michael: [00:08:04]** And I think the  the key thing here is, you know, you get to make these decisions about how you design  your schema. You're not forced to normalize data in one way across the entire database, as you are in the relational world.\n\n And so you're going to make a decision about  the number of elements in a potential array versus the cost of storing that data in separate collections and doing a lookup. And. Obviously, you know, you may start, you may embark on your journey to develop an application, thinking that your arrays are going to be within scope within a relative, relatively low number.\n \nAnd maybe the use pattern changes or the number of users changes the number of developers using your application changes. And at some point you may need to change that. So let me ask the question about the.  The user case when I'm interacting with Mongo DB Atlas, and my use case does change. My  user pattern does change.\n\nHow will that appear? How will it surface in the product that now  I've breached the limits of what is an acceptable  pattern. And now it's, I'm in the scope of an anti-pattern. \n\n**Julia: [00:09:16]** Right. So when that happens, the best place for it to be flagged is our performance advisor tab. So we'll have, we have a little card that says improve your schema.\nAnd if we have anti-patterns that we flagged  we'll show the number of suggestions there. You can click it to learn more about them.  And what we do there is it's based on. A sample of your data. So we kind of try to catch these in a reactive sense. We'll see that something is going on  and we'll give you a suggestion to improve it.\n\nSo to do that, we like analyze your data. We try to determine which  collections matter, which collections you're really using.  So based on the number of reads and writes to the collections, we'll kind of identify your top 20 collections and then. We'll see what's going on. We'll look for, you know, the edgy pattern, some of which I've mentioned and kind of just collect, this is all going on behind the scenes, by the way, we'll kind of collect  you know, distributions of, you know, average data size, our look ups happening  you know, just looking for some of those anti-patterns that I've mentioned, and then we'll determine which ones.\nYou can actually fix and  which ones are most impactful, which ones are actually a problem. And then we surface that to the user. \n\n**Nic: [00:10:35]** So is it monitoring what type of queries you're doing or is it just looking at, based on how  your documents are structured when it's suggesting a schema? \n\n**Julia: [00:10:46]** Yeah. It's mainly looking for how your documents are structured.\n The dollar lookup is a little tricky because it is, you know, an operation that's kind of happening under the hood, but it's based on the fact that you're referencing things within the document.\n \n**Michael: [00:11:00]** Okay. So we talked about the unbounded arrays.  We talked about  three anti-patterns so far. Do you want to continue on the journey  of anti-patterns? \n\n**Julia: [00:11:10]** Okay. Yeah. Yeah, no, definitely.  So one that we also flag is at the index level, and this is something that is also available in porphyry performance advisor in general.\n\nSo  if you have unnecessary indexes on the collection, that's something that is problematic because an index just existing is  you know, it consumes resources, it takes up space and. It can slow down, writes, even though it does slow down  speed up reads. So  that's like for indexes in general, but then there's the case where the index isn't actually doing anything and it may be kind of stale.\n\nMaybe your query patterns have changed and things like that.  So if you have excessive indexes on your collection, we'll flag that, but I will say in performance advisor  we do now have index removal recommendations that. We'll say this is the actual index that you should remove. So a little more granular  which is nice.\n\nThen another one we have is reducing the number of collections you have in general. So  at a certain point, collections again, consume a lot of resources. You have indexes on the collections. You have a lot of documents.  Maybe you're referencing things that could be embedded. So  that's just kind of another sign that you might want to refactor your data landscape within Mongo DB.\n\n**Michael: [00:12:36]** Okay. So we've talked about a number of, into patterns so far, we've talked about a use of dollar lookup, storing unbounded arrays in your documents. We've talked about having too many indexes. We've talked about having a large document sizes in your collections. We've talked about too many collections.\n\nAnd then I guess the last one we need to cover off is around case insensitive rejects squares. You want to talk a little bit about that? \n\n**Julia: [00:13:03]** Yeah. So. Like with the other anti-patterns we'll kind of look to see when you have queries that are using case insensitive red jacks and recommend that you have the appropriate index.\n\nSo it could be  case insensitive. Index, it could be a search index, things like that.  That is, you know, the last anti-pattern we flag. \n\n**Michael: [00:13:25]** Okay. Okay, great. And obviously, you know, any kind of operation against the database is going to require resource. And the whole idea here is there's a balancing act between leveraging the resource and  and operating efficiently.\n\n So, so these are, this is a product feature that's available in Mongo, DB, Atlas. All of these things are available today. Correct? Yeah. And you would get to, to see these suggestions in the performance advisor tab, right? \n \n**Julia: [00:13:55]** Yes. Performance advisor.  And also as I mentioned, our data Explorer, which is our collections.\nYeah. Right. \n\n**Michael: [00:14:02]** Yeah. Fantastic. The whole entire goal of. Automating database management is to make it easier for the developer to interact with the database. What else do we want to tell the audience about a schema suggestions or anything in this product space? So \nJulia: [00:14:19] I think definitely want to highlight what you just mentioned, that, you know, your schema changes the anti-patterns that  could be, you know, more damaging to your performance.\n\nChange over time and it really does depend on your workload and how you're accessing the data. I know that, you know, some of this FEMA anti-patterns do conflict with each other.  We do say that some cases you S you should  reduce references and some cases you shouldn't, it really depends on, you know, is the data that you want to access together, actually being stored together.\n\nAnd does that. You know, it makes sense. So they won't all always apply. It will be kind of situational and that's, you know  why we're here to help. \n\n**Nic: [00:15:01]** So when people are using Mongo DB to create documents in their collections, I imagine that they have some pretty intense looking document schemas, like I'm talking objects that are nested eight levels deep.\nWill the schema suggestions help in those scenarios to try to improve how people have created their data? \n\n**Julia: [00:15:23]** Schema suggestions are still definitely in their early days. I think we released this product almost a year ago. We'll definitely capture any of the six anti-patterns that we just mentioned if they're happening on a high level.\n\nSo if you're nesting a lot of stuff within the document, that would probably increase. You know, document size and we would flag it.  We might not be able to get that targeted to say, this is why your document sizes this large.  But I think that that's a really good call-out and it's safe to say, we know that we are not capturing every scenario that a user could encounter with their schema.\n\n You can truly do whatever you want  you know, designing your Mongo DB documents.  Were actively researching, which schema suggestions it makes sense to look for in our next iteration of this product. So if you have feedback, you know, always don't hesitate to reach out. We'd love to hear your thoughts.\n \n So yeah, there are definitely  some limitations we're working on it.  We're looking into it. \n\n**Michael: [00:16:27]** Okay. Let's say I'm a developer and I have a number of collections that  maybe they're not accessed as frequently, but I am concerned about the patterns in them. How can I force the performance advisor to look at a specific collection?\n\n**Julia: [00:16:43]** Yeah, that's a really good question.  So as I mentioned before, we do surface the anti-patterns in two places. One is performance advisor and that's for the more reactive use case  where doing a sweep, seeing what's going on and those 20 most active collections and kind of. Doing some logic to determine where the most impactful changes could be made.\n\nAnd then there's also the collections tab in Atlas. And this is where you can go say you're actively developing or adding documents to collection. They aren't heavily used yet, but you want to make sure you're on the right track.  If you view the schema, anti-patterns there, it basically runs our algorithm for you.\n\nAnd we'll. Search  a sample of collections for that, or sorry, a sample of documents for that collection and surface the suggestions there. So it's a little more targeted.  And I would say very useful for when you're actively developing something or have a small workload. \n\n**Michael: [00:17:39]** We've got a huge conference coming up in July.\nIt's Mongo, db.live. My first question is, are you going to be there? Are you perhaps presenting a talk on  on this subject at.live? \n\n**Julia: [00:17:50]** I am not presenting a talk on this subject at.live, but I will be there. I'm very, very excited for it. \n\n**Michael: [00:17:56]** Fantastic. Well, maybe we can get you  to come to community day, which is the week after where we've got talks and sessions and games and all sorts of fun stuff for the community.\nMaybe we can get you to  to talk a little bit about this at the  at the event that would be. That would be fantastic. I'm going to be.live is our biggest user conference of the year.  Joined us July 13th and 14th. It's free. It's all online. There's a huge lineup of cutting edge keynotes and breakout sessions.\n\nAll sorts of ask me anything, panels and brain breaking activities so much more. You can get more information@mongodb.com slash live. All right, Nick, anything else to add before we begin to wrap? \n \n**Nic: [00:18:36]** Nothing for me. I mean, Julia, is there any other last minute words of wisdom or anything that you want to tell the audience about schemas suggestions with the Mongo DB or anything that'll help them?\nYeah, \n\n**Julia: [00:18:47]** I don't think so. I think we covered a lot again. I would just emphasize  you know, don't be overwhelmed. Scheme is very important for Mongo DB.  And it is meant to be flexible. We're just here to help you. \n\n**Nic: [00:19:00]** I think that's the key word there. It's not a, it's not schema less. It's just flexible schema, right?\n\n**Julia: [00:19:05]** Yes, yes, yes. \n\n**Michael: [00:19:05]** Yes. Well, Julia, thank you so much. This has been a great conversation. \n\n**Julia: [00:19:09]** Awesome. Thanks for having me.\n","description":"Today, we are joined by Julia Oppenheim, Associate Product Manager at MongoDB. Julia chats with us and shares details of a set of features within MongoDB Atlas designed to help developers improve the design of their schemas to avoid common anti-patterns. ","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt8e1a63441b12ad9c/644c464b45f0985a159389bc/og-mongodb-podcast.png?branch=prod","description":null}}]},"slug":"/schema-suggestions-julia-oppenheim","title":"*Schema Suggestions with Julia Oppenheim - Podcast Episode 59","original_publish_date":"2022-05-20T18:14:02.400Z","strapi_updated_at":"2022-05-20T18:14:02.482Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Podcast","calculated_slug":"/podcasts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Schema","calculated_slug":"/products/mongodb/schema"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:32.945Z","publish_details":{"time":"2023-04-28T22:52:51.715Z"}}},{"calculated_slug":"/products/realm/realm-meetup-kotlin-multiplatform","content":"Didn't get a chance to attend the Realm Kotlin Multiplatform for modern mobile apps Meetup? Don't worry, we recorded the session and you can now watch it at your leisure to get you caught up.\n\n:youtube[]{vid=F1cEI9OKI-E}\n\nIn this meetup, Claus Rørbech, software engineer on the Realm Android team, will walk us through some of the constraints of the RealmJava SDK, the thought process that went into the decision to build a new SDK for Kotlin, the benefits developers will be able to leverage with the new APIs, and how the RealmKotlin SDK will evolve.\n\nIn this 50-minute recording, Claus spends about 35 minutes presenting an overview of the Realm Kotlin Multiplatfrom. After this, we have about 15 minutes of live Q&A with Ian, Nabil and our Community. For those of you who prefer to read, below we have a full transcript of the meetup too. As this is verbatim, please excuse any typos or punctuation errors!\n\nThroughout 2021, our Realm Global User Group will be planning many more online events to help developers experience how Realm makes data stunningly easy to work with. So you don't miss out in the future, join our Realm Global Community and you can keep updated with everything we have going on with events, hackathons, office hours, and (virtual) meetups. Stay tuned to find out more in the coming weeks and months.\n\nTo learn more, ask questions, leave feedback, or simply connect with other Realm developers, visit our [community forums](https://developer.mongodb.com/community/forums/c/realm-sdks/58). Come to learn. Stay to connect.\n\n### Transcript\n\nClaus:\nYeah, hello. I'm Claus Rorbech, welcome to today's talk on Realm Kotlin. I'm Claus Rorbech and a software engineer at MongoDB working in the Java team and today I'm going to talk about Realm Kotlin and why we decided to build a complete new SDK and I'll go over some of the concepts of this.\n\nWe'll do this with a highlight of what Realm is, what triggered this decision of writing a new SDK instead of trying to keep up with the Realm Java. Well go over some central concepts as it has some key significant changes compared to Realm Java. We'll look into status, where we are in the process. We'll look into where and how it can be used and of course peek a bit into the future.\n\nJust to recap, what is Realm? Realm is an object database with the conventional ACID properties. It's implemented in a C++ storage engine and exposed to various language through language specific SDKs. It's easy to use, as you can define your data model directly with language constructs. It's also performant. It utilizes zero copying and lazy loading to keep the memory footprint small. Which is still key for mobile development.\n\nHistorically we have been offering live objects, which is a consistent view of data within each iteration of your run loop. And finally we are offering infrastructure for notifications and easy on decide encryption and easy synchronization with MongoDB Realm. So, Realm Java already works for Kotlin on Android, so why bother doing a new SDK? The goal of Realm is to simplify app development. Users want to build apps and not persistent layers, so we need to keep up providing a good developer experience around Realm with this ecosystem.\n\nWhy not just keep up with the Realm Java? To understand the challenge of keeping up with Realm Java, we have to have in mind that it has been around for almost a decade. Throughout that period, Android development has changed significantly. Not only has the language changed from Java to Kotlin, but there's also been multiple iterations of design guidelines. Now, finally official Android architectural guidelines and components. We have kept up over the years. We have constantly done API adjustments. Both based on new language features, but also from a lot of community feedback. What users would like us to do. We have tried to accommodate this major new design approach by adding support for the reactive frameworks.\n\nBoth RX Java and lately with coroutine flows. But keeping up has become increasingly harder. Not only just by the growing features of Realm itself, but also trying to constantly fit into this widening set of frameworks. We thought it was a good time to reassess some of these key concepts of Realm Java. The fundamentals of Realm Java is built around live objects. They provide a consistent updated view of your data within each run loop iterations. Live data is actually quite nice, but it's also thread confined.\n\nThis means that each thread needs its own instance of Realm. This Realm instance needs to be explicitly close to free up resources and all objects obtained from this Realm are also thread confined. These things have been major user obstacles because accessing objects on other threads will flow, and failing to close instances of Realm on background threads can potentially lead to growing file sizes because we cannot clean up this updated data.\n\nSo, in general it was awesome for early days Android app architectures that were quite simple, but it's less attractive for the current dominant designs that rely heavily on mutable data streams in frameworks and very flexible execution and threading models. So, besides this wish of trying to redo some of the things, there're other motivations for doing this now. Namely, being Kotlin first. Android has already moved there and almost all other users are also there.\n\nWe want to take advantage of being able to provide the cleaner APIs with nice language features like null safety and in line and extension functions and also, very importantly, co routines for asynchronous operations. Another key feature of Kotlin is the Kotlin Compiler plugin mechanism. This is a very powerful mechanism that can substitute our current pre processor approach and byte manipulation approach.\n\nSo, instead of generating code along the user classes, we can do in place code transformation. This reduces the amount of code we need to generate and simplifies our code weaving approach and therefore also our internal APIs. The Kotlin Compiler plugin is also faster than our current approach because we can eliminate the very slow KAPT plugin that acts as an annotation processor, but does it by requiring multiple passes to first generate stops and then the actual code for the implementation.\n\nWe can also target more platforms with the Compiler plugin because we eliminate the need for Google's Transform API that was only available to Android. Another key opportunity for doing this now is that we can target the Kotlin multi platform ecosystem. In fact, Realm's C++ storage engine is already multi platform. It's already running on the Kotlin multi platform targets with the exception of JavaScript for web. Secondly, we also find Kotlin's multi platform approach very appealing. It allows code sharing but also allows you to target specific portions of your app in native platform frameworks when needed.\n\nFor example, UI. We think Realm fits well into this Kotlin multi platform library suite. There's often no need for developer platform differentiation for persistence and there's actually already quite good coverage in this library suite. Together with the Kotlin serialization and Realm, you can actually do full blown apps as shared code and only supply a platform specific UI.\n\nSo, let's look into some of the key concepts of this new SDK. We'll do that by comparing it to Realm Java and we'll start by defining a data model. Throughout all these code snippets I've used the Kotlin syntax even for the Realm Java examples just to highlight the semantic changes instead of bothering with the syntactical differences. So, I just need some water...\n\nSo, as you see it looks more or less like Java. But there are some key things there. The compiler plugins enable us access the classes directly. This means we no longer need the classes to be open, because we are not internally inheriting from the user classes. We can also just add a marker interface that we fill out by the compiler plugin instead of requiring some specific base classes. And we can derive nullability directly from the types in Kotlin instead of requiring this required annotation.\n\nNot all migration are as easy to cut down. For our Realm configurations, we're not opting in for pure Kotlin with the named parameters, but instead keeping the binder approach. This is because it's easier to discover from inside the ID and we also need to have in mind that we need to be interoperable with the Java. We only offer some specific constructors with named parameters for very common use cases. Another challenge from this new tooling is that the compiler plug-in has some constraints that complicates gathering default schemas.\n\nWe're not fully in place with the constraints for this yet, so for now, we're just required explicit schema definition in the configuration. For completion, I'll just also highlight the current API for perform inquiries on the realm. To get immediate full query capabilities with just exposed the string-based parcel of the underlying storage engine, this was a quick way to get full capabilities and we'll probably add a type safe query API later on when there's a bigger demand.\n\nActually, this string-based query parcel is also available from on Java recently, but users are probably more familiar with the type-based or type safe query system. All these changes are mostly syntactical but the most dominant change for realm Kotlin is the new object behavior. In Realm Kotlin, objects are no longer live, but frozen. Frozen objects are data objects tied to a specific version of the realm. They are immutable. You cannot update them and they don't change over time.\n\nWe still use the underlying zero-copying and lazy loading mechanism, so we still keep the memory footprints small. You can still use a frozen object to navigate the full object graph from this specific version. In Realm Kotlin, the queries also just returns frozen objects. Similarly, notifications also returns new instances of frozen objects, and with this, we have been able to lift the thread confinement constraint. This eases lifecycle management, because we can now only have a single global instance of the realm.\n\nWe can also pass these objects around between threads, which makes it way easier to use it in reactive frameworks. Again, let's look into some examples by comparing is to Realm Java. The shareable Realm instances eases this life cycle management. In Realm Java, we need to obtain an instance on each thread, and we also need to explicitly close this realm instance. On Realm Kotlin, we can now do this upfront with a global instance that can be passed around between the threads. We can finally close it later on this single instance. Of course, it has some consequences. With these shareable instances, changes are immediately available too on the threads.\n\nIn Realm Java, the live data implementation only updated our view of data with in between our run loop iterations. Same query in the same scope would always yield the same result. For Realm Kotlin with our frozen objects, so in Realm Kotlin, updates from different threads are immediately visible. This means that two consecutive queries might reveal different results. This also applies for local or blocking updates. Again, Realm Java with live results local updates were instantly visible, and didn't require refresh. For Realm Java, the original object is frozen and tied to a specific version of the Realm.\n\nWhich means that the update weren't reflected in our original object, and to actually inspect the updates, we would have to re-query the Realm again. In practice, we don't expect access to these different versions to be an issue. Because the primary way of reacting to changes in Realm Kotlin will be listening for changes.\n\nIn Realm Kotlin, updates are delivered as streams of immutable objects. It's implemented by coroutine flows of these frozen instances. In Realm Java, when you got notified about changes, you could basically throw away the notification object, because you could still access all data from your old existing live reference. With Realm Kotlin, your original instance is tied to a specific version of the Realm. It means that you for each update, you would need to access the notify or the new instance supplied by the coroutine flow.\n\nBut again, this updated instance, it gives you full access to the Realm version of that object. It gives you full access to the object graph from this new frozen instance. Here we start to see some of the advantage of coroutine based API. With Realm Java, this code snippet, it was run on a non-loop of thread. It would actually not even give you any notification because it was hard to tie the user code with our underlying notification mechanism. For Realm Kotlin, since we're using coroutines, we use the flexibilities of this. This gives the user flexibility to supply this loop like dispatching context and it's easy for us to hook our event delivery off with that.\n\nSecondly, we can also now spread the operations on a flow over varying contexts. This means that we can apply all the usual flow operations, but we can also actually change the context as our objects are not tied to a specific thread. Further, we can also with the structural concurrency of codes routines, it's easier to align our subscription with the existing scopes. This means that you don't have to bother with closing the underlying Realm instance here.\n\nWith the shareable Realm instances of Realm Kotlin, updates must be done atomically. With Realm Java, since we had a thread confined instance of the Realm, we could just modify it. In Realm Kotlin, we have to be very explicit on when the state is changed. We therefore provide this right transaction on a separate mutable Realm within a managed scope. Inside the scope, it allows us to create an update objects just as in Realm Java. But the changes are only visible to other when once the scope is exited. We actually had a similiar construct in Realm Java, but this is now the only way to update the Realm.\n\nInside the transaction blocks, things almost works as in Realm Java. It's the underlying same starch engine principles. Transactions are still performed on single thread confined live Realm, which means that inside this transaction block, the objects and queries are actually live. The major key difference for the transaction block is how to update existing objects when they are now frozen. For both STKs, it applies that we can only do one transaction at a time. This transaction must be done on the latest version of the Realm. Therefore, when updating existing objects, we need to ensure that we have an instance that is tied to the latest version of the object.\n\nFor Realm Java, we could just pass in the live objects to our transaction block but since it could actually have ... we always had to check the object for its validity. A key issue here was that since object couldn't be passed around on arbitrary threads, we had to get a non-local object, we would have to query the Realm and find out a good filtering pattern to identify objects uniquely. For Realm, we've just provided API for obtaining the latest version of an object. This works for both primary key and non-primary key objects due to some internal identifiers.\n\nSince we can now pass objects between a thread, we can just pass our frozen objects in and look up the latest version. To complete the tour, we'll just close the Realm. As you've already seen, it's just easier to manage the life cycle of Realm when there's one single instance, so closing an instance to free up resources and perform exclusive operations on the Realm, it's just a matter of closing the shared global instance.\n\nInteracting with any object instance after you closed the Realm will still flow, but again, the structural concurrency of coroutine flows should assist you in stopping accessing the objects following the use cases of your app. Besides the major shift to frozen objects, we're of course trying to improve the STKs in a lot of ways, and first of all, we're trying to be idiomatic Kotlin. We want to take advantage of all the new features of the language. We're also trying to reduce size both of our own library but also the generated code. This is possible with the new compiler plug-in as we've previously touched. We can just modify the user instance and not generate additional classes.\n\nWe're also trying to bundle up part of functionality and modularize it into support libraries. This is way easier with the extension methods. Now we should be able to avoid having everybody to ship apps with the JSON import and export functionality and stuff like that. This also makes it easier to target future frameworks by offering support libraries with extension functions. As we've already also seen, we are trying to ensure that our STK is as discoverable from the ID directly, and we're also trying to ensure that the API is backward compatible with the Java.\n\nThis might not be the most idiomatic Java, but at least we try to do it without causing major headaches. We also want to improve testability, so we're putting in places to inject dispatchers. But most notably, we're also supplying JBM support for off-device testing. Lastly, since we're redoing an STK, we of course have a lot of insight in the full feature set, so we also know what to target to make a more maintainable STK. You've already seen some of the compromises for these flights, but please feel free to provide feedback if we can improve something.\n\nWith this new multiplatform STK, where and how to use. We're providing our plug-in and library as a Kotlin multiplatform STK just to be absolutely clear for people not familiar with the multiplatform ecosystem. This still means that you can just apply your project or apply this library and plug-in on Android only projects. It just means that we can now also target iOS and especially multiplatform and later desktop JBM. And I said there's already libraries out there for sterilization and networking.\n\nWith Realm, we can already build full apps with the shared business logic, and only have to supply platform dependent UI. Thanks to the layout and metadata of our artifacts, there's actually no difference in how to apply the projects depending on which setting you are in. It integrates seamlessly with both Android and KMM projects. You just have to apply the plug-in. It's already available in plug-in portal, so you can just use this new plug-in syntax. Then you have to add the repository, but you most likely already have Maven Central as part of your setup, and then at our dependency. There's a small caveat for Android projects before Kotlin or using Kotlin, before 1.5, because the IR backend that triggers our compiler plug-in is not default before that.\n\nYou would have to enable this feature in the compiler also. Yeah. You've already seen a model definition that's a very tiny one. With this ability or this new ability to share our Realm instances, we can now supply one central instance and here have exemplified it by using a tiny coin module. We are able to share this instance throughout the app, and to show how it's all tied together, I have a very small view model example. These users are central Realm instance supplied by Kotlin.\n\nIt sets up some live data to feed the view. This live data is built up from our observable flows. You can apply the various flow operators on it, but most importantly you can also control this context for where it's executing. Lastly, you are handling this in the view model scope. It's just that subscription is also following the life cycle of your view model. Lastly, for completion, there's a tiny method to put some data in there.\n\nAs you might have read between the lines, this is not all in place yet, but I'll try to give a status. We're in the middle of maturing this prove of concept of the proposed frozen architecture. This is merged into our master branch bit by bit without exposing it to the public API. There's a lot of pieces that needs to fit together before we can trigger and migrate completely to this new architecture. But you can still try our library out. We have an initial developer preview in what we can version 0.1.0. Maybe the best label is Realm Kotlin Multiplatform bring-up.\n\nBecause it sort of qualifies the overall concept in this mutliplatform setting with our compiler plug-in being on multi platforms. Also a mutliplatform [inaudible 00:30:09] with collecting all these native objects in the various [inaudible 00:30:14] management domains. A set, it doesn't include the full frozen architecture yet, so the Realm instances are still thread confined, and objects are live. There's only limited support, but we have primitive types, links to other Realm objects and primary keys. You can also register for notifications.\n\nWe use this string-based queries also briefly mentioned. It operates on Kotlin Mutliplatform mobile, which means that it's both available for Android and iOS, but only for 64 bit on both platforms. It's already available on Maven Central, so you can go and try it out either by using our own KMM example in the repository or build your own project following the read me in the repository.\n\nWhat's next? Yeah. Of course in these weeks, we're stabilizing the full frozen architecture. Our upcoming milestones are first we want to target the release with the major Java features. It's a lot of the other features of Realm like lists, indexes, more detailed changelistener APIs, migration for schema updates and dynamic realms and also desktop JVM support. After that, we'll head off to build support for MongoDB Realm. To be able to sync this data remotely.\n\nWhen this is in place, we'll target the full feature set of Realm Java. There's a lot of more exotic types embedded objects and there's we also just introduced new types like sets and dictionaries and [inaudible 00:32:27] types to Realm Java. These will come in a later version on Kotlin. We're also following the evolution of Kotlin Mutliplatform. It's still only alpha so we have to keep track of what they're doing there, and most notably, following the memory management model of Kotlin native, there are constraints that once you pass objects around, Kotlin is freezing those.\n\nRight now, you cannot just pass Realm instances around because they have to be updated. But these frozen objects can be passed around threads and throughout this process, we'll do incremental releases, so please keep an eye open and provide feedback.\n\nTo keep up with our progress, follow us on GitHub. This is our main communication channel with the community. You can try out the sample, a set. You can also ... there's instructions how to do your own Kotlin Mutliplatform project, and you can peek into our public design docs. They're also linked from our repository. If you're more interested into the details of building this Mutliplatform STK, you can read a blog post on how we've addressed some of this challenge with the compiler plug-in and handling Mutliplatform C Interrupts, memory management, and all this.\n\nThank you for ... that's all.\n\n**Ian:**\nThank you Claus, that was very enlightening. Now, we'll take some of your questions, so if you have any questions, please put them in the chat. The first one here will mention, we've answered some of them, but the first one here is regarding the availability of the. It is available now. You can go to GitHub/Realm/Realm.Kotlin, and get our developer preview. We plan to have iterative releases over the next few quarters. That will add more and more functionality.\n\nThe next one is regarding the migration from I presume this user has ... or James, you have a Realm Java application using Realm Java and potentially, you would be looking to migrate to Realm Kotlin. We don't plan to have an automatic feature that would scan your code and change the APIs. Because the underlying semantics have changed so much. But it is something that we can look to have a migration guide or something like that if more users are asking about it.\n\nReally the objects have changed from being live objects to now being frozen objects. We've also removed the threading constraint and we've also have a single shared Realm instance. Whereas before, with every thread, you had to open up a new Realm instance in order to do work on that thread. The semantics have definitely changed, so you'll have to do this with developer care in order to migrate your applications over. Okay. Next question here, we'll go through some of these.\n\nDoes the Kotlin STK support just KMM for syncing or just local operations? I can answer this one. We do plan to offer our sync, and so if you're not familiar, Realm also offers a synchronization from the local store Realm file to MongoDB Atlas through Realm Sync, through the Realm Cloud. This is a way to bidirectionally sync any documents that you have stored on MongoDB Atlas down and transformed into Realm objects and vice versa.\n\nWe don't have that today, but it is something that you can look forward to the future in next quarters, we will be releasing our sync support for the new Realm Kotlin STK. Other questions here, so are these transactions required to be scoped or suspended? I presume this is using the annotations for the Kotlin coroutines keywords. The suspend functions, the functions, Claus, do you have any thoughts on that one?\n\n**Claus:**\nYeah. We are providing a default mechanism but we are also probably adding at least already in our current prototype, we already have a blocking right. You will be able to do it without suspending. Yeah.\n\n**Ian:**\nOkay. Perfect. Also, in the same vein, when running a right transaction, do you get any success or failed result back in the code if the transaction was successful? I presume this is having some sort of callback or on success or on failure if the right transaction succeeded or failed. We plan that to our API at all?\n\n**Claus:**\nUsually, we just have exceptions if things doesn't go right, and they will propagate throughout normal suspend ... throughout coroutine mechanisms. Yeah.\n\n**Ian:**\nYeah. It is a good thought though. We have had other users request this, so it's something if we continue to get more user feedback on this, potentially we could add in the future. Another question here, is it possible to specify a path to where all the entities are loaded instead of declaring each on a set method?\n\n**Ian:**\nNot sure I fully follow that.\n\n**Claus:**\nIt's when defining the schema. Yeah. We have some options of gathering this schema information, but as I stated, we are not completely on top of which constraints we want to put into it. Right now, we are forced to actually define all the classes, but we have issues for addressing this, and we have investigating various options. But some of these come with different constraints, and we have to judge them along the way to see which fits best or maybe we'll find some other ways around this hopefully.\n\n**Nabil:**\nJust to add on top of this, we could use listOf or other data structure. Just the only constraint at the compiler level are using class literal to specify the. Since there's no reflection in Kotlin Native, we don't have the mechanism like we do in Java to infer from your class path, the class that are annotating to Java that will participate in your schema. The lack of reflection in Kotlin Native forces us to just use class and use some compiler classes to build the schema for you constraint.\n\n**Ian:**\nYeah. Just to introduce everyone, this is Nabil. Nabil also works on the Realm Android team. He's one of the lead architects and designers of our new Realm Kotlin STK, so thank you Nabil.\n\n**Ian:**\nIs there any known limitations with the object relation specifically running on a KMM project? My understanding is no. There shouldn't be any restrictions on object relations. Also, for our types, because Realm is designed to be a cross-platform SDK where you can use the same Realm file in both iOS, JavaScript, Android applications, the types are also cross-platform. My understanding is we shouldn't have any restrictions for object relations. I don't know Nabil or Claus if you can confirm or deny that.\n\n**Nabil:**\nSorry. I lost you for a bit.\n\n**Claus:**\nI also lost the initial. No, but we are supporting the full feature set, so I can't immediately come up with any constraints \naround Java.\n\n**Ian:**\nOther questions here, will the Realm SDK support other platforms not just mobile? We talked a little bit about desktop, so I think JVM is something that we think we can get out of the box with our implementations of course. I think for desktop JVM applications is possible.\n\n**Nabil:**\nInternally like I mentioned already compiling for JVM and and also for. But we didn't expose it other public API yet. We just wanted object support in iOS and Android. The only issue for JVM, we have the tool chain compiling on desktop file, is to add the Android specific component, which are like the which you don't have for desktops. We need to find way either to integrate with Swing or to provide a hook for you to provide your looper, so it can deliver a notification for you. That's the only constraint since we're not using any other major Android specific API besides the context. The next target that we'll try to support is JVM, but we're already supported for internally, so it's not going to be a big issue.\n\n**Ian:**\nI guess in terms of web, we do have ... Realm Core, the core database is written in C++. We do have some projects to explore what it would take to compile Realm Core into Wasm so that it could then be run into a browser, and if that is successful, then we could potentially look to have web as a target. But currently, it's not part of our target right now.\n\nOther questions here, will objects still be proxies or will they now be the same object at runtime? For example, the object Realm proxy.\n\n**Nabil:**\nThere will be no proxy for Realm Kotlin. It's one of the benefit of using a compiler, is we're modifying the object itself. Similar to what composes doing with the add compose when you write a compose UI. It's modifying your function by adding some behavior. We do the same thing. Similar to what Kotlin sterilization compiler is doing, so we don't use proxy objects anymore.\n\n**Ian:**\nOkay. Perfect. And then I heard at the end that Realm instances can't be frozen on Native, just wanted to confirm that. Will that throw off if a freeze happens? Also wondering if that's changing or if you're waiting on the Native memory model changes.\n\n**Nabil:**\nThere's two aspects to what we're observing to what are doing. It's like first it's the garbage collector. They introduce an approach, where you could have some callbacks when you finalize the objects and we'll rely on this to free the native resources. By native, I mean the C++ pointer. The other aspect of this is the memory model itself of the Kotlin Native, which based on frozen object similiar concept as what we used to do in Realm Java, which is a thread confinement model to achieve.\n\n**Nabil:**\nActually, what we're doing is like we're trying to freeze the object graph similarly to what Kotlin it does. You can pass object between threads. The only sometimes issue you is like how you interface with multi-threaded coroutine on Kotlin Native. This part is not I think stable yet. But in theory, our should overlap, should work in a similiar way.\n\n**Claus:**\nI guess we don't really expect this memory management scheme from Kotlin Native to be instantly solved, but we have maybe options of providing the Realm instance in like one specific instance that doesn't need to be closed on each thread, acting centrally with if our Native writer and notification thread. It might be possible in the future to define Realms on each thread and interact with the central mechanisms. But I wouldn't expect the memory management constraints on Native to just go away.\n\n**Ian:**\nRight. Other question here will Realm plan on supporting the Android Paging 3 API?\n\n**Nabil:**\nYou could ask the same question with Realm Java. The actual lazy loading capability of Realm doesn't require you to implement the paging library. The paging library problem is like how you can load efficiently pages of data without loading the entire dataset. Since both Realm Java and Realm Kotlin uses just native pointers to the data, so as you traverse your list or collection will only loads this object you're trying to access. And not like 100 or 200 similiar to what a cursor does in SQLite for instance. There's no similiar problem in Realm in general, so it didn't try to come up with a paging solution in the first place.\n\n**Ian:**\nThat's just from our lazy loading and memory map architecture, right?\n\n**Nabil:**\nCorrect.\n\n**Ian:**\nYeah. Okay. Other question here, are there any plans to support polymorphic objects in the Kotlin SDK? I can answer this. I have just finished a product description of adding inheritance in polymorphism to not only the Kotlin SDK, but to all of our SDKs. This is targeted to be a medium term task. It is an expensive task, but it is something that has been highly requested for a while now. Now, we have the resources to implement it, so I'd expect we would get started in the medium term to implement that.\n\n**Nabil:**\nWe also have released for in Java what we call the polymorphic type, which is. You can and then install in it the supported Realm have JSON with the dynamic types, et cetera. Go look at it. But it's not like the polymorphic, a different polymorphic that Ian was referring -\n\n**Ian:**\nIt's a first step I guess you could say into polymorphism. What it enables is kind of what Nabil described is you could have an owner field and let's say that owner could be a business type, it could be a person, it could be an industrial, a commercial, each of these are different classes. You now have the ability to store that type as part of that field. But it doesn't allow for true inheritance, which I think is what most people are looking for for polymorphism. That's something that is after this is approved going to be underway. Look forward to that. Other questions here? Any other questions? Anything I missed? I think we've gone through all of them. Thank you to the Android team. Here's Christian, the lead of our Android team on as well. He's been answering a lot of questions. Thank you Christian. But if any other questions here, please reach out. Otherwise, we will close a little bit early.\n\nOkay. Well, thank you so much everyone. Claus, thank you. I really appreciate it. Thank you for putting this together. This will be posted on YouTube. Any other questions, please go to /Realm/Realm.Kotlin on our GitHub. You can file an issue there. You can ask questions. There's also Forums.Realm.io.\n\nWe look forward to hearing from you. Okay. Thanks everyone. Bye.\n\n**Nabil:**\nSee you online. Cheers.","description":"In this talk, Claus Rørbech, software engineer on the Realm Android team, will walk us through some of the constraints of the RealmJava SDK, the  thought process that went into the decision to build a new SDK for Kotlin, the benefits developers will be able to leverage with the new APIs, and how the RealmKotlin SDK will evolve.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltecbf4adf0b3bf202/644c464c06be8e2681f54a65/Realm_Kotlin.png?branch=prod","description":null}}]},"slug":"/realm-meetup-kotlin-multiplatform","title":"*Realm Meetup - Realm Kotlin Multiplatform for Modern Mobile Apps","original_publish_date":"2021-06-10T23:25:26.260Z","strapi_updated_at":"2023-03-21T16:11:00.296Z","expiry_date":"2022-06-10T23:14:23.944Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Kotlin","calculated_slug":"/languages/kotlin"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Android","calculated_slug":"/technologies/android"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:32.502Z","publish_details":{"time":"2023-04-28T22:52:51.742Z"}}},{"calculated_slug":"/products/realm/realm-meetup-swiftui-testing-and-realm-with-projections","content":"Didn't get a chance to attend the SwiftUI Testing and Realm with Projections Meetup? Don't worry, we recorded the session and you can now watch it at your leisure to get you caught up.\n\n:youtube[]{vid=fxar75-7ZbQ}\n\nIn this meetup, Jason Flax, Lead iOS Engineer, makes a return to explain how the testing landscape has changed for iOS apps using the new SwiftUI framework. Learn how to write unit tests with SwiftUI apps powered by Realm, where to put your business logic with either ViewModels or in an app following powered by Model-View-Intent, and witness the power of Realm's new Projection feature. \n\nIn this 50-minute recording, Jason spends about 40 minutes presenting \n\n- Testing Overview for iOS Apps\n\n- What's Changed in Testing from UIKit to SwiftUI\n\n- Unit Tests for Business Logic - ViewModels or MVI?\n\n- Realm Projections - Live Realm Objects that Power your View\n\nAfter this, we have about 10 minutes of live Q&A with Ian & Jason and our community . For those of you who prefer to read, below we have a full transcript of the meetup too. \n\nThroughout 2021, our Realm Global User Group will be planning many more online events to help developers experience how Realm makes data stunningly easy to work with. So you don't miss out in the future, join our [Realm Global Community](https://live.mongodb.com/realm-global-community/) and you can keep updated with everything we have going on with events, hackathons, office hours, and (virtual) meetups. Stay tuned to find out more in the coming weeks and months.\n\nTo learn more, ask questions, leave feedback, or simply connect with other Realm developers, visit our [community forums](https://developer.mongodb.com/community/forums/c/realm-sdks/58). Come to learn. Stay to connect.\n\n### Transcript\n(*As this is verbatim, please excuse any typos or punctuation errors!*)\n\n**Ian:**\nWe’re going to talk about our integration into SwiftUI, around the Swift integration into SwiftUI and how we're making that really tight and eliminating boilerplate for developers. We're also going to show off a little bit of a feature that we're thinking about called Realm projections. And so we would love your feedback on this new functionality. We have other user group conference meetings coming up in the next few weeks. So, we're going to be talking about Realm JavaScript for React Native applications next week, following that we're talking about how to integrate with the Realm cloud and AWS EventBridge and then later on at the end of this month, we will have two other engineers from the iOS team talk about key path filtering and auto open, which will be new functionality that we deliver as part of the Realm Swift SDK.\n\nWe also have MongoDB.live, this is taking place on July 13th and 14th. This is a free event and we have a whole track of talks that are dedicated to mobile and mobile development. So, you don't need to know anything about MongoDB as a server or anything like that. These talks will be purely focused on mobile development. So you can definitely join that and get some benefit if you're just a mobile developer. A little bit about housekeeping here. This is the Bevy platform. In a few slides, I'm going to turn it back over to Jason. Jason's going to run through a presentation. If you have any questions during the presentation, there's a little chat box in the window, so just put them in there. We have other team members that are part of the Realm team that can answer them for you. And then at the end, I'll run through them as well as part of a Q&A session that you can ask any questions.\n\nAlso, if you want to make this more interactive, we're happy to have you come on to the mic and do your camera and you can ask a question live as well. So, please get connected with us. You can join our forums.realm.io. Ask any questions that you might have. We also have our community get hubs where you can file an issue. Also, if you want to win some free swag, you can go on our Twitter and tweet about this event or upcoming events. We will be sending swag for users that tweet about us. And without further ado, I will stop sharing my screen and turn it over to Jason.\n\n**Jason:**\nHello, everyone. Hope everyone's doing well. I'm going to figure out how to share my screen with this contraption. Can people see my screen?\n\n**Ian:**\nI can see it.\n\n**Jason:**\nCool stuff. All right. Is the thing full screen? Ian?\n\n**Ian:**\nSorry. I was muted, but I was raising my finger.\n\n**Jason:**\nYeah, I seem to always do these presentations when I'm visiting the States. I normally live in Dublin and so I'm out of my childhood bedroom right now. So, I don't have all of the tools I would normally have. For those of you that do not know me, my name is Jason Flax. I'm the lead engineer on the Realm Cocoa team. I've been at MongoDB for about five years, five years actually in three days, which is crazy. And we've been working with Realm for about two years now since the acquisition. And we've been trying to figure out how to better integrate Realm into SwiftUI, into all the new stuff coming out so that it's easier for people to use. We came up with a feature not too long ago to better integrate with the actual life cycle of SwiftUI ideas.\n\nThat's the ObservedRealmObject out observed results at state Realm object, the property rappers that hook into the view make things easy. We gave a presentation on the architectures that we want to see people using what SwiftUI, ruffled some feathers by saying that everybody should rewrite 50,000 lines of code and change the architecture that we see fit with SwiftUI. But a lot of people are mainly asking about testing. How do you test with SwiftUI? There aren't really good standards and practices out there yet. It's two years old. And to be honest, parts of it still feel a bit beta ish. So, what we want to do today is basically to go over, why do we test in the first place? How should you be testing with Realm?\n\nHow should you be testing the SwiftUI and Realm? What does that look like in a real world scenario? And what's coming next for Realm to better help you out in the future? Today's agenda. Why bother testing? We've all worked in places where testing doesn't happen. I encourage everybody to test their code. How to test a UI application? We're talking about iOS, macOS, TBOS, watchOS, they would be our primary users here. So we are testing a UI application. Unit and integration testing, what those are, how they differ, how Realm fits in? Testing your business logic with Realm. We at least internally have a pretty good idea of where we see business logic existing relative to the database where that sits between classes and whatnot. And then finally a sneak peek for Projections and a Q&A.\n\nSo, Projections is a standalone feature. I'll talk more about it later, but it should greatly assist in this case based on how we've seen people using Realm in SwiftUI. And for the Q&A part, we really want to hear from everybody in the audience. Testing, I wouldn't call it a hotly contested subject but it's something that we sometimes sit a bit too far removed from not building applications every day. So, it's really important that we get your feedback so that we can build better features or provide better guidance on how to better integrate Realm into your entire application life cycle. So why bother testing? Structural integrity, minimize bugs, prevent regressions, improve code quality and creating self-documenting code, though that turned to be dangerous as I have seen it used before, do not write documentation at all. I won't spend too long on this slide.\n\nI can only assume that if you're here at the SwiftUI testing talk that you do enjoy the art of testing your code. But in general, it's going to create better code. I, myself and personal projects test my code. But I certainly didn't when I first started out as a software engineer, I was like, \"Ah, sure. That's a simple function there. It'll never break.\" Lo and behold, three months later, I have no idea what that function is. I have no idea what it did, I was supposed to do and now I have a broken thing that I have to figure out how to fix and I'll spend an extra week on it. It's not fun for anybody involved. So, gesture code. How to test a UI application. That is a UI application. So, unit tests, unit tests are going to be your most basic test.\n\nIt tests functions that test simple bodies of work. They're going to be your smallest test, but you're going to a lot of them. And I promise that I'll try to go quickly through the basics testing for those that are more seasoned here. But unit tests are basically input and output. If I give you this, I expect this back in return. I expect the state of this to look like this based on these parameters. Probably, some of our most important tests, they keep the general structure of the code sound. Integration tests, these are going to, depending on the context of how you're talking about it could be integrating with the backend, it could be integrating with the database. Today, we're going to focus on the latter. But these are the tests that actually makes sure that some of the like external moving parts are working as you'd expect them to work.\n\nAcceptance tests are kind of a looser version of integration tests. I won't really be going over them today. End-to-end tests, which can be considered what I was talking about earlier, hitting an actual backend. UI testing can be considered an end to end test if you want to have sort of a loose reasoning about it or UI testing that actually tests the back. And it basically does the whole system work? I have a method that sends this to the server and I get this back, does the thing I get back look correct and is the server state sound? And then smoke tests, these are your final tests where your manager comes to you at 8:00 PM on the day that you were supposed to ship the thing and he's like, \"Got to get it out there.\n\nDid you test it?\" And you're like, \"Oh, we smoke tested.\" It's the last few checks I suppose. And then performance testing, which is important in most applications, making sure that everything is running as it should, everything is running as quickly as it should. Nothing is slowing it down where it shouldn't. This can catch a lot of bugs in code. XC test provides some really simple mechanisms for performance testing that we use as well. It'd be fairly common as well, at least for libraries to have regression testing with performance testing, to make sure that code you introduced didn't slow things down 100X because that wouldn't be fun for anyone involved. So, let's start with unit tests. Again, unit tests focus on the smallest possible unit of code, typically a function or class, they're fast to run and you'll usually have a lot of them.\n\nSo, the example we're going to be going over today is a really simple application, a library application. There's going to be a library. There's going to be users. Users can borrow books and use just can return book. I don't know why I pick this, seemed like a nice thing that wasn't a to do app. Let's start going down and just explaining what's happening here. You have the library, you have an error enum, which is a fairly common thing to do in Swift. Sorry. You have an array of books. You have an array of member IDs. These are people with assumably library cards. I don't know, that's the thing in every country. You're going to have initialize it, that takes in an array of books and an array of member IDs that initialize the library to be in the correct state.\n\nYou're going to have a borrow method that is going to take an ISBAN, which is ... I don't exactly remember what it stands for. International something book number, it's the internationally recognized idea, the book and then the library memberUid, it is going to be a throwing method that returns a book. And just to go over to the book for a second, a book contains an ISBAN, ID, a title and an author. The borrow method is going to be the main thing that we look at here. This is an individual body of work, there's clear input and output. It takes an ISBAN and the library memberUid, and it gives you back a book if all was successful. Let's walk down what this method does and how we want to test it.\n\nAgain, receive an ISBAN, received a library memberUid. We're going to check if that book actually exists in the available books. If it doesn't, we throw an error, we're going to check if a member actually exists in our library memberUid, it doesn't, throw an error. If we've gotten to this point, our state is correct. We remove the book from the books array, and we return it back to the color. So, it can be a common mistake to only test the happy path there, I give you the right ISBAN, I give you the right Uid, I get the right book back. We also want to test the two cases where you don't have the correct book, you don't have the correct member. And that the correct error is thrown. So, go to import XC test and write our first unit test.\n\nThrowing method, it is going to ... I'll go line by line. I'm not going to do this for every single slide. But because we're just kind of getting warmed up here, it'll make it clear what I'm talking about as we progress with the example because we're going to build on the example as the presentation goes on. So, we're going to create a new library. It's going to have an empty array of books and empty memberUids. We're going to try to borrow a book with an ISBAN that doesn't exist in the array and a random Uid which naturally does not exist in the empty number ID. That's going to throw an error. We're asserting that it throws an error. This is bad path, but it's good that we're testing it. We should also be checking that it's the correct error.\n\nI did not do that to save space on the slide. The wonders of presenting. After that, we're going to create a library now with a book, but not a Uid, that book is going to make sure that the first check passes, but the lack of memberUids is going to make sure that the second check fails. So we're going to try to borrow that book again. That book is Neuromancer, which is great book. Everybody should read it. Add it to your summer reading lists, got plenty of time on our hands. We're going to assert that, that throws an error. After that we're going to actually create the array of memberUids finally, we're going to create another library with the Neuromancer book and the memberUids properly initialized this time. And we're going to finally successfully borrow the book using the first member of that members array of IDs.\n\nThat book, we're going to assert that has been the correct title and the correct author. We tested both bad paths in the happy path. There's probably more we could have tested here. We could have tested the library, initialized it to make sure that the state was set up soundly. That gets a bit murky though, when you have private fields, generally a big no, no in testing is to avoid unprivate things that should be private. That means that you're probably testing wrong or something was structured wrong. So for the most part, this test is sound, this is the basic unit test. Integration tests, integration tests ensure that the interlocking pieces of your application work together as designed. Sometimes this means testing layers between classes, and sometimes this means testing layer between your database and application. So considering that this is the Realm user group, let's consider Realm as your object model and the database that we will be using and testing against.\n\nSo, we're going to switch some things around to work with Realm. It's not going to be radically different than what we had before, but it's going to be different enough that it's worth going over. So our book and library classes are going to inherit an object now, which is a Realm type that you inherit from so that you can store that type in the database. Everything is going to have our wonderful Abruzzi Syntex attached to it, which is going away soon, by the way, everyone, which is great. The library class has changed slightly and so far is that has a library ID now, which is a Uid generated initialization. It has a Realm list of available books and a Realm list of library members. Library member is another Realm object that has a member ID, which is a Uid generated on initialization.\n\nA list of borrowed books, as you can borrow books from the library and the member ID is the primary key there. We are going to change our borrow method on the library to work with Realm now. So it's still going to stick and it has been in a memberUid. This is mainly because we're slowly migrating to the world where the borrow function is going to get more complex. We're going to have a check here to make sure that the Realm is not invalidated. So every Realm object has an exposed Realm property on it that you can use. That is a Realm that is associated with that object. We're going to make sure that that's valid. We're going to check if the ISBAN exists within our available books list. If that passes, we're going to check that the member ID exists within our members list of library members. We're going to grab the book from the available books list. We're going to remove it from the available books list and we're going to return it to the color. As you can see, this actually isn't much different than the previous bit of code.\n\nThe main difference here is that we're writing to a Realm. Everything else is nearly the same, minor API differences. We're also going to add a return method to the library member class that is new. You should always return your library books. There's fines if you don't. So it's going to take a book and the library, we're going to, again, make sure that the Realm is not validated. We're going to make sure that our list of borrowed books because we're borrowing books from a library contains the correct book. If it does, we're going to remove it from our borrowed books list and we're going to append it back to the list of bell books in the library. So, what we're already doing here in these two methods is containing business logic. We're containing these things that actually change our data and in effect we'll eventually when we actually get to that part change the view.\n\nSo, let's test the borrow function now with Realm. Again, stepping through line by line, we're going to create an in-memory Realm because we don't actually want to store this stuff, we don't want state to linger between tests. We're going to open the Realm. We're going to create that Neuromancer book again. We're going to create a library member this time. We're going to create a library. We don't need to pass anything in this time as the state is going to be stored by the Realm and should be messed with from the appropriate locations, not necessarily on initialization, this is a choice.\n\nThis is not a mandate simplicity sake or a presentation. We're going to add that library to the Realm and we're going to, because there's no books in the library or members in the library assert that it's still froze that error. We don't have that book. Now, we're going to populate the library with the books in a right transaction. So, this is where Rome comes into play. We're going to try to borrow again, but because it doesn't have any members we're going to throw the air. Let's add members. Now we can successfully borrow the book with the given member and the given book, we're going to make sure that the ISBAN and title and author are sound, and that's it. It's nearly the same as the previous test.\n\nBut this is a super simple example and let's start including a view and figuring out how that plays in with your business logic and how Realm fits in all that. Testing business logic with Realm. Here's a really simple library view. There's two observed objects on it, a library and a library member. They should actually be observed Realm objects but it's not a perfect presentation. And so for each available book in the library, display a text for the title, a text for the author and a button to borrow the book. We're going to try to borrow, and do catch. If it succeeds, great. If it doesn't, we should actually show an error. I'm not going to put that in presentation code and we're going to tag the button with an identifier to be able to test against it later.\n\nThe main thing that we want to test in this view is the borrow button. It's the only thing that actually isn't read only. We should also test the read only things to make sure that the text user sound, but for again, second presentation, make sure that borrowing this book removes the book from the library and gives it to the member. So the thing that we at Realm have been talking about a lot recently is this MBI pattern, it meshes nicely with SwiftUI because of two-way data binding because of the simplicity of SwiftUI and the fact that we've been given all of the scaffolding to make things simpler, where we don't necessarily need few models, we don't necessarily need routers. And again, you might, I'm not mandating anything here, but this is the simplest way. And you can create a lot of small components and a lot of very clear methods on extensions on your model that make sure that this is fairly sound.\n\nYou have a user, the user has intent. They tap a button. That button changes something in the model. The model changes something in the view, the user sees the view fairly straightforward. It's a circular pattern, it's super useful in simpler circumstances. And as I found through my own dog fooding, in a new application, I can't speak to applications that have to migrate to SwiftUI, but in a new application, you can intentionally keep things simple regardless of the size of your code base, keep things small, keep your components small, create objects as you see fit, have loads of small functions that do exactly what they're supposed to do relative to that view, still a way to keep things simple. And in the case of our application, the user hits the borrow button. It's the tech button that we have.\n\nIt's going to borrow from the library from that function, that function is going to change our data. That data is going to be then reflected in the view via the Realm. The Realm is going to automatically update the view and the user's going to see that view. Fairly straightforward, fairly simple, again, works for many simple use cases. And yeah, so we're also going to add here a method for returning books. So it's the same exact thing. It's just for the member. I could have extracted this out, but wanted to show everybody it's the same thing. Member.borrowed books, texts for the title, text for the author, a return button with an accessibility identifier called return button that actually should have been used in the previous slide instead of tag. And that member is going to return that book to the library.\n\nWe also want to test that and for us in the case of the test that I'm about to show, it's kind of the final stage in the test where not only are we testing that we can borrow the book properly, but testing that we can back properly by pressing the borrow and return. So we're going to create a simple UI test here. The unit tests here that should be done are for the borrow and return methods. So, the borrow tests, we've already done. The return test, I'm going to avoid showing because it's the exact same as the borrow test, just in the case of the user. But having UI test is also really nice here because the UI in the case of MDI is the one that actually triggers the intent, they trigger what happens to the view model ... the view. Sorry, the model.\n\nIn the case of UI tests, it's actually kind of funky how you have to use it with Realm, you can't use your classes from the executable, your application. So, in the case of Realm, you'll actually have to not necessarily copy and paste, but you'll have to share a source file with your models. Realm is going to read those models and this is a totally different process. You have to think of it as the way that we're going to have to use Realm here is going to be a bit funky. That said, it's covered by about five lines of code.\n\nWe're going to use a Realm and the temporary directory, we're going to store that Realm path in the launch environment. That's going to be an environment variable that you can read from your application. I wouldn't consider that test code in your app. I would just consider it an injection required for a better structured application. The actual last line there is M stakes, everyone. But we're going to read that Realm from the application and then use it as we normally would. We're going to then write to that Realm from the rest of this test.\n\nAnd on the right there is a little gift of the test running. It clicks the borrow button, it then clicks the return button and moves very quickly and they don't move as slow as they used to move. But let's go over the test. So, we create a new library. We create a new library member. We create a new book. At the library, we add the member and we append the book to the library. We then launch the application. Now we're going to launch this application with all of that state already stored. So we know exactly what that should look like. We know that the library has a book, but the user doesn't have a book. So, UI testing with SwiftUI is pretty much the same as UI kit. The downside is that it doesn't always do what you expect it to do.\n\nIf you have a heavily nested view, sometimes the identifier isn't properly exposed and you end up having to do some weird things just for the sake of UI testing your application. I think those are actually bugs though. I don't think that that's how it's supposed to work, I guess keep your eyes peeled after WWDC. But yeah, so we're going to tap the borrow.button. That's the tag that you saw before? That's going to trigger the fact that that available book is going to move to the member, so that list is going to be empty. We're going to assert then that the library.members.first.borrowbooks.firststudy is the same as the book that has been.\n\nSo, the first and only member of the library's first and only book is the same as the book that we've injected into this application. We're then going to hit the return button, that's going to return the book to the library and run through that return function that you saw as an extension on the library member class. We're going to check that the library.members.borrowbooks is empty. So, the first and only member of the library no longer has a borrowed book and that the library.borrowbook at first, it has been the only available book in the library is the same as the book that we inject into the application state. Right. So, we did it, we tested everything, the application's great. We're invincible. We beat the game, we got the high score and that's it.\n\nBut what about more complex acts, you say? You can't convert your 50,000 line app that is under concentrator to these simple MVI design pattern now? It's really easy to present information in this really sterile, simple environment. It's kind of the nature of the beast when it comes to giving a presentation in the first place. And unfortunately, sometimes it can also infect the mind when coming up with features and coming up with ways to use Realm. We don't get to work with these crazy complex applications every day, especially ones that are 10 years old.\n\nOccasionally, we actually do get sent people's apps and it's super interesting for us. And we've got enough feedback at this point that we are trying to work towards having Realm be more integrated with more complex architectures. We don't want people to have to work around Realm, which is something we've seen, there are people that completely detach their app from Realm and use Realm as this dummy data store. That's totally fine, but there's often not a point at this point in doing something like that. There's so many better ways to use Realm that we want to introduce features that make it really obvious that you don't have to do some of these crazy things that people do. And yes, we have not completely lost our minds. We know there are more complex apps out there. So let's talk about MVVM.\n\nIt is just totally off the top of my head, not based on any factual truth and only anecdotal evidence, but it seems to be the most popular architecture these days. It is model view view model. So, the view gives commands to the view model, the view model updates the model, the view model reads from the model and it binds it to the view. I have contested in the past that it doesn't make as much sense with SwiftUI because it's two way data binding because what ends up happening with the models in SwiftUI is that you write from the view to the view model and then the view model just passes that information off to the model without doing anything to it. There's not really a transformation that generally happens between the view model and the model anymore. And then you have to then manually update the view, and especially with Realm where we're trying to do all that stuff for you, where you update the Realm and that updates the view without you having to do anything outside of placing a property wrapper on your view, it kind of breaks what we're trying to do.\n\nBut that said, we do understand that there is a nice separation here. And not only that, sometimes what is in your model isn't necessarily what you want to display on the view. Probably, more times than not, your model is not perfectly aligned with your view. What happens to you if you have multiple models wrong, doesn't support joins. More often than not, you have used with like a bunch of different pieces. Even in the example I showed, you have a library and you have a library member, somebody doing MVVM would want only a view model property and any like super simple state variables on that view. They wouldn't want to have their objects directly supplanted onto the view like that. They'd have a library view view model with a library member and a library. Or even simpler than that. They can take it beyond that and do just the available books and the borrowed books, since those are actually the only things that we're working with in that view.\n\nSo this is one thing that we've seen people do, and this is probably the simplest way to do view models with Realm. In this case, because this view specifically only handles available books and borrowed books, those are the things that we're going to read from the library and the library member. We're going to initialize the library view view model with those two things. So you're probably do that in the view before, and then pass that into the next view. You're going to assign the properties of that from the library available books and the member borrowed books, you're then going to observe the available books and observe the borrowed books because of the way that ... now that you're abstracting out some of the functionality that we added in, as far as observation, you're going to have to manually update the view from the view model.\n\nSo in that case, you're going to observe, you don't care, what's changing. You just care that there's change. You're going to send that to the object will change, which is a synthesized property on an observable object. That's going to tell the view, please update. Your borrow function is going to look slightly differently now. In this case, you're going to check for any available books, if the ISBAN exists you can still have the same errors. You're going to get the Realm off of the available books which, again, if the Realm has been invalidated or something happened, you are going to have to throw an error. You're going to grab the book out of the available books and you're going to remove it from the available books and then append it to the borrowed books in the right transaction from the Realm, and then return the book.\n\nSo, it's really not that different in this case. The return function, similarly, it does the opposite, but the same checks and now it even has the advantage of both of these are on the singular model associated with a view. And assuming that this is the only view that does this thing, that's actually not a bad setup. I would totally understand this as a design pattern for simplifying your view and not separating things too much and keeping like concepts together. But then we've seen users do some pretty crazy things, like totally map everything out of Realm and just make their view model totally Realm agnostic. I get why in certain circumstances this happens. I couldn't name a good reason why to do this outside of like there are people that totally abstract out the database layer in case they don't want to be tied to Realm.\n\nThat's understandable. We don't want people to be handcuffed to us. We want people to want to use us and assume that we will be around to continue to deliver great features and work with everyone to make building apps with Realm great. But we have seen this where ... Sure, you have some of the similar setup here where you're going to have a library and a library member, but you're going to save out the library ID and the member ID for lookup later. You're going to observe the Realm object still, but you're going to map out the books from the lists and put them into plain old Swift arrays.\n\nAnd then basically what you're going to end up doing is it's going to get a lot more complex or you're going to have to look up the primary keys in the Realm. You're going to have to make sure that those objects are still sound, you're then going to have to modify the Realm objects anyway, in a right transaction. And then you're going to have to re-map out the Realm lists back into their arrays and it gets really messy and it ends up becoming quintessential spaghetti code and also hard to test, which is the point of this presentation. So, this is not something we'd recommend unless there's good reason for it. So there's a big cancel sign for you.\n\nWe understand that there are infinite use cases and 1,000 design patterns and so many different ways that you can write code, these design patterns or social constructs, man. There's no quick and easy way to do this stuff. So we're trying to come up with ways to better fit in. And for us that's projections, this is a pre-alpha feature. It's only just been scoped out. We still have to design it fully. But this is from the prototype that we have now. So what is a projection? So in database land projection is when you grab a bunch of data from different sources and put it into a single structure, but it's not actually stored in the database. So, if I have a person and that person has a name and I have a dog and that dog has a name and I want to project those two names into a single structure I would have like a structure called person and dog name.\n\nI would do queries on the database to grab those two things. And in Mongo, there's a project operator that you can use to make sure that that object comes out with the appropriate fields and values. For us, it's going to look slightly different. At some point in the future, we would like a similar super loose projection syntax, where you can join across multiple objects and get whatever object you want back. That's kind of far future for us. So in the near future, we want to come up with something a bit simpler where you're essentially reattaching existing properties onto this new arbitrary structure. And arbitrary is kind of the key word here. It's not going to be directly associated with a single Realm object. It's going to be this thing that you associate with whatever you want to associate it with.\n\nSo if you want to associate it with the view, we've incidentally been working on sort of a view model for people then it becomes your view model. If the models are one-to-one with the view, you grab the data from the sources that you want to grab it from. And you stick it on that projection and associate that with the view. Suddenly, you have a view model. In this case, we have our library view view model, it inherits from the projection class or protocol, we're not sure yet. It's going to have two protective properties. It's going to have available books and borrowed books. These are going to be read directly from the library and member classes. These properties are going to be live. Think of this as a Realm object. This is effectively a Realm object with reattached successors.\n\nIt should be treated no differently, but it's much more flexible and lightweight. You can attach to anything on here, and you could attach the member IDs on here, if you had overdue fees and that was supposed to go on this view, you could attach overdue fees to it. There's things you can query. Right now we're trying to stick mainly to things that we can access with keypads. So, for those familiar with keypads, which I think was Swift 52.\n\nI can't remember which version of Swift it was, but it was a really neat feature that you can basically access a chain of keypads on an object and then read those properties out. The initial version of projections will be able to do that where that available books is going to be read from that library and any updates to the library, we'll update available books, same thing with borrowed books and library member. And it's going to have a similar borrow function that the other view model had, this case it's just slightly more integrated with Realm, but I think the code is nearly identical. Same thing with return code is nearly identical, slightly more integrated with Realm.\n\nAnd the view is nearly the same, except now you have the view model, sorry for some of the formatting there. In this case, you call borrow on the view model and you call return on the view model. It is very close to what we had. It's still a Realmy thing that's going to automatically update your view when any of the things that you have attached update so that if the library updates, if the user ... Oops, sorry, not user. If the member updates, if the books update, if the borrowed books update, that view is again going to automatically update. And now we've also created a single structure, which is easier to test or for you to test. Integration testing is going to be, again, very, very similar. The differences is that instead of creating a library and a member, creating we're also creating a library view model.\n\nWe're going to borrow from that, we're going to make sure that it throws the appropriate error. We're going to refill the state, mess with the state, do all the same stuff, except this time on view model. And now what we've done here is that if this is the only place where you need to return and borrow, we've created this nice standalone structure that does that for you. And it's associated with Realm, which means that it's closer to your model, since we are encouraging people to have Realm B of the model as a concept. Your testing is the exact same because this is a view associated thing and not actually a Realm object, you don't need to change these tests at all. They're the same. That's pretty much it. I hope I've left enough time for questions. I have not had a chance to look at the chat yet. \n\n**Ian:**\nI'm going to see to that, Jason, but thank you so much. I guess one of the comments here, Sebastian has never seen the objective C declaration that we have in our Realm models. Maybe tell them a little bit about the history there and then tell him what's in plan for future.\n\n**Jason:**\nSure. So, just looking at the question now, I've never used an ob C member. Obviously, members prevents you from having to put at ob C on all of your properties that need to use objective C reflection. The reason that you have to do that with Realm and Swift is because we need to take advantage of objective C reflection. It's the only way that we're able to do that. When you put that tag there, sorry, annotation. When you put that there, it gives objective C, the objective C runtime access to that property. And we still need that. However, in the future, we are going to be taking advantage of property wrappers to make it a much nicer, cleaner, more obvious with syntax. Also, it's going to have compile time checks. That's going to look like Swift instead of an ob C whatever. That is actually coming sooner than later. I hesitate to ever promise a date, but that one should be pretty, pretty soon.\n\n**Ian:**\nExcellent. Yeah, we're looking forward to being able to clean up those Realm model definitions to make it more swifty. Richard had a question here regarding if there's a recommendation or proper way to automate the user side input for some of the UI testing?\n\n**Jason:**\nUI testing, for UI test proper, is there a way to automate the user input side of the equation since you weren't going to? I'm not entirely sure what you mean, Richard. If you could explain a bit more.\n\n**Ian:**\nI mean, I think maybe this is about having variable input into what the user puts into the field. Could this also be maybe something around a fuzzer, having different inputs and testing different fields and how they accept certain inputs and how it goes through different tests?\n\n**Jason:**\nYeah. I mean, I didn't go over fuzz testing, but that's absolutely something that you should do. There's no automated mouse input on text input. You can automate some of that stuff. There's no mouse touch yet. You can touch any location on the screen, you can make it so that if you want to really, not load test is the wrong word, but batch up your application, just have it touch everywhere and see what happens and make sure nothing crashes, you could do that. It's actually really interesting if you have these UI tests. So yes, you can do that, Richard. I don't know if there's a set of best standards and practices, but at least with macOS, for instance, it was bizarre the first time I ran it. When you a UI test on macOS it actually completely takes control from your mouse, and it will go all over the screen wherever you tell it to and click anywhere. Obviously, on the iPhone simulator, it has a limited space of where it can touch, but yes, that can be automated. But I guess it depends on what you're trying to test.\n\n**Ian:**\nI guess, another question for me is what's your opinion on test coverage? I think a lot of people would look to have everything be unit tested, but then there's also integration tests. Should everything be having integration tests? And then end to end tests, there's kind of a big, a wide berth of different things you can test there. So, what's your opinion on how much coverage you should have for each type of test?\n\n**Jason:**\nThat's a tough question, because at Realm, I suppose we tell ourselves that there can never be too many tests, so we're up to us, every single thing would be tested within reason. You can't really go overkill unless you start doing weird things to your code to accommodate weird testing patterns. I couldn't give you a number as to what appropriate test coverage is. Most things I know for us at Realm, we don't make it so that every single method needs to be tested. So, if you have a bunch of private methods, those don't need to be tested, but for us, anything by the public API needs to be heavily tested, every single method and that's not an exaggeration. We're also a library. So in a UI application, you have to look at it a bit differently and consider what your public API is, which were UI applications, really the entry points to the model, any entry point that transforms data. And in my opinion, all of those should be tested. So, I don't know if that properly answers the question, for integration tests and end to end tests, same thing. What?\n\n**Ian:**\nYeah, I think so. I mean, I think it says where's your public API and then a mobile application, your public API is a lot of the UI interfaces that they can interact with and that's how they get into your code paths. Right?\n\n**Jason:**\nYeah.\n\n**Ian:**\nI guess another question from me, and this is another opinion question is what's your opinion on flaky tests? And so these are tests that sometimes pass sometimes fail and is it okay? A lot of times relate to, should we release, should we not release? Maybe you could give us a little bit of your thoughts on that.\n\n**Jason:**\nYeah. That's a tricky one because even on the Realm Cocoa, if you follow the pull requests, we still have a couple of flaky tests. To be honest, those tests are probably revealing some race condition. They could be in the test themselves though, which I think in the case of some of the recent ones, that was the case. More often flaky tests are revealing something wrong. I don't want to be on a recording thing that that's okay. But very occasionally, yes, you do have to look at a test and be like, \"Maybe this is specific to the testing circumstance,\" but if you're trying to come out with like the most high quality product, you should have all your tests passing, you should make sure that there's no race conditions, you should make sure that everything is clean cut sound, all that kind of thing.\n\n**Ian:**\nYeah. Okay, perfect. There's a final question here, will be docs on best practices for testing? I think we're looking as this presentation is a little bit of our, I wouldn't say docs, but a presentation on best practices for testing. It is something potentially in the future we can look to ask to our docs. So yeah, I think if we have other things covered, we can look to add testing best practices as well to our docs as well. And then last question here from Shane what are we hoping for for WWDC next week?\n\n**Jason:**\nSure. Well, just to add one thing to Ian's question, if there's ever a question that you or anybody else here has, feel free to ask on GitHub or forums or something like that. For things that we can't offer through API or features or things that might take a long time to work on, we're happy to offer guidance. We do have an idea of what those best practices are and are happy to share them. As far as WWDC, what we're hoping for is ..., yes, we should add more docs, Richard, sorry. There are definitely some things there that are got yous. But with WWDC next week, and this ties to best practices on multi-threading using Realm, we're hoping for a sync await which we've been playing with for a few weeks now. We're hoping for actors, we're hoping for a few minor features as well like property wrappers in function parameters and property rappers in pretty much Lambdas and everywhere. We're hoping for Sendable as well, Sendable will prevent you from passing unsafe things into thread safe areas, basically. But yeah, that's probably our main wishlist right now.\n\n**Ian:**\nWow. Okay. That's a substantial wishlist. Well, I hope you get everything you wish for. Perfect. Well, if there's no other questions, thank you so much, everyone, and thank you so much, Jason. This has been very informative yet again.\n\n**Jason:**\nThanks everyone for coming. I always have-\n\n**Ian:**\nThank you.\n\n**Jason:**\n... to thank everyone.\n\n**Ian:**\nBye.\n","description":"Jason Flax, Lead iOS Engineer, makes a return to explain how the testing landscape has changed for iOS apps using the new SwiftUI framework. Learn how to write unit tests with SwiftUI apps powered by Realm, where to put your business logic with either ViewModels or in an app following powered by Model-View-Intent, and witness the power of Realm's new Projection feature. ","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf6a3980e45514a9c/644c464d1fdb099939054c60/dev-tools-swift.png?branch=prod","description":null}}]},"slug":"/realm-meetup-swiftui-testing-and-realm-with-projections","title":"*Realm Meetup - SwiftUI Testing and Realm with Projections","original_publish_date":"2021-06-15T23:50:31.409Z","strapi_updated_at":"2023-03-21T16:11:47.223Z","expiry_date":"2022-06-15T23:28:54.569Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*iOS","calculated_slug":"/technologies/ios"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:32.065Z","publish_details":{"time":"2023-04-28T22:52:51.779Z"}}},{"calculated_slug":"/products/realm/adding-realm-as-dependency-ios-framework","content":"# Adding Realm as a Dependency to an iOS Framework\n\n\n## Introduction\n\nIn this post we’ll review how we can add RealmSwift as a dependency to our libraries, using two different methods: Xcode assistants and the Swift Package Manager.\n\n\n## The Problem \n\nI have a little, nice Binary Tree library. I know that I will use it for a later project, and that it'll be used at least in a macOS and iOS app. Maybe also a Vapor web app. So I decided to create a Framework to hold this code. But some of my model classes there need to be persisted in some way locally in the phone later. The Realm library is perfect for this, as I can start working with regular objects and store them locally and, later, if I need a ~~no code~~ really simple & quick to implement backend solution I can use Atlas Device Sync.\n\nBut the problem is, how do we add Realm as a dependency in our Frameworks?\n\n\n## Solution 1: Use Xcode to Create the Framework and Add Realm with SPM\n\nThe first way to create the Framework is just to create a new Xcode Project. Start Xcode and select `File > New > Project`. In this case I’ll change to the iOS tab, scroll down to the Framework & Library section, then select Framework. This way I can share this Framework between my iOS app and its extensions, for instance.\n\n![Choosing a template for the project](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/realm_dependency_framework_3_38b52507fe.png)\n\nNow we have a new project that holds our code. This project has two targets, one to build the Framework itself and a second one to run our Unit Tests. Every time we write code we should test it, but this is especially important for reusable code, as one bug can propagate to multiple places.\n\nTo add Realm/Swift as a dependency, open your project file in the File Navigator. Then click on the Project Name and change to the Swift Packages tab. Finally click on the + button to add a new package. \n\n![Adding Realm as a Package to the Project, step by step](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/add_realm_swift_framework_c6525c705c.gif)\n\nIn this case, we’ll add Realm Cocoa, a package that contains two libraries. We’re interested in  Realm Swift: [https://github.com/realm/realm-cocoa](https://github.com/realm/realm-cocoa). We want one of the latest versions, so we’ll choose “Up to major version” 10.0.0. Once the resolution process is done, we can select RealmSwift.\n\nNice! Now that the package is added to our Framework we can compile our code containing Realm Objects without any problems!\n\n\n## Solution 2: create the Framework using SPM and add the dependency directly in Package.swift\n\nThe other way to author a framework is to create it using the Swift Package Manager. We need to add a Package Manifest (the Package.swift file), and follow a certain folder [structure](https://developer.apple.com/documentation/xcode/creating_a_standalone_swift_package_with_xcode). We have two options here to create the package:\n\n\n\n*   Use the Terminal\n*   Use Xcode\n\n\n### Creating the Package from Terminal\n\n\n\n*   Open Terminal / CLI\n*   Create a folder with `mkdir yourframeworkname`\n*   Enter that folder with `cd yourframeworkname`\n*   Run `swift package init`\n*   Once created, you can open the package with `open Package.swift`\n\n    \n![Terminal showing the results of running above commands](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/realm_dependency_framework_2_b01503b34b.png)\n\n\n### Creating the Package using Xcode\n\nYou can also use Xcode to do all this for you. Just go to `File > New > Swift Package`, give it a name and you’ll get your package with the same structure.\n\n\n![Xcode showing the newly created Package.swift](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/realm_dependency_framework_1_146e545d92.png)\n\n\n\n### Adding Realm as a dependency\n\nSo we have our Framework, with our library code and we can distribute it easily using Swift Package Manager. Now, we need to add Realm Swift. We don’t have the nice assistant that Xcode shows when you create the Framework using Xcode, so we need to add it manually to `Package.swift`\n\nThe complete `Package.swift` file\n\n\n```swift\nlet package = Package(\n    name: \"BinaryTree\",\n    platforms: [\n        .iOS(.v14)\n    ],\n    products: [\n        // Products define the executables and libraries a package produces, and make them visible to other packages.\n        .library(\n            name: \"BinaryTree\",\n            targets: [\"BinaryTree\"]),\n    ],\n    dependencies: [\n        // Dependencies declare other packages that this package depends on.\n        .package(name: \"Realm\", url: \"https://github.com/realm/realm-cocoa\", from: \"10.7.0\")\n    ],\n    targets: [\n        // Targets are the basic building blocks of a package. A target can define a module or a test suite.\n        // Targets can depend on other targets in this package, and on products in packages this package depends on.\n        .target(\n            name: \"BinaryTree\",\n            dependencies: [.product(name: \"RealmSwift\", package: \"Realm\")]),\n        .testTarget(\n            name: \"BinaryTreeTests\",\n            dependencies: [\"BinaryTree\"]),\n    ]\n)\n```\n\n\nHere, we declare a package named “BinaryTree”, supporting iOS 14\n\n\n```swift\nlet package = Package(\n    name: \"BinaryTree\",\n    platforms: [\n        .iOS(.v14)\n    ],\n```\n\n\nAs this is a library, we declare the products we’re going to build, in this case it’s just one target called `BinaryTree`.\n\n\n```swift\nproducts: [\n        // Products define the executables and libraries a package produces, and make them visible to other packages.\n        .library(\n            name: \"BinaryTree\",\n            targets: [\"BinaryTree\"]),\n    ],\n```\n\n\nNow, the important part: we declare Realm as a dependency in our library. We’re giving this dependency the short name “Realm” so we can refer to it in the next step.\n\n\n```swift\ndependencies: [\n        // Dependencies declare other packages that this package depends on.\n        .package(name: \"Realm\", url: \"https://github.com/realm/realm-cocoa\", from: \"10.7.0\")\n    ],\n```\n\n\nIn our target, we use the previously defined `Realm` dependency.\n\n\n```swift\n.target(\n            name: \"BinaryTree\",\n            dependencies: [.product(name: \"RealmSwift\", package: \"Realm\")]),\n```\n\n\nAnd that’s all! Now our library can be used as a Swift Package normally, and it will include automatically Realm.\n\n\n## Recap\n\nIn this post we’ve seen different ways to create a Framework, directly from Xcode or as a Swift Package, and how to add `Realm` as a dependency to that Framework. This way, we can write code that uses Realm and distribute it quickly using SPM. \n\nIn our next post in this series we’ll document this library using the new Documentation Compiler (DocC) from Apple. Stay tuned and thanks for reading!\n\nIf you have questions, please head to our [developer community website](https://developer.mongodb.com/) where the Realm engineers and the Realm/MongoDB community will help you build your next big idea with Realm and MongoDB.\n","description":"Adding Realm to a Project is how we usually work. But sometimes we want to create a Framework (could be the data layer of a bigger project) that uses Realm. So... how do we add Realm as a dependency to said Framework? ","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt788d5b71223935b3/644c4643bc3f0a0307815104/realm-logo.jpg?branch=prod","description":null}}]},"slug":"/adding-realm-as-dependency-ios-framework","title":"*Adding Realm as a dependency to an iOS Framework","original_publish_date":"2021-06-29T07:19:48.335Z","strapi_updated_at":"2022-09-02T14:31:08.329Z","expiry_date":"2022-06-17T12:51:24.184Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*iOS","calculated_slug":"/technologies/ios"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:31.659Z","publish_details":{"time":"2023-04-28T22:52:51.815Z"}}},{"calculated_slug":"/products/realm/realm-meetup-javascript-react-native","content":"Didn't get a chance to attend the Realm JavaScript for React Native applications Meetup? Don't worry, we recorded the session and you can now watch it at your leisure to get you caught up.\n\n:youtube[]{vid=6nqMCAR_v7U}\n\nIn this event, recorded on June 10th, Andrew Meyer, Software Engineer, on the Realm JavaScript team, walks us through the React Native ecosystem as it relates to persisting data with Realm. We discuss things to consider when using React Native, best practices to implement and gotcha's to avoid, as well as what's next for the JavaScript team at Realm.\n\nIn this 55-minute recording, Andrew spends about 45 minutes presenting \n\n- React Native Overview & Benefits\n\n- React Native Key Concepts and Architecture\n\n- Realm Integration with React Native\n\n- Realm Best Practices / Tips&Tricks with React Native\n\nAfter this, we have about 10 minutes of live Q&A with Ian & Andrew and our community . For those of you who prefer to read, below we have a full transcript of the meetup too. \n\nThroughout 2021, our Realm Global User Group will be planning many more online events to help developers experience how Realm makes data stunningly easy to work with. So you don't miss out in the future, join our [Realm Global Community](https://live.mongodb.com/realm-global-community/) and you can keep updated with everything we have going on with events, hackathons, office hours, and (virtual) meetups. Stay tuned to find out more in the coming weeks and months.\n\nTo learn more, ask questions, leave feedback, or simply connect with other Realm developers, visit our [community forums](https://developer.mongodb.com/community/forums/c/realm-sdks/58). Come to learn. Stay to connect.\n\n### Transcript\n(*As this is verbatim, please excuse any typos or punctuation errors!*)\n\n**Ian:**\nI'm Ian Ward. I'm a product manager that focuses on the Realm SDKs. And with me today, I'm joined by Andrew Meyer, who is an engineer on our React Native development team, and who is focusing on a lot of the improvements we're looking to make for the React Native SDK in the future. And so I just went through a few slides here to just kick it off. So we've been running these user group sessions for a while now, we have some upcoming meetups next week we are going to be joined by a AWS engineer to talk about how to integrate MongoDB Realm, our serverless platform with AWS EventBridge. A couple of weeks after that, we will also be joined by the Swift team to talk about some of the new improvements they've made to the SDK and developer experience. So that's key path filtering as well as automatic open for Realms.\n\nWe also have MongoDB.live, which is happening on July 13th and 14th. This is a free virtual event and we will have a whole track set up for Realm and mobile development. So if you are interested in mobile development, which I presume you are, if you're here, you can sign up for that. No knowledge or experience with MongoDB is necessary to learn something from some of these sessions that we're going to have.\n\nA little bit of housekeeping here. So we're using this Bevy platform. You'll see, in the web view here that there's a chat. If you have questions during the program, feel free to type in the question right there, and we'll look to answer it if we can in the chat, as well as we're going to have after Andrew goes through his presentation, we're going to have a Q&A session. So we'll go through some of those questions that have been accumulating across the presentation. And then at the end you can also ask other questions as well. We'll go through each one of those as well. If you'd like to get more connected we have our developer hub. This is our developer blog, We post a bunch of developer focused articles there. Please check that out at developer.mongodb.com, many of them are mobile focus.\n\nSo if you have questions on Swift UI, if you have questions on Kotlin multi-platform we have articles for you. If you have a question yourself come to forums.realm.io and ask a question, we patrol that regularly and answer a lot of those questions. And of course our Twitter @realm please follow us. And if you're interested in getting Swag please tweet about us. Let us know your comments, thoughts, especially about this program that you're watching right now. We would love to give away Swag and we'd love to see the community talk about us in the Twitter sphere. And without further ado, I'll stop sharing my screen here and pass it over to Andrew. Andrew, floor's yours.\n\n**Andrew:**\nHello. Just one second, I'll go find my slides. Okay. I think that looks good. So hello. My name is Andrew Meyer, I'm a software engineer at MongoDB on the Realm-JS team, just joined in February. I have been working with React Native for the past four years. In my last job I worked for HORNBACH, which is one of the largest hardware stores in Germany making the HORNBACH shopping app. It also allowed you to scan barcodes in the store and you could fill up a cart and checkout in the store and everything. So I like React Native, I've been using it as I said for four years and I'm excited to talk to you all about it. So my presentation is called Realm JavaScript for React Native applications. I'm hoping that it inspires you if you haven't used React Native to give it a shot and hopefully use realm for your data and persistence.\n\nLet's get started. So my agenda today, I'm going to go over React Native. I'm also going to go over some key concepts in React. We're going to go over how to integrate Realm with React Native, some best practices and tips when using Realm with React Native. And I'm also going to go over some upcoming changes to our API. So what is React Native? I think we've got a pretty mixed group. I'm not sure how many of you are actually React Native developers now or not. But I'm going to just assume that you don't know what React Native is and I'm going to give you a quick overview. So React Native is a framework made from Facebook. It's a cross platform app development library, you can basically use it for developing both Android and iOS applications, but it doesn't end there; there is also the ability to make desktop applications with React Native windows and React Native Mac OS.\n\nIt's pretty nice because with one team you can basically get your entire application development done. As it is written in JavaScript, if your backend is written in Node.JS, then you don't have a big context switch from jumping from front end development to back end development. So at my last job I think a lot of us started as front end developers, but by the end of a couple of years, we basically were full stack developers. So we were constantly going back and forth from front end to backend. And it was pretty easy, it's really a huge context switch when you have to jump into something like Ruby or Java, and then go back to JavaScript and yeah, it takes more time. So basically you just stay in one spot, but when you're using a JavaScript for the full stack you can hop back and forth really fast.\n\nAnother cool feature about React Native is fast refresh. This was implemented a few years ago, basically, as you develop your code, you can see the changes real time in your simulator, actually on your hardware as well. It can actually handle multiple simulators and hardware at the same time. I've tested Android, iOS phones in multiple languages and sizes and was able to see my front end changes happen in real time. So that's super useful if you've ever done a native development in iOS or an Android, you have to compile your changes and that takes quite a bit of time.\n\nSo this is an example of what a component looks like in React Native. If you're familiar with HTML and CSS it's really not a big jump to use React Native, basically you have a view and you apply styles to it, which is a JavaScript object that looks eerily similar to CSS except that it has camelCase instead of dash-case. One thing is if you do use React Native, you are going to want to become a very big friend of Flexbox. They use Flex quite adamantly, there's no CSS grid or anything like that, but I've been able to get pretty much anything I need to get done using Flexbox. So this is just a basic example of how that looks.\n\nSo, we're going to move on to React. So the React portion of React Native; it is using the React framework under the hood which is a front end web development framework. Key stomped concepts about React are JSX, that's what we just saw over here in the last example, this is JSX basically it's HTML and JavaScript. It resolves to basically a function call that will manipulate the DOM. If you're doing front end development and React Native world, it's actually going to bridge over into objective C for iOS and Java for Android. So that's one concept of it. The next is properties, pretty much every component you write is going to have properties and they're going to be managed by state. State is very important too. You can make basically say a to-do list and you need to have a state that's saving all its items and you need to be able to manipulate that. And if you manipulate that state, then it will re-render any changes through the properties that you pass down to sub components. And I'll show you an example of that now.\n\nSo this is an example of some React code. This is basically just a small piece of text to the button that allows you to change it from lowercase to uppercase. This is an example of a class component. There's actually two ways that you can make components in React, class components and functional components. So this is an example of how you do it with a class component, basically you make an instructor where you set your initial state then you have a rendering function that returns JSX. So this JSX reacts on that state. So in this case, we have a toUpper state with just a Boolean. If I change this toUpper Boolean to true, then that's going to change the text property that was passed in to uppercase or lowercase. And that'll be displayed here in the text. To set that state, I call this dot set state and basically just toggle that Boolean from true to false or false to true, depending on what state it's in.\n\nSo, as I said, this is class components. There's a lot more to this. Basically there's some of these life cycle methods that you had to override. You could basically before your component is mounted, make a network request and maybe initiate your state with some of that data. Or if you need to talk to a database that's where you would handle that. There's also a lot of... Yeah, the lifecycle methods get pretty confusing and that's why I'm going to move on to functional components, which are quite simpler. Before we could use this, but I think three years ago React introduced hooks, which is a way that we can do state management with functional programming. This gets rid of all those life cycle methods that are a bit confusing to know what's happening when.\n\nSo this is an example of what a functional component looks like. It's a lot less code, your state is actually being handled by a function and this function is called useState. Basically, you initialize it with some state and you get back that state and a function to set that state with. So in this case, I can look at that toUpper Boolean here and call this function to change that state. I want to go back real quick, that's how it was looking before, and that's how it is now. So I'm just going to go quickly through some of the hooks that are available to you because these are pretty much the basics of what you need to work with React and React Native. So as I talked about useState before this is just an example of showing a modal, but it's not too different than changing the case of a text.\n\nSo basically you'd be able to press a button and say, show this modal, you pass that in as a property to your modal. And you could actually pass that set modal, visible function to your modal components so that something inside of that modal can close that. And if you don't know what the modal is, it's basically an overlay that shows up on top of your app.\n\nSo then the next one is called useEffect. This is basically going to replace all your life cycle methods that I talked about before. And what you can do with useEffect is basically subscribe to changes that are happening. So that could be either in the state or some properties that are being passed down. There's an array at the end that you provide with the dependencies and every time something changes this function will be called. In this case, it's just an empty array, which basically means call this once and never call it again. This would be if you need to initialize your state with some data that's stored in this case in persistent storage then you'd be able to get that data out and store it to your state. We're going to see a lot more of this in the next slides.\n\nUseContext is super useful. It's a bit confusing, but this is showing how to use basically a provider pattern to apply a darker light mode to your application. So basically you would define the styles that you want to apply for your component. You create your context with the default state and that create gives you a context that you can call the provider on, and then you can set that value. So this one's basically overriding that light with dark, but maybe you have some sort of functionality or a switch that would change this value of state and change it on the fly. And then if you wrap your component or your entire application with this provider, then you can use the useContext hook to basically get that value out.\n\nSo this could be a very complex app tree and some button way deep down in that whole tree structure that can just easily get this theme value out and say, \"Okay, what am I, dark or light?\" Also, you can define your own hooks. So if you notice that one of your components is getting super complex or that you created a use effect that you're just using all over the place, then that's probably a good chance for you to do a little bit of dry coding and create your own hooks. So this one is basically one that will check a friend status. If you have some sort of chat API, so you'd be able to subscribe to any changes to that. And for trends it's Boolean from that to let you know that friends online or not. There's also a cool thing about useEffect. It has a tear down function. So if that component that's using this hook is removed from the tree, this function will be called so that those subscription handlers are not going to be called later on.\n\nA couple of other hooks useCallback and useMemo. These are a bit nice, this is basically the concept of memorization. So if you have a component that's doing some sort of calculation like averaging an array of items and maybe they raise 5,000 items long. If you just call the function to do that in your component, then every time your component got re-rendered from a state change, then it would do that computation again, and every single time it got re-rendered. You actually only want to do that if something in that array changes. So basically if you use useMemo you can provide dependencies and then compute that expensive value. Basically it helps you not have an on performance app.\n\nUseCallback is similar, but this is on return to function, this function will basically... Well, this is important because if you were to give a function to a component as a property, and you didn't call useCallback to do that, then every time that function re-rendered any component that was using that function as a property would also be re-rendered. So this basically keeps that function reference static, basically makes sure it doesn't change all the time. We're going to go into that a little bit more on the next slides. UseRef is also quite useful in React Native. React Native has to sometimes have some components that take advantage of data features on your device, for instance, the camera. So you have a camera component that you're using typically there might be some functions you want to call on that component. And maybe you're not actually using properties to define that, but you actually have functions that you can call in this case, maybe something that turns the flashlight on.\n\nIn that case, you would basically define your reference using useRef and you would be able to basically get a reference from useRef and you can the useRef property on this component to get a reference of that. Sorry, it's a bit confusing. But if you click this button, then you'd be able to basically call function on that reference. Cool. And these are the rest of the hooks. I didn't want to go into detail on them, but there are other ones out there. I encourage you to take a look at them yourselves and see what's useful but the ones that I went through are probably the most used, you get actually really far with useState, useEffect, useContext.\n\nSo one thing I want to go over that's super important is if you're going to use a JavaScript object for state manipulation. So, basically objects in JavaScript are a bit strange. I'll go up here. So if I make an object, like say, we have this message here and I changed something on that object and set that state with that changed object, the reference doesn't change. So basically react doesn't detect that anything changed, it's not looking at did the values inside this object change it's actually looking at is the address value of this object different. So going back to that, let me go back to the previous slide. So basically that's what immutability is. Sorry, let me get back here. And the way we fix that is if you set that state with an object, you want to make a copy of it, and if you make a copy, then the address will change and then the state will be detected as new, and then you'll get your message where you rendered.\n\nAnd you can either use object data sign, but basically the best method right now is to use the spread operator and what this does is basically take all the properties of that object and makes a copy of them here and then overrides that message with the texts that you entered into this text input. Cool. And back to that concept of memorization. There's actually a pretty cool function from React to that, it's very useful. When we had class components, there used to be a function you could override that compared the properties of what was changing inside of your component. And then you would be able to basically compare your previous properties with your next properties and decide, should I re-render this or not, should I return false, it's going to re-render. If you return true, then it's just going to stay the same.\n\nTo do that with functional components, we get a function called the React.memo. Basically, if you wrap your component React.memo it's going to automatically look at those base level properties and just check if they're equal to each other. With objects that becomes a little bit problematic, if it's just strings and Booleans, then it's going to successfully pull that off and only re-render that component if that string changes or that Boolean changes. So if you do wrap this and you're using objects, then you can actually make a function called... Well, in this case, it's equal or are equal, which is the second argument of this memo function. And that will give you the access to previous prompts and next prompts. So if you're coming into the hooks world and you already have a class component, this is a way to basically get that functionality back. Otherwise hooks is just going to re-render all the time.\n\nSo I have an example of this right here. So basically if I have in like my example before that text input if I wrap that in memo and I have this message and my setMessage, you state functions past in here, then this will only re-render if those things change. So back to this, the setMessage, if this was defined by me, not from useState, this is something that you definitely want to wrap with useCallback by the way making sure that this doesn't potentially always re-render, just like objects, functions are also... Actually functions in JavaScript are objects. So if you change a function or re-render the definition of a function, then its address is going to change and thus your component is going to be unnecessarily re-rendering.\n\nSo let's see if there's any questions at the moment, nothing. Okay, cool. So that brings us to Realm. Basically, how do you persist state? We have our hooks, like useState that's all great, but you need a way to be able to save that state and persist it when you close your app and open it again. And that's where Realm comes in. Realm has been around for 10 years, basically started as an iOS library and has moved on to Native Android and .NET and React Native finally. It's a very fast database it's actually written in C++, so that's why it's easily cross-platform. Its offline first, so most data that you usually have in an application is probably going to be talking to a server and getting that data off that, you can actually just store the data right on the phone.\n\nWe actually do offer a synchronization feature, it's not free, but if you do want to have cloud support, we do offer that as well. I'm not going to go over that in this presentation, but if that's something that you're really interested, in I encourage you to take a look at that and see if that's right for your application. And most databases, you have to know some sort of SQL language or some sort of query language to do anything. I don't know if anybody has MySQL or a PostgreSQL. Yeah, that's how I started learning what the DOM is. And you don't need to know a query language to basically use Realm. It's just object oriented data model. So you'll be using dots and texts and calling functions and using JavaScript objects to basically create and manipulate your data. It's pretty easy to integrate, if you want to add Realm to your React Native application either use npm or Yarn, whatever your flavor is to install Realm, and then just update your pods.\n\nThis is a shortcut, if anybody wanted to know how to install your pods without jumping into the iOS directory, if you're just getting to React Native, you'll know what I'm talking about later. So there's a little bit of an introduction around, so basically if you want to get started using Realm, you need to start modeling your data. Realm has schemas to do that, basically any model you have needs to have a schema. You provide a name for that schema, you define properties for this. Properties are typically defined with just a string to picking their type. This also accepts an object with a few other properties. This would be, if we were using the objects INTAX, then this would be type colon object ID, and then you could also make this the primary key if you wanted to, or provide some sort of default value.\n\nWe also have a bit of TypeScript support. So here's an example of how you would define a class using this syntax to basically make sure that you have that TypeScript support and whatever you get back from your Realm queries is going to be properly typed. Basically, so this is example of a journal from my previous schema definition here. And what's important to notice is that you have to add this exclamation point, basically, this is just telling TypeScript that something else is going to be defining how these properties are being set, which Realm is going to be doing for you. It's important to know that Realm objects, their properties are actually pointers to a memory address. So those will be automatically propagated as soon as you connect this to Realm.\n\nIn this example, I created a generate function. This is basically just a nice syntax, where if you wanted to basically define an object that you can use to create Realm objects you can do that here and basically provide some values, you'll see what I mean in a second how that works. So once you have your schema defined then you can put that into a configuration and open the Realm, and then you get this Realm object. When you create a Realm, then it's going to actually create that database on your phone. If you close it, then it'll just make sure that that's saved and everything's good to go. So I'm going to show you some tips on how to keep that open and close using hooks here in a second.\n\nAnother thing that's pretty useful though, is when you're starting to get getting started with defining your definitions, your schema definitions, you're getting started with your app, it's pretty useful to put this deleteRealmMigrationNeeded to true. Basically that's if you're adding new properties to your Realm in development, and it's going to yell at you because it needs to have a migration path. If you've put this to true, then it's just going to ignore that, it's going to delete all that data and start from scratch. So this is pretty useful to have in development when you're constantly tweaking changes and all that to your data models.\n\nHere's some examples about how you can basically create, edit and delete anything in Realm. So that Realm object that you get basically any sort of manipulation you have to put inside of a right transaction that basically ensures that you're not going to have any sort of problems with concurrency. So if I do realm.write that takes a call back and within that callback, you can start manipulating data. So this is an example of how I would create something using that journal class. So if I give this thing, that journal class, it's going to actually already be typed for me, and I'm going to call that generate function. I could actually just give this a plain JavaScript object as well. And if I provide this journal in the front then it'll start type checking that whatever I'm providing as a second argument.\n\nIf you want to change anything, say that display journal in this case, it's just the journal that I'm working on in some component, then if I wrap this in a right transaction, I can immediately manipulate that that property and it'll automatically be written to the database. I'll show you how to manage state with that in a second because it's a bit tricky. And then if you want to delete something, then basically you just provide what's coming back from realm.object creation into this, or realm.query into this delete function and then it'll remove that from the database. In this example, I'm just grabbing a journal by the ID primary key.\n\nAnd last but not least how to read data. There's two main functions I basically use to get data out of Realm, one is using realm.objects. Oops, I have a little bit of code there. If you call realm.objects and journal and forget about this filtered part basically it'll just get everything in the database for that model that you defined. If you want to filter it by something, say if you have an author field and it's got a name, then you could say, I just want everything that was authored by Andrew then this filter would basically return a model that's filtered and then you can also sort it. But you can chain these as well as you see, you can just be filtered or realm.object.filtered.sorted, that'd be the better syntax, but for readability sake, I kept it on one line. And if you want to get a single object, you can use object for primary and provide that ID.\n\nSo I'm going to go through a few best practices and tips to basically combine this knowledge of Realm and hooks, it's a lot, so bear with me. So if you have an app and you need to access Realm you could either use a singleton or something to provide that, but I prefer to make sure to just provide it once and I found that using useContext is the best way to do that. So if you wanted to do that, you could write your own Realm provider, basically this is a component, it's going to be wrapping. So if you make any sort of component, that's wrapping other components, you have to give children and you have to access the children property and make sure that what you're returning is implementing those children otherwise you won't have an app it'll just stop here. So this Realm provider is going to have children and it's going to have a configuration just like where you defined in the previous slide.\n\nAnd basically I have a useEffect that basically detects changes on the configuration and opens the Realm and then it sets that Realm to state and adds that to that provider value. And then if you do that, you'll be able to use that useContext to retrieve that realm at any point in your app or any component. So if you wrap that component with Realm provider, then you'll be able to get that Realm. I would recommend making a hook for this called useRealm or something similar where you can have error checking and any sort of extra logic that you need when you're accessing that Realm here and have that return that context for you to use.\n\nSo another thing, initializing data. So if you have a component and it's the very first time your app is opened you might want to initialize it with some data. The way I recommend doing that is making an effect for it, basically calling realm.objects and setting that to your state, having this useEffect listen for that state and just check, do we have any entries? If we don't have any entries then I would initialize some data and then set that journal up. So going on the next slide. And another very important thing is subscribing to changes. Yeah, basically if you are making changes to your collection in Realm, it's not going to automatically re-render. So I recommend using useState to do that and keeping a copy of that realm.object in state and updating with set state. And basically all you need to do is create an effect with a handle change function. This handle change function can be given to these listeners and basically it will be called any time any change happens to that Realm collection.\n\nYou want to make sure though that you do check if there are any modifications before you start setting state especially if you're subscribing to changes to that collection, because you could find yourself into an infinite loop. Because as soon as you call ad listener, there will be an initial event that fires and the length of all the changes is zero. So this is pretty important, make sure you check that there actually are changes before you set that state. So here's an example of basically providing or using a FlatList to display around data. FlatList is one of the main components from React Native that I've used to basically display any list of data. FlatList basically takes an array of data, in our case, it'll also take a Realm collection, which is almost an array. It works like an array. So it works in this case. So you can provide that collection.\n\nI recommend sorting it because one thing about Realm collections is the order is not guaranteed. So you should sort it by some sort of timestamp or something to make sure that when you add new entries, it's not just showing up in some random spot in the list. It's just showing up in this case at the creation date. And then it's also recommended to use a key extractor and do not set it to the index of the array. That's a bad idea, set it to something that is that's unique. In this case, the idea that we were using for our Realm is object ID, in the future we'll have a UUID property coming out, but in the meantime, object ID is our best option for providing that for you to have basically a unique ID that you can define your data with. And if you use that, I recommend using that. You can call it the two check string function on here because key extractor wants a string. He's not going to work with an object. And then basically this will make sure that your items are properly rendered and not rerunning the whole list all the time.\n\nAlso, using React.memo is going to help with that as well, which I'm going to show you how to do that. This item in this case is actually a React.memo. I recommend instead of just passing that whole item as a property to maybe just get what you need out of it and passing that down and that way you'll avoid any necessary re-renders. I did intentionally put a mistake in here. ID is an object, so you will have to be careful if you do it like this and I'll show you how that's done. you could just set it to string and then you wouldn't have to provide this extra function that on purpose I decided to put the object and to basically show you how you can check the properties and, and update this. So this is using React.memo and basically it will only render once. It will only render if that title changes or if that ID changes, which it shouldn't change.\n\nBasically, this guy will look at is title different? Are the IDs different? If they're not return true, if any of them changed return false. And that'll basically cause a re-render. So I wrote quite a bit of sample code to basically make these slides, if you want to check that out, my GitHub is Takameyer, T-A-K-A-M-E-Y-E-R. And I have a Realm and React Native example there. You can take a look there and I'll try to keep that updated with some best practices and things, but a lot of the sample code came from there. So I recommend checking that out. So that's basically my overview on React and Realm. I'll just want to take an opportunity to show up what's coming up for these upcoming features. Yeah, you just saw there was quite a lot of boiler plate in setting up those providers and schemas and things.\n\nAnd yeah, if you're setting up TypeScript types, you got to set up your schemers, you got to set up your types and you're doing that in multiple places. So I'm going to show you some things that I'm cooking up in the near future. Well, I'm not sure when they're coming out, but things that are going to make our lives a little bit easier. One goal I had is I wanted to have a single source of truth for your types. So we are working on some decorators. This is basically a feature of a JavaScript. It's basically a Boolean that you have to hit in TypeScript or in Babel to get working. And basically what that does is allow you to add some more context to classes and properties on that class. So in this case this one is going to allow you to define your models without a schema. And to do that, you provide a property, a decorator to your attributes. And that property is basically as an argument taking those configuration values I talked about before.\n\nSo basically saying, \"Hey, this description is a type string, or this ID is primary key and type object ID.\" My goal eventually when TypeScript supports it, I would like to infer the types from the TypeScript types that you're defining here. So at the moment we're probably going to have to live with just defining it twice, but at least they're not too far from each other and you can immediately see if they're not lining up. I didn't go over relations, but you can set up relations between Realms models. And that's what I'm going to revive with this link from property, this is bit easier, send texts, get that done. You can take a look at our documentation to see how you do that with normal schemas. But basically this is saying I'm linking lists from todoLists because a TodoItem on the items property from todoList link from todoList items, reads kind of nice.\n\nYeah, so those are basically how we're going to define schemas in the future. And we're also going to provide some mutator functions for your business logic in your classes. So basically if you define the mutator, it'll basically wrap this in a right transaction for you. So I'm running out of time, so I'm just going to go for the next things quick. We have Realm context generator. This is basically going to do that whole provider pattern for you. You call createRealmContext, give it your schemas, he's going to give you a context object back, you can call provider on that, but you can also use that thing to get hooks. I'm going to provide some hooks, so you don't have to do any sort of notification handling or anything like that. You basically call the hook on that context. You give it that Realm class.\n\nAnd in this case use object, he's just going to be looking at the primary key. You'll get that object back and you'll be able to render that and display it and subscribe to updates. UseQuery is also similar. That'll provide a sorting and filter function for you as well. And that's how you'd be able to get lists of items and display that. And then obviously you can just call, useRealm to get your Realm and then you can do all your right transactions. So that's coming up and that's it for me. Any questions?\n\n**Ian:**\nYeah. Great. Well, thank you, Andrew. We don't have too many questions, but we'll go through the ones we have. So there's one question around the deleteRealmIfMigrationNeeded and the user said this should only be used in dev. And I think, yes, we would agree with that, that this is for iterating your schema while you're developing your application. Is that correct Andrew?\n\n**Andrew:**\nYeah, definitely. You don't want to be using that in production at all. That's just for development. So Yeah.\n\n**Ian:**\nDefinitely. Next question here is how has Realm integrated with static code analyzers in order to give better dev experience and show suggestions like if a filtered field doesn't exist? I presume this is for maybe you're using Realm objects or maybe using regular JavaScript objects and filtered wouldn't exist on those, right? It's the regular filter expression.\n\n**Andrew:**\nYeah. If you're using basically that syntax I showed to you, you should still see the filtered function on all your collections. If you are looking at using filtered in that string, we don't have any sort of static analysis for those query strings yet, but definitely for the future, we could look at that in the future.\n\n**Ian:**\nYeah, I think the Vs code is definitely plugin heavy. And as we start to replatform the JavaScript SDK, especially for React Native, some of these new features that Andrew showed we definitely want to get into creating a plugin that is Realm specific. That'll help you create some of your queries and give you suggestions. So that's definitely something to look forward to in the future. Please give us feedback on some of these new features and APIs that you're looking to come out with, especially as around hooks? Because we interviewed quite a few users of Realm and React Native, and we settled on this, but if you have some extra feedback, we are a community driven product, so please we're looking for the feedback and if it could work for you or if it needed an extra parameter to take your use case into account, we're in the stage right now where we're designing it and we can add more functionality as we come.\n\nSome of the other things we're looking to develop for the React Native SDK is we're replatforming it to take advantage of the new Hermes JavaScripts VM, right interpreter, so that not just using JavaScript core, but also using Hermes with that. Once we do that, we'll also be able to get a new debugger right now, the debugging experience with Realm and React Native is a little bit... It's not great because of the way that we are a C++ database that runs on the device itself. And so with the Chrome debugger, right, it wants to run all your JavaScript code in Chrome. And so there's no Native context for it to attach to, and had to write this RPC layer to work around that. But with our new Hermes integration, we'll be able to get in much better debugging experience. And we think we'll actually look to Flipper as the debugger in the future, once we do that.\n\nOkay, great. What is your opinion benefit, would you say makes a difference better to use than the likes of PouchDB? So certainly noted here that this is a SDK for React Native. We're depending on the hard drive to be available in a mobile application. So for PouchDB, it's more used in web browsers. This SDK you can't use it in a web browser. We do have a Realm web SDK that is specific for querying data from Atlas, and it gives some convenience methods for logging into our serverless platform. But I will say that we are doing a spike right now to allow for compilation of our Realm core database into. And if we do that, we'll be able to then integrate into browsers and have the ability to store and persist data into IndexedDB, which is a browser available or is the database available in browsers. Right. And so you can look forward to that because then we could then be integrated into PWAs for instance in the web.\n\nOther Question here, is there integration, any suggestions talk around Realm sync? Is there any other, I guess, tips and tricks that we can suggest the things may be coming in the future API regarding a React Native application for Realm sync? I know one of the things that came out in our user interviews was partitions. And being able to open up multiple Realms in a React Native SDK, I believe we were looking to potentially add this to our provider pattern, to put in multiple partition key values. Maybe you can talk a little bit to that.\n\n**Andrew:**\nYeah. Basically that provider you'd be able to actually provide that configuration as properties as well to the provider. So if you initiate your context with the configuration and something needs to change along the line based on some sort of state, or maybe you open a new screen and it's like a detailed view. And that parameter, that new screen is taking an ID, then you'd be able to basically set the partition to that ID and base the data off that partition ID.\n\n**Ian:**\nYeah, mostly it's our recommendation here to follow a singleton pattern where you put everything in the provider and that when you call that in a new view, it basically gives you an already open Realm reference. So you can boot up the app, you open up all the rounds that you need to, and then depending on the view you're on, you can call to that provider to get the Realm reference that you'd need.\n\n**Andrew:**\nRight. Yeah. That's another way to do it as well. So you can do it as granular as you want. And so you can use your provider on a small component on your header of your app, or you could wrap the whole app with it. So many use cases. So I would like to go a little bit more into detail someday about how to like use Realm with React navigation and multiple partitions and Realms and stuff like that. So maybe that's something we could look at in the future.\n\n**Ian:**\nYeah. Absolutely. Great. Are there any other questions from anyone here? Just to let everyone know this will be recorded, so we've recorded this and then we'll post this on YouTube later, so you can watch it from there, but if there's any other questions, please ask them now, otherwise we'll close early. Are there any issues with multiple independently installed applications accessing the same database? So I think it's important to note here that with Realm, we do allow multi-process access. We do have a way, we have like a lot file and so there is the ability to have Realm database be used and access by multiple applications if you are using a non-sync Realm. With sync Realms, we don't have multi-process support, it is something we'll look to add in the future, but for right now we don't have it. And that's just from the fact that our synchronization runs in a background thread. And it's hard for us to tell when that thread has done to at work or not.\n\nAnother question is the concept behind partitions. We didn't cover this. I'd certainly encourage you to go to docs@mongodb.com/realm we have a bunch of documentation around our sync but a partition corresponds to the Realm file on the client side. So what you can do with the Realm SDK is if you enable sync, you're now sinking into a MongoDB Atlas server or cluster. This is the database as a service managed offering that is for the cloud version of MongoDB. And you can have multiple collections within this MongoDB instance. And you could have, let's say a hundred thousand documents. Those a hundred thousand documents are for the amalgamation of all of your Realm clients. And so a partition allows you to specify which documents are for which clients. So you can boot up and say, \"Okay, I logged in. My user ID is Ian Ward. Therefore give me all documents that are for Ian Ward.\" And that's where you can segment your data that's all stored together in MongoDB. Interesting Question.\n\n**Andrew:**\nYeah. I feel like a simple application, it's probably just going to be partitioned by a user ID, but if you're making an app for a logistics company that has multiple warehouses and you have an app that has the inventory for all those warehouses, then you might probably want to partition on those warehouses, the warehouse that you're in. So that'd be a good example of where you could use that partition in a more complex environment.\n\n**Ian:**\nYeah, definitely. Yeah. It doesn't need to be user ID. It could also be store ID. We have a lot of logistics customer, so it could be driver ID, whatever packages that driver is supposed to deliver on that day will be part of their partition. Great. Well if there's no other... Oops, got another one in, can we set up our own sync on existing Realm database and have it sync existing data i.e. the user used the app without syncing, but later decides to sync the data after signing up? So right now the file format for a Realm database using non-sync and a syncing database is different basically because with sync, we need to keep track of the operations that are happening when you're occurring offline. So it keeps a queue of those operations.\n\n**Ian:**\nAnd then once you connect back online, it automatically sends those operations to the service side to apply the state. Right now, if you wanted to move to a synchronized brown, you would need to copy that data from the non-sync Realm to the sync Realm. We do have a project that I hope to get to in the next quarter for automatically doing that conversion for you. So you'll basically be able to write all the data and copy it over to the sync and make it a lot easier for developers to do that if they wish to. But it is something that we get some requests for. So we would like to make it easier. Okay. Well, thank you very much, everyone. Thank you, Andrew. I really appreciate it. And thank you everyone for coming. I hope you found this valuable and please reach out to us if you have any further questions. Okay. Thanks everyone. Bye.\n","description":"In this event, recorded on June 10th, Andrew Meyer, Software Engineer, on the Realm JavaScript team, walks us through the React Native ecosystem as it relates to persisting data with Realm. We discuss things to consider when using React Native, best practices to implement and gotcha's to avoid, as well as what's next for the JavaScript team at Realm.\n\n","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltce70f1d229063657/644c464e2aebaa5d93274b21/speaker-grant.png?branch=prod","description":null}}]},"slug":"/realm-meetup-javascript-react-native","title":"*Realm Meetup - Realm JavaScript for React Native Applications","original_publish_date":"2021-06-24T15:34:45.838Z","strapi_updated_at":"2023-03-21T16:12:09.552Z","expiry_date":"2022-06-24T15:23:33.923Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*React","calculated_slug":"/technologies/react"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:31.222Z","publish_details":{"time":"2023-04-28T22:52:51.847Z"}}},{"calculated_slug":"/products/realm/realm-swiftui-maps-location","content":"## Introduction\nEmbedding Apple Maps and location functionality in SwiftUI apps used to be a bit of a pain. It required writing your own SwiftUI wrapper around UIKit code—see these examples from the [O-FISH app](https://github.com/WildAid/o-fish-ios):\n\n* [Location helper](https://github.com/WildAid/o-fish-ios/blob/main/o-fish-ios/Helpers/LocationHelper.swift)\n* [Map views](https://github.com/WildAid/o-fish-ios/tree/main/o-fish-ios/Views/Components/Maps)\n\nIf you only need to support iOS14 and later, then you can **forget most of that messy code 😊**. If you need to support iOS13—sorry, you need to go the O-FISH route!\n\niOS14 introduced the [Map](https://developer.apple.com/documentation/mapkit/map) SwiftUI view (part of [Mapkit](https://developer.apple.com/documentation/mapkit/map)) allowing you to embed maps directly into your SwiftUI apps without messy wrapper code.\n\nThis article shows you how to embed Apple Maps into your app views using Mapkit's Map view. We'll then look at how you can fetch the user's current location—with their permission, of course!\n\nFinally, we'll see how to store the location data in Realm in a format that lets MongoDB Atlas Device Sync it to MongoDB Atlas. Once in Atlas, you can add a [geospatial index](https://docs.mongodb.com/manual/core/2dsphere/) and use [MongoDB Charts](https://www.mongodb.com/products/charts) to plot the data on a map—we'll look at that too.\n\nMost of the code snippets have been extracted from the [RChat app](https://github.com/realm/RChat). That app is a good place to see maps and location data in action. [Building a Mobile Chat App Using Realm – The New and Easier Way](https://developer.mongodb.com/how-to/building-a-mobile-chat-app-using-realm-new-way/) is a good place to learn more about the RChat app—including how to enable [MongoDB Atlas Device Sync](https://www.mongodb.com/docs/atlas/app-services/sync/learn/overview/#std-label-sync-overview).\n\n## Prerequisites\n\n* [Realm-Cocoa 10.8.0](https://github.com/realm/realm-cocoa/releases)+ (may work with some 10.7.X versions)\n* iOS 14.5+ ([Mapkit](https://developer.apple.com/documentation/mapkit/map) was introduced in iOS 14.0 and so most features should work with earlier iOS 14.X versions)\n* [XCode12+](https://developer.apple.com/xcode/)\n\n## How to Add an Apple Map to Your SwiftUI App\n\nTo begin, let's create a simple view that displays a map, the coordinates of the center of that map, and the zoom level:\n\n![Gif of scrolling around an embedded Apple Map and seeing the reported coordinates changing](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/realm_map1_7ea7ecbf15.gif)\n\nWith Mapkit and SwiftUI, this only takes a few lines of code:\n\n``` swift\nimport MapKit\nimport SwiftUI\n\nstruct MyMapView: View {\n    @State private var region: MKCoordinateRegion = MKCoordinateRegion(\n        center: CLLocationCoordinate2D(latitude: MapDefaults.latitude, longitude: MapDefaults.longitude),\n        span: MKCoordinateSpan(latitudeDelta: MapDefaults.zoom, longitudeDelta: MapDefaults.zoom))\n        \n    private enum MapDefaults {\n        static let latitude = 45.872\n        static let longitude = -1.248\n        static let zoom = 0.5\n    }\n\n    var body: some View {\n        VStack {\n            Text(\"lat: \\(region.center.latitude), long: \\(region.center.longitude). Zoom: \\(region.span.latitudeDelta)\")\n            .font(.caption)\n            .padding()\n            Map(coordinateRegion: $region,\n                interactionModes: .all,\n                showsUserLocation: true)\n        }\n    }\n}\n```\n\nNote that `showsUserLocation` won't work unless the user has already given the app permission to use their location—we'll get to that.\n\n`region` is initialized to a starting location, but it's updated by the `Map` view as the user scrolls and zooms in and out.\n\n### Adding Bells and Whistles to Your Maps (Pins at Least)\n\nPins can be added to a map in the form of \"annotations.\" Let's start with a single pin:\n\n![Embedded Apple Map showing a red pin](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/realm_map2_3812086195.png)\n\nAnnotations are provided as an array of structs where each instance must contain the coordinates of the pin. The struct must also conform to the [Identifiable](https://developer.apple.com/documentation/swift/identifiable) protocol:\n\n``` swift\nstruct MyAnnotationItem: Identifiable {\n    var coordinate: CLLocationCoordinate2D\n    let id = UUID()\n}\n```\n\nWe can now create an array of `MyAnnotationItem` structs:\n\n``` swift\nlet annotationItems = [\n    MyAnnotationItem(coordinate: CLLocationCoordinate2D(\n        latitude: MapDefaults.latitude,\n        longitude: MapDefaults.longitude))]\n```\n\nWe then pass `annotationItems` to the `MapView` and indicate that we want a `MapMarker` at the contained coordinates:\n\n``` swift\nMap(coordinateRegion: $region,\n    interactionModes: .all,\n    showsUserLocation: true,\n    annotationItems: annotationItems) { item in\n        MapMarker(coordinate: item.coordinate)\n    }\n```\n\nThat gives us the result we wanted.\n\nWhat if we want multiple pins? Not a problem. Just add more `MyAnnotationItem` instances to the array.\n\nAll of the pins will be the same default color. But, what if we want different colored pins? It's simple to extend our code to produce this:\n\n![Embedded Apple Map showing red, yellow, and plue pins at different locations](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/realm_map3_669f6ec179.png)\n\nFirstly, we need to extend `MyAnnotationItem` to include an optional `color` and a `tint` that returns `color` if it's been defined and \"red\" if not:\n\n``` swift\nstruct MyAnnotationItem: Identifiable {\n    var coordinate: CLLocationCoordinate2D\n    var color: Color?\n    var tint: Color { color ?? .red }\n    let id = UUID()\n}\n```\n\nIn our sample data, we can now choose to provide a color for each annotation:\n\n``` swift\nlet annotationItems = [\n    MyAnnotationItem(\n        coordinate: CLLocationCoordinate2D(\n            latitude: MapDefaults.latitude,\n            longitude: MapDefaults.longitude)),\n    MyAnnotationItem(\n        coordinate: CLLocationCoordinate2D(\n            latitude: 45.8827419,\n            longitude: -1.1932383),\n        color: .yellow),\n    MyAnnotationItem(\n        coordinate: CLLocationCoordinate2D(\n            latitude: 45.915737,\n            longitude: -1.3300991),\n        color: .blue)\n]\n```\n\nThe `MapView` can then use the `tint`:\n\n``` swift\nMap(coordinateRegion: $region,\n    interactionModes: .all,\n    showsUserLocation: true,\n    annotationItems: annotationItems) { item in\n    MapMarker(\n        coordinate: item.coordinate,\n        tint: item.tint)\n}\n```\n\nIf you get bored of pins, you can use `MapAnnotation` to use any view you like for your annotations:\n\n``` swift\nMap(coordinateRegion: $region,\n    interactionModes: .all,\n    showsUserLocation: true,\n    annotationItems: annotationItems) { item in\n    MapAnnotation(coordinate: item.coordinate) {\n        Image(systemName: \"gamecontroller.fill\")\n            .foregroundColor(item.tint)\n    }\n}\n```\n\nThis is the result:\n\n![Apple Map showing red, yellow and blue game controller icons at different locations on the map](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/realm_map5_8509602cd7.png)\n\nYou could also include the name of the system image to use with each annotation.\n\nThis [gist](https://gist.github.com/am-MongoDB/3073226a84bf6deb6b2df7afcb21ca92) contains the final code for the view.\n\n## Finding Your User's Location\n\n### Asking for Permission\n\nApple is pretty vocal about respecting the privacy of their users, and so it shouldn't be a shock that your app will have to request permission before being able to access a user's location.\n\nThe first step is to add a key-value pair to your Xcode project to indicate that the app may request permission to access the user's location, and what text should be displayed in the alert. You can add the pair to the \"Info.plist\" file:\n\n```\nPrivacy - Location When In Use Usage Description : We'll only use your location when you ask to include it in a message\n```\n\n![Screenshot from Xcode showing the key-value pair for requesting permission for the app to access the user's location](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Info_plist_1500a15e11.png)\n\nOnce that setting has been added, the user should see an alert the first time that the app attempts to access their current location:\n\n![iPhone screenshot – app is requesting permission to access the user's location](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/request_access_to_location_08d1be66d1.png)\n\n### Accessing Current Location\n\nWhile Mapkit has made maps simple and native in SwiftUI, the same can't be said for location data.\n\nYou need to create a SwiftUI wrapper for Apple's [Core Location](https://developer.apple.com/documentation/corelocation) functionality. There's not a lot of value in explaining this boilerplate code—just copy this code from RChat's [LocationHelper.swift](https://github.com/realm/RChat/blob/main/RChat-iOS/RChat/Helpers/LocationHelper.swift) file, and paste it into your app:\n\n``` swift\nimport CoreLocation\n\nclass LocationHelper: NSObject, ObservableObject {\n\n    static let shared = LocationHelper()\n    static let DefaultLocation = CLLocationCoordinate2D(latitude: 45.8827419, longitude: -1.1932383)\n\n    static var currentLocation: CLLocationCoordinate2D {\n        guard let location = shared.locationManager.location else {\n            return DefaultLocation\n        }\n        return location.coordinate\n    }\n\n    private let locationManager = CLLocationManager()\n\n    private override init() {\n        super.init()\n        locationManager.delegate = self\n        locationManager.desiredAccuracy = kCLLocationAccuracyBest\n        locationManager.requestWhenInUseAuthorization()\n        locationManager.startUpdatingLocation()\n    }\n}\n\nextension LocationHelper: CLLocationManagerDelegate {\n    func locationManager(_ manager: CLLocationManager, didUpdateLocations locations: [CLLocation]) { }\n\n    public func locationManager(_ manager: CLLocationManager, didFailWithError error: Error) {\n        print(\"Location manager failed with error: \\(error.localizedDescription)\")\n    }\n\n    public func locationManager(_ manager: CLLocationManager, didChangeAuthorization status: CLAuthorizationStatus) {\n        print(\"Location manager changed the status: \\(status)\")\n    }\n}\n```\n\nOnce added, you can access the user's location with this simple call:\n\n``` swift\nlet location = LocationHelper.currentLocation\n```\n\n### Store Location Data in Your Realm Database\n\n#### The Location Format Expected by MongoDB\n\nRealm doesn't have a native type for a geographic location, and so it's up to us how we choose to store it in a Realm Object. That is, unless we want to synchronize the data to MongoDB Atlas using Device Sync, and go on to use MongoDB's geospatial functionality.\n\nTo make the best use of the location data in Atlas, we need to add a [geospatial index](https://docs.mongodb.com/manual/geospatial-queries/#geospatial-indexes) to the field (which we’ll see how to do soon.) That means storing the location in a [supported format](https://docs.mongodb.com/manual/geospatial-queries/#geospatial-data). Not all options will work with Atlas Device Sync (e.g., it's not guaranteed that attributes will appear in the same order in your Realm Object and the synced Atlas document). The most robust approach is to use an array where the first element is longitude and the second is latitude:\n\n``` json\nlocation: [<longitude>, <latitude>]\n```\n\n#### Your Realm Object\n\nThe RChat app gives users the option to include their location in a chat message—this means that we need to include the location in the [ChatMessage](https://github.com/realm/RChat/blob/main/RChat-iOS/RChat/Model/ChatMessage.swift) Object:\n\n``` swift\nclass ChatMessage: Object, ObjectKeyIdentifiable {\n  …\n    @Persisted let location = List<Double>()\n  …\n    convenience init(author: String, text: String, image: Photo?, location: [Double] = []) {\n        ...\n\tlocation.forEach { coord in\n            self.location.append(coord)\n      }\n        ...\n        }\n    }\n   ….\n}\n```\n\nThe `location` array that's passed to that initializer is formed like this:\n\n``` swift\nlet location = LocationHelper.currentLocation\nself.location = [location.longitude, location.latitude]\n```\n\n## Location Data in Your Backend MongoDB Atlas Application Services App\n\nThe easiest way to create your backend MongoDB Atlas Application Services schema is to enable [Development Mode](https://docs.mongodb.com/realm/sync/enable-development-mode/)—that way, the schema is automatically generated from your Swift Realm Objects.\n\nThis is the generated [schema](https://github.com/realm/RChat/blob/main/RChat-Realm/RChat/services/mongodb-atlas/rules/RChat.ChatMessage.json) for our \"ChatMessage\" collection:\n\n``` swift\n{\n    \"bsonType\": \"object\",\n    \"properties\": {\n      \"_id\": {\n        \"bsonType\": \"string\"\n      },\n      ...\n      \"location\": {\n        \"bsonType\": \"array\",\n        \"items\": {\n          \"bsonType\": \"double\"\n        }\n      }\n    },\n    \"required\": [\n      \"_id\",\n      ...\n    ],\n    \"title\": \"ChatMessage\"\n}\n```\n\nThis is a document that's been created from a synced Realm `ChatMessage` object:\n\n![Screen capture of an Atlas document, which includes an array named location](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/atlas_document_d09cb3be1d.png)\n\n### Adding a Geospatial Index in Atlas\n\nNow that you have location data stored in Atlas, it would be nice to be able to work with it—e.g., running [geospatial queries](https://docs.mongodb.com/manual/geospatial-queries/). To enable this, you need to add a [geospatial index](https://docs.mongodb.com/manual/geospatial-queries/#geospatial-indexes) to the `location` field.\n\nFrom the Atlas UI, select the \"Indexes\" tab for your collection and click \"CREATE INDEX\":\n\n![Atlas screen capture of creating a new index](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/geo_index1_604fcbcd13.png)\n\nYou should then configure a `2dsphere` index:\n\n![Atlas screen capture of creating a new 2dsphere index](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/geo_index2_8878556cfc.png)\n\nMost chat messages won't include the user's location and so I set the `sparse` option for efficiency.\n\nNote that you'll get an error message if your ChatMessage collection contains any documents where the value in the location attribute isn't in a valid geospatial format.\n\nAtlas will then build the index. This will be very quick, unless you already have a huge number of documents containing the location field. Once complete, you can move onto the next section.\n\n### Plotting Your Location Data in MongoDB Charts\n\n[MongoDB Charts](https://www.mongodb.com/products/charts) is a simple way to visualize MongoDB data. You can access it through the same UI as Application Services and Atlas. Just click on the \"Charts\" button:\n\n![Atlas screen capture of MongoDB Charts button](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/select_charts_9c8caa71c0.png)\n\nThe first step is to click the \"Add Data Source\" button:\n\n![Charts screen capture of adding a new data source](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/charts_data_source_4cbf88fea5.png)\n\nSelect your Atlas cluster:\n\n![Charts screen capture of adding Atlas cluster as a data source](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/charts_select_cluster_b6d35ea00e.png)\n\nSelect the `RChat.ChatMessage` collection:\n\n![Charts screen capture of selecting the ChatMessage collection in the RChat database](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/charts_select_collections_4f0dc0577d.png)\n\nClick “Finish.” You’ll be taken to the default Dashboards view, which is empty for now. Click \"Add Dashboard\":\n\n![Charts screen capture of adding a new dashboard](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/charts_dashboard1_b25eb3e015.png)\n\nIn your new dashboard, click \"ADD CHART\":\n\n![Charts screen capture of adding a new chart](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/charts_add_chart_f00639d1a1.png)\n\nConfigure your chart as shown here by:\n- Setting the chart type to \"Geospatial\" and the sub-type to \"Scatter.\"\n- Dragging the \"location\" attribute to the coordinates box.\n- Dragging the \"author\" field to the \"Color\" box.\n\n![Charts screen capture of configuring a new chart](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/charts_create_chart_81189874ac.png)\n\nOnce you've created your chart, you can [embed it](https://docs.mongodb.com/charts/saas/embed-chart-anon-auth/#std-label-anon-embedding-charts) in web apps, etc. That's beyond the scope of this article, but check out the [MongoDB Charts docs](https://docs.mongodb.com/charts/saas/) if you're interested.\n\n## Conclusion\n\nSwiftUI makes it easy to embed Apple Maps in your SwiftUI apps. As with most Apple frameworks, there are extra maps features available if you break out from SwiftUI, but I'd suggest that the simplicity of working with SwiftUI is enough incentive for you to avoid that unless you have a compelling reason.\n\nAccessing location information from within SwiftUI still feels a bit of a hack, but in reality, you cut and paste the helper code once, and then you're good to go.\n\nBy storing the location as a `[longitude, latitude]` array (`List`) in your Realm database, it's simple to sync it with MongoDB Atlas. Once in Atlas, you have the full power of MongoDB's geospatial functionality to work your location data.\n\nIf you have questions, please head to our [developer community website](https://community.mongodb.com/) where the MongoDB engineers and the MongoDB community will help you build your next big idea with MongoDB.","description":"Learn how to use the new Map view from iOS Map Kit in your SwiftUI/Realm apps. Also see how to use iOS location in Realm, Atlas, and Charts.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt0474c23872f1df2e/644c46501023732e29966c68/realm-swiftui-maps.jpg?branch=prod","description":null}}]},"slug":"/realm-swiftui-maps-location","title":"*Using Maps and Location Data in Your SwiftUI (+Realm) App","original_publish_date":"2021-07-08T13:52:22.516Z","strapi_updated_at":"2022-08-26T20:32:32.313Z","expiry_date":"2022-06-29T07:57:06.580Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":"https://github.com/realm/RChat","l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Sync","calculated_slug":"/products/realm/sync"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*iOS","calculated_slug":"/technologies/ios"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to use the new Map view from iOS Map Kit in your SwiftUI/Realm apps. Also see how to use iOS location in Realm, Atlas, and Charts.","og_description":"Learn how to use the new Map view from iOS Map Kit in your SwiftUI/Realm apps. Also see how to use iOS location in Realm, Atlas, and Charts.","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt5269e992ea00a100/644c4651bc3f0a2551815108/realm-swiftui-maps-og.jpg?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@mongodb"},"system":{"updated_at":"2023-04-28T22:52:30.831Z","publish_details":{"time":"2023-04-28T22:52:51.884Z"}}},{"calculated_slug":"/products/atlas/5-year-atlas-anniversary-episode-1-on-ramp","content":"My name is [Michael Lynn](linkedin.com/in/mlynn/), and I’m a developer advocate at MongoDB.\n\nI’m excited to welcome you to this, the first in a series of episodes created to celebrate the five year anniversary of the launch of MongoDB [Atlas](https://www.mongodb.com/cloud/atlas), our Database as a Service Platform.\n\nIn this series, my co-hosts, Jesse Hall, and Nic Raboy will talk with some of the people responsible for building, and launching the platform that helped to transform MongoDB as a company.\n\nbeginning with [Episode 1](https://mongodb.libsyn.com/ep-63-the-road-to-atlas-1-onramp-with-sahir-azam-and-andrew-davidson), the On ramp to Atlas talking with [Sahir Azam](https://www.linkedin.com/in/sahirazam/), Chief Product Officer, and [Andrew Davidson](https://www.linkedin.com/in/andrewad/), VP of Product about the strategic shift from a software company to a software as a service business.\n\nIn episode 2, Zero to Database as a Service, we’ll chat with Cailin Nelson, SVP of Engineering, and Cory Mintz, VP of Engineering - about Atlas as a product and how it was built and launched.\n\nIn episode 3, we’ll Go Mobile, talking with Alexander Stigsen, Founder of the Realm Mobile Database which has become a part of the Atlas Platform. \n\nIn episode 4, we’ll wrap the series up with a panel discussion and review some of our valued customer comments about the platform. \n\nThanks so much for tuning in and reading, please take a moment to [subscribe](https://podcasts.apple.com/us/podcast/the-mongodb-podcast/id1500452446) for more episodes and if you enjoy what you hear, please don’t forget to provide a comment, and a rating to help us continue to improve.\n\n:youtube[]{vid=FqhAB7nH6hE}\n\nWithout further adue, here is the transcript from episode one of this series.\n\nSahir: [00:00:00] Hi Everyone. My name is Sahir Azam and I'm the chief product officer at Mongo DB. Welcome to the Mongo DB podcast. \n\nMike: [00:00:07] Okay. Today, we're going to be talking about Mongo to be Atlas and the journey that has taken place to bring us to this point, the five-year anniversary of MongoDB Atlas of a launch of MongoDB Atlas. And I'm joined in the studio today by a couple of guests. And we'll start by introducing Sahir Azam chief product officer at Mongo DB.\nSahir, welcome to the show. It's great to have you on the podcast.\n\nSahir: [00:00:31] Hey, Hey Mike. Great to be here.\n\nMike: [00:00:33] Terrific. And we're also joined by Andrew Davidson. Andrew is vice-president of product cloud products at Mongo DB. Is it, do I have that right?\n\nAndrew: [00:00:41] That's right? Good to be here, Mike. How you doin? \n\nMike: [00:00:44] Doing great. It's great to have you on the show. And of course are my co-hosts for the day. Is Jesse Hall also known as codeSTACKr. Welcome back to the show, Jesse.\nIt's great to have you on\n\nJesse: [00:00:54] I'm fairly new here. So I'm excited to hear about the,  history of Atlas\n\nMike: [00:00:58] fantastic. Yeah. W we're gonna, we're gonna get into that. But before we do so here, I guess we'll maybe introduce yourself to the audience, talk a little bit about who you are and what you.\n\nSahir: [00:01:09] Yeah. So, I mentioned earlier, I run the product organization at Mongo and as part of my core focus, I think about the products we build the roadmaps of those products and how they serve customers and ultimately help us grow our business. And I've been with the company for about five years.\nCoincidentally I was recruited to lead basically the transition of the business. Open source enterprise software company to becoming a SAS vendor. And so I came on right before the launch of Atlas, Andrew on the line here certainly has the history of how Atlas came to be even prior to me joining.\nBut, uh, it's been a heck of a ride.\n\nMike: [00:01:46] Fantastic. Well, Andrew, that brings us to you once, yet, let folks know who you are and what you do.\n\nAndrew: [00:01:52] sure. Yeah. Similar to Sahir, I focus on product management, but a really more specifically focused on our cloud product suite. And if you think about it, that was something that five years ago, when we first launched Atlas was just an early kernel, a little bit of a startup inside of our broader company.\nAnd so before that time, I was very focused on our traditional more private cloud management experience from marketing the and it's really just been this amazing journey to really transform this company with Sahir and so many others into being a cloud company. So really excited to be here on this milestone. \n\nMike: [00:02:25] Fantastic. And Jesse, so you've been with Mongo to be, I guess, relatively the least amount of time among the four of us, but maybe talk about your experience with Mongo to be and cloud in general.\n\nJesse: [00:02:36] Yeah. So I've used it several times in some tutorials that I've created on the Atlas portion of it. Going through the onboarding experience and\nlearning how it actually works, how the command line and all of that was amazing to understand it from that perspective as well.\nSo, yeah, I'm excited to see how you took it from that to the cloud.\n\nMike: [00:02:58] Yeah. Yeah, me too. And if you think about the journey I'm going to be was a successful open source product. I was a project that was largely used by developers. To increase agility. It represented a different way to store data and it wasn't a perfect journey. There were some challenges early on, specifically around the uniqueness of the mechanism that it's using to store data is different from traditional forms.\nAnd. So I guess Andrew you've been here the longest over eight years. Talk about the challenges of transitioning from a software product to an online database, as a service.\n\nAndrew: [00:03:37] Yeah. Sure. When you think back to where we were, say eight and a half years ago, to your point, we had this kind of almost new category of data experience for developers that gave them this natural way to interface with data in a way that was totally reflective of the way they wanted to think about their data, the objects in there. And we came in and revolutionized the world with this way of interfacing with data. And that's what led to them. I'm going to be just exploding in popularity. It was just mind boggling to see millions of people every month, experiencing MongoDB for the first time as pure open source software on their laptops.\nBut as we move forward over the years, we realized. We could be this phenomenal database that gave developers exactly the way they want to interface with data. We could be incredibly scalable. We could go up to any level of scale with vertical and horizontal kind of linear cost economics, really built for cloud.\nWe could do all of that, but if our customers continued to have to self manage all of this software at scale, we realized, frankly, we might get left behind in the end. We might get beaten by databases that weren't as good. But then we're going to be delivered at a higher level of abstraction, fully managed service.\nSo we went all in as a company recognizing we need to make this just so easy for people to get started and to go up to any level of scale. And that's really what Atlas was about. It was all about democratizing this incredible database, which had already democratize a new data model, but making it accessible for production use cases in the cloud, anywhere in the room.\nAnd I think when you see what's happened today with just millions of people who have now used Atlas, the same magnitude of number of have had used our self-managed software. It's just amazing to see how far. \n\nMike: [00:05:21] Yeah. Yeah. It's been quite a ride and it is interesting timing. So here, so you joined right around the same time. I think it was, I think a couple of months prior to the launch of Atlas. Tell us about like your role early.\n\nSahir: [00:05:36] Yeah, I think what attracted me to Mongo DB in the first place, certainly the team, I knew there was a strong team here and I absolutely knew of the sort of popularity and. Just disruption that the open source technology and database had created in the market just as, somebody being an it and technology.\nAnd certainly it'd be hard to miss. So I had a very kind of positive impression overall of the business, but the thing that really did it for me was the fact that the company was embarking on this strategic expansion to become a SAS company and deliver this database as a service with Atlas, because I had certainly built. In my own mind sort of conviction that for open source companies, the right business model that would ultimately be most successful was distributing tech technology as a matter of service so that it can get the reach global audiences and, really democratize that experiences as Andrew mentioned.\nSo that was the most interesting challenge. And when I joined the company, I think. Part of everyone understands is okay, it's a managed version of bongo DB, and there's a whole bunch of automation, elasticity and pay as you go pricing and all of the things that you would expect in the early days from a managed service.\nBut the more interesting thing that I think is sometimes hidden away is how much it's really transformed Mongo DB. The company's go to market strategy. As well, it's allowed us to really reach, tens of thousands of customers and millions of developers worldwide. And that's a function of the fact that it's just so easy to get started.\nYou can start off on our free tier or as you start building your application and it scales just get going on a credit card and then ultimately engaged and, in a larger level with our organization, as you start to get to mission criticality and scale. That's really hard to do in a, a traditional sort of enterprise software model.\nIt's easy to do for large customers. It's not easy to do for the broad base of them. Mid-market and the SMB and the startups and the ecosystem. And together with the team, we put a lot of focus into thinking about how do we make sure we widen the funnel as much as possible and get as many developers to try Atlas as the default experience we're using Mongo DB, because we felt a, it was definitely the best way to use the technology, but also for us as a company, it was the most powerful way for us to scale our overall operations.\n\nMike: [00:07:58] Okay.\n\nJesse: [00:08:00] Okay. \n\nMike: [00:08:00] So obviously there's going to be some challenges early on in the minds of the early adopters. Now we've had some relatively large names. I don't know if we can say any names of customers that were early adopters, but there were obviously challenges around that. What are some of the challenges that were particularly difficult when you started to talk to some of these larger name companies?\nWhat are some of the things that. Really concerned about early \non. \n\nSahir: [00:08:28] Yeah I'll try them a little bit. And Andrew, I'm sure you have thoughts on this as well. So I think in the, when we phased out sort of the strategy for Atlas in the early years, when we first launched, it's funny to think back. We were only on AWS and I think we were in maybe four or five regions at the time if I remember correctly and the first kind of six to 12 months was really optimized for. Let's call it lower end use cases where you could come in. You didn't necessarily have high-end requirements around security or compliance guarantees. And so I think the biggest barrier to entry for larger customers or more mission critical sort of sensitive applications was. We as ourselves had not yet gotten our own third-party compliance certifications, there were certain enterprise level security capabilities like encryption, bring your own key encryption things like, private networking with with peering on the cloud providers that we just hadn't built yet on our roadmap.\nAnd we wanted to make sure we prioritize correctly. So I think that was the. Internal factor. The external factor was, five years ago. It wasn't so obvious that for the large enterprise, that databases of service would be the default way to consume databases in the cloud. Certainly there was some of that traction happening, but if you look at it compared to today, it was still early days.\nAnd I laugh because early on, we probably got positively surprised by some early conservative enterprise names. Maybe Thermo Fisher was one of them. We had I want to say AstraZeneca, perhaps a couple of like really established brand names who are, bullish on the cloud, believed in Mongo DB as a key enabling technology.\nAnd in many ways where those early partners with us in the enterprise segment were to help develop the maturity we needed to scale over time.\n\nMike: [00:10:23] Yeah, \n\nAndrew: [00:10:23] I remember the, these this kind of wake up call moment where you realized the pace of being a cloud company is just so much higher than what we had traditionally been before, where it was, a bit more of a slow moving enterprise type of sales motion, where you have a very big, POC phase and a bunch of kind of setup time and months of delivery.\nThat whole model though, was changing. The whole idea of Atlas was to enable our customer to very rapidly and self-service that service matter build amazing applications. And so you had people come in the matter of hours, started to do really cool, amazing stuff. And sometimes we weren't even ready for that.\nWe weren't even ready to be responsive enough for them. So we had to develop these new muscles. Be on the pulse of what this type of new speed of customer expected. I remember in one of our earliest large-scale customers who would just take us to the limits, it was, we had, I think actually funny enough, multiple cricket league, fantasy sports apps out of India, they were all like just booming and popularity during the India premier league. \n\nMike: [00:11:25] Okay. \n\nAndrew: [00:11:26] Cricket competition. And it was just like so crazy how many people were storming into this application, the platform at the same time and realizing that we had a platform that could, actually scale to their needs was amazing, but it was also this constant realization that every new level of scale, every kind of new rung is going to require us to build out new operational chops, new muscles, new maturity, and we're still, it's an endless journey, a customer today.\nA thousand times bigger than what we could accommodate at that time. But I can imagine that the customers of, five years from now will be a, yet another couple of order magnitude, larger or orders meant to larger. And it's just going to keep challenging us. But now we're in this mindset of expecting that and always working to get that next level, which is exciting. \n\nMike: [00:12:09] Yeah. I'm sure it hasn't always been a smooth ride. I'm sure there were some hiccups along the way. And maybe even around scale, you mentioned, we got surprised. Do you want to talk a little bit about maybe some of that massive uptake. Did we have trouble offering this product as a service?\nJust based on the number of customers that we were able to sign up?\n\nSahir: [00:12:30] I'd say by and large, it's been a really smooth ride. I think one of the ones, the surprises that kind of I think is worth sharing \nis we have. I think just under or close to 80 regions now in Atlas and the promise of the cloud at least on paper is endless scale and availability of resources, whether that be compute or networking or storage. That's largely true for most customers in major regions where the cloud providers are. But if you're in a region that's not a primary region or you've got a massive rollout where you need a lot of compute capacity, a lot of network capacity it's not suddenly available for you on demand all the time. There are supply chain data center or, resources backing all of this and our partners, do a really great job, obviously staying ahead of that demand, but there are sometimes constraints.\nAnd so I think we reached a certain scale inflection point where we were consistently bumping up. The infrastructure cloud providers limits in terms of availability of capacity. And, we've worked with them on making sure our quotas were set properly and that we were treated in a special case, but there were definitely a couple of times where, we had a new application launching for a customer. It's not like it was a quota we were heading there literally was just not there weren't enough VMs and underlying physical infrastructure is set up and available in those data centers. And so we had some teething pains like working with our cloud provider friends to make sure that we were always projecting ahead with more and more I think, of a forward look to them so that we can, make sure we're not blocking our customers. Funny cloud learnings, I would say.\n\nMike: [00:14:18] Well, I guess that answers that, I was going to ask the question, why not? Build our own cloud, why not build, a massive data center and try and meet the demands with something like, an ops manager tool and make that a service offering. But I guess that really answers the question that the demand, the level of demand around the world would be so difficult.\nWas that ever a consideration though? Building our own\n\nSahir: [00:14:43] so ironically, we actually did run our own infrastructure in the early days for our cloud backup service. So we had spinning disks and\nphysical devices, our own colo space, and frankly, we just outgrew it. I think there's two factors for us. One, the database is pretty. Low in the stack, so to speak.\nSo it needs to, and as an operational transactional service, We need to be really close to where the application actually runs. And the power of what the hyperscale cloud providers has built is just immense reach. So now any small company can stand up a local site or a point of presence, so to speak in any part of the world, across those different regions that they have.\nAnd so the idea that. Having a single region that we perhaps had the economies of scale in just doesn't make sense. We're very dispersed because of all the different regions we support across the major cloud providers and the need to be close to where the application is. So just given the dynamic of running a database, the service, it is really important that we sit in those public major public cloud providers, right by the side, those those customers, the other.\nIs really just that we benefit from the innovation that the hyperscale cloud providers put out in the market themselves. Right. There's higher levels of abstraction. We don't want to be sitting there. We have limited resources like any company, would we rather spend the dollars on racking and stacking hardware and, managing our own data center footprint and networking stack and all of that, or would we rather spend those reasons?\nConsuming as a service and then building more value for our customers. So the same thing we, we just engage with customers and why they choose Atlas is very much true to us as we build our cloud platforms.\n\nAndrew: [00:16:29] Yeah. I If you think about it, I'm going to be is really the only company that's giving developers this totally native data model. That's so easy to get started with at the prototyping phase. They can go up to any level of scale from there that can read and write across 80 regions across the big three cloud providers all over the world.\nAnd for us to not stay laser-focused on that level. Making developers able to build incredible global applications would just be to pull our focus away from really the most important thing for us, which is to be obsessed with that customer experience rather than the infrastructure building blocks in the backend, which of course we do optimize them in close partnership with our cloud provider partners to Sahir's point..  . \n\nJesse: [00:17:09] So along with all of these challenges to scale over time, there was also other competitors trying to do the same thing. So how does Mongo DB, continue to have a competitive advantage?\n\nSahir: [00:17:22] Yeah, I think it's a consistent investment in engineering, R and D and innovation, right? If you look at the capabilities we've released, the core of the database surrounding the database and Atlas, the new services that integrated simplify the architecture for applications, some of the newer things we have, like search or realm or what we're doing with analytics with that was data lake.\nI'll put our ability to push out more value and capability to customers against any competitor in the world. I think we've got a pretty strong track record there, but at a more kind of macro level. If you went back kind of five years ago to the launch of Atlas, most customers and developers, how to trade off to make you either go with a technology that's very deep on functionality and best of breed.\nSo to speak in a particular domain. Like a Mongo DB, then you have to, that's typically all software, so you tend to have to operate it yourself, learn how to manage and scale and monitor and all those different things. Or you want to choose a managed service experience where you get, the ease of use of just getting started and scaling and having all the pay as you go kind of consumption models.\nBut those databases are nowhere close to as capable as the best of breed players. That was the state of the mark. Five years ago, but now, fast forward to 2021 and going forward customers no longer have to make that trade. You have multicloud and sort of database and service offerings analytics as a service offerings, which you learning players that have not only the best of breed capability, that's a lot deeper than the first party services that are managed by the cloud providers, but are also delivered in this really amazing, scalable manner.\nConsumption-based model so that trade-off is no longer there. And I think that's a key part of what drives our success is the fact that, we have the best capabilities. That's the features and the costs that at the cost of developers and organizations want. We deliver it as a really fluid elastic managed service.\nAnd then guess what, for enterprises, especially multicloud is an increasingly strategic sort of characteristic they look for in their major providers, especially their data providers. And we're available on all three of the major public clouds with Atlas. That's a very unique proposition. No one else can offer that.\nAnd so that's the thing that really drives in this\n\nMike: [00:19:38] Yeah.\n\nSahir: [00:19:39] powering, the acceleration of the Atlas business.\n\nMike: [00:19:42] Yeah. And so, Andrew, I wonder if for the folks that are not familiar with Atlas, the architecture you want to just give an overview of how Atlas works and leverages the multiple cloud providers behind the scenes.\nAndrew: [00:19:56] Yeah, sure. Look, anyone who's not used not going to be Atlas, I encourage you just, sign up right away. It's the kind of thing where in just a matter of five minutes, you can deploy a free sandbox cluster and really start building your hello world. Experience your hello world application on top of MongoDB to be the way Atlas really works is essentially we try and make it as simple as possible.\nYou sign up. Then you decide which cloud provider and which region in that cloud provider do I want to launch my database cluster into, and you can choose between those 80 regions to hear mentioned or you can do more advanced stuff, you can decide to go multi-region, you can decide to go even multicloud all within the same database cluster.\nAnd the key thing is that you can decide to start really tiny, even at the free level or at our dedicated cluster, starting at $60. Or you can go up to just massive scale sharded clusters that can power millions of concurrent users. And what's really exciting is you can transition those clusters between those states with no downtime.\nAt any time you can start single region and small and scale up or scale to multiple regions or scale to multiple clouds and each step of the way you're meeting whatever your latest business objectives are or whatever the needs of your application are. But in general, you don't have to completely reinvent the wheel and rearchitect your app each step of the way.\nThat's where MongoDB makes it just so as you to start at that prototyping level and then get up to the levels of scale. Now on the backend, Atlas does all of this with of course, huge amount of sophistication. There's dedicated virtual, private clouds per customer, per region for a dedicated clusters.\nYou can connect into those clusters using VPC, Piering, or private link, offering a variety of secure ways to connect without having to deal with public IP access lists. You can also use the. We have a wide variety of authentication and authorization options, database auditing, like Sahir mentioned, bring your own key encryption and even client-side  field level encryption, which allows you to encrypt data before it even goes into the database for the subsets of your schema at the highest classification level.\nSo we make it, the whole philosophy here is to democratize making it easy to build applications in a privacy optimized way to really ultimately make it possible, to have millions of end consumers have a better experience. And use all this wonderful digital experiences that everyone's building out there. \n\nJesse: [00:22:09] So here we talked about how just the Mongo DB software, there was a steady growth, right. But once we went to the cloud \nwith Atlas, the success of that, how did that impact our business?\n\nSahir: [00:22:20] Yeah, I think it's been obviously Quite impactful in terms of just driving the acceleration of growth and continued success of MongoDB.  We were fortunate, five, six years ago when Atlas was being built and launched that, our business was quite healthy. We were about a year out from IPO.\nWe had many enterprise customers that were choosing our commercial technology to power, their mission, critical applications. That continues through today. So the idea of launching outlets was although certainly strategic and, had we saw where the market was going. And we knew this would in many ways, be the flagship product for the company in the term, it was done out of sort of an offensive view to getting to market.\nAnd so if you look at it now, Atlas is about 51% of our revenue. It's, the fastest growing product in our portfolio, Atlas is no longer just a database. It's a whole data platform where we've collapsed a bunch of other capabilities in the architecture of an application. So it's much simpler for developers.\nAnd over time we expected that 51% number is only going to continue to be, a larger percentage of our business, but it's important to know. Making sure that we deliver a powerful open source database to the market, that we have an enterprise version of the software for customers who aren't for applications or customers that aren't yet in the crowd, or may never go to the cloud for certain workloads is super critical.\nThis sort of idea of run anywhere. And the reason why is, oftentimes. Timeline for modernizing an application. Let's say you're a large insurance provider or a bank or something. You've got thousands of these applications on legacy databases. There's an intense need to monitor modernize.\nThose that save costs to unlock developer, agility, that timeline of choosing a database. First of all, it's a decision that lasts typically seven to 10 years. So it's a long-term investment decision, but it's not always timed with a cloud model. So the idea that if you're on premises, that you can modernize to an amazing database, like Mongo DB, perhaps run it in Kubernetes, run it in virtual machines in your own data center.\nBut then, two years later, if that application needs to move to the cloud, it's just a seamless migration into Atlas on any cloud provider you choose. That's a very unique and powerful, compelling story for, especially for large organizations, because what they don't want is to modernize or rewrite an application twice, once to get the value on pro-business and then have to think about it again later, if the app moves to the cloud, it's one seamless journey and that hybrid model.\nOf moving customers to words outlets over time is really been a cohesive strategies. It's not just Atlas, it's open source and the enterprise version all seamlessly playing in a uniform model.\n\nMike: [00:25:04] Hmm. Fantastic. And, I love that, the journey that. Atlas has been on it's really become a platform. It's no longer just a database as a service. It's really, an indispensable tool that developers can use to increase agility. And, I'm just looking back at the kind of steady drum beat of additional features that have been added to, to really transform Atlas into a platform starting with free tier and increasing the regions and the coverage and.\nClient side field level encryption. And just the list of features that have been added is pretty incredible. I think I would be remiss if I didn't ask both of you to maybe talk a little bit about the future. Obviously there's things like, I don't know, like invisibility of the service and AI and ML and what are some of the things that you're thinking about, I guess, without, tipping your cards too much.\nTalk about what's interesting to you in the future of cloud.\n\nAndrew: [00:25:56] I'll take a quick pass. Just I love the question to me, the most important thing for us to be just laser focused on always going forward. Is to deliver a truly integrated, elegant experience for our end customers that is just differentiated from essentially a user experience perspective from everything else that's out there.\nAnd the platform is such a fundamental part of that, being a possibility, it starts with that document data model, which is this super set data model that can express within it, everything from key value to, essentially relational and object and. And then behind making it possible to access all of those different data models through a single developer native interface, but then making it possible to drive different physical workloads on the backend of that.\nAnd what by workloads, I mean, different ways of storing the data in different algorithms used to analyze that data, making it possible to do everything from operational transactional to those search use cases to here mentioned a data lake and mobile synchronization. Streaming, et cetera, making all of that easily accessible through that single elegant interface.\nThat is something that requires just constant focus on not adding new knobs, not adding new complex service area, not adding a millions of new permutations, but making it elegant and accessible to do all of these wonderful data models and workload types and expanding out from there. So you'll just see us keep, I think focusing. Yeah. \n\nMike: [00:27:15] Fantastic. I'll just give a plug. This is the first in the series that we're calling on ramp to Mongo to be Atlas. We're going to go a little bit deeper into the architecture. We're going to talk with some engineering folks. Then we're going to go into the mobile space and talk with Alexander Stevenson and and talk a little bit about the realm story.\nAnd then we're going to wrap it up with a panel discussion where we'll actually have some customer comments and and we'll provide a little bit. Detail into what the future might look like in that round table discussion with all of the guests. I just want to thank both of you for taking the time to chat with us and I'll give you a space to, to mention anything else you'd like to talk about before we wrap the episode up. Sahir, anything?\n\nSahir: [00:27:54] Nothing really to add other than just a thank you. And it's been humbling to think about the fact that this product is growing so fast in five years, and it feels like we're just getting started. I would encourage everyone to keep an eye out for our annual user conference next month.\nAnd some of the exciting announcements we have and Atlas and across the portfolio going forward, certainly not letting off the gas.\n\nMike: [00:28:15] Great. Any final words Andrew? \n\nAndrew: [00:28:18] yeah, I'll just say, mom going to be very much a big ten community. Over a hundred thousand people are signing up for Atlas every month. We invest so much in making it easy to absorb, learn, dive into to university courses, dive into our wonderful documentation and build amazing things on us.\nWe're here to help and we look forward to seeing you on the platform. \n\nMike: [00:28:36] Fantastic. Jesse, any final words?\n\nJesse: [00:28:38] No. I want just want to thank both of you for joining us. It's been very great to hear about how it got started and look forward to the next episodes.\n\nMike: [00:28:49] right.\n\nSahir: [00:28:49] Thanks guys.\n\nMike: [00:28:50] Thank you.\n\n","description":"My name is Michael Lynn, and I’m a developer advocate at MongoDB.\n\nI’m excited to welcome you to this, the first in a series of episodes created to celebrate the five year anniversary of the launch of MongoDB Atlas, our Database as a Service Platform.\n\nIn this series, my co-hosts, Jesse Hall, and Nic Raboy will talk with some of the people responsible for building, and launching the platform that helped to transform MongoDB as a company.\n\nbeginning with Episode 1, the On ramp to Atlas talking with Sahir Azam, Chief Product Officer, and Andrew Davidson, VP of Product about the strategic shift from a software company to a software as a service business.\n\nIn episode 2, Zero to Database as a Service, we’ll chat with Cailin Nelson, SVP of Engineering, and Cory Mintz, VP of Engineering - about Atlas as a product and how it was built and launched.\n\nIn episode 3, we’ll Go Mobile, talking with Alexander Stigsen, Founder of the Realm Mobile Database which has become a part of the Atlas Platform. \n\nIn episode 4, we’ll wrap the series up with a panel discussion and review some of our valued customer comments about the platform. \n\nThanks so much for tuning in, please take a moment to subscribe for more episodes and if you enjoy what you hear, please don’t forget to provide a comment, and a rating to help us continue to improve.\n","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt4fa2ca3f961971ae/644c46552c3b2bc675eba835/5-years.png?branch=prod","description":null}}]},"slug":"/5-year-atlas-anniversary-episode-1-on-ramp","title":"*Atlas 5-Year Anniversary Podcast Series Episode 1 - Onramp to Atlas","original_publish_date":"2021-06-30T18:35:14.461Z","strapi_updated_at":"2022-05-16T18:47:14.935Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Podcast","calculated_slug":"/podcasts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:30.433Z","publish_details":{"time":"2023-04-28T22:52:51.930Z"}}},{"calculated_slug":"/products/atlas/atlas-serverless-quick-start","content":"MongoDB Atlas serverless instances are now GA (generally available)!\n\nWhat is a serverless instance you might ask? In short, *it’s an on-demand serverless database*. In this article, we'll deploy a MongoDB Atlas serverless instance and perform some basic CRUD operations. You’ll need a MongoDB Atlas account. If you already have one [sign-in](https://account.mongodb.com/account/login), or [register now](https://www.mongodb.com/cloud/atlas/register).\n\n## Demand Planning\n\nWhen you deploy a MongoDB Atlas cluster, you need to understand what compute and storage resources your application will require so that you pick the correct tier to accommodate its needs.\n\nAs your storage needs grow, you will need to adjust your cluster’s tier accordingly. You can also enable auto-scaling between a minimum and maximum tier.\n\n## Ongoing Management\n\nOnce you’ve set your tiering scale, what happens when your app explodes and gets tons of traffic and exceeds your estimated maximum tier? It’s going to be slow and unresponsive because there aren’t enough resources.\n\nOr, maybe you’ve over-anticipated how much traffic your application would get but you’re not getting any traffic. You still have to pay for the resources even if they aren’t being utilized.\n\nAs your application scales, you are limited to these tiered increments but nothing in between.\n\nThese tiers tightly couple compute and storage with each other. You may not need 3TB of storage but you do need a lot of compute. So you’re forced into a tier that isn’t balanced to the needs of your application.\n\n## The Solve\n\nMongoDB Atlas serverless instances solve all of these issues:\n\n-   Deployment friction\n-   Management overhead\n-   Performance consequences\n-   Paying for unused resources\n-   Rigid data models\n\nWith MongoDB Atlas serverless instances, you will get seamless deployment and scaling, a reliable backend infrastructure, and an intuitive pricing model.\n\nIt’s even easier to deploy a serverless instance than it is to deploy a free cluster on MongoDB Atlas. All you have to do is choose a cloud provider and region. Once created, your serverless instance will seamlessly scale up and down as your application demand fluctuates.\n\nThe best part is you only pay for the compute and storage resources you use, leaving the operations to MongoDB’s best-in-class automation, including end-to-end security, continuous uptime, and regular backups.\n\n## Create Your First Serverless Instance\n\nLet’s see how it works…\n\nIf you haven’t already signed up for a [MongoDB Atlas account](https://www.mongodb.com/cloud/atlas/register), go ahead and do that first, then select \"Build a Database\".\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_07_16_at_3_52_21_PM_25c61de281.png)\n\nNext, choose the Serverless deployment option.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_07_16_at_3_52_50_PM_203c12265e.png)\n\nNow, select a cloud provider and region, and then optionally modify your instance name. Create your new deployment and you’re ready to start using your serverless instance!\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_07_16_at_3_55_23_PM_2ec017a847.png)\n\nYour serverless instance will be up and running in just a few minutes. Alternatively, you can also use the [Atlas CLI](https://www.mongodb.com/docs/atlas/cli/stable/) to create and deploy a new serverless instance.\n\nWhile we wait for that, let’s set up a quick Node.js application to test out the CRUD operations.\n\n## Node.js CRUD Example\n\nPrerequisite: You will need [Node.js](https://nodejs.org/) installed on your computer.\n\nConnecting to the serverless instance is just as easy as a tiered instance.\n\n1.  Click “Connect.”\n\n    ![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/EXTERNAL_Cluster_Card_Close_Up_2x_38c2a5e381.png)\n\n3.  Set your IP address and database user the same as you would a tiered instance.\n4.  Choose a connection method.\n\t-   You can choose between mongo shell, Compass, or “Connect your application” using MongoDB drivers.\n    \n    ![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/EXTERNAL_Connect_to_Cluster_2x_eccf2b8761.png)\n    \nWe are going to “Connect your application” and choose Node.js as our driver. This will give us a connection string we can use in our Node.js application. Check the “Include full driver code example” box and copy the example to your clipboard.\n\nTo set up our application, open VS Code (or your editor of choice) in a blank folder. From the terminal, let’s initiate a project:\n\n`npm init -y`\n\nNow we’ll install MongoDB in our project:\n\n`npm i mongodb`\n\n### Create\n\nWe’ll create a `server.js` file in the root and paste the code example we just copied.\n\n```js\nconst  MongoClient  =  require('mongodb').MongoClient;\nconst  uri  =  \"mongodb+srv://mongo:<password>@serverlessinstance0.xsel4.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\";\nconst  client  =  new  MongoClient(uri, { useNewUrlParser:  true, useUnifiedTopology:  true });\n\nclient.connect(err  => {\n    const  collection  =  client.db(\"test\").collection(\"devices\");\n    \n    // perform actions on the collection object\n    \n    client.close();\n});\n```\n\nWe’ll need to replace `<password>` with our actual user password and `myFirstDatabase` with the database name we’ll be connecting to.\n\nLet’s modify the `client.connect` method to create a database, collection, and insert a new document.\n\nNow we’ll run this from our terminal using `node server`.\n\n```js\nclient.connect((err) => {\n    const  collection  =  client.db(\"store\").collection(\"products\");\n    collection\n        .insertOne(\n        {\n            name:  \"JavaScript T-Shirt\",\n            category:  \"T-Shirts\",\n        })\n        .then(() => {\n            client.close();\n        });\n});\n```\n\nWhen we use the `.db` and `.collection` methods, if the database and/or collection does not exist, it will be created. We also have to move the `client.close` method into a `.then()` after the `.insertOne()` promise has been returned. Alternatively, we could wrap this in an async function.\n\nWe can also insert multiple documents at the same time using `.insertMany()`.\n\n```js\ncollection\n    .insertMany([\n    {\n        name:  \"React T-Shirt\",\n        category:  \"T-Shirts\",\n    },\n    {\n        name:  \"Vue T-Shirt\",\n        category:  \"T-Shirts\",\n    }\n    ])\n    .then(() => {\n        client.close();\n    });\n```\n\nMake the changes and run `node server` again.\n\n### Read\n\nLet’s see what’s in the database now. There should be three documents. The `find()` method will return all documents in the collection.\n\n```js\nclient.connect((err) => {\n    const  collection  =  client.db(\"store\").collection(\"products\");\n    collection.find().toArray((err, result) =>  console.log(result))\n    .then(() => {\n        client.close();\n    });\n});\n```\n\nWhen you run `node server` now, you should see all of the documents created in the console.\n\nIf we wanted to find a specific document, we could pass an object to the `find()` method, giving it something to look for.\n\n```js\nclient.connect((err) => {\n    const  collection  =  client.db(\"store\").collection(\"products\");\n    collection.find({name: “React T-Shirt”}).toArray((err, result) =>  console.log(result))\n    .then(() => {\n        client.close();\n    });\n});\n```\n\n### Update\n\nTo update a document, we can use the `updateOne()` method, passing it an object with the search parameters and information to update.\n\n```js\nclient.connect((err) => {\n    const  collection  =  client.db(\"store\").collection(\"products\");\n    collection.updateOne(\n        { name:  \"Awesome React T-Shirt\" },\n        { $set: { name:  \"React T-Shirt\" } }\n    )\n    .then(() => {\n        client.close();\n    });\n});\n```\n\nTo see these changes, run a `find()` or `findOne()` again.\n\n### Delete\n\nTo delete something from the database, we can use the `deleteOne()` method. This is similar to `find()`. We just need to pass it an object for it to find and delete.\n\n```js\nclient.connect((err) => {\n    const  collection  =  client.db(\"store\").collection(\"products\");\n    collection.deleteOne({ name:  \"Vue T-Shirt\" }).then(() =>  client.close());\n});\n```\n\n## Conclusion\n\nIt’s super easy to use MongoDB Atlas serverless instances! You will get seamless deployment and scaling, a reliable backend infrastructure, and an intuitive pricing model. We think that serverless instances are a great deployment option for new users on Atlas.\n\nI’d love to hear your feedback or questions. Let’s chat in the [MongoDB Community](https://developer.mongodb.com/community/forums/).","description":"MongoDB Atlas serverless instances are now generally available! What is a serverless instance you might ask? In short, it’s an on-demand serverless database. In this article, we'll deploy a MongoDB Atlas serverless instance and perform some basic CRUD operations.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltc7d740353b3a6f3c/644c4659a353e2d2166bafec/docs.png?branch=prod","description":null}}]},"slug":"/atlas-serverless-quick-start","title":"*MongoDB Atlas Serverless Instances: Quick Start","original_publish_date":"2021-07-13T12:54:26.239Z","strapi_updated_at":"2022-06-28T20:22:06.589Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Serverless","calculated_slug":"/technologies/serverless"}},{"node":{"title":"*Nodejs","calculated_slug":"/technologies/nodejs"}}]}},"seo":{"canonical_url":"","meta_description":"MongoDB Atlas serverless instances are now available in preview! What is a serverless instance you might ask? In short, it’s an on-demand serverless database. In this article, we'll deploy a MongoDB Atlas serverless instance and perform some basic CRUD operations.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt37ac2c25102c0e22/644c465badd18e3cb9aa9278/og-docs.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@codestackr"},"system":{"updated_at":"2023-04-28T22:52:29.696Z","publish_details":{"time":"2023-04-28T22:52:52.049Z"}}},{"calculated_slug":"/products/atlas/influence-search-result-ranking-function-scores-atlas-search","content":"When it comes to natural language searching, it's useful to know how the order of the results for a query were determined. Exact matches might be obvious, but what about situations where not all the results were exact matches due to a fuzzy parameter, the `$near` operator, or something else?\n\nThis is where the document score becomes relevant.\n\nEvery document returned by a `$search` query in MongoDB Atlas Search is assigned a score based on relevance, and the documents included in a result set are returned in order from highest score to lowest.\n\nYou can choose to rely on the [scoring](https://docs.atlas.mongodb.com/reference/atlas-search/scoring/) that Atlas Search determines based on the query operators, or you can customize its behavior using function scoring and optimize it towards your needs. In this tutorial, we're going to see how the `function` option in Atlas Search can be used to rank results in an example.\n\nPer the [documentation](https://docs.atlas.mongodb.com/reference/atlas-search/scoring/#function), the `function` option allows the value of a numeric field to alter the final score of the document. You can specify the numeric field for computing the final score through an expression. With this in mind, let's look at a few scenarios where this could be useful.\n\nLet's say that you have a review system like Yelp where the user needs to provide some search criteria such as the type of food they want to eat. By default, you're probably going to get results based on relevance to your search term as well as the location that you defined. In the examples below, I’m using the [sample restaurants data](https://developer.mongodb.com/article/atlas-sample-datasets/) available in MongoDB Atlas.\n\nThe `$search` query (expressed as an aggregation pipeline) to make this search happen in MongoDB might look like the following:\n\n```json\n[\n    {\n        \"$search\": {\n            \"text\": {\n                \"query\": \"korean\",\n                \"path\": [ \"cuisine\" ],\n                \"fuzzy\": {\n                    \"maxEdits\": 2\n                }\n            }\n        }\n    },\n    {\n        \"$project\": {\n            \"_id\": 0,\n            \"name\": 1,\n            \"cuisine\": 1,\n            \"location\": 1,\n            \"rating\": 1,\n            \"score\": {\n                \"$meta\": \"searchScore\"\n            }\n        }\n    }\n]\n```\n\nThe above query is a two-stage aggregation pipeline in MongoDB. The first stage is searching for \"korean\" in the \"cuisine\" document path. A fuzzy factor is applied to the search so spelling mistakes are allowed. The document results from the first stage might be quite large, so in the second stage, we're specifying which fields to return for every document. This includes a search score that is not part of the original document, but part of the search results.\n\nAs a result, you might end up with the following results:\n\n```json\n[\n    {\n        \"location\": \"Jfk International Airport\",\n        \"cuisine\": \"Korean\",\n        \"name\": \"Korean Lounge\",\n        \"rating\": 2,\n        \"score\": 3.5087265968322754\n    },\n    {\n        \"location\": \"Broadway\",\n        \"cuisine\": \"Korean\",\n        \"name\": \"Mill Korean Restaurant\",\n        \"rating\": 4,\n        \"score\": 2.995847225189209\n    },\n    {\n        \"location\": \"Northern Boulevard\",\n        \"cuisine\": \"Korean\",\n        \"name\": \"Korean Bbq Restaurant\",\n        \"rating\": 5,\n        \"score\": 2.995847225189209\n    }\n]\n```\n\nThe default ordering of the documents returned is based on the `score` value in descending order. The higher the score, the closer your match.\n\nIt's very unlikely that you're going to want to eat at the restaurants that have a rating below your threshold, even if they match your search term and are within the search location. With the `function` option, we can assign a point system to the rating and perform some arithmetic to give better rated restaurants a boost in your results.\n\nLet's modify the search query to look like the following:\n\n```json\n[\n    {\n        \"$search\": {\n            \"text\": {\n                \"query\": \"korean\",\n                \"path\": [ \"cuisine\" ],\n                \"fuzzy\": {\n                    \"maxEdits\": 2\n                },\n                \"score\": {\n                    \"function\": {\n                        \"multiply\": [\n                            {\n                                \"score\": \"relevance\"\n                            },\n                            {\n                                \"path\": {\n                                    \"value\": \"rating\",\n                                    \"undefined\": 1\n                                }\n                            }\n                        ]\n                    }\n                }\n            }\n        }\n    },\n    {\n        \"$project\": {\n            \"_id\": 0,\n            \"name\": 1,\n            \"cuisine\": 1,\n            \"location\": 1,\n            \"rating\": 1,\n            \"score\": {\n                \"$meta\": \"searchScore\"\n            }\n        }\n    }\n]\n```\n\nIn the above two-stage aggregation pipeline, the part to pay attention to is the following:\n\n```json\n\"score\": {\n    \"function\": {\n        \"multiply\": [\n            {\n                \"score\": \"relevance\"\n            },\n            {\n                \"path\": {\n                    \"value\": \"rating\",\n                    \"undefined\": 1\n                }\n            }\n        ]\n    }\n}\n```\n\nWhat we're saying in this part of the `$search` query is that we want to take the relevance score that we had already seen in the previous example and multiply it by whatever value is in the `rating` field of the document. This means that the score will potentially be higher if the rating of the restaurant is higher. If the restaurant does not have a rating, then we use a default multiplier value of 1.\n\nIf we run this query on the same data as before, we might now get results that look like this:\n\n```json\n[\n    {\n        \"location\": \"Northern Boulevard\",\n        \"cuisine\": \"Korean\",\n        \"name\": \"Korean Bbq Restaurant\",\n        \"rating\": 5,\n        \"score\": 14.979236125946045\n    },\n    {\n        \"location\": \"Broadway\",\n        \"cuisine\": \"Korean\",\n        \"name\": \"Mill Korean Restaurant\",\n        \"rating\": 4,\n        \"score\": 11.983388900756836\n    },\n    {\n        \"location\": \"Jfk International Airport\",\n        \"cuisine\": \"Korean\",\n        \"name\": \"Korean Lounge\",\n        \"rating\": 2,\n        \"score\": 7.017453193664551\n    }\n]\n```\n\nSo now, while \"Korean BBQ Restaurant\" might be further in terms of location, it appears higher in our result set because the rating of the restaurant is higher.\n\nIncreasing the score based on rating is just one example. Another scenario could be to give search result priority to restaurants that are sponsors. A `function` multiplier could be used based on the sponsorship level.\n\nLet's look at a different use case. Say you have an e-commerce website that is running a sale. To push search products that are on sale higher in the list than items that are not on sale, you might use a `constant` score in combination with a relevancy score.\n\nAn aggregation that supports the above example might look like the following:\n\n```\ndb.products.aggregate([\n    {\n        \"$search\": {\n            \"compound\": { \n                \"should\": [\n                    { \n                        \"text\": { \n                            \"path\": \"promotions\", \n                            \"query\": \"July4Sale\", \n                            \"score\": { \n                                \"constant\": { \n                                    \"value\": 1 \n                                }\n                            }\n                        }\n                    }\n                ],\n                \"must\": [ \n                    { \n                        \"text\": { \n                            \"path\": \"name\", \n                            \"query\": \"bose headphones\"\n                        }\n                    }\n                ]\n            }\n        }\n    },\n    {\n        \"$project\": {\n            \"_id\": 0,\n            \"name\": 1,\n            \"promotions\": 1,\n            \"score\": { \"$meta\": \"searchScore\" }\n        }\n    }\n]);\n```\n\nTo get into the nitty gritty of the above two-stage pipeline, the first stage uses the [compound operator](https://docs.atlas.mongodb.com/reference/atlas-search/compound/) for searching. We're saying that the search results `must` satisfy \"bose headphones\" and if the result-set `should` contain \"July4Sale\" in the `promotions` path, then add a `constant` of one to the score for that particular result item to boost its ranking.\n\nThe `should` operator doesn't require its contents to be satisfied, so you could end up with headphone results that are not part of the \"July4Sale.\" Those result items just won't have their score increased by any value, and therefore would show up lower down in the list. The second stage of the pipeline just defines which fields should exist in the response.\n\n## Conclusion\n\nBeing able to customize how search result sets are scored can help you deliver more relevant content to your users. While we looked at a couple examples around the `function` option with the `multiply` operator, there are other ways you can use function scoring, like replacing the value of a missing field with a constant value or boosting the results of documents with search terms found in a specific path. You can find more information in the [Atlas Search documentation](https://docs.atlas.mongodb.com/reference/atlas-search/scoring/#function).\n\nDon't forget to check out the [MongoDB Community Forums](https://community.mongodb.com) to learn about what other developers are doing with Atlas Search.","description":"Learn how to influence the score of your Atlas Search results using a variety of operators and options.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt4e47a65a504cb6e2/644c465f783e05d903ae3e02/atlas-search.png?branch=prod","description":null}}]},"slug":"/influence-search-result-ranking-function-scores-atlas-search","title":"*Influence Search Result Ranking with Function Scores in Atlas Search","original_publish_date":"2021-07-09T15:58:36.040Z","strapi_updated_at":"2023-02-03T16:11:51.130Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Search","calculated_slug":"/products/atlas/search"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to influence the score of your Atlas Search results using a variety of operators and options.","og_description":"Learn how to influence the score of your Atlas Search results using a variety of operators and options.","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltae306dbf7d066da3/644c4662b92bcd50e6787e3a/og-atlas-search.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@nraboy"},"system":{"updated_at":"2023-04-28T22:52:29.314Z","publish_details":{"time":"2023-04-28T22:52:52.081Z"}}},{"calculated_slug":"/products/mongodb/new-time-series-collections","content":"## What is Time Series Data?\n\nTime-series data are measurements taken at time intervals. Sometimes time-series data will come into your database at high frequency - use-cases like financial transactions, stock market data, readings from smart meters, or metrics from services you're hosting over hundreds or even thousands of servers. In other cases, each measurement may only come in every few minutes. Maybe you're tracking the number of servers that you're running every few minutes to estimate your server costs for the month. Perhaps you're measuring the soil moisture of your favourite plant once a day.\n\n|           | Frequent                              | Infrequent                                  |\n| --------- | ------------------------------------- | ------------------------------------------- |\n| Regular   | Service metrics                       | Number of sensors providing weather metrics |\n| Irregular | Financial transactions, Stock prices? | LPWAN data                                  |\n\nHowever, when it comes to time-series data, it isn’t all about frequency, the only thing that truly matters is the presence of time so whether your data comes every second, every 5 minutes, or every hour isn’t important for using MongoDB for storing and working with time-series data.\n\n### Examples of Time-Series Data\n\nFrom the very beginning, developers have been using MongoDB to store time-series data. MongoDB can be an extremely efficient engine for storing and processing time-series data, but you'd have to know how to correctly model it to have a performant solution, but that wasn't as straightforward as it could have been.\n\nStarting in MongoDB 5.0 there is a new collection type, time-series collections, which are specifically designed for storing and working with time-series data without the hassle or need to worry about low-level model optimization.\n\n## What are Time series Collections?\n\nTime series collections are a new collection type introduced in MongoDB 5.0. On the surface, these collections look and feel like every other collection in MongoDB. You can read and write to them just like you do regular collections and even create secondary indexes with the createIndex command. However, internally, they are natively supported and optimized for storing and working with time-series data.\n\nUnder the hood, the creation of a time series collection results in a collection and an automatically created writable non-materialized view which serves as an abstraction layer. This abstraction layer allows you to always work with their data as single documents in their raw form without worry of performance implications as the actual time series collection implements a form of the bucket pattern you may already know when persisting data to disk, but these details are something you no longer need to care about when designing your schema or reading and writing your data. Users will always be able to work with the abstraction layer and not with a complicated compressed bucketed document.\n\n## Why Use MongoDB's Time Series Collections?\n\nWell because you have time-series data, right?\n\nOf course that may be true, but there are so many more reasons to use the new time series collections over regular collections for time-series data.\n\nEase of use, performance, and storage efficiency were paramount goals when creating time series collections. Time series collections allow you to work with your data model like any other collection as single documents with rich data types and structures. They eliminate the need to model your time-series data in a way that it can be performant ahead of time - they take care of all this for you!\n\nYou can design your document models more intuitively, the way you would with other types of MongoDB collections. The database then optimizes the storage schema  for ingestion, retrieval, and storage by providing native compression to allow you to efficiently store your time-series data without worry about duplicated fields alongside your measurements.\n\nDespite being implemented in a different way from the collections you've used before, to optimize for time-stamped documents, it's important to remember that you can still use the MongoDB features you know and love, including things like nesting data within documents, secondary indexes, and the full breadth of analytics and data transformation functions within the aggregation framework, including joining data from other collections, using the `$lookup` operator, and creating materialized views using `$merge`.\n\n## How to Create a Time-Series Collection\n\n### All It Takes is Time \n\nCreating a time series collection is straightforward, all it takes is a field in your data that corresponds to time, just pass the new \"timeseries'' field to the createCollection command and you’re off and running. However, before we get too far ahead,  let’s walk through just how to do this and all of the options that allow you to optimize time series collections.\n\nThroughout this post, we'll show you how to create a time series collection to store documents that look like the following:\n\n```js\n{\n   \"_id\" : ObjectId(\"60c0d44894c10494260da31e\"),\n   \"source\" : {sensorId: 123, region: \"americas\"},\n   \"airPressure\" : 99 ,\n   \"windSpeed\" : 22,\n   \"temp\" : { \"degreesF\": 39,\n              \"degreesC\": 3.8\n            },\n   \"ts\" : ISODate(\"2021-05-20T10:24:51.303Z\")\n}\n\n```\n\nAs mentioned before, a time series collection can be created with just a simple time field. In order to store documents like this in a time series collection, we can pass the following to the *createCollection* command:\n\n```js\ndb.createCollection(\"weather\", {\n  timeseries: {\n    timeField: \"ts\",\n  },\n});\n```\n\nYou probably won't be surprised to learn that the timeField option declares the name of the field in your documents that stores the time, in the example above, \"ts\" is the name of the timeField. The value of the field specified by timeField must be a [<u>date type</u>](https://docs.mongodb.com/manual/reference/bson-types/#:~:text=BSON%20Date%20is%20a%2064,BSON%20Date%20type%20is%20signed.).\n\nPretty fast right? While timeseries collections only require a timeField, there are other optional parameters that can be specified at creation or in some cases at modification time which will allow you to get the most from your data and time series collections. Those optional parameters are metaField, granularity, and expireAfterSeconds.\n\n### metaField\nWhile not a required parameter, metaField allows for better optimization when specified, including the ability to create secondary indexes.\n\n```js\ndb.createCollection(\"weather\", {\n  timeseries: {\n    timeField: \"ts\",\n    metaField: \"source\",\n  }});\n```\n\nIn the example above, the metaField would be the \"source\" field: \n\n```js\n\"source\" : {sensorId: 123, region: \"americas\"}\n```\n\nThis is an object consisting of key-value pairs which describe our time-series data. In this example, an identifying ID and location for a sensor collecting weather data.\n\nThe metaField field can be a complicated document with nested fields, an object, or even simply a single GUID or string. The important point here is that the metaField is really just metadata which serves as a label or tag which allows you to uniquely identify the source of a time-series, and this field should never or rarely change over time. \n\nIt is recommended to always specify a metaField, but you would especially want to use this when you have multiple sources of data such as sensors or devices that share common measurements.\n\nThe metaField, if present, should partition the time-series data, so that measurements with the same metadata relate over time. Measurements with a common metaField for periods of time will be grouped together internally to eliminate the duplication of this field at the storage layer. The order of metadata fields is ignored in order to accommodate drivers and applications representing objects as unordered maps. Two metadata fields with the same contents but different order are considered to be identical. \n\nAs with the timeField, the metaField is specified as the top-level field name when creating a collection. However, the metaField can be of any BSON data type except *array* and cannot match the timeField required by timeseries collections. When specifying the metaField, specify the top level field name as a string no matter its underlying structure or data type.\n\nData in the same time period and with the same metaField will be colocated on disk/SSD, so choice of metaField field can affect query performance.\n\n### Granularity\n\nThe granularity parameter represents a string with the following options:\n\n- \"seconds\"\n- \"minutes\"\n- \"hours\"\n\n```js\ndb.createCollection(\"weather\", {\n  timeseries: {\n    timeField: \"ts\",\n    metaField: \"source\",\n    granularity: \"minutes\",\n  },\n});\n```\n\nGranularity should be set to the unit that is closest to rate of ingestion for a unique metaField value. So, for example, if the collection described above is expected to receive a measurement every 5 minutes from a single source, you should use the \"minutes\" granularity, because source has been specified as the metaField.\n\nIn the first example, where only the timeField was specified and no metaField was identified (try to avoid this!), the granularity would need to be set relative to the *total* rate of ingestion, across all sources.\n\nThe granularity should be thought about in relation to your metadata ingestion rate, not just your overall ingestion rate. Specifying an appropriate value allows the time series collection to be optimized for your usage.\n\nBy default, MongoDB defines the granularity to be \"seconds\", indicative of a high-frequency ingestion rate or where no metaField is specified.\n\n### expireAfterSeconds\n\nTime series data often grows at very high rates and becomes less useful as it ages. Much like last week leftovers or milk you will want to manage your data lifecycle and often that takes the form of expiring old data.\n\nJust like TTL indexes, time series collections allow you to manage your data lifecycle with the ability to automatically delete old data at a specified interval in the background. However, unlike TTL indexes on regular collections, time series collections do not require you to create an index to do this. \n\nSimply specify your retention rate in seconds during creation time, as seen below, or modify it at any point in time after creation with collMod. \n\n```js\ndb.createCollection(\"weather\", {\n  timeseries: {\n    timeField: \"ts\",\n    metaField: \"source\",\n    granularity: \"minutes\"\n  },\n    expireAfterSeconds: 9000 \n}); \n```\n\nThe expiry of data is only one way MongoDB natively offers you to manage your data lifecycle. In a future post we will discuss ways to automatically archive your data and efficiently read data stored in multiple locations for long periods of time using MongoDB Online Archive.\n\n### Putting it all Together \n\nPutting it all together, we’ve walked you through how to create a timeseries collection and the different options you can and should specify to get the most out of your data.\n\n```js\n{\n   \"_id\" : ObjectId(\"60c0d44894c10494260da31e\"),\n   \"source\" : {sensorId: 123, region: \"americas\"},\n   \"airPressure\" : 99 ,\n   \"windSpeed\" : 22,\n   \"temp\" : { \"degreesF\": 39,\n              \"degreesC\": 3.8\n            },\n   \"ts\" : ISODate(\"2021-05-20T10:24:51.303Z\")\n}\n```\n\nThe above document can now be efficiently stored and accessed from a time series collection using the below createCollection command.\n\n```js\ndb.createCollection(\"weather\", {\n  timeseries: {\n    timeField: \"ts\",\n    metaField: \"source\",\n    granularity: \"minutes\"\n  },\n    expireAfterSeconds: 9000 \n}); \n```\n\nWhile this is just an example, your document can look like nearly anything. Your schema is your choice to make with the freedom that you need not worry about how that data is compressed and persisted to disk. Optimizations will be made automatically and natively for you.\n\n## Limitations of Time Series Collections in MongoDB 5.0\n\nIn the initial MongoDB 5.0 release of time series collection there are some limitations that exist. The most notable of these limitations is that the timeseries collections are considered append only, so we do not have support on the abstraction level for update and/or delete operations. Update and/delete operations can still be performed on time series collections, but they must go directly to the collection stored on disk using the optimized storage format and a user must have the proper permissions to perform these operations.\n\nIn addition to the append only nature, in the initial release, time series collections will not work with Change Streams, Realm Sync, or Atlas Search. Lastly, time series collections allow for the creation of secondary indexes as discussed above. However, these secondary indexes can only be defined on the metaField and/or timeField.\n\nFor a full list of limitations, please consult the official MongoDB documentation page.\n\nWhile we know some of these limitations may be impactful to your current use case, we promise we're working on this right now and would love for you to provide your feedback!\n\n## Next Steps\n\nNow that you know what time series data is, when and how you should create a timeseries collection and some details of how to set parameters when creating a collection. Why don't you go create a timeseries collection now? Our next blog post will go into more detail on how to optimize your time series collection for specific use-cases.\n\nYou may be interested in migrating to a time series collection from an existing collection! We'll be covering this in a later post, but in the meantime, you should check out the official documentation for a list of migration tools and examples.\n","description":"Learn all about MongoDB's new time series collection type! This post will teach you what time series data looks like, and how to best configure time series collections to store your time series data.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltc7d740353b3a6f3c/644c4659a353e2d2166bafec/docs.png?branch=prod","description":null}}]},"slug":"/new-time-series-collections","title":"*MongoDB's New Time Series Collections","original_publish_date":"2021-07-13T21:22:23.645Z","strapi_updated_at":"2022-05-13T14:01:14.574Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*News & Announcements","calculated_slug":"/news"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Time series","calculated_slug":"/products/mongodb/time-series"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:28.929Z","publish_details":{"time":"2023-04-28T22:52:52.110Z"}}},{"calculated_slug":"/products/mongodb/generate-mql-with-mongosh-and-openai","content":"# Generating MQL Shell Commands Using OpenAI and New mongosh Shell\n\n[OpenAI](https://beta.openai.com/) is a fascinating and growing AI platform sponsored by Microsoft, allowing you to digest text cleverly to produce AI content with stunning results considering how small the “learning data set” you actually provide is.\n\nMongoDB’s Query Language (MQL) is an intuitive language for developers to interact with MongoDB Documents. For this reason, I wanted to put OpenAI to the test of quickly learning the MongoDB language and using its overall knowledge to build queries from simple sentences. The results were more than satisfying to me. Github is already working on a project called [Github copilot](https://copilot.github.com/) which uses the same OpenAI engine to code.\n\nIn this article, I will show you my experiment, including the game-changing capabilities of the new [MongoDB Shell](https://docs.mongodb.com/mongodb-shell/) (`mongosh`) which can extend scripting with npm modules integrations.\n\n## What is OpenAI and How Do I Get Access to It?\n\nOpenAI is a unique project aiming to provide an API for many AI tasks built mostly on Natural Language Processing today. You can read more about their projects in this [blog](https://openai.com/blog/openai-api/).\n\nThere are a variety of examples for its [text processing capabilities](https://beta.openai.com/examples).\n\nIf you want to use OpenAI, you will need to get a trial API key first by joining the [waitlist](https://share.hsforms.com/1Lfc7WtPLRk2ppXhPjcYY-A4sk30) on their [main](https://beta.openai.com/) page. Once you are approved to get an API key, you will be granted about $18 for three months of testing. Each call in OpenAI is  [billed](https://beta.openai.com/pricing) and this is something to consider when using in production. For our purposes, $18 is more than enough to test the most expensive engine named “davinci.”\n\n\nOnce you get the API key, you can use various [clients](https://beta.openai.com/docs/developer-quickstart/community-libraries) to run their AI API from your script/application. \n\nSince we will be using the new `mongosh` shell, I have used the\n [JS API](https://www.npmjs.com/package/openai-api).\n\n## Preparing the mongosh to Use OpenAI\n\nFirst, we need to [install](https://docs.mongodb.com/mongodb-shell/install/) the new shell, if you haven’t done it so far. On my Mac laptop, I just issued:\n\n``` bash\nbrew install mongosh\n```\n\nWindows users should download the MSI installer from our [download page](https://www.mongodb.com/try/download/shell) and follow the Windows instructions.\n\nOnce my mongosh is ready, I can start using it, but before I do so, let’s install [OpenAI JS](https://www.npmjs.com/package/openai-api), which we will import in the shell later on:\n\n``` bash\n$ mkdir openai-test\n$ cd openai-test\nOpenai-test $ npm i openai-api\n```\n\nI’ve decided to use the Questions and Answers pattern, in the form of `Q: <Question>` and `A: <Answer>`, provided to the [text to command](https://beta.openai.com/examples/default-text-to-command) completion API to provide the learning material about MongoDB queries for the AI engine. To better feed it, I placed the training questions and answers in a file called `AI-input.txt` and its content:\n\n```\nQ: What is the query syntax?\nA: db.collection.find(<filter>, <projection> , <options>)\nQ:  Query users collection for username with value \"boy\"\nA: db.users.find({\"username\" : \"boy\"})\nQ:  Query users collection for username with value \"girl\"A:  db.users.find({\"username\" : \"girl\"})\nQ: Query users collection for username with age bigger than 16\nA:  db.users.find({\"age\" : {$gt : 16}})n;\nQ: Query author collection for username with value \"boy\"\nA: db.authors.find({\"username\" : \"boy\"})\nQ:Query author collection for age lower than 7\nA: db.authors.find({\"age\" : {$lt : 7}});\n\nQ:insert a json document into collection authors with username equal to \"girl\"\nA: db.authors.insert({\"username\" : \"girl\"}, {\"age\" : 10, \"gender\" : \"female\"})\nQ: insert many documents into users collections\nA: db.users.insertMany([{username : \"john doe\"}, {username : \"joe doe\"}]);\nQ: The following aggregation uses $geoNear to find documents with a location at most 2 meters from the center [ -73.99279 , 40.719296 ] and a category equal to Parks.\nA: db.places.aggregate([{ $geoNear: {near: { type: \"Point\", coordinates: [ -73.99279 , 40.719296 ] },distanceField: \"dist.calculated\",maxDistance: 2, query: { category: \"Parks\" },includeLocs: \"dist.location\", spherical: true}}])\nQ: How to build atlas $search text query?\nA: db.collection.aggregate({$search: {\"index\": <index name> \"text\": {\"query\": \"<search-string>\",\"path\": \"<field-to-search>\",  \"fuzzy\": <options>,\"score\": <options>  }  }})\n\nQ: What is the aggregate syntax?\nA: db.collection.aggregate([<stage1>,<stage2>,<stage3>], <projection> , <options>);\nQ: aggregate users collection to calculate salary sum per user\nA: db.users.aggregate([{$group : { _id : \"$username\" , salary_sum : { $sum : \"$salary\" }}}]);\nQ: aggregate person collection to calculate salary sum per person\nA: db.persons.aggregate([{$group : { _id : \"$person\" , salary_sum : { $sum : \"$salary\" }}}]);\nQ: Lookup users and orders collection\nA: db.users.aggregate([{$lookup : {from: 'orders', localField : \"_id\", foreignField : \"userId\", as : \"result\" }} ]);\n\nQ: What is the update syntax?\nA:db.collection.update(query, update, options)\nQ:  How to edit collection sports where sportname is 'football' and match is 'england vs portugal' to score of '3-3' and date to current date?\nA: db.sports.update({ sportname: \"football\", match: \"england vs portugal\"} , {$set : {score: \"3-3\" , date : new Date()}} })\nQ: Query and atomically update collection zoo where animal is \"bear\" with a counter increment on eat field, if the data does not exist user upsert\nA: db.zoo.findOneAndUpdate({animal : \"bear\"}, {$inc: { eat : 1 }} , {upsert : true})\n```\n\nWe will use this file later in our code.\n\nThis way, the completion will be based on a similar pattern.\n\n### Prepare Your Atlas Cluster\n\n\n\n[MongoDB Atlas](https://www.mongodb.com/cloud/atlas),  the database-as-a-platform service, is a great way to have a running cluster in seconds with a sample dataset already there for our test. To prepare it, please use the following steps:\n\n1. Create an Atlas account (if you don’t have one already) and use/start a cluster. For detailed steps, follow this [documentation](https://docs.atlas.mongodb.com/getting-started/).\n2. [Load the sample data set](https://docs.atlas.mongodb.com/sample-data/).\n3. [Get your connection string](https://docs.atlas.mongodb.com/connect-to-cluster/).\n\nUse the copied connection string, providing it to the `mongosh` binary to connect to the pre-populated Atlas cluster with sample data. Then, switch to `sample_restaurants`\ndatabase.\n\n\n``` js\nmongosh \"mongodb+srv://<u>:<p>@<atlas-uri>/sample_restaurants\"\nUsing Mongosh : X.X.X\nUsing MongoDB:\t\tX.X.X\n\nFor mongosh info see: https://docs.mongodb.com/mongodb-shell/\n\nATLAS atlas-ugld61-shard-0 [primary]> use sample_restaurants;\n```\n\n## Using OpenAI Inside the mongosh Shell\n\nNow, we can build our `textToMql` function by pasting it into the `mongosh`. The function will receive a text sentence, use our generated OpenAI API key, and will try to return the best MQL command for it:\n\n``` js\nasync function textToMql(query){\n\nconst OpenAI = require('openai-api');\nconst openai-client = new OpenAI(\"<YOUR-OPENAI-API-KEY>\");\n\nconst fs = require('fs');\n\nvar data =  await  fs.promises.readFile('AI-input.txt', 'utf8');\n\nconst learningPath = data;\n\nvar aiInput =  learningPath + \"Q:\" + query + \"\\nA:\";\n\n const gptResponse = await openai-client.complete({\n     engine: 'davinci',\n     prompt: aiInput,\n     \"temperature\": 0.3,\n     \"max_tokens\": 400,\n     \"top_p\": 1,\n     \"frequency_penalty\": 0.2,\n     \"presence_penalty\": 0,\n     \"stop\": [\"\\n\"]\n });\n\n console.log(gptResponse.data.choices[0].text);\n}\n```\n\nIn the above function, we first load the OpenAI npm module and initiate a client with the relevant API key from OpenAI. \n\n``` js\nconst OpenAI = require('openai-api');\nconst openai-client = new OpenAI(\"<YOUR-OPENAI-API-KEY>\");\n\nconst fs = require('fs');\n```\n\nThe new shell allows us to import built-in and external [modules](https://docs.mongodb.com/mongodb-shell/write-scripts/#require-a-native-module) to produce an unlimited flexibility with our scripts.\n\nThen, we read the learning data from our `AI-input.txt` file. Finally we add our `Q: <query>` input to the end followed by the `A:` value which tells the engine we expect an answer based on the provided learningPath and our query. \n\n\nThis data will go over to an OpenAI API call:\n\n``` js\n const gptResponse = await openai.complete({\n     engine: 'davinci',\n     prompt: aiInput,\n     \"temperature\": 0.3,\n     \"max_tokens\": 400,\n     \"top_p\": 1,\n     \"frequency_penalty\": 0.2,\n     \"presence_penalty\": 0,\n     \"stop\": [\"\\n\"]\n });\n```\n\nThe call performs a completion API and gets the entire initial text as a `prompt` and receives some additional parameters, which I will elaborate on:\n\n* `engine`: OpenAI supports a few AI engines which differ in quality and purpose as a tradeoff for pricing. The “davinci” engine is the most sophisticated one, according to OpenAI, and therefore is the most expensive one in terms of billing consumption.\n* `temperature`: How creative will the AI be compared to the input we gave it? It can be between 0-1. 0.3 felt like a down-to-earth value, but you can play with it.\n* `Max_tokens`: Describes the amount of data that will be returned.\n* `Stop`: List of characters that will stop the engine from producing further content. Since we need to produce MQL statements, it will be one line based and “\\n” is a stop character.\n\nOnce the content is returned, we parse the returned JSON and print it with `console.log`.\n\n### Lets Put OpenAI to the Test with MQL\n\nOnce we have our function in place, we can try to produce a simple query to test it:\n\n``` js\nAtlas atlas-ugld61-shard-0 [primary] sample_restaurants> textToMql(\"query all restaurants where cuisine is American and name starts with 'Ri'\")\n db.restaurants.find({cuisine : \"American\", name : /^Ri/})\n\nAtlas atlas-ugld61-shard-0 [primary] sample_restaurants> db.restaurants.find({cuisine : \"American\", name : /^Ri/})\n[\n  {\n    _id: ObjectId(\"5eb3d668b31de5d588f4292a\"),\n    address: {\n      building: '2780',\n      coord: [ -73.98241999999999, 40.579505 ],\n      street: 'Stillwell Avenue',\n      zipcode: '11224'\n    },\n    borough: 'Brooklyn',\n    cuisine: 'American',\n    grades: [\n      {\n        date: ISODate(\"2014-06-10T00:00:00.000Z\"),\n        grade: 'A',\n        score: 5\n      },\n      {\n        date: ISODate(\"2013-06-05T00:00:00.000Z\"),\n        grade: 'A',\n        score: 7\n      },\n      {\n        date: ISODate(\"2012-04-13T00:00:00.000Z\"),\n        grade: 'A',\n        score: 12\n      },\n      {\n        date: ISODate(\"2011-10-12T00:00:00.000Z\"),\n        grade: 'A',\n        score: 12\n      }\n    ],\n    name: 'Riviera Caterer',\n    restaurant_id: '40356018'\n  }\n...\n```\n\nNice! We never taught the engine about the  `restaurants` collection or how to filter with [regex](https://docs.mongodb.com/manual/reference/operator/query/regex/) operators but it still made the correct AI decisions. \n\nLet's do something more creative.\n\n``` js\nAtlas atlas-ugld61-shard-0 [primary] sample_restaurants> textToMql(\"Generate an insert many command with random fruit names and their weight\")\n db.fruits.insertMany([{name: \"apple\", weight: 10}, {name: \"banana\", weight: 5}, {name: \"grapes\", weight: 15}])\nAtlas atlas-ugld61-shard-0 [primary]sample_restaurants>  db.fruits.insertMany([{name: \"apple\", weight: 10}, {name: \"banana\", weight: 5}, {name: \"grapes\", weight: 15}])\n{\n  acknowledged: true,\n  insertedIds: {\n    '0': ObjectId(\"60e55621dc4197f07a26f5e1\"),\n    '1': ObjectId(\"60e55621dc4197f07a26f5e2\"),\n    '2': ObjectId(\"60e55621dc4197f07a26f5e3\")\n  }\n}\n```\n\nOkay, now let's put it to the ultimate test: [aggregations](https://docs.mongodb.com/manual/aggregation/)!\n\n``` js\nAtlas atlas-ugld61-shard-0 [primary] sample_restaurants> use sample_mflix;\nAtlas atlas-ugld61-shard-0 [primary] sample_mflix> textToMql(\"Aggregate the count of movies per year (sum : 1) on collection movies\")\n db.movies.aggregate([{$group : { _id : \"$year\", count : { $sum : 1 }}}]);\n\nAtlas atlas-ugld61-shard-0 [primary] sample_mflix>  db.movies.aggregate([{$group : { _id : \"$year\", count : { $sum : 1 }}}]);\n[\n  { _id: 1967, count: 107 },\n  { _id: 1986, count: 206 },\n  { _id: '2006è2012', count: 2 },\n  { _id: 2004, count: 741 },\n  { _id: 1918, count: 1 },\n  { _id: 1991, count: 252 },\n  { _id: 1968, count: 112 },\n  { _id: 1990, count: 244 },\n  { _id: 1933, count: 27 },\n  { _id: 1997, count: 458 },\n  { _id: 1957, count: 89 },\n  { _id: 1931, count: 24 },\n  { _id: 1925, count: 13 },\n  { _id: 1948, count: 70 },\n  { _id: 1922, count: 7 },\n  { _id: '2005è', count: 2 },\n  { _id: 1975, count: 112 },\n  { _id: 1999, count: 542 },\n  { _id: 2002, count: 655 },\n  { _id: 2015, count: 484 }\n]\n```\n\nNow *that* is the AI power of MongoDB pipelines!\n\n## DEMO\n\n[![asciicast](https://asciinema.org/a/424297.svg)](https://asciinema.org/a/424297)\n\n## Wrap-Up\n\nMongoDB's new shell allows us to script with enormous power like never before by utilizing npm external packages. Together with the power of OpenAI sophisticated AI patterns, we were able to teach the shell how to prompt text to accurate complex MongoDB commands, and with further learning and tuning, we can probably get much better results.\n\nTry this today using the new MongoDB shell.","description":"Learn how new mongosh external modules can be used to generate MQL language via OpenAI engine. Transform simple text sentences into sophisticated queries. ","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt56c60f6adc3c5492/644c46668bfaaa7aea424a60/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/generate-mql-with-mongosh-and-openai","title":"*Generating MQL Shell Commands Using OpenAI and New mongosh Shell","original_publish_date":"2021-07-23T15:28:33.451Z","strapi_updated_at":"2022-12-19T18:34:45.594Z","expiry_date":"2022-07-11T07:40:14.319Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Shell","calculated_slug":"/products/mongodb/shell"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt6e3038b8dc0e29f1/644c466850ad36cfb7e3a4a5/OG_1200x630.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:28.546Z","publish_details":{"time":"2023-04-28T22:52:53.067Z"}}},{"calculated_slug":"/products/realm/realm-flexible-sync-preview","content":"> Atlas Device Sync's flexible sync mode is now GA. Learn more [here](https://www.mongodb.com/docs/atlas/app-services/sync/data-access-patterns/flexible-sync/).\n\n## Introduction\n\nWhen MongoDB acquired Realm in 2019, we knew we wanted to give developers the easiest and fastest way to synchronize data on-device with a backend in the cloud.\n\n:youtube[]{vid=6WrQ-f0dcIA}\n\nIn an offline-first environment, edge-to-cloud data sync typically requires thousands of lines of complex conflict resolution and networking code, and leaves developers with code bloat that slows the development of new features in the long-term. MongoDB’s Atlas Device Sync simplifies moving data between the Realm Mobile Database and MongoDB Atlas. With huge amounts of boilerplate code eliminated, teams are able to focus on the features that drive 5-star app reviews and happy users. \n\nSince bringing Atlas Device Sync GA in February 2021, we’ve seen it transform the way developers are building data synchronization into their mobile applications. But we’ve also seen developers creating workarounds for complex sync use cases. With that in mind, we’ve been hard at work building the next iteration of Sync, which we’re calling Flexible Sync.\n\nFlexible Sync takes into account a year’s worth of user feedback on partition-based sync, and aims to make syncing data to MongoDB Atlas a simple and idiomatic process by using a client-defined query to define the data synced to user applications.\n\n> **Build better mobile apps with Atlas Device Sync**: Atlas Device Sync is a fully-managed mobile backend-as-a-service. Leverage out-of-the-box infrastructure, data synchronization capabilities, built-in network handling, and much more to quickly launch enterprise-grade mobile apps. [Get started now by build: Deploy Sample for Free](https://www.mongodb.com/realm/register?tck=devhub_previewflexiblesync)!\n\n## How Flexible Sync Works\n\nFlexible Sync lets developers start writing code that syncs data more quickly – allowing you to choose which data is synced via a language-native query and to change the queries that define your syncing data at any time.\n\nWith Flexible Sync, developers can enable devices to define a query on the client side using the Realm SDK’s query-language, which will execute on MongoDB Atlas to identify the set of data to Sync. Any documents that match the query will be translated to Realm Objects and saved to the client device’s local disk. The query will be maintained on the server, which will check in real-time to identify if new document insertions, updates, or deletions on Atlas change the query results. Relevant changes on the server-side will be replicated down to the client in real-time, and any changes from the client will be similarly replicated to Atlas.\n\n## New Capabilities\n\nFlexible Sync is distinctly different from the [partition-based sync](https://docs.mongodb.com/realm/sync/partitioning/#std-label-partitioning) used by Device Sync today. \n\nWith partition-based sync, developers must configure a partition field for their Atlas database. This partition field lives on each document within the Atlas database that the operator wants to sync. Clients can then request access to different partitions of the Atlas database, using the different values of the partition key field. When a client opens a synchronized Realm they pass in the partition key value as a parameter. The sync server receives the value from the client, and sends any documents down to the client that match the partition key value. These documents are automatically translated as Realm Objects and stored on the client’s disk for offline access. \n\nPartition-based sync works well for applications where data is static and compartmentalized, and where permissions models rarely need to change. With Flexible Sync, we’re making fine-grained and flexible permissioning possible, and opening up new app use cases through simplifying the syncing of data that requires ranged or dynamic queries.\n\n## Flexible Permissions\n\nUnlike with partition-based sync, Flexible Sync makes it seamless to implement the [document-level permission model](https://docs.mongodb.com/realm/mongodb/define-roles-and-permissions/) when syncing data - meaning synced fields can be limited based on a user’s role. We expect this to be available at beta, and with field-level permissions coming after that.\n\nConsider a healthcare app, with different field-level permissions for Patients, Doctors, and Administrative staff using the application. A patient collection contains user data about the patient, their health history, procedures undergone, and prognosis. The patient accessing the app would only be able to see their full healthcare history, along with their own personal information. Meanwhile, a doctor using the app would be able to see any patients assigned to their care, along with healthcare history and prognosis. But doctors viewing patient data would be unable to view certain personal identifying information, like social security numbers. Administrative staff who handle billing would have another set of field-level permissions, seeing only the data required to successfully bill the patient. \n\nUnder the hood, this is made possible when Flexible Sync runs the query sent by the client, obtains the result set, and then subtracts any data from the result set sent down to the client based on the permissions. The server guards against clients receiving data they aren’t allowed to see, and developers can trust that the server will enforce compliance, even if a query is written with mistakes. In this way, Flexible Sync simplifies sharing subsets of data across groups of users and makes it easier for your application's permissions to mirror complex organizations and business requirements.\n\nFlexible Sync also allows clients to share some documents but not others, based on the ResultSet of their query. Consider a company where teams typically share all the data within their respective teams, but not across teams. When a new project requires teams to collaborate, Flexible Sync makes this easy. The shared project documents could have a field called allowedTeams: [ marketing, sales]. Each member of the team would have a client-side query, searching for all documents on allowedTeams matching marketing or sales using an $in operator, depending on what team that user was a member of.\n\n## Ranged & Dynamic Queries\n\nOne of Flexible Sync's primary benefits is that it allows for simple synchronization of data that falls into a range – such as a time window – and automatically adds and removes documents as they fall in and out of range.   \n\nConsider an app used by a company’s workforce, where the users only need to see the last seven days of work orders. With partition-based sync, a time-based trigger needed to fire daily to move work orders in and out of the relevant partition. With Flexible Sync, a developer can write a ranged query that automatically includes and removes data as time passes and the 7-day window changes. By adding a time based range component to the query, code is streamlined. The sync resultset gets a built-in TTL, which previously had to be implemented by the operator on the server-side. \n\nFlexible Sync also enables much more dynamic queries, based on user inputs. Consider a shopping app with millions of products in its Inventory collection. As users apply filters in the app – viewing only pants that are under $30 dollars and size large – the query parameters can be combined with logical ANDs and ORs to produce increasingly complex queries, and narrow down the search result even further. All of these query results are combined into a single realm file on the client’s device, which significantly simplifies code required on the client-side. \n\n## Looking Ahead\n\nUltimately, our decision to build Flexible Sync is driven by the Realm team’s desire to eliminate every possible piece of boilerplate code for developers. We’re motivated by delivering a sync service that can fit any use case or schema design pattern you can imagine, so that you can spend your time building features rather than implementing workarounds. \n\nThe Flexible Sync project represents the next evolution of Atlas Device Sync. We’re working hard to get to a public beta by the end of 2021, and believe this query-based sync has the potential to become the standard for Sync-enabled applications. We won’t have every feature available on day one, but iterative releases over the course of 2022 will continuously bring you more query operators and permissions integrations.\n\nInterested in joining the early access program? [Sign-up here](https://www.mongodb.com/realm/opt-in) and we’ll let you know when Flexible Sync is available in beta. \n\n","description":"Flexible Sync lets developers start writing code that syncs data more quickly – allowing you to choose which data is synced via a language-native query and to change the queries that define your syncing data at any time.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt04671d607e7e92fb/644c466a1023738922966c6e/realm-logo.jpg?branch=prod","description":null}}]},"slug":"/realm-flexible-sync-preview","title":"*A Preview of Flexible Sync","original_publish_date":"2021-07-13T07:57:28.603Z","strapi_updated_at":"2023-03-22T00:57:37.506Z","expiry_date":"2022-07-12T18:57:31.633Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Sync","calculated_slug":"/products/realm/sync"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*React Native","calculated_slug":"/technologies/react-native"}},{"node":{"title":"*Mobile","calculated_slug":"/technologies/mobile"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt7c48db4187beabb8/644c466cf3337d69b202ceeb/collection-globbing.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:28.188Z","publish_details":{"time":"2023-04-28T22:52:53.094Z"}}},{"calculated_slug":"/products/realm/realm-kotlin-041-announcement","content":"In this blogpost we are announcing v0.4.1 of the Realm Kotlin Multiplatform SDK. This release contains a significant architectural departure from previous releases of Realm Kotlin as well as other Realm SDK’s, making it much more compatible with modern reactive frameworks like [Kotlin Flows](https://kotlinlang.org/docs/flow.html). We believe this change will hugely benefit users in the Kotlin ecosystem.\n\n> **Build better mobile apps with Atlas Device Sync**: Atlas Device Sync is a fully-managed mobile backend-as-a-service. Leverage out-of-the-box infrastructure, data synchronization capabilities, built-in network handling, and much more to quickly launch enterprise-grade mobile apps. [Get started now by build: Deploy Sample for Free](https://www.mongodb.com/realm/register?tck=devhub_realmkotlin041)!\n\n## **Some background**\nThe Realm Java and Kotlin SDK’s have historically exposed a model of interacting with data we call Live Objects. Its primary design revolves around database objects acting as Live Views into the underlying database. \n\nThis was a pretty novel approach when Realm Java was first released 7 years ago. It had excellent performance characteristics and made it possible to avoid a wide range of nasty bugs normally found in concurrent systems.\n\nHowever, it came with one noticeable drawback: Thread Confinement.\n\nThread-confinement was not just an annoying restriction. This was what guaranteed that users of the API would always see a consistent view of the data, even across decoupled queries. Which was also the reason that Kotlin Native adopted a [similar memory model](https://kotlinlang.org/docs/native-concurrency.html)\n\nBut it also meant that you manually had to open and close realms on each thread where you needed data, and it was impossible to pass objects between threads [without additional boilerplate](https://docs.mongodb.com/realm/sdk/android/advanced-guides/threading/#communication-across-threads). \n\nBoth of which put a huge burden on developers.\n\nMore importantly, this approach conflicts with another model for working with concurrent systems, namely [Functional Reactive Programming (FRP)](https://en.wikipedia.org/wiki/Functional_reactive_programming). In the Android ecosystem this was popularized by the [RxJava framework](https://github.com/ReactiveX/RxJava) and also underpins Kotlin Flows.\n\nIn this mode, you see changes to data as immutable events in a stream, allowing complex mapping and transformations. Consistency is then guaranteed by the semantics of the stream; each operation is carried out in sequence so no two threads operate on the same object at the same time. \n\nIn this model, however, it isn’t uncommon for different operations to happen on different threads, breaking the thread-confinement restrictions of Realm. \n\nLooking at the plethora of frameworks that support this model (React JS, RxJava, Java Streams, Apple Combine Framework and Kotlin Flows) It becomes clear that this way of reasoning about concurrency is here to stay.\n\nFor that reason we decided to change our API to work much better in this context.\n\n## The new API\n\nSo today we are introducing a new architecture, which we internally have called the Frozen Architecture. It looks similar to the old API, but works in a fundamentally different way.\n\nRealm instances are now thread-safe, meaning that you can use the same instance across the entire application, making it easier to pass around with e.g. dependency injection.\n\nAll query results and objects from the database are frozen or immutable by default. They can now be passed freely between threads. This also means that they no longer automatically are kept up to date. Instead you must register change listeners in order to be notified about any change.\n\nAll modifications to data must happen by using a special instance of a `MutableRealm`, which is only available inside write transactions. Objects inside a write transaction are still live.\n\n## Opening a Realm\n\nOpening a realm now only needs to happen once. It can either be stored in a global variable or made available via dependency injection.\n\n```\n// Global App variable\nclass MyApp: Application() {\n   companion object {\n       private val config = RealmConfiguration(schema = setOf(Person::class))\n       public val REALM = Realm(config)\n   }\n}\n\n\n\n// Using dependency injection\nval koinModule = module {\n   single { RealmConfiguration(schema = setOf(Person::class)) }\n   single { Realm(get()) }\n}\n\n// Realms are now thread safe\nval realm = Realm(config)\nval t1 = Thread {\n   realm.writeBlocking { /* ... */ }\n}\nval t2 = Thread {\n   val queryResult = realm.objects(Person::class)\n}\n\n```\n\nYou can now safely keep your realm instance open for the lifetime of the application. You only need to close your realm when interacting with the realm file itself, such as when deleting the file or compacting it.\n\n```\n// Close Realm to free native resources\nrealm.close()\n```\n\n## Creating Data\nYou can only write within write closures, called `write` and `writeBlocking`. Writes happen through a [MutableRealm](https://docs.mongodb.com/realm-sdks/kotlin/latest/-realm%20-kotlin%20-multiplatform%20-s-d-k/io.realm/-mutable-realm/index.html) which is a [receiver](https://stackoverflow.com/questions/45875491/what-is-a-receiver-in-kotlin) of the  `writeBlocking` and `write` lambdas. \n\nBlocking:\n\n```\nval jane = realm.writeBlocking { \n   val unmanaged = Person(\"Jane\")\n   copyToRealm(unmanaged)\n}\n```\n\nOr run as a suspend function. Realm automatically dispatch writes to a write dispatcher backed by a background thread, so launching this from a scope on the UI thread like [`viewModelScope`](https://developer.android.com/topic/libraries/architecture/coroutines#viewmodelscope\n) is safe:\n\n```\nCoroutineScope(Dispatchers.Main).launch {\n\n   // Write automatically happens on a background dispatcher\n   val jane = realm.write {\n       val unmanaged = Person(\"Jane\")\n       // Add unmanaged objects\n       copyToRealm(unmanaged)\n   }\n\n   // Objects returned from writes are automatically frozen\n   jane.isFrozen() // == true\n\n   // Access any property.\n   // All properties are still lazy-loaded.\n   jane.name // == \"Jane\"\n}\n```\n\n## **Updating data**\n\nSince everything is frozen by default, you need to retrieve a live version of the object that you want to update, then write to that live object to update the underlying data in the realm.\n\n```\nCoroutineScope(Dispatchers.Main).launch {\n   // Create initial object   \n   val jane = realm.write {\n       copyToRealm(Person(\"Jane\"))\n   }\n  \n   realm.write {\n       // Find latest version and update it\n       // Note, this always involves a null-check\n       // as another thread might have deleted the\n       // object.\n       // This also works on objects without\n       // primary keys.\n       findLatest(jane)?.apply {\n           name = \"Jane Doe\"\n       }\n   }\n}\n```\n\n## Observing Changes\n\nChanges to all Realm classes are supported through Flows. Standard change listener API support is coming in a future release. \n\n```\nval jane = getJane()\nCoroutineScope(Dispatchers.Main).launch {\n   // Updates are observed using Kotlin Flow\n   val flow: Flow<Person> = jane.observe()\n   flow.collect {\n       // Listen to changes to the object\n       println(it.name)\n   }\n}\n```\n\nAs all Realm objects are now frozen by default, it is now possible to pass objects between different dispatcher threads without any additional boilerplate:\n\n```\nval jane = getJane()\nCoroutineScope(Dispatchers.Main).launch {\n\n   // Run mapping/transform logic in the background\n   val flow: Flow<String> = jane.observe()\n       .filter { it.name.startsWith(\"Jane\") }\n       .flowOn(Dispatchers.Unconfined)\n\n   // Before collecting on the UI thread\n   flow.collect {\n       println(it.name)\n   }\n}\n```\n\n## Pitfalls\n\nWith the change to frozen architecture, there are some new pitfalls to be aware of:\n\nUnrelated queries are no longer guaranteed to run on the same version.\n\n```\n// A write can now happen between two queries\nval results1: RealmResults<Person> = realm.objects(Person::class)\nval results2: RealmResults<Person> = realm.objects(Person::class)\n\n// Resulting in subsequent queries not returning the same result\nresults1.version() != results2.version()\nresults1.size != results2.size\n\n```\nWe will introduce API’s in the future that can guarantee that all operations within a certain scope are guaranteed to run on the same version. Making it easier to combine the results of multiple queries.\n\nDepending on the schema, it is also possible to navigate the entire object graph for a single object. It is only unrelated queries that risk this behaviour. \n\nStoring objects for extended periods of time can lead to [Version Pinning](https://docs.mongodb.com/realm/sdk/android/fundamentals/realms/#realm-file-size). This results in an increased realm file size. It is thus not advisable to store Realm Objects in global variables unless they are unmanaged. \n\n```\n// BAD: Store a global managed object\nMyApp.GLOBAL_OBJECT = realm.objects(Person::class).first()\n\n// BETTER: Copy data out into an unmanaged object\nval person = realm.objects(Person::class).first()\nMyApp.GLOBAL_OBJECT = Person(person.name)\n\n```\n\nWe will monitor how big an issue this is in practise and will introduce future API’s that can work around this if needed. It is currently possible to detect this happening by setting `RealmConfiguration.Builder.maxNumberOfActiveVersions()`\n \nUltimately we believe that these drawbacks are acceptable given the advantages we otherwise get from this architecture, but we’ll keep a close eye on these as the API develops further.\n\n## Conclusion \nWe are really excited about this change as we believe it will fundamentally make it a lot easier to use Realm Kotlin in Android and will also enable you to use Realm in Kotlin Multilplatform projects.\n\nYou can read more about how to get started at https://docs.mongodb.com/realm/sdk/kotlin-multiplatform/. We encourage you to try out this new version and leave any feedback at https://github.com/realm/realm-kotlin/issues/new. Sample projects can be found [here](https://github.com/realm/realm-kotlin-samples).\n\nThe SDK is still in alpha and as such none of the API’s are considered stable, but it is possible to follow our progress at https://github.com/realm/realm-kotlin. \n\nIf you are interested about learning more about how this works under the hood, you can also read more [here](https://docs.google.com/document/d/1bGfjbKLD6DSBpTiVwyorSBcMqkUQWedAmmS_VAhL8QU/edit)\n\nHappy hacking!","description":"In this blogpost we are announcing v0.4.1 of the Realm Kotlin Multiplatform SDK. This release contains a significant architectural departure from previous releases of Realm Kotlin as well as other Realm SDK’s, making it much more compatible with modern reactive frameworks like Kotlin Flows. We believe this change will hugely benefit users in the Kotlin ecosystem.\n","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltcc488f6f270d0c15/644c467057a8351f3664a6a2/realm-image.png?branch=prod","description":null}}]},"slug":"/realm-kotlin-041-announcement","title":"*Realm Kotlin 0.4.1 Announcement","original_publish_date":"2021-07-16T17:04:58.575Z","strapi_updated_at":"2023-03-22T00:57:56.367Z","expiry_date":"2022-07-16T16:51:32.882Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*News & Announcements","calculated_slug":"/news"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Kotlin","calculated_slug":"/languages/kotlin"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*Mobile","calculated_slug":"/technologies/mobile"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:27.784Z","publish_details":{"time":"2023-04-28T22:52:53.126Z"}}},{"calculated_slug":"/products/mongodb/document-enrichment-and-schema-updates","content":"So your business needs have changed and there’s additional data that needs to be stored within an existing dataset. Fear not! With MongoDB, this is no sweat.\n\n> In this article, I’ll show you how to quickly add and populate additional fields into an existing database collection.\n\n## The Scenario\n\nLet’s say you have a “Netflix” type application and you want to allow users to see which movies they have watched. We’ll use the sample\\_mflix database from the [sample datasets](https://docs.atlas.mongodb.com/sample-data/) available in a MongoDB Atlas cluster.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/I_Veb_N_Rm3_Ab3xi_F5c_Phpa3n_3zzf5pvv_I_Fsgnu3_X_6m_U_Knyev_Mrkvy_ZCTUF_5y_LM_Sv_WN_8_E11nbqzv3v_Mo95_C7_WYFGR_3l_OK_4_N2b_Rr9_Z4_Bu_U4_K_Cheipu_V_Rcvcq3_N_Wh_Zd3bu_J_Yafd_V_69c906938e)\n\nHere is the existing schema for the user collection in the sample\\_mflix database:\n\n``` js\n{\n    _id: ObjectId(),\n    name: <string>,\n    email: <string>,\n    password: <string>\n}\n```\n\n## The Solution\n\nThere are a few ways we could go about this. Since MongoDB has a flexible data model, we can just add our new data into existing documents.\n\nIn this example, we are going to assume that we know the user ID. We’ll use [`updateOne`](https://docs.mongodb.com/manual/reference/method/db.collection.updateOne/#mongodb-method-db.collection.updateOne) and the [`$addToSet`](https://docs.mongodb.com/manual/reference/operator/update/addToSet/) operator to add our new data.\n\n``` js\nconst { db } = await connectToDatabase();\nconst collection = await db.collection(“users”).updateOne(\n    { _id: ObjectID(“59b99db9cfa9a34dcd7885bf”) },\n    {\n        $addToSet: {\n            moviesWatched: {\n                <movieId>,\n                <title>,\n                <poster>\n            }\n        }\n    }\n);\n```\n\nThe `$addToSet` operator adds a value to an array avoiding duplicates. If the field referenced is not present in the document, `$addToSet` will create the array field and enter the specified value. If the value is already present in the field, `$addToSet` will do nothing.\n\nUsing `$addToSet` will prevent us from duplicating movies when they are watched multiple times.\n\n## The Result\n\nNow, when a user goes to their profile, they will see their watched movies.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/d_I_Tvq_WAL_4_Ap_Z_j_GN_Qx_Vu_Vxw6_3_Dt3_Krn_O_Fl_L9_OI_3be_DD_1_Q58dk_J39f7s0_Vn3p_Ptgcs_P_Ss3_XT_0_B_Kjcqh_H_Erw_J6if2_Ueeb_GA_Gc20_Fmfgt8j9g_H_Tm_Vow_Kk2_Hgb_Kbj_Eg9f_Xg_ACZ_Be_A_bb92f06e85)\n\nBut what if the user has not watched any movies? The user will simply not have that field in their document.\n\nI’m using Next.js for this application. I simply need to check to see if a user has watched any movies and display the appropriate information accordingly.\n\n``` js\n{ moviesWatched\n    ? \"Movies I've Watched\"\n    : \"I have not watched any movies yet :(\"\n}\n```\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/78ye_Oz_Njq_O_Rd2_Ngt_YQ_0ab_X_Fm_Vn_Eriu9_F_Bh_K8_Zx9_FD_Ls_ST_Uk_IR_Ee8_B_Iz_H_Tw_WI_Or_KB_Kzp_Ve_H_Eti_O66_fyb_MA_Eu_Yxa3gf2iq_Lg_Mska_O1w_W_Jc_Gk1_C7_Uk_F_Mud28_Sx_U4yse_0_Tb3_S9_F_e0d789b4bf)\n\n## Conclusion\n\nBecause of MongoDB’s flexible data model, we can have multiple schemas in one collection. This allows you to easily update data and fields in existing schemas.\n\nIf you would like to learn more about schema validation, take a look at the [Schema Validation](https://docs.mongodb.com/manual/core/schema-validation/) documentation.\n\nI’d love to hear your feedback or questions. Let’s chat in the [MongoDB Community](https://developer.mongodb.com/community/forums/).","description":"So your business needs have changed and there’s additional data that needs to be stored within an existing dataset. Fear not! With MongoDB, this is no sweat. In this article, I’ll show you how to quickly add and populate additional fields into an existing database collection.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt6d40d7e88b4b9bc6/644c46713dcc487df4f2dff9/https_raw.githubusercontent.com_10gen_devhub-content_master_source_images_atf-images_illustrations_schema-design-patterns.png_token_AMRZ4M4QANSX265CUPKYEHDBALERK?branch=prod","description":null}}]},"slug":"/document-enrichment-and-schema-updates","title":"*Document Enrichment and Schema Updates","original_publish_date":"2021-07-22T18:44:58.795Z","strapi_updated_at":"2022-05-12T20:59:15.377Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Schema","calculated_slug":"/products/mongodb/schema"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"So your business needs have changed and there’s additional data that needs to be stored within an existing dataset. Fear not! With MongoDB, this is no sweat.  In this article, I’ll show you how to quickly add and populate additional fields into an existing database collection.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt79cc8193ec73067b/644c46733ac5438a94a3f637/https_raw.githubusercontent.com_10gen_devhub-content_master_source_images_social_open-graph_og-schema-design-patterns.png_token_AMRZ4M73XI3XMVY77YMC4E3BALEBC?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@codestackr"},"system":{"updated_at":"2023-04-28T22:52:27.404Z","publish_details":{"time":"2023-04-28T22:52:53.154Z"}}},{"calculated_slug":"/products/mongodb/mongodb-podcast-doug-eck-google-brain","content":"[Doug Eck](https://www.linkedin.com/in/douglaseck/) is a principal scientist at Google and a research director on the Brain Team. He created the ongoing research project, Magenta, which focuses on the role of machine learning in the process of creating art and music. He is joining [Anaiya Raisinghani](https://www.linkedin.com/in/anaiyaraisinghani/), [Michael Lynn](https://www.linkedin.com/in/mlynn/), and [Nic Raboy](https://www.linkedin.com/in/nraboy/) today to discuss all things artificial intelligence, machine learning, and to give us some insight into his role at Google. \n\nWe are going to be diving head first into HCI (Human Computer Interaction), Google’s new GPT-3 language model, and discussing some of the hard issues with combining databases and deep learning. With all the hype surrounding AI, you may have some questions as to its past and potential future, so stay tuned to hear from one of Google’s best. \n\n:youtube[]{vid=Wge-1tcRQco}\n\n*Doug Eck* :[00:00:00] Hi everybody. My name is Doug Eck and welcome to the MongoDB podcast.\n\n*Michael Lynn* : [00:00:08] Welcome to the show. Today we're talking with [Doug Eck](https://www.linkedin.com/in/douglaseck). He's a principal scientist at [Google and a research director on the Brain Team](https://research.google/teams/brain/). He also created and helps lead the Magenta team, an ongoing research project exploring the role of machine learning and the process of creating art and music. Today's episode was produced and the interview was led by Anaiya Raisinghani  She's a summer intern here at MongoDB. She's doing a fantastic job. I hope you enjoy this episode.\n\nWe've got a couple of guests today and our first guest is a summer intern at MongoDB.\n\n*Anaiya Raisinghani* : [00:00:55] Hi everyone. My name is [Anaiya Raisinghani](https://www.linkedin.com/in/anaiyaraisinghani/) and I am the developer advocacy intern here at MongoDB.\n\n\n*Michael Lynn* : [00:01:01] Well, welcome to the show. It's great to have you on the podcast. Before we begin, why don't you tell the folks a little bit about yourself?\n\n*Anaiya Raisinghani* : [00:01:08] Yeah, of course. I'm from the Bay Area. I grew up here and I go to school in LA at the [University of Southern California](https://www.usc.edu/). My undergrad degree is in Computational Linguistics, which is half CS, half linguistics. And I want to say my overall interest in artificial intelligence, really came from the cool classes I have the unique opportunity to take, like speech recognition, natural language processing, and just being able to use machine learning libraries like TensorFlow in some of my school projects. So I feel very lucky to have had an early exposure to AI than most.\n\n\n*Michael Lynn* : [00:01:42] Well, great. And I understand that you brought a guest with you today. Do you want to talk a little bit about who that is and what we're going to discuss today?\n\n*Anaiya Raisinghani* : [00:01:48] Yes, definitely. So today we have a very, very special guest Doug Eck, who is a principal scientist at Google, a research director on the Brain Team and the creator of Magenta, so today we're going to be chatting about machine learning, AI, and some other fun topics.\nThank you so much, Doug, for being here today.\n\n*Doug Eck* :[00:02:07] I'm very happy to be here, Anaiya.\n\n\n*Michael Lynn* : [00:02:08] Well, Doug, it's great to have you on the show. Thanks so much for taking the time to talk with us. And at this point, I kind of want to turn it over to Anaiya. She's got some prepared questions. This is kind of her field of study, and she's got some passion and interest around it.\nSo we're going to get into some really interesting topics in the machine learning space. And Anaiya, I'll turn it over to you.\n\n*Anaiya Raisinghani* : [00:02:30] Perfect. Thank you so much, Mike. Just to get us started, Doug, could you give us a little background about what you do at Google? \n\n*Doug Eck* :[00:02:36] Sure, thanks, Anaiya. Well, right now in my career, I go to a lot of meetings. By that, I mean I'm running a large team of researchers on the Google brain team, and I'm trying to help keep things going. Sometimes it feels like herding cats because we hire very talented and very self motivated researchers who are doing fundamental research in machine learning. Going back a bit, I've been doing something like this, God, it's terrifying to think about, but almost 30 years. In a previous life when I was young, like you Anaiya, I was playing a lot of music, playing guitar. I was an English major as an undergrad, doing a lot of writing and I just kept getting drawn into technology. And once I finished my undergrad, I worked as a database programmer.\n\nWell, well, well before MongoDB. And, uh, I did that for a few years and really enjoyed it. And then I decided that my passion was somewhere in the overlap between music and artificial intelligence. And at that point in my life, I'm not sure I could have provided a crisp definition of artificial intelligence, but I knew I wanted to do it.\n\nI wanted to see if we can make intelligent computers help us make music. And so I made my way back into grad school. Somehow I tricked a computer science department into letting an English major do a PhD in computer science with a lot of extra math. And, uh, I made my way into an area of AI called machine learning, where our goal is to build computer programs that learn to solve problems, rather than kind of trying to write down the recipe ourselves.\n\nAnd for the last 20 years, I've been active in machine learning as a post-doc doing a post-doctoral fellowship in Switzerland. And then I moved to Canada and became a professor there and worked with some great people at the University of Montreal, just like changing my career every, every few years.\n\nSo, uh, after seven years there, I switched and came to California and became a research scientist at Google. And I've been very happily working here at Google, uh, ever since for 11 years, I feel really lucky to have had a chance to be part of the growth and the, I guess, Renaissance of neural networks and machine learning across a number of really important disciplines and to have been part of spearheading a bit of interest in AI and creativity.\n\n*Anaiya Raisinghani* : [00:04:45] That's great. Thank you so much. So there's currently a lot of hype around just AI in general and machine learning, but for some of our listeners who may not know what it is, how would you describe it in the way that you understand it?\n\n*Doug Eck* :[00:04:56] I was afraid you were going to ask that because I said, you know, 30 years ago, I couldn't have given you a crisp definition of AI and I'm not sure I can now without resorting to Wikipedia and cheating, I would define artificial intelligence as the task of building software that behaves intelligently.\nAnd traditionally there have been two basic approaches to AI in the past, in the distant past, in the eighties and nineties, we called this neat versus scruffy. Where neat was the idea of writing down sets of rules, writing down a recipe that defined complex behavior like translate a translation maybe, or writing a book, and then having computer programs that can execute those rules.\nContrast that with scruffy scruffy, because it's a bit messier. Um, instead of thinking we know the rules, instead we build programs that can examine data can look at large data sets. Sometimes datasets that have labels, like this is a picture, this is a picture of an orangutan. This is a picture of a banana, et cetera, and learn the relationship between those labels and that data.\nAnd that's a kind of machine learning where our goal is to help the machine learn, to solve a problem, as opposed to building in the answer. And long-term at least the current evidence where we are right now in 2021, is that for many, many hard tasks, probably most of them it's better to teach the machine how to learn rather than to try to provide the solution to the problem.\nAnd so that's how I would define a machine learning is writing software that learns to solve problems by processing information like data sets, uh, what might come out of a camera, what might come out of a microphone. And then learn to leverage what it's learned from that data, uh, to solve specific sub problems like translation or, or labeling, or you pick it.\nThere are thousands of possible examples. \n\n*Anaiya Raisinghani* : [00:06:51] That's awesome. Thank you so much. So I also wanted to ask, because you said from 30 years ago, you wouldn't have known that definition. What has it been like to see how machine learning has improved over the years? Especially now from an inside perspective at Google. \n\n*Doug Eck* :[00:07:07] I think I've consistently underestimated how fast we can move.\nPerhaps that's human nature. I noticed a statistic that, this isn't about machine learning, but something less than 70 years, 60, 61 years passed between the first flight, the Wright brothers and landing on the moon. And like 60 years, isn't very long. That's pretty shocking how fast we moved. And so I guess it shouldn't be in retrospect, a surprise that we've, we've moved so fast.\nI did a retrospective where I'm looking at the quality of image generation. I'm sure all of you have seen these hyper-realistic faces that are not really faces, or maybe you've heard some very realistic sounding music, or you've seen a machine learning algorithm able to generate really realistic text, and this was all happening.\nYou know, in the last five years, really, I mean, the work has been there and the ideas have been there and the efforts have been there for at least two decades, but somehow I think the combination of scale, so having very large datasets and also processing power, having large or one large computer or many coupled computers, usually running a GPU is basically, or TPU is what you think of as a video card, giving us the processing power to scale much more information.\nAnd, uh, I don't know. It's been really fun. I mean, every year I'm surprised I get up in the morning on Monday morning and I don't dread going to work, which makes me feel extremely lucky. And, uh, I'm really proud of the work that we've done at Google, but I'm really proud of what what's happened in the entire research community.\n\n\n*Michael Lynn* : [00:08:40] So Doug, I want to ask you and you kind of alluded to it, but I'm curious about the advances that we've made. And I realize we are very much standing on the shoulders of giants and the exponential rate at which we increase in the advances. I'm curious from your perspective, whether you think that's software or hardware and maybe what, you know, what's your perspective on both of those avenues that we're advancing in.\n\n*Doug Eck* :[00:09:08] I think it's a trade off. It's a very clear trade off. When you have slow hardware or not enough hardware, then you need to be much, much more clever with your software. So arguably the, the models, the approaches that we were using in the late 1990s, if you like terminology, if your crowd likes buzzwords support, vector machines, random forests, boosting, these are all especially SVM support vector machines are all relatively complicated. There's a lot of machinery there. And for very small data sets and for limited processing power, they can outperform simpler approaches, a simpler approach, it may not sound simple because it's got a fancy name, a neural network, the underlying mechanism is actually quite simple and it's all about having a very simple rule to update a few numbers.\nWe call them parameters, or maybe we call them weights and neural networks don't work all that well for small datasets and for small neural networks compared to other solutions. So in the 1980s and 1990s, it looked like they weren't really very good. If you scale these up and you run a simple, very simple neural network on with a lot of weights, a lot of parameters that you can adjust, and you have a lot of data allowing the model to have some information, to really grab onto they work astonishingly well, and they seem to keep working better and better as you make the datasets larger and you add more processing power. And that could be because they're simple. There's an argument to be made there that there's something so simple that it scales to different data sets, sizes and different, different processing power. We can talk about calculus, if you want. We can dive into the chain rule.\nIt's only two applications on the chain rule to get to backprop. \n\n\n*Michael Lynn* : [00:10:51] I appreciate your perspective. I do want to ask one more question about, you know, we've all come from this conventional digital, you know, binary based computing background and fascinating things are happening in the quantum space. I'm curious, you know, is there anything happening at Google that you can talk about in that space?\n\n*Doug Eck* :[00:11:11] Well, absolutely. We have. So first caveat, I am not an expert in quantum. We have a top tier quantum group down in Santa Barbara and they have made a couple of. It had been making great progress all along a couple of breakthroughs last year, my understanding of the situation that there's a certain class of problems that are extraordinarily difficult to solve with the traditional computer, but which a quantum computer will solve relatively easily.\nAnd that in fact, some of these core problems can form the basis for solving a much broader class of problems if you kind of rewrite these other problems as one of these core problems, like factorizing prime numbers, et cetera. And I have to admit, I am just simply not a quantum expert. I'm as fascinated about it as you are, we're invested.\nI think the big question mark is whether the class of problems that matter to us is big enough to warrant the investment and basically I've underestimated every other technological revolution. Right. You know, like I didn't think we'd get to where we are now. So I guess, you know, my skepticism about quantum is just, this is my personality, but I'm super excited about what it could be.\nIt's also, you know, possible that we'll be in a situation where Quantum yield some breakthroughs that provides us with some challenges, especially with respect to security and cryptography. If we find new ways to solve massive problems that lead indirectly for us to be able to crack cryptographic puzzles.\nBut if there's any quantum folks in the audience and you're shrugging your shoulders and be like, this guy doesn't know what he's talking about. This guy admits he doesn't really know what he's talking about.\n\n\n*Michael Lynn* : [00:12:44] I appreciate that. So I kind of derailed the conversation Anaiya, you can pick back up if you like.\n\n*Anaiya Raisinghani* : [00:12:51] Perfect. Thank you. Um, I wanted to ask you a little bit about HCI which is human computer interaction and what you do in that space. So a lot of people may not have heard about human computer interaction and the listeners. I can get like a little bit of a background if you guys would like, so it's really just a field that focuses on the design of computer technology and the way that humans and computers interact.\nAnd I feel like when people think about artificial intelligence, the first thing that they think about are, you know, robots or big spaces. So I wanted to ask you with what you've been doing at Google. Do you believe that machine learning can really help advance human computer interaction and the way that human beings and machines interact ethically?\n\n*Doug Eck* :[00:13:36] Thank you for that. That's an amazingly important question. So first a bit of a preface. I think we've made a fairly serious error in how we talk about AI and machine learning. And specifically I'm really turned off by the personification of AI. Like the AI is going to come and get you, right?\nLike it's a conscious thing that has volition and wants to help you or hurt you. And this link with AI and robotics, and I'm very skeptical of this sort of techno-utopian folks who believe that we can solve all problems in the world by building a sentient AI. Like there are a lot of real problems in front of us to solve.\nAnd I think we can use technology to help help us solve them. But I'm much more interested in solving the problems that are right in front of us, on the planet, rather than thinking about super intelligence or AGI, which is artificial general intelligence, meaning something smarter than us. So what does this mean for HCI human computer interaction?\nI believe fundamentally. We use technology to help us solve problems. We always have, we have from the very beginning of humanity with things like arrowheads and fire, right. And I fundamentally don't see AI and machine learning as any different. I think what we're trying to do is use technology to solve problems like translation or, you know, maybe automatic identification of objects and images and things like that.\nIdeally many more interesting problems than that. And one of the big roadblocks comes from taking a basic neural network or some other model trained on some data and actually doing something useful with it. And often it's a vast, vast, vast distance between a model and a lab that can, whatever, take a photograph and identify whether there's an orangutan or a banana in it and build something really useful, like perhaps some sort of medical software that will help you identify skin cancer. Right. And that, that distance ends up being more and more about how to actually make the software work for people deal with the messy real-world constraints that exist in our real, you know, in our actual world.\nAnd, you know, this means that like I personally and our team in general, the brain team we've become much more interested in HCI. And I wouldn't say, I think the way you worded it was can machine learning help revolutionize HCI or help HCI or help move HCI along. It's the wrong direction we need there like we need HCI's help. So, so we've, we've been humbled, I think by our inability to take like our fancy algorithms and actually have them matter in people's lives. And I think partially it's because we haven't engaged enough in the past decade or so with the HCI community. And, you know, I personally and a number of people on my, in my world are trying really hard to address that.\nBy tackling problems with like joint viewpoints, that viewpoint of like the mathematically driven AI researcher, caring about what the data is. And then the HCI and the user interface folks were saying, wait, what problem are you trying to solve? And how are you going to actually take what this model can do and put it in the hands of users and how are you going to do it in a way that's ethical per your comment Anaiya?\nAnd I hope someone grabbed the analogy of going from an image recognition algorithm to identifying skincancers. This has been one topic, for example, this generated a lot of discussion because skin cancers and skin color correlates with race and the ability for these algorithms to work across a spectrum of skin colors may differ, um, and our ability to build trust with doctors so that they want to use the software and patients, they believe they can trust the software.\nLike these issues are like so, so complicated and it's so important for us to get them right. So you can tell I'm a passionate about this. I guess I should bring this to a close, which is to say I'm a convert. I guess I have the fervor of a convert who didn't think much about HCI, maybe five, six years ago.\nI just started to see as these models get more and more powerful that the limiting factor is really how we use them and how we deploy them and how we make them work for us human beings. We're the personified ones, not the software, not the AI. \n\n*Anaiya Raisinghani* : [00:17:37] That's awesome. Thank you so much for answering my question, that was great. And I appreciate all the points you brought up because I feel like those need to be talked about a lot more, especially in the AI community. \nI do want to like pivot a little bit and take part of what you said and talk about some of the issues that come with deep learning and AI, and kind of connect them with neural networks and databases, because I would love to hear about some of the things that have come up in the past when deep learning has been tried to be integrated into databases. And I know that there can be a lot of issues with deep learning and tabular databases, but what about document collection based databases? And if the documents are analogous to records or rows in a relational database, do you think that machine learning might work or do you believe that the same issues might come up? \n\n*Doug Eck* :[00:18:24] Another great question.\nSo, so first to put this all in content, arguably a machine learning researcher. Who's really writing code day to day, which I did in the past and now I'm doing more management work, but you're, you know, you're writing code day-to-day, you're trying to solve a hard problem. Maybe 70 or 80% of your time is spent dealing with data and how to manage data and how to make sure that you don't have data errors and how to move the data through your system.\nProbably like in, in other areas of computer science, you know, we tend to call it plumbing. You spend a lot of time working on plumbing. And this is a manageable task. When you have a dataset of the sort we might've worked with 15 years ago, 10,000, 28 by 28 pixel images or something like that. I hope I got the pixels, right. Something called eminence, a bunch of written digits. \nIf we start looking at datasets that are all of the web basically represented in some way or another, all of the books in the library of Congress as a, as a hypothetical massive, massive image, data sets, massive video data sets, right? The ability to just kind of fake it.\nRight, write a little bit of Python code that processes your data and throws it in a flat file of some sort becomes, you know, becomes basically untraceable. And so I think we're at an inflection point right now maybe we were even at that inflection point a year or two ago. Where a lot of machine learning researchers are thinking about scalable ways to handle data.\nSo that's the first thing. The second thing is that we're also specifically with respect to very large neural networks, wanting predictions to be factual. If we have a chat bot that chats with you and that chat bot is driven by a neural network and you ask it, what's the capital of Indiana, my home state.\nWe hope it says Indianapolis every time. Uh, we don't want this to be a roll of the dice. We don't want it to be a probabilistic model that rolls the dice and says Indianapolis, you know, 50 times, but 51 time that 51st time instead says Springfield. So there's this very, very active and rich research area of bridging between databases and neural networks, which are probabilistic and finding ways to land in the database and actually get the right answer.\nAnd it's the right answer because we verify that it's the right answer. We have a separate team working with that database and we understand how to relate that to some decision-making algorithm that might ask a question: should I go to Indianapolis? Maybe that's a probabilistic question. Maybe it's role as a dice.\nMaybe you all don't want to come to Indianapolis. It's up to you, but I'm trying to make the distinction between, between these two kinds of, of decisions. Two kinds of information. One of them is probabilistic. Every sentence is unique. We might describe the same scene with a million different sentences.\nBut we don't want to miss on facts, especially if we want to solve hard problems. And so there's an open challenge. I do not have an answer for it. There are many, many smarter people than me working on ways in which we can bridge the gap between products like MongoDB and machine learning. It doesn't take long to realize there are a lot of people thinking about this.\nIf you do a Google search and you limit to the site, reddit.com and you put them on MongoDB and machine learning, you see a lot of discussion about how can we back machine learning algorithms with, with databases. So, um, it's definitely an open topic. Finally. Third, you mentioned something about rows and columns and the actual structure of a relational database.\nI think that's also very interesting because algorithms that are sensitive, I say algorithm, I mean a neural network or some other model program designed to solve a problem. You know, those algorithms might actually take advantage of that structure. Not just like cope with it, but actually understand in some ways how, in ways that it's learning how to leverage the structure of the database to make it easier to solve certain problems.\nAnd then there's evidence outside of, of databases for general machine learning to believe that's possible. So, for example, in work, for example, predicting the structure of proteins and other molecules, we have some what we might call structural prior information we have some idea about the geometry of what molecules should look like.\nAnd there are ways to leverage that geometry to kind of limit the space of predictions that the model would make. It's kind of given that structure as, as foundation for, for, for the productions, predictions is making such that it won't likely make predictions that violate that structure. For example, graph neural networks that actually work on a graph.\nYou can write down a database structure as a graph if you'd like, and, and take advantage of that graph for solving hard problems. Sorry, that was, it's like a 10 minute answer. I'll try to make them shorter next time, Anaiya, but that's my answer.\n\n*Anaiya Raisinghani* : [00:23:03] Yeah. Cause I, well, I was researching for this and then also when I got the job, a lot of the questions during the interview were, like how you would use machine learning, uh, during my internship and I saw articles like stretching all the way back the early two thousands talking about just how applying, sorry, artificial neural networks and ANN's to large modern databases seems like such a great idea in theory, because you know, like they, they offer potential fault tolerance, they're inherently parallel. Um, and the intersection between them just looks really super attractive. But I found this article about that and like, the date was 2000 and then I looked for other stuff and everything from there was the issues between connecting databases and deep learning.\nSo thank you so much for your answer. I really appreciate that. I feel like, I feel like, especially on this podcast, it was a great, great answer to a hard question. \n\n*Doug Eck* :[00:23:57] Can I throw, can I throw one more thing before you move on? There are also some like what I call low hanging fruit. Like a bunch of simpler problems that we can tackle.\nSo one of the big areas of machine learning that I've been working in is, is that of models of, of language of text. Right? And so think of translation, you type in a string in one language, and we translate it to another language or if, and if, if your listeners have paid attention to some, some new um, machine learning models that can, you can chat with them like chatbots, like Google's Lambda or some large language models that can write stories.\nWe're realizing we can use those for data augmentation and, and maybe indirectly for data verification. So we may be able to use neural networks to predict bad data entries. We may be able to, for example, let's say your database is trying to provide a thousand different ways to describe a scene. We may be able to help automate that.\nAnd then you'd have a human who's coming in. Like the humans always needs to be there I think to be responsible, you know, saying, okay, here's like, you know, 20 different ways to describe this scene at different levels of complexity, but we use the neural network to help make their work much, much faster.\nAnd so if we move beyond trying to solve the entire problem of like, what is a database and how do we generate it, or how do we do upkeep on it? Like, that's one thing that's like the holy grail, but we can be thinking about using neural networks in particularly language models to, to like basically super charge human data, data quality people in ways that I think are just gonna go to sweep through the field and help us do a much, much better job of, of that kind of validation. And even I remember from like a long time ago, when I did databases, data validation is a pain, right? Everybody hates bad data. It's garbage in, garbage out.\nSo if we can make cleaner, better data, then we all win.\n\n*Anaiya Raisinghani* : [00:25:39] Yeah. And on the subject of language models, I also wanted to talk about the GPT 3 and I saw an article from  MIT recently about how they're thinking it can replace Google's page rank. And I would just love to hear your thoughts on what you think might happen in the future and if language models actually could replace indexing. \n\n*Doug Eck* :[00:25:58] So to be clear, we will still need to do indexing, right? We still need to index the documents and we have to have some idea of what they mean. Here's the best way to think about it. So we, we talked to IO this year about using some large language models to improve our search in our products.\nAnd we've talked about it in other blogs. I don't want to get myself in trouble by poorly stating what has already been stated. I'd refer you there because you know, nobody wants, nobody wants to have to talk to their boss after the podcast comes out and says, why did you say that? You know, but here's the thing.\nThis strikes me. And this is just my opinion. Google's page rank. For those of you who don't know what page rank is, the basic idea is instead of looking at a document and what the document contains. We decide the value of the document by other documents that link into that document and how much we trust the other documents.\nSo if a number of high profile websites link to a document that happens to be about automobiles, we'll trust that that document is about automobiles, right? Um, and so it's, it's a graph problem where we assign trust and propagate it from, from incoming links. Um, thank you, Larry and Sergei. Behind that is this like fundamental mistrust of being able to figure out what's in a document.\nRight, like the whole idea is to say, we don't really know what's in this document. So we're going to come up with a trick that allows us to value this document based upon what other documents think about it. Right. And one way you could think about this revolution and large language models, um, like GPT-3 which came from open AI and, um, which is based upon some core technology that came from our group called transformer. That's the T in GPT-3 with there's always friendly rivalries that the folks at Open AI are great. And I think our team is great too. We'll kind of ratcheting up who can, who can move faster, um, cheers to Open AI.\nNow we have some pretty good ways of taking a document full of words. And if you want to think about this abstractly, projecting it into another space of numbers. So maybe for that document, which may have like as many words as you need for the document, let's say it's between 500 and 2,000 words, right. We take a neural network and we run that sequence through the neural network.\nAnd we come out with this vector of numbers that vector, that sequence of numbers maybe it's a thousand numbers right, now, thanks to the neural network that thousand numbers actually does a really good job of describing what's in the document. We can't read it with our eyes, cause it's just a sequence of numbers.\nBut if we take that vector and compare it to other vectors, what we'll find is similar vectors actually contain documents that contain very similar information and they might be written completely differently. Right. But topically they're similar. And so what we get is the ability to understand massive, massive data sets of text vis-a-vis what it's about, what it means, who it's for. And so we have a much better job of what's in a document now, and we can use that information to augment what we know about how people use documents, how they link to them and how much they trust them. And so that just gives us a better way to surface relevant documents for people.\nAnd that's kind of the crux in my mind, or at least in my view of why a large language model might matter for a search company. It helps us understand language and fundamentally most of search is about language.\n\n*Anaiya Raisinghani* : [00:29:11] I also wanted to talk to you about, because language is one of the big things with AI, but then now there's been a lot of movement towards art and music.\nAnd I know that you're really big into that. So I wanted to ask you about for the listeners, if you could explain a little bit behind Magenta, and then I also wanted to talk to you about Yacht because I heard that they used Magenta for yeah. For their new album. And so like, what are your thoughts on utilizing AI to continue on legacies in art and music and just creation?\n\n*Doug Eck* :[00:29:45] Okay, cool. Well, this is a fun question for me. Uh, so first what's Magenta? Magenta is an open source project that I'm very proud to say I created initially about six years ago. And our goal with Magenta is to explore the role of machine learning as a tool in the creative process. If you want to find it, it's at g.co/magenta.\nWe've been out there for a long time. You could also just search for Google Magenta and you'll find us, um, everything we do goes in open source basically provide tools for musicians and artists, mostly musicians based upon the team. We are musicians at heart. That you can use to extend your musical, uh, your musical self.\nYou can generate new melodies, you can change how things sound you can understand more, uh, the technology. You can use us to learn JavaScript or Python, but everything we do is about extending people and their music making. So one of the first things I always say is I think it would be, it's kind of cool that we can generate realistic sounding melodies that, you know, maybe sound like Bach or sound like another composer, but that's just not the point. That's not fun. Like, I think music is about people communicating with people. And so we're really more in the, in the heritage of, you know, Les Paul who invented was one of the inventors of the electric guitar or the cool folks that invented guitar pedals or amplifiers, or pick your favorite technology that we use to make a new kind of music.\nOur real question is can we like build a new kind of musical instrument or a new kind of music making experience using machine learning. And we've spent a lot of time doing fundamental research in this space, published in conferences and journals of the sort that all computer scientists do. And then we've done a lot of open source work in JavaScript so that you can do stuff really fast in the browser.\nAlso plugins for popular software for musicians like Ableton and then sort of core hardcore machine learning in Python, and we've done some experimental work with some artists. So we've tried to understand better on the HCI side, how this all works for real artists. And one of the first groups we worked with is in fact, thank you for asking a group called Yacht.\nThey're phenomenal in my mind, a phenomenal pop band. I think some part LCD sound system. I don't know who else to even add. They're from LA their front person. We don't say front man, because it's Claire is Claire Evans. She's an amazing singer, an utterly astonishing presence on stage. She's also a tech person, a tech writer, and she has a great book out that everybody should read, especially every woman in tech, Anaiya, called BroadBand the story of, um, of women in the internet. I mean, I don't remember if I've got the subtitle, right. So anyway very interesting people and what they did was they came to us and they worked with a bunch of other AI folks, not just Google at all. Like we're one of like five or six collaborators and they just dove in headfirst and they just wrestled with the technology and they tried to do something interesting.\nAnd what they did was they took from us, they took a machine learning model. That's able to generate variations on a theme. So, and they use pop music. So, you know, you give it right. And then suddenly the model is generating lots of different variations and they can browse around the space and they can play around and find different things.\nAnd so they had this like a slight AI extension of themselves. Right. And what they did was utterly fascinating. I think it's important. Um, they, they first just dove in and technically dealt with the problems we had. Our HCI game was very low then like we're like quite, quite literally first type this pro type this command into, into, into a console.\nAnd then it'll generate some midi files and, you know, there are musicians like they're actually quite technically good, but another set of musicians of like what's a command line. Right. You know, like what's terminal. So, you know, you have these people that don't work with our tooling, so we didn't have anything like fancy for them.\nBut then they also set constraints. So, uh, Jona and Rob the other two folks in the band, they came up with kind of a rule book, which I think is really interesting. They said, for example, if we take a melody generated by the Magenta model, we won't edit it ever, ever, ever. Right. We might reject it. Right. We might listen to a bunch of them, but we won't edit it.\nAnd so in some sense, they force themselves to like, and I think if they didn't do that, it would just become this mush. Like they, they wouldn't know what the AI had actually done in the end. Right. So they did that and they did the same with another, uh, some other folks, uh, generating lyrics, same idea.\nThey generated lots and lots of lyrics. And then Claire curated them. So curation was important for them. And, uh, this curation process proved to be really valuable for them. I guess I would summarize it as curation, without editing. They also liked the mistakes. They liked when the networks didn't do the right thing.\nSo they liked breakage like this idea that, oh, this didn't do what it was supposed to. I like that. And so this combination of like curiosity work they said it was really hard work. Um, and in a sense of kind of building some rules, building a kind of what I would call it, grammar around what they're doing the same way that like filmmakers have a grammar for how you tell a story.\nThey told a really beautiful story, and I don't know. I'm I really love Chain Tripping. That's the album. If you listened to it, every baseline was written by a magenta model. The lyrics were written by, uh, an LSTM network by another group. The cover art is done by this brilliant, uh, artists in Australia, Tom white, you know, it's just a really cool album overall.\n\n*Anaiya Raisinghani* : [00:35:09] Yeah, I've listened to it. It's great. I feel like it just alludes to how far technology has come. \n\n*Doug Eck* :[00:35:16] I agree. Oh, by the way that the, the drum beats, the drum beats come from the same model. But we didn't actually have a drum model. So they just threw away the notes and kept the durations, you know, and the baselines come from a model that was trained on piano, where the both of, both of both Rob and Jona play bass, but Rob, the guy who usually plays bass in the band is like, it would generate these baselines that are really hard to play.\nSo you have this like, idea of like the AI is like sort of generating stuff that they're just physically not used to playing on stage. And so I love that idea too, that it's like pushing them, even in ways that like onstage they're having to do things slightly differently with their hands than they would have to do.\nUm, so it's kind of pushes them out. \n\n\n*Michael Lynn* : [00:35:54] So I'm curious about the authoring process with magenta and I mean, maybe even specifically with the way Yacht put this album together, what are the input files? What trains the system. \n\n*Doug Eck* :[00:36:07] So in this case, this was great. We gave them the software, they provided their own midi stems from their own work.\nSo, that they really controlled the process. You know, our software has put out and is licensed for, you know, it's an Apache license, but we make no claims on what's being created. They put in their own data, they own it all. And so that actually made the process much more interesting. They weren't like working with some like weird, like classical music, piano dataset, right.\nThey were like working with their own stems from their own, um, their own previous recordings. \n\n\n*Michael Lynn* : [00:36:36] Fantastic. \n\n*Anaiya Raisinghani* : [00:36:38] Great. For my last question to kind of round this out, I just wanted to ask, what do you see that's shocking and exciting about the future of machine learning. \n\n*Doug Eck* :[00:36:49] I'm so bad at crystal ball. Um, \n\n\n*Michael Lynn* : [00:36:53] I love the question though.\n\n*Doug Eck* :[00:36:56] Yeah. So, so here, I think, I think first, we should always be humble about what we've achieved. If you, if you look, you know, humans are really smart, like way smarter than machines. And if you look at the generated materials coming from deep learning, for example, faces, when they first come out, whatever new model first comes out, like, oh my God, I can't tell them from human faces.\nAnd then if you play with them for a while, you're like, oh yeah, they're not quite right. They're not quite right. And this has always been true. I remember reading about like when the phonograph first came out and they would, they would demo the phonograph on, on like a stage in a theater. And this is like a, with a wax cylinder, you know?\nPeople will leave saying it sounds exactly like an orchestra. I can't tell it apart. Right. They're just not used to it. Right. And so like first I think we should be a little bit humble about what we've achieved. I think, especially with like GPT-3, like models, large language models, we've achieved a kind of fluency that we've never achieved before.\nSo the model sounds like it's doing something, but like it's not really going anywhere. Right. And so I think, I think by and large, the real shocking new, new breakthroughs are going to come as we think about how to make these models controllable so can a user really shape the output of one of these models?\nCan a policymaker add layers to the model that allow it to be safer? Right. So can we really have like use this core neural network as, you know, as a learning device to learn the things that needs to define patterns in data, but to provide users with much, much more control about how, how those patterns are used in a product.\nAnd that's where I think we're going to see the real wins, um, an ability to actually harness this, to solve problems in the right way.\n\n*Anaiya Raisinghani* : [00:38:33] Perfect. Doug, thank you so much for coming on today. It was so great to hear from you. \n\n*Doug Eck* :[00:38:39] That was great. Thanks for all the great questions, Anaiya, was fantastic \n\n*Michael Lynn* : [00:38:44] I'll reiterate that. Thanks so much, Doug. It's been great chatting with you. \nThanks for listening. If you enjoyed this episode, please like, and subscribe, have a question or a suggestion for the show? Visit us in the MongoDB community forums at community.Mongodb.com.\n\nThank you so much for taking the time to listen to our episode today. If you would like to learn more about Doug’s work at Google, you can find him through his [LinkedIn profile](https://www.linkedin.com/in/douglaseck/) or his [Google Research profile](https://research.google/people/author39086/). If you have any questions or comments about the episode, please feel free to reach out to Anaiya Raisinghani, Michael Lynn, or Nic Raboy. \n\nYou can also find this, and all episodes of the MongoDB Podcast on your favorite podcast network.\n\n* [Apple Podcasts](https://podcasts.apple.com/us/podcast/the-mongodb-podcast/id1500452446) \n* [Google Podcasts](https://podcasts.google.com/feed/aHR0cHM6Ly9tb25nb2RiLmxpYnN5bi5jb20vcnNz)\n* [Spotify](https://open.spotify.com/show/0ibUtrJG4JVgwfvB2MXMSb)\n\n","description":"Douglas Eck is a Principal Scientist at Google Research and a research director on the Brain Team. His work lies at the intersection of machine learning and human-computer interaction (HCI). Doug created and helps lead Magenta (g.co/magenta), an ongoing research project exploring the role of machine learning in the process of creating art and music. This article is a transcript of the podcast episode where Anaiya Rasinghani leads an interview with Doug to learn more about the intersection between AI, ML, HCI, and Databases.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt8f17e1732ae61b80/644c4677b4288c64b4fb3962/MongoDB_Podcast_Cover_Doug_Eck.png?branch=prod","description":null}}]},"slug":"/mongodb-podcast-doug-eck-google-brain","title":"*At the Intersection of AI/ML and HCI with Douglas Eck of Google (MongoDB Podcast)","original_publish_date":"2021-08-10T15:29:35.877Z","strapi_updated_at":"2023-02-27T14:30:56.828Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Podcast","calculated_slug":"/podcasts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:26.897Z","publish_details":{"time":"2023-04-28T22:52:53.179Z"}}},{"calculated_slug":"/products/realm/jetpack-compose-experience-android","content":"\n# Introduction \n\n### What is Jetpack Compose?\n\nAs per [Google](https://developer.android.com/jetpack/compose), *“Jetpack Compose is Android’s modern toolkit for building native UI. It simplifies and accelerates UI development on Android. Quickly bring your app to life with less code, powerful tools, and intuitive Kotlin APIs”.*\n\nIn my words, it’s a revolutionary **declarative way** of creating (or should I say composing 😄) UI in Android using Kotlin. Until now, we created layouts using XML and never dared to create via code (except for custom views, no choice) due to its complexity, non-intuitiveness, and maintenance issues.\n\nBut now it’s different!\n\n### What is Declarative UI?\n\n\n>  You know, imperative is like **how** you do something, and declarative is more like **what** you do, or something.\n\nDoesn’t that make sense? It didn’t to me as well in the first go 😄. In my opinion, imperative is more like an algorithm to perform any operation step by step, and declarative is the code that is built using the algorithm, more to do ***what*** works.\n\nIn Android, we normally create an XML of a layout and then update (sync) each element every time based on the input received, business rules using findViewById/Kotlin Semantics/View Binding/ Data Binding 😅.\n\nBut with **Compose**, we simply write a function that has both elements and rules, which is called whenever information gets updated. In short, a part of the UI is recreated every time **without** **performance** **issues**.\n\nThis philosophy or mindset will in turn help you write smaller (Single Responsibility principle) and reusable functions.\n\n\n### Why is Compose Getting So Popular?\n\nI’m not really sure, but out of the many awesome features, the ones I’ve loved most are:\n\n1. **Faster release cycle**: Bi-weekly, so now there is a real chance that if you get any issue with **composing,** it can be fixed soon. Hopefully!\n\n2. **Interoperable**: Similar to Kotlin, Compose is also interoperable with earlier UI design frameworks.\n\n3. **Jetpack library and material component built-in support**: Reduce developer efforts and time in building beautiful UI with fewer lines of code ❤️.\n\n4. **Declarative UI**: With a new way of building UI, we are now in harmony with all other major frontend development frameworks like SwiftUI, Flutter, and React Native, making it easier for the developer to use concepts/paradigms from other platforms.\n\n### Current state\n\nAs of 29th July, the first stable version was [released 1.0](https://android-developers.googleblog.com/2021/07/jetpack-compose-announcement.html), meaning **Compose is production-ready**.\n\n\n# Get Started with Compose\n\n### For using Compose, we need to set up a few things:\n\n\n 1. Kotlin v*1.5.10* and above, so let’s update our dependency in the project-level `build.gradle` file.\n\n    ```kotlin\n    plugins {\n            id 'org.jetbrains.kotlin:android' version '1.5.10'\n    } \n    ```\n\n2. Minimum  *API level 21*\n\n    ```kotlin\n    android {\n         defaultConfig {\n             ...\n             minSdkVersion 21\n        }\n    }    \n    ```\n\n3. Enable Compose \n\n    ```kotlin\n        android { \n\n            defaultConfig {\n                ...\n                minSdkVersion 21\n            }\n\n            buildFeatures {\n                // Enables Jetpack Compose for this module\n                compose true\n            }\n        }\n    ```\n    \n4. Others like min Java or Kotlin compiler and compose compiler\n\n    ```kotlin\n        android {\n            defaultConfig {\n                ...\n                minSdkVersion 21\n            }\n\n            buildFeatures {\n                // Enables Jetpack Compose for this module\n                compose true\n            }\n            ...\n\n            // Set both the Java and Kotlin compilers to target Java 8.\n           compileOptions {\n                sourceCompatibility JavaVersion.VERSION_1_8\n                targetCompatibility JavaVersion.VERSION_1_8\n            }\n            kotlinOptions {\n                jvmTarget = \"1.8\"\n            }\n\n            composeOptions {\n                kotlinCompilerExtensionVersion '1.0.0'\n            }\n        }\n    ```\n\n5. At last compose dependency for build UI\n\n    ```kotlin\n        dependencies {\n\n            implementation 'androidx.compose.ui:ui:1.0.0'\n            // Tooling support (Previews, etc.)\n            implementation 'androidx.compose.ui:ui-tooling:1.0.0'\n            // Foundation (Border, Background, Box, Image, Scroll, shapes, animations, etc.)\n            implementation 'androidx.compose.foundation:foundation:1.0.0'\n            // Material Design\n            implementation 'androidx.compose.material:material:1.0.0'\n            // Material design icons\n            implementation 'androidx.compose.material:material-icons-core:1.0.0'\n            implementation 'androidx.compose.material:material-icons-extended:1.0.0'\n            // Integration with activities\n            implementation 'androidx.activity:activity-compose:1.3.0'\n            // Integration with ViewModels\n            implementation 'androidx.lifecycle:lifecycle-viewmodel-compose:1.0.0-alpha07'\n            // Integration with observables\n            implementation 'androidx.compose.runtime:runtime-livedata:1.0.0'\n\n        }\n    ```\n\n\n### Mindset \n\nWhile composing UI, you need to unlearn various types of layouts and remember just one thing: Everything is a composition of *rows* and *columns*.\n\n![](https://cdn-images-1.medium.com/max/2108/1*VhWkcj7aUWNdE7e3CYJdQw.png)\n\nBut what about ConstraintLayout, which makes life so easy and is very useful for building complex UI? We can still use it ❤️, but in a little different way.\n\n\n### First Compose Project — Tweet Details Screen\n\n\n\nFor our learning curve experience, I decided to re-create this screen in Compose.\n\n![](https://cdn-images-1.medium.com/max/2000/1*yeFRhDs5R1CP4zaEomCmjQ.png)\n\n\n\nSo let’s get started. \n\nCreate a new project with Compose project as a template and open MainActivity.\n\nIf you don’t see the Compose project, then update Android Studio to the latest version.\n\n```kotlin\n    class MainActivity : ComponentActivity() {\n    \n        override fun onCreate(savedInstanceState: Bundle?) {\n            super.onCreate(savedInstanceState)\n            setContent {\n                ComposeTweetTheme {\n               \n                    .... \n                }\n            }\n        }\n    }\n```\n\nNow to add a view to the UI, we need to create a function with `@Composable` annotation, which makes it a Compose function.\n\nCreating our first layout of the view, toolbar\n\n```kotlin\n    @Composable\n    fun getAppTopBar() {\n        TopAppBar(\n            title = {\n                Text(text = stringResource(id = R.string.app_name))\n            },\n            elevation = 0.*dp\n        )\n    }\n```\n\nTo preview the UI rendered in Android Studio, we can use `@Preview` annotation.\n\n![](https://cdn-images-1.medium.com/max/4856/1*ukkhEHOGJ2B7WlPpDxHHzg.png)\n\nTopAppBar is an inbuilt material component for adding a topbar to our application.\n\nLet’s create a little more complex view, user profile view\n\n![](https://cdn-images-1.medium.com/max/2000/1*EgdJyqYqrpsl2w67GSrHIw.png)\n\nAs discussed earlier, in Compose, we have only rows and columns, so let’s break our UI 👇, where the red border represents columns and green is rows, and complete UI as a row in the screen.\n\n![](https://cdn-images-1.medium.com/max/2000/1*ICvhTxCdsy8RXR8jsMfO3g.png)\n\nSo let’s create our compose function for user profile view with our root row.\n\n![](https://cdn-images-1.medium.com/max/3984/1*v-SIoDDh9pQSTtJ6ooSyqQ.png)\n\nYou will notice the modifier argument in the Row function. This is the Compose way of adding formatting to the elements, which is uniform across all the elements.\n\nCreating a round imageview is very simple now. No need for any library or XML drawable as an overlay.\n\n```kotlin\nImage(\n            painter = painterResource(id = R.drawable.ic_profile),\n            contentDescription = \"Profile Image\",\n            modifier = Modifier\n                .size(36.dp)\n                .clip(CircleShape)\n                .border(1.dp, Color.Transparent, CircleShape),\n            contentScale = ContentScale.Crop\n        )\n ```\n\nAgain we have a `modifier` for updating our Image (AKA ImageView) with `clip` to make it rounded and `contentScale` to scale the image. \n\nSimilarly, adding a label will be a piece of cake now.\n\n```kotlin\n     Text (text = userName, fontSize = 20.sp)\n```\n\nNow let’s put it all together in rows and columns to complete the view.\n\n```kotlin\n@Composable\nfun userProfileView(userName: String, userHandle: String) {\n    Row(\n        modifier = Modifier\n            .fillMaxWidth()\n            .wrapContentHeight()\n            .padding(all = 12.dp),\n        verticalAlignment = Alignment.CenterVertically\n    ) {\n        Image(\n            painter = painterResource(id = R.drawable.ic_profile),\n            contentDescription = \"Profile Image\",\n            modifier = Modifier\n                .size(36.dp)\n                .clip(CircleShape)\n                .border(1.dp, Color.Transparent, CircleShape),\n            contentScale = ContentScale.Crop\n        )\n        Column(\n            modifier = Modifier\n                .padding(start = 12.dp)\n        ) {\n            Text(text = userName, fontSize = 20.sp, fontWeight = FontWeight.Bold)\n            Text(text = userHandle, fontSize = 14.sp)\n        }\n    }\n}\n```\n\n\nAnother great example is to create a Text Label with two styles. We know that traditionally doing that is very painful.\n\nLet’s see the Compose way of doing it.\n\n```kotlin\nText(\n            text = buildAnnotatedString {\n                withStyle(style = SpanStyle(fontWeight = FontWeight.ExtraBold)) {\n                    append(\"3\")\n                }\n                append(\" \")\n                withStyle(style = SpanStyle(fontWeight = FontWeight.Normal)) {\n\n                    append(stringResource(id = R.string.retweets))\n                }\n            },\n            modifier = Modifier.padding(end = 8.dp)\n        )\n\n```\n\n![](https://cdn-images-1.medium.com/max/2008/1*Sam96dWBFuNELaztMUAf0g.png)\n\nThat’s it!! I hope you’ve seen the ease of use and benefit of using Compose for building UI.\n\n![](https://cdn-images-1.medium.com/max/2000/1*eLrryz9etGCak2M3YPi1Ww.png)\n\nJust remember everything in Compose is rows and columns, and the order of attributes matters. You can check out my [Github repo](https://github.com/mongodb-developer/compose-tweets-android) complete example which also demonstrates the rendering of data using `viewModel`.\n\n>If you have questions, please head to our [developer community website](https://www.mongodb.com/community/forums/) where the MongoDB engineers and the MongoDB community will help you build your next big idea with MongoDB.","description":"Learn how to get started with Jetpack Compose on Android","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt9c0060da2175a78c/644c4678a633eca0667fb74b/realm-for-android.png?branch=prod","description":null}}]},"slug":"/jetpack-compose-experience-android","title":"*Unboxing Jetpack Compose: My First Compose App","original_publish_date":"2021-07-30T14:48:43.231Z","strapi_updated_at":"2022-05-09T19:20:25.734Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Kotlin","calculated_slug":"/languages/kotlin"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Android","calculated_slug":"/technologies/android"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:26.494Z","publish_details":{"time":"2023-04-28T22:52:53.214Z"}}},{"calculated_slug":"/products/atlas/get-hyped-synonyms-atlas-search","content":"Sometimes, the word you’re looking for is on the tip of your tongue, but you can’t quite grasp it. For example, when you’re  trying to find a really funny tweet you saw last night to show your friends. If you’re sitting there reading this and thinking, \"Wow, Anaiya and Nic, you’re so right. I wish there was a fix for this,\" strap on in! We have just the solution for those days when your precise linguistic abilities fail you, but you have an idea of what you’re looking for: **Synonyms in Atlas Search**. \n\nIn this tutorial, we are going to be showing you how to index a MongoDB collection to capture searches for words that mean similar things. For the specifics, we’re going to search through content written with Generation Z (Gen-Z) slang. The slang will be mapped to common words with synonyms and as a result, you’ll get a quick Gen-Z lesson without having to ever open TikTok. \n\nIf you’re in the mood to learn a few new words, alongside how effortlessly synonym mappings can be integrated into Atlas Search, this is the tutorial for you. \n\n## Requirements\n\nThere are a few requirements that must be met to be successful with this tutorial:\n\n- MongoDB Atlas M0 (or higher) cluster running MongoDB version 4.4 (or higher)\n- Node.js\n- A Twitter developer account\n\nWe’ll be using Node.js to load our Twitter data, but a Twitter developer account is required for accessing the APIs that contain Tweets.\n\n## Load Twitter Data into a MongoDB Collection\n\n![Example Tweet Data for Slang Synonyms](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/tweets_7ac7bd49a0.png \"Example Tweet Data for Slang Synonyms\")\n\nBefore starting this section of the tutorial, you’re going to need to have your Twitter API Key and API Secret handy. These can both be generated from the [Twitter Developer Portal](https://developer.twitter.com/en/portal/petition/use-case).\n\nThe idea is that we want to store a bunch of tweets in MongoDB that contain Gen-Z slang that we can later make sense of using Atlas Search and properly defined synonyms. Each tweet will be stored as a single document within MongoDB and will look something like this:\n\n```json\n{\n    \"_id\": 1420091624621629400,\n    \"created_at\": \"Tue Jul 27 18:40:01 +0000 2021\",\n    \"id\": 1420091624621629400,\n    \"id_str\": \"1420091624621629443\",\n    \"full_text\": \"Don't settle for a cheugy database, choose MongoDB instead 💪\",\n    \"truncated\": false,\n    \"entities\": {\n        \"hashtags\": [],\n        \"symbols\": [],\n        \"user_mentions\": [],\n        \"urls\": []\n    },\n    \"metadata\": {\n        \"iso_language_code\": \"en\",\n        \"result_type\": \"recent\"\n    },\n    \"source\": \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\",\n    \"in_reply_to_status_id\": null,\n    \"in_reply_to_status_id_str\": null,\n    \"in_reply_to_user_id\": null,\n    \"in_reply_to_user_id_str\": null,\n    \"in_reply_to_screen_name\": null,\n    \"user\": {\n        \"id\": 1400935623238643700,\n        \"id_str\": \"1400935623238643716\",\n        \"name\": \"Anaiya Raisinghani\",\n        \"screen_name\": \"anaiyaraisin\",\n        \"location\": \"\",\n        \"description\": \"Developer Advocacy Intern @MongoDB. Opinions are my own!\",\n        \"url\": null,\n        \"entities\": {\n            \"description\": {\n                \"urls\": []\n            }\n        },\n        \"protected\": false,\n        \"followers_count\": 11,\n        \"friends_count\": 29,\n        \"listed_count\": 1,\n        \"created_at\": \"Fri Jun 04 22:01:07 +0000 2021\",\n        \"favourites_count\": 8,\n        \"utc_offset\": null,\n        \"time_zone\": null,\n        \"geo_enabled\": false,\n        \"verified\": false,\n        \"statuses_count\": 7,\n        \"lang\": null,\n        \"contributors_enabled\": false,\n        \"is_translator\": false,\n        \"is_translation_enabled\": false,\n        \"profile_background_color\": \"F5F8FA\",\n        \"profile_background_image_url\": null,\n        \"profile_background_image_url_https\": null,\n        \"profile_background_tile\": false,\n        \"profile_image_url\": \"http://pbs.twimg.com/profile_images/1400935746593202176/-pgS_IUo_normal.jpg\",\n        \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/1400935746593202176/-pgS_IUo_normal.jpg\",\n        \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/1400935623238643716/1622845231\",\n        \"profile_link_color\": \"1DA1F2\",\n        \"profile_sidebar_border_color\": \"C0DEED\",\n        \"profile_sidebar_fill_color\": \"DDEEF6\",\n        \"profile_text_color\": \"333333\",\n        \"profile_use_background_image\": true,\n        \"has_extended_profile\": true,\n        \"default_profile\": true,\n        \"default_profile_image\": false,\n        \"following\": null,\n        \"follow_request_sent\": null,\n        \"notifications\": null,\n        \"translator_type\": \"none\",\n        \"withheld_in_countries\": []\n    },\n    \"geo\": null,\n    \"coordinates\": null,\n    \"place\": null,\n    \"contributors\": null,\n    \"is_quote_status\": false,\n    \"retweet_count\": 0,\n    \"favorite_count\": 1,\n    \"favorited\": false,\n    \"retweeted\": false,\n    \"lang\": \"en\"\n}\n```\n\nThe above document model is more extravagant than we need. In reality, we’re only going to be paying attention to the `full_text` field, but it’s still useful to know what exists for any given tweet.\n\nNow that we know what the document model is going to look like, we just need to consume it from Twitter.\n\nWe’re going to use two different Twitter APIs with our API Key and API Secret. The first API is the authentication API and it will give us our access token. With the access token we can get tweet data based on a Twitter query.\n\nSince we’re using Node.js, we need to install our dependencies. Within a new directory on your computer, execute the following commands from the command line:\n\n```bash\nnpm init -y\nnpm install mongodb axios --save\n```\n\nThe above commands will create a new **package.json** file and install the MongoDB Node.js driver as well as Axios for making HTTP requests.\n\nTake a look at the following Node.js code which can be added to a **main.js** file within your project:\n\n```javascript\nconst { MongoClient } = require(\"mongodb\");\nconst axios = require(\"axios\");\n\nrequire(\"dotenv\").config();\n\nconst mongoClient = new MongoClient(process.env.MONGODB_URI);\n\n(async () => {\n    try {\n        await mongoClient.connect();\n        const tokenResponse = await axios({\n            \"method\": \"POST\",\n            \"url\": \"https://api.twitter.com/oauth2/token\",\n            \"headers\": {\n                \"Authorization\": \"Basic \" + Buffer.from(`${process.env.API_KEY}:${process.env.API_SECRET}`).toString(\"base64\"),\n                \"Content-Type\": \"application/x-www-form-urlencoded\"\n            },\n            \"data\": \"grant_type=client_credentials\"\n        });\n        const tweetResponse = await axios({\n            \"method\": \"GET\",\n            \"url\": \"https://api.twitter.com/1.1/search/tweets.json\",\n            \"headers\": {\n                \"Authorization\": \"Bearer \" + tokenResponse.data.access_token\n            },\n            \"params\": {\n                \"q\": \"mongodb -filter:retweets filter:safe (from:codeSTACKr OR from:nraboy OR from:kukicado OR from:judy2k OR from:adriennetacke OR from:anaiyaraisin OR from:lauren_schaefer)\",\n                \"lang\": \"en\",\n                \"count\": 100,\n                \"tweet_mode\": \"extended\"\n            }\n        });\n        console.log(`Next Results: ${tweetResponse.data.search_metadata.next_results}`)\n        const collection = mongoClient.db(process.env.MONGODB_DATABASE).collection(process.env.MONGODB_COLLECTION);\n        tweetResponse.data.statuses = tweetResponse.data.statuses.map(status => {\n            status._id = status.id;\n            return status;\n        });\n        const result = await collection.insertMany(tweetResponse.data.statuses);\n        console.log(result);\n    } finally {\n        await mongoClient.close();\n    }\n})();\n```\n\nThere’s quite a bit happening in the above code so we’re going to break it down. However, before we break it down, it's important to note that we’re using environment variables for a lot of the sensitive information like tokens, usernames, and passwords. For security reasons, you really shouldn’t hard-code these values.\n\nInside the asynchronous function, we attempt to establish a connection to MongoDB. If successful, no error is thrown, and we make our first HTTP request.\n\n```javascript\nconst tokenResponse = await axios({\n    \"method\": \"POST\",\n    \"url\": \"https://api.twitter.com/oauth2/token\",\n    \"headers\": {\n        \"Authorization\": \"Basic \" + Buffer.from(`${process.env.API_KEY}:${process.env.API_SECRET}`).toString(\"base64\"),\n        \"Content-Type\": \"application/x-www-form-urlencoded\"\n    },\n    \"data\": \"grant_type=client_credentials\"\n});\n```\n\nOnce again, in this first HTTP request, we are exchanging our API Key and API Secret with an access token to be used in future requests.\n\nUsing the access token from the response, we can make our second request to the tweets API endpoint:\n\n```javascript\nconst tweetResponse = await axios({\n    \"method\": \"GET\",\n    \"url\": \"https://api.twitter.com/1.1/search/tweets.json\",\n    \"headers\": {\n        \"Authorization\": \"Bearer \" + tokenResponse.data.access_token\n    },\n    \"params\": {\n        \"q\": \"mongodb -filter:retweets filter:safe\",\n        \"lang\": \"en\",\n        \"count\": 100,\n        \"tweet_mode\": \"extended\"\n    }\n});\n```\n\nThe tweets API endpoint expects a Twitter specific query and some other optional parameters like the language of the tweets or the expected result count. You can check the query language in the [Twitter documentation](https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators).\n\nAt this point, we have an array of tweets to work with.\n\nThe next step is to pick the database and collection we plan to use and insert the array of tweets as documents. We can use a simple `insertMany` operation like this:\n\n```javascript\nconst result = await collection.insertMany(tweetResponse.data.statuses);\n```\n\nThe `insertMany` takes an array of objects, which we already have. We have an array of tweets, so each tweet will be inserted as a new document within the database.\n\nIf you have the MongoDB shell handy, you can validate the data that was inserted by executing the following:\n\n```javascript\nuse(\"synonyms\");\ndb.tweets.find({ });\n```\n\nNow that there’s data to work with, we can start to search it using slang synonyms.\n\n## Creating Synonym Mappings in MongoDB \n\nWhile we’re using a `tweets` collection for our actual searchable data, the synonym information needs to exist in a separate source collection in the same database. \n\nYou have two options for how you want your synonyms to be mapped–explicit or equivalent. You are not stuck with choosing just one type. You can have a combination of both explicit and equivalent as synonym documents in your collection. Choose the explicit format for when you need a set of terms to show up as a result of your inputted term, and choose equivalent if you want all terms to show up bidirectionally regardless of your queried term. \n\nFor example, the word \"basic\" means \"regular\" or \"boring.\" If we decide on an explicit (one-way) mapping for \"basic,\" we are telling Atlas Search that if someone searches for \"basic,\" we want to return all documents that include the words \"basic,\" \"regular,\" and \"boring.\" But! If we query the word \"regular,\" we would not get any documents that include \"basic\" because \"regular\" is not explicitly mapped to \"basic.\" \n\nIf we decide to map \"basic\" equivalently to \"regular\" and \"boring,\" whenever we query any of these words, all the documents containing \"basic,\" \"regular,\" **and** \"boring\" will show up regardless of the initial queried word. \n\nTo learn more about explicit vs. equivalent synonym mappings, check out the [official documentation](https://docs.atlas.mongodb.com/reference/atlas-search/synonyms/). \n\nFor our demo, we decided to make all of our synonyms equivalent and formatted our synonym data like this: \n\n```json\n[\n    {\n        \"mappingType\": \"equivalent\",\n        \"synonyms\": [\"basic\", \"regular\", \"boring\"]  \n    },\n    {\n        \"mappingType\": \"equivalent\",\n        \"synonyms\": [\"bet\", \"agree\", \"concur\"]\n    },\n    {\n        \"mappingType\": \"equivalent\",\n        \"synonyms\": [\"yikes\", \"embarrassing\", \"bad\", \"awkward\"]\n    },\n    {\n        \"mappingType\": \"equivalent\",\n        \"synonyms\": [\"fam\", \"family\", \"friends\"]\n    }\n]\n```\n\nEach object in the above array will exist as a separate document within MongoDB. Each of these documents contains information for a particular set of synonyms.\n\nTo insert your synonym documents into your MongoDB collection, you can use the ‘insertMany()’ MongoDB raw function to put all your documents into the collection of your choice. \n\n```javascript\nuse(\"synonyms\");\n\ndb.slang.insertMany([\n    {\n        \"mappingType\": \"equivalent\",\n        \"synonyms\": [\"basic\", \"regular\", \"boring\"]\n    },\n    {\n        \"mappingType\": \"equivalent\",\n        \"synonyms\": [\"bet\", \"agree\", \"concur\"]\n    }\n]);\n```\n\nThe `use(\"synonyms\");` line is to ensure you’re in the correct database before inserting your documents. We’re using the `slang` collection to store our synonyms and it doesn’t need to exist in our database prior to running our query.\n\n## Create an Atlas Search Index that Leverages Synonyms\n\nOnce you have your collection of synonyms handy and uploaded, it's time to create your search index! A search index is crucial because it allows you to use full-text search to find the inputted queries in that collection. \n\nWe have included screenshots below of what your MongoDB Atlas Search user interface will look like so you can follow along: \n\nThe first step is to click on the \"Search\" tab, located on your cluster page in between the \"Collections\" and \"Profiler\" tabs.\n\n![Find the Atlas Search Tab](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/atlas_search_tab_830c2159ce.png \"Find the Atlas Search Tab\")\n\nThe second step is to click on the \"Create Index\" button in the upper right hand corner, or if this is your first Index, it will be located in the middle of the page. \n\n![Create a New Atlas Search Index](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/create_atlas_search_index_e03316726a.png \"Create a New Atlas Search Index\")\n\nOnce you reach this page, go ahead and click \"Next\" and continue on to the page where you will name your Index and set it all up! \n\n![Name the Atlas Search Index](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/name_atlas_search_index_e8284eb612.png \"Name the Atlas Search Index\")\n\nClick \"Next\" and you’ll be able to create your very own search index! \n\n![Finalize the Atlas Search Index](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/finalize_atlas_search_index_b475015a58.png \"Finalize the Atlas Search Index\")\n\nOnce you create your search index, you can go back into it and then edit your index definition using the JSON editor to include what you need. The index we wrote for this tutorial is below: \n\n```json\n{\n    \"mappings\": {\n        \"dynamic\": true\n    },\n    \"synonyms\": [\n        {\n            \"analyzer\": \"lucene.standard\",\n            \"name\": \"slang\",\n            \"source\": {\n                \"collection\": \"slang\"\n            }\n        }\n    ]\n}\n```\n\nLet’s run through this! \n\n```json\n{\n    \"mappings\": {\n    \"dynamic\": true\n},\n```\n\nYou have the option of choosing between dynamic and static for your search index, and this can be up to your discretion. To find more information on the difference between dynamic and static mappings, check out the [documentation](https://docs.atlas.mongodb.com/reference/atlas-search/synonyms/).\n\n```json\n\"synonyms\": [\n    {\n        \"analyzer\": \"lucene.standard\",\n        \"name\": \"slang\",\n        \"source\": {\n            \"collection\": \"slang\"\n        }\n    }\n]\n```\n\nThis section refers to the synonyms associated with the search index. In this example, we’re giving this synonym mapping a name of \"slang,\" and we’re using the default index analyzer on the synonym data, which can be found in the slang collection. \n\n## Searching with Synonyms with the MongoDB Aggregation Pipeline\n\nOur next step is to put together the search query that will actually filter through your tweet collection and find the tweets you want using synonyms! \n\nThe code we used for this part is below:\n\n```javascript\nuse(\"synonyms\");\n\ndb.tweets.aggregate([\n   {\n       \"$search\": {\n           \"index\": \"synsearch\",\n           \"text\": {\n               \"query\": \"throw\",\n               \"path\": \"full_text\",\n               \"synonyms\": \"slang\"\n           }\n       }\n   }\n]);\n```\n\nWe want to search through our tweets and find the documents containing synonyms for our query \"throw.\" This is the synonym document for \"throw\":\n\n```json\n{\n    \"mappingType\": \"equivalent\",\n    \"synonyms\": [\"yeet\", \"throw\", \"agree\"]\n},\n```\n\nRemember to include the name of your search index from earlier (synsearch). Then, the query we’re specifying is \"throw.\" This means we want to see tweets that include \"yeet,\" \"throw,\" and \"agree\" once we run this script. \n\nThe ‘path’ represents the field we want to search within, and in this case, we are searching for \"throw\" only within the ‘full_text’ field of the documents and no other field. Last but not least, we want to use synonyms found in the collection we have named \"slang.\" \n\nBased on this query, any matches found will include the entire document in the result-set. To better streamline this, we can use a `$project` aggregation stage to specify the fields we’re interested in. This transforms our query into the following aggregation pipeline:\n\n```javascript\ndb.tweets.aggregate([\n    {\n        \"$search\": {\n            \"index\": \"synsearch\",\n            \"text\": {\n                \"query\": \"throw\",\n                \"path\": \"full_text\",\n                \"synonyms\": \"slang\"\n            }\n        }\n    },\n    {\n        \"$project\": {\n            \"_id\": 1,\n            \"full_text\": 1,\n            \"username\": \"$user.screen_name\"\n        }\n    }\n]);\n```\n\nAnd these are our results! \n\n```json\n[\n    {\n        \"_id\": 1420084484922347500,\n        \"full_text\": \"not to throw shade on SQL databases, but MongoDB SLAPS\",\n        \"username\": \"codeSTACKr\"\n    },\n    {\n        \"_id\": 1420088203499884500,\n        \"full_text\": \"Yeet all your data into a MongoDB collection and watch the magic happen! No cap, we are efficient 💪\",\n        \"username\": \"nraboy\"\n    }\n]\n```\n\nJust as we wanted, we have tweets that include the word \"throw\" and the word \"yeet!\" \n\n## Conclusion\n\nWe’ve accomplished a **ton** in this tutorial, and we hope you’ve enjoyed following along. Now, you are set with the knowledge to load in data from external sources, create your list of explicit or equivalent synonyms and insert it into a collection, and write your own index search script. Synonyms can be useful in a multitude of ways, not just isolated to Gen-Z slang. From figuring out regional variations (e.g., soda = pop), to finding typos that cannot be easily caught with autocomplete, incorporating synonyms will help save you time and a thesaurus. \n\nUsing synonyms in Atlas Search will improve your app’s search functionality and will allow you to find the data you’re looking for, even when you can’t quite put your finger on it. \n\nIf you want to take a look at the code, queries, and indexes used in this blog post, check out the project on [GitHub](https://github.com/mongodb-developer/twitter-slang-search). If you want to learn more about synonyms in Atlas Search, check out the [documentation](https://docs.atlas.mongodb.com/reference/atlas-search/synonyms/).\n\nIf you have questions, please head to our [developer community website](https://www.mongodb.com/community/forums/) where the MongoDB engineers and the MongoDB community will help you build your next big idea with MongoDB.","description":"Learn how to define your own custom synonyms for use with MongoDB Atlas Search in this example with features searching within slang found in Twitter messages.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltcf19c4d4bc0239df/644c467a118e75934febb63a/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/get-hyped-synonyms-atlas-search","title":"*Get Hyped: Synonyms in Atlas Search","original_publish_date":"2021-08-03T17:15:05.256Z","strapi_updated_at":"2023-02-27T14:30:56.828Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Search","calculated_slug":"/products/atlas/search"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Nodejs","calculated_slug":"/technologies/nodejs"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to define custom synonyms for MongoDB Atlas Search and search for data with them.","og_description":"Learn how to define custom synonyms for MongoDB Atlas Search and search for data with them.","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt51b2825b40200341/644c467d47e9ccf6e7cd0931/OG_1200x630.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@nraboy"},"system":{"updated_at":"2023-04-28T22:52:26.117Z","publish_details":{"time":"2023-04-28T22:52:53.242Z"}}},{"calculated_slug":"/languages/go/get-hyped-using-docker-go-mongodb","content":"In the developer community, ensuring your projects run accurately regardless of the environment can be a pain. Whether it’s trying to recreate a demo from an online tutorial or working on a code review, hearing the words, “Well, it works on my machine…” can be frustrating. Instead of spending hours debugging, we want to introduce you to a platform that will change your developer experience: [Docker](https://www.docker.com/). \n\nDocker is a great tool to learn because it provides developers with the ability for their applications to be used easily between environments, and it's resource-efficient in comparison to virtual machines. This tutorial will gently guide you through how to navigate Docker, along with how to integrate [Go](https://golang.org/) on the platform. We will be using this project to connect to our previously built MongoDB Atlas Search Cluster made for using [Synonyms in Atlas Search](https://www.mongodb.com/developer/how-to/get-hyped-synonyms-atlas-search/). Stay tuned for a fun read on how to learn all the above while also expanding your Gen-Z slang knowledge from our synonyms cluster. Get hyped! \n\n## The Prerequisites\n\nThere are a few requirements that must be met to be successful with this tutorial.\n\n- A M0 or better MongoDB Atlas cluster\n- Docker Desktop\n\nTo use MongoDB with the Golang driver, you only need a free M0 cluster. To create this cluster, follow the instructions listed on the [MongoDB documentation](https://docs.atlas.mongodb.com/tutorial/create-new-cluster/). However, we’ll be making many references to a [previous tutorial](https://www.mongodb.com/developer/how-to/get-hyped-synonyms-atlas-search/) where we used Atlas Search with custom synonyms.\n\nSince this is a Docker tutorial, you’ll need [Docker Desktop](https://www.docker.com/products/docker-desktop). You don’t actually need to have Golang configured on your host machine because Docker can take care of this for us as we progress through the tutorial.\n\n## Building a Go API with the MongoDB Golang Driver \n\nLike previously mentioned, you don’t need Go installed and configured on your host computer to be successful. However, it wouldn’t hurt to have it in case you wanted to test things prior to creating a Docker image.\n\nOn your computer, create a new project directory, and within that project directory, create a **src** directory with the following files:\n\n- go.mod\n- main.go\n\nThe **go.mod** file is our dependency management file for Go modules. It could easily be created manually or by using the following command:\n\n```bash\ngo mod init\n```\n\nThe **main.go** file is where we’ll keep all of our project code.\n\nStarting with the **go.mod** file, add the following lines:\n\n```\nmodule github.com/mongodb-developer/docker-golang-example\ngo 1.15\nrequire go.mongodb.org/mongo-driver v1.7.0\nrequire github.com/gorilla/mux v1.8.0\n```\n\nEssentially, we’re defining what version of Go to use and the modules that we want to use. For this project, we’ll be using the MongoDB Go driver as well as the [Gorilla Web Toolkit](https://www.gorillatoolkit.org/).\n\nThis brings us into the building of our simple API.\n\nWithin the **main.go** file, add the following code:\n\n```golang\npackage main\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"os\"\n\t\"time\"\n\n\t\"github.com/gorilla/mux\"\n\t\"go.mongodb.org/mongo-driver/bson\"\n\t\"go.mongodb.org/mongo-driver/mongo\"\n\t\"go.mongodb.org/mongo-driver/mongo/options\"\n)\n\nvar client *mongo.Client\nvar collection *mongo.Collection\n\ntype Tweet struct {\n\tID       int64  `json:\"_id,omitempty\" bson:\"_id,omitempty\"`\n\tFullText string `json:\"full_text,omitempty\" bson:\"full_text,omitempty\"`\n\tUser     struct {\n\t\tScreenName string `json:\"screen_name\" bson:\"screen_name\"`\n\t} `json:\"user,omitempty\" bson:\"user,omitempty\"`\n}\n\nfunc GetTweetsEndpoint(response http.ResponseWriter, request *http.Request) {}\nfunc SearchTweetsEndpoint(response http.ResponseWriter, request *http.Request) {}\n\nfunc main() {\n\tfmt.Println(\"Starting the application...\")\n\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n\tdefer cancel()\n\tclient, err := mongo.Connect(ctx, options.Client().ApplyURI(os.Getenv(\"MONGODB_URI\")))\n\tdefer func() {\n\t\tif err = client.Disconnect(ctx); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}()\ncollection = client.Database(\"synonyms\").Collection(\"tweets\")\n\trouter := mux.NewRouter()\n\trouter.HandleFunc(\"/tweets\", GetTweetsEndpoint).Methods(\"GET\")\n\trouter.HandleFunc(\"/search\", SearchTweetsEndpoint).Methods(\"GET\")\n\thttp.ListenAndServe(\":12345\", router)\n}\n```\n\nThere’s more to the code, but before we see the rest, let’s start breaking down what we have above to make sense of it.\n\nYou’ll probably notice our `Tweets` data structure:\n\n```golang\ntype Tweet struct {\n\tID       int64  `json:\"_id,omitempty\" bson:\"_id,omitempty\"`\n\tFullText string `json:\"full_text,omitempty\" bson:\"full_text,omitempty\"`\n\tUser     struct {\n\t\tScreenName string `json:\"screen_name\" bson:\"screen_name\"`\n\t} `json:\"user,omitempty\" bson:\"user,omitempty\"`\n}\n```\n\nEarlier in the tutorial, we mentioned that this example is heavily influenced by a [previous tutorial](https://www.mongodb.com/developer/how-to/get-hyped-synonyms-atlas-search/) that used Twitter data. We highly recommend you take a look at it. This data structure has some of the fields that represent a tweet that we scraped from Twitter. We didn’t map all the fields because it just wasn’t necessary for this example.\n\nNext, you’ll notice the following:\n\n```golang\nfunc GetTweetsEndpoint(response http.ResponseWriter, request *http.Request) {}\nfunc SearchTweetsEndpoint(response http.ResponseWriter, request *http.Request) {}\n```\n\nThese will be the functions that hold our API endpoint logic. We’re going to skip these for now and focus on understanding the connection and configuration logic.\n\nAs of now, most of what we’re interested in is happening in the `main` function.\n\nThe first thing we’re doing is connecting to MongoDB:\n\n```golang\nctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\ndefer cancel()\nclient, err := mongo.Connect(ctx, options.Client().ApplyURI(os.Getenv(\"MONGODB_URI\")))\ndefer func() {\n\tif err = client.Disconnect(ctx); err != nil {\n\t\tpanic(err)\n\t}\n}()\ncollection = client.Database(\"synonyms\").Collection(\"tweets\")\n```\n\nYou’ll probably notice the `MONGODB_URI` environment variable in the above code. It’s not a good idea to hard-code the MongoDB connection string in the application. This prevents us from being flexible and it could be a security risk. Instead, we’re using environment variables that we’ll pass in with Docker when we deploy our containers.\n\nYou can visit the MongoDB Atlas dashboard for your URI string.\n\nThe database we plan to use is `synonyms` and we plan to use the `tweets` collection, both of which we talked about in that previous tutorial.\n\nAfter connecting to MongoDB, we focus on configuring the Gorilla Web Toolkit:\n\n```golang\nrouter := mux.NewRouter()\nrouter.HandleFunc(\"/tweets\", GetTweetsEndpoint).Methods(\"GET\")\nrouter.HandleFunc(\"/search\", SearchTweetsEndpoint).Methods(\"GET\")\nhttp.ListenAndServe(\":12345\", router)\n```\n\nIn this code, we are defining which endpoint path should route to which function. The functions are defined, but we haven’t yet added any logic to them. The application itself will be serving on port 12345.\n\nAs of now, the application has the necessary basic connection and configuration information. Let’s circle back to each of those endpoint functions.\n\nWe’ll start with the `GetTweetsEndpoint` because it will work fine with an M0 cluster:\n\n```golang\nfunc GetTweetsEndpoint(response http.ResponseWriter, request *http.Request) {\n\tresponse.Header().Set(\"content-type\", \"application/json\")\n\tvar tweets []Tweet\n\tctx, _ := context.WithTimeout(context.Background(), 30*time.Second)\n\tcursor, err := collection.Find(ctx, bson.M{})\n\tif err != nil {\n\t\tresponse.WriteHeader(http.StatusInternalServerError)\n\t\tresponse.Write([]byte(`{ \"message\": \"` + err.Error() + `\" }`))\n\t\treturn\n\t}\n\tif err = cursor.All(ctx, &tweets); err != nil {\n\t\tresponse.WriteHeader(http.StatusInternalServerError)\n\t\tresponse.Write([]byte(`{ \"message\": \"` + err.Error() + `\" }`))\n\t\treturn\n\t}\n\tjson.NewEncoder(response).Encode(tweets)\n}\n```\n\nIn the above code, we’re saying that we want to use the `Find` operation on our collection for all documents in that collection, hence the empty filter object.\n\nIf there were no errors, we can get all the results from our cursor, load them into a `Tweet` slice, and then JSON encode that slice for sending to the client. The client will receive JSON data as a result.\n\nNow we can look at the more interesting endpoint function.\n\n```golang\nfunc SearchTweetsEndpoint(response http.ResponseWriter, request *http.Request) {\n\tresponse.Header().Set(\"content-type\", \"application/json\")\n\tqueryParams := request.URL.Query()\n\tvar tweets []Tweet\n\tctx, _ := context.WithTimeout(context.Background(), 30*time.Second)\n\tsearchStage := bson.D{\n\t\t{\"$search\", bson.D{\n\t\t\t{\"index\", \"synsearch\"},\n\t\t\t{\"text\", bson.D{\n\t\t\t\t{\"query\", queryParams.Get(\"q\")},\n\t\t\t\t{\"path\", \"full_text\"},\n\t\t\t\t{\"synonyms\", \"slang\"},\n\t\t\t}},\n\t\t}},\n\t}\n\tcursor, err := collection.Aggregate(ctx, mongo.Pipeline{searchStage})\n\tif err != nil {\n\t\tresponse.WriteHeader(http.StatusInternalServerError)\n\t\tresponse.Write([]byte(`{ \"message\": \"` + err.Error() + `\" }`))\n\t\treturn\n\t}\n\tif err = cursor.All(ctx, &tweets); err != nil {\n\t\tresponse.WriteHeader(http.StatusInternalServerError)\n\t\tresponse.Write([]byte(`{ \"message\": \"` + err.Error() + `\" }`))\n\t\treturn\n\t}\n\tjson.NewEncoder(response).Encode(tweets)\n}\n```\n\nThe idea behind the above function is that we want to use an aggregation pipeline for Atlas Search. It does use the synonym information that we outlined in the [previous tutorial](https://www.mongodb.com/developer/how-to/get-hyped-synonyms-atlas-search/).\n\nThe first important thing in the above code to note is the following:\n\n```golang\n​​queryParams := request.URL.Query()\n```\n\nWe’re obtaining the query parameters passed with the HTTP request. We’re expecting a `q` parameter to exist with the search query to be used.\n\nTo keep things simple, we make use of a single stage for the MongoDB aggregation pipeline:\n\n```golang\nsearchStage := bson.D{\n\t{\"$search\", bson.D{\n\t\t{\"index\", \"synsearch\"},\n\t\t{\"text\", bson.D{\n\t\t\t{\"query\", queryParams.Get(\"q\")},\n\t\t\t{\"path\", \"full_text\"},\n\t\t\t{\"synonyms\", \"slang\"},\n\t\t}},\n\t}},\n}\n```\n\nIn this stage, we are doing a text search with a specific index and a specific set of synonyms. The query that we use for our text search comes from the query parameter of our HTTP request.\n\nAssuming that everything went well, we can load all the results from the cursor into a `Tweet` slice, JSON encode it, and return it to the client that requested it.\n\nIf you have Go installed and configured on your computer, go ahead and try to run this application. Just don’t forget to add the `MONGODB_URI` to your environment variables prior.\n\nIf you want to learn more about API development with the Gorilla Web Toolkit and MongoDB, check out [this tutorial](https://www.thepolyglotdeveloper.com/2019/02/developing-restful-api-golang-mongodb-nosql-database/) on the subject.\n\n## Configuring a Docker Image for Go with MongoDB\n\nLet’s get started with Docker! If it’s a platform you’ve never used before, it might seem a bit daunting at first, but let us guide you through it, step by step. We will be showing you how to download Docker and get started with setting up your first Dockerfile to connect to our Gen-Z Synonyms Atlas Cluster. \n\nFirst things first. Let’s download Docker. This can be done through their [website](https://www.docker.com/products/docker-desktop) in just a couple of minutes. \n\nOnce you have that up and running, it’s time to create your very first Dockerfile. \n\nAt the root of your project folder, create a new **Dockerfile** file with the following content:\n\n```\n#get a base image\nFROM golang:1.16-buster\n\nMAINTAINER anaiya raisinghani <anaiya.raisinghani@mongodb.com>\n\nWORKDIR /go/src/app\nCOPY ./src .\n\nRUN go get -d -v\nRUN go build -v\n\nCMD [\"./docker-golang-example\"]\n```\n\nThis format is what many Dockerfiles are composed of, and a lot of it is heavily customizable and can be edited to fit your project's needs. \n\nThe first step is to grab a base image that you’re going to use to build your new image. You can think of using Dockerfiles as layers to a cake. There are a multitude of different base images out there, or you can use `FROM scratch` to start from an entirely blank image. Since this project is using the programming language Go, we chose to start from the `golang` base image and add the tag `1.16` to represent the version of Go that we plan to use. Whenever you include a tag next to your base image, be sure to set it up with a colon in between, just like this: `golang:1.16`. To learn more about which tag will benefit your project the best, check out [Docker’s documentation](https://hub.docker.com/_/golang?tab=tags&page=1&ordering=last_updated) on the subject.\n\nThis site holds a lot of different tags that can be used on a Golang base image. Tags are important because they hold very valuable information about the base image you’re using such as software versions, operating system flavor, etc. \n\nLet’s run through the rest of what will happen in this Dockerfile!\n\nIt's optional to include a `MAINTAINER` for your image, but it’s good practice so that people viewing your Dockerfile can know who created it. It's not necessary, but it’s helpful to include your full name and your email address in the file. \n\nThe `WORKDIR /go/src/app` command is crucial to include in your Dockerfile since `WORKDIR` specifies which working directory you’re in. All the commands after will be run through whichever directory you choose, so be sure to be aware of which directory you’re currently in.\n\nThe `COPY ./src .` command allows you to copy whichever files you want from the specified location on the host machine into the Docker image. \n\nNow, we can use the `RUN` command to set up exactly what we want to happen at image build time before deploying as a container. The first command we have is `RUN go get -d -v`, which will download all of the Go dependencies listed in the **go.mod** file that was copied into the image.. \n\nOur second `RUN` command is `RUN go build -v`, which will build our project into an executable binary file. \n\nThe last step of this Dockerfile is to use a `CMD` command, `CMD [“./docker-golang-example”]`. This command will define what is run when the container is deployed rather than when the image is built. Essentially we’re saying that we want the built Go application to be run when the container is deployed.\n\nOnce you have this Dockerfile set up, you can build and execute your project using your entire MongoDB URI link:\n\nTo build the Docker image and deploy the container, execute the following from the command line:\n\n```bash\ndocker build -t docker-syn-image .\ndocker run -d -p 12345:12345 -e “MONGODB_URI=YOUR_URI_HERE” docker-syn-image\n```\n\nFollowing these instructions will allow you to run the project and access it from http://localhost:12345. **But**! It’s so tedious. What if we told you there was an easier way to run your application without having to write in the entire URI link? There is! All it takes is one extra step: setting up a Docker Compose file. \n\n## Setting Up a Docker Compose File to Streamline Deployments\n\nA Docker Compose file is a nice little step to run all your container files and dependencies through a simple command: `docker compose up`.\n\nIn order to set up this file, you need to establish a YAML configuration file first. Do this by creating a new file in the root of your project folder, naming it **docker-compose**, and adding **.yml** at the end. You can name it something else if you like, but this is the easiest since when running the `docker compose up` command, you won’t need to specify a file name. Once that is in your project folder, follow the steps below.\n\nThis is what your Docker Compose file will look like once you have it all set up: \n\n```yaml\nversion: \"3.9\" \nservices:\n   web:\n       build: .\n       ports:\n           - \"12345:12345\"\n       environment:\n           MONGODB_URI: your_URI_here\n```\n\nLet’s run through it!\n\nFirst things first. Determine which schema version you want to be running. You should be using the most recent version, and you can find this out through [Docker’s documentation](https://docs.docker.com/compose/compose-file/).\n\nNext, define which services, otherwise known as containers, you want to be running in your project. We have included `web` since we are attaching to our Atlas Search cluster. The name isn’t important and it acts more as an identifier for that particular service. Next, specify that you are building your application, and put in your `ports` information in the correct spot. For the next step, we can set up our `environment` as our MongoDB URI and we’re done! \n\nNow, run the command `docker compose up` and watch the magic happen. Your container should build, then run, and you’ll be able to connect to your port and see all the tweets!\n\n## Conclusion\n\nThis tutorial has now left you equipped with the knowledge you need to build a Go API with the MongoDB Golang driver, create a Dockerfile, create a Docker Compose file, and connect your newly built container to a MongoDB Atlas Cluster. \n\nUsing these new platforms will allow you to take your projects to a whole new level. \n\nIf you’d like to take a look at the code used in our project, you can access it on [GitHub](https://github.com/mongodb-developer/docker-golang-example).\n\nUsing Docker or Go, but have a question? Check out the [MongoDB Community Forums](https://community.mongodb.com)!","description":"Learn how to create and deploy Golang-powered micro-services that interact with MongoDB using Docker.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt495822ec19d2384a/644c4682a89f624b81ec581c/Golang_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/get-hyped-using-docker-go-mongodb","title":"*Get Hyped: Using Docker + Go with MongoDB","original_publish_date":"2021-08-06T20:13:48.139Z","strapi_updated_at":"2023-02-27T14:30:56.828Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Go","calculated_slug":"/languages/go"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Go","calculated_slug":"/languages/go"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Docker","calculated_slug":"/technologies/docker"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to use Docker to deploy MongoDB compatible Go micro-services.","og_description":"Learn how to use Docker to deploy MongoDB compatible Go micro-services.","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt87439192f1f7f0fa/644c4683d676973cbb0af233/Golang_Tile_360x360.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@nraboy"},"system":{"updated_at":"2023-04-28T22:52:25.715Z","publish_details":{"time":"2023-04-28T22:52:53.268Z"}}},{"calculated_slug":"/products/realm/realm-cocoa-data-types","content":"In this blog post we will discover the new data types that Realm has to offer.\n\nOver the past year we have worked hard to bring three new datatypes to the Realm SDK: `MutableSet`, `Map`, and `AnyRealmValue`.\n\n> **Build better mobile apps with Atlas Device Sync**: Atlas Device Sync is a fully-managed mobile backend-as-a-service. Leverage out-of-the-box infrastructure, data synchronization capabilities, built-in network handling, and much more to quickly launch enterprise-grade mobile apps. [Get started now by build: Deploy Sample for Free](https://www.mongodb.com/realm/register?tck=devhub_newrealmcocoadatatypes)!\n\n## MutableSet\n\n`MutableSet` allows you to store a collection of unique values in an unordered fashion. This is different to `List` which allows you to store duplicates and persist the order of items.\n\n`MutableSet` has some methods that many will find useful for data manipulation and storage:\n\n- `Intersect`\n    - Gets the common items between two `MutableSet`s.\n- `Union`\n    - Combines elements from two `MutableSet`s, removing any duplicates.\n- `Subtract`\n    - Removes elements from one `MutableSet` that are present in another given `MutableSet`.\n- `isSubset`\n    - Checks to see if the elements in a `MutableSet` are children of a given super `MutableSet`.\n\nSo why would you use a `MutableSet` over a `List`?\n- You require a distinct collection of elements.\n- You do not rely on the order of items.\n- You need to perform mathematical operations such as `Intersect`, `Union`, and `Subtract`.\n- You need to test for membership in other Set collections using `isSubset` or `intersects`.\n\n### Practical example\n\nUsing our Movie object, we want to store and sync certain properties that will never contain duplicates and we don't care about ordering. Let's take a look below:\n\n```swift\nclass Movie: Object, ObjectKeyIdentifiable {\n   @Persisted(primaryKey: true) var _id: ObjectId\n   @Persisted var _partitionKey: String\n   // we will want to keep the order of the cast, so we will use a `List`\n   @Persisted var cast: List<String>\n   @Persisted var countries: MutableSet<String>\n   @Persisted var genres: MutableSet<String>\n   @Persisted var languages: MutableSet<String>\n   @Persisted var writers: MutableSet<String>\n}\n```\n\nStraight away you can see the use case, we never want to have duplicate elements in the `countries`, `genres`, `languages`, and `writers` collections, nor do we care about their stored order. `MutableSet` does support sorting so you do have the ability to rearrange the order at runtime, but you can't persist the order.\n\nYou query a `MutableSet` the same way you would with List:\n```swift\nlet danishMovies = realm.objects(Movie.self).filter(\"'Danish' IN languages\")\n```\n### Under the hood\n\n`MutableSet` is based on the `NSSet` type found in Foundation. From the highest level we mirror the `NSMutableSet / Set API` on `RLMSet / MutableSet`. \n\nWhen a property is unmanaged the underlying storage type is deferred to `NSMutableSet`.\n\n## Map\n\nOur new `Map` data type is a Key-Value store collection type. It is similar to Foundation's `Dictionary` and shares the same call semantics. You use a `Map` when you are unsure of a schema and need to store data in a structureless fashion. NOTE: You should not use `Map` over an `Object` where a schema is known.\n\n### Practical example\n\n```swift\n@Persisted phoneNumbers: Map<String, String>\n\nphoneNumbers[\"Charlie\"] = \"+353 86 123456789\"\nlet charliesNumber = phoneNumbers[\"Charlie\"] // \"+353 86 123456789\"\n```\n\n`Map` also supports aggregate functions so you can easily calculate data:\n\n```swift\n@Persisted testScores: Map<String, Int>\n\ntestScores[\"Julio\"] = 95\ntestScores[\"Maria\"] = 95\ntestScores[\"John\"] = 70\n\nlet averageScore = testScores.avg()\n```\n\nAs well as filtering with NSPredicate:\n\n```swift\n@Persisted dogMap: Map<String, Dog>\n\nlet spaniels = dogMap.filter(NSPredicate(\"breed = 'Spaniel'\")) // Returns `Results<Dog>`\n\n```\n\nYou can observe a `Map` just like the other collection types:\n\n```swift\nlet token = map.observe(on: queue) { change in\n    switch change {\n    case .initial(let map):\n        ...\n    case let .update(map, deletions: deletions, insertions: insertions, modifications: modifications):\n        // `deletions`, `insertions` and `modifications` contain the modified keys in the Map\n        ...\n    case .error(let error):\n\t...\n    }\n}\n```\n\nCombine is also supported for observation:\n\n```swift\ncancellable = map.changesetPublisher\n    .sink { change in\n        ...\n    }\n```\n\n### Under the hood\n\n`Map` is based on the `NSDictionary` type found in Foundation. From the highest level, we mirror the `NSMutableDictionary / Dictionary API` on `RLMDictionary / Map`. \n\nWhen a property is unmanaged the underlying storage type is deferred to `NSMutableDictionary`.\n\n## AnyRealmValue\n\nLast but not least, a datatype we are very excited about, `AnyRealmValue`. No this is not another collection type but one that allows you to store various different types of data under one property. Think of it like `Any` or `AnyObject` in Swift or a union in C.\n\nTo better understand how to use `AnyRealmValue`, let's see some practical examples.\n\nLet's say we have a Settings class which uses a `Map` for storing the user preferences, because the types of references we want to store are changing all the time, we are certain that this is schemaless for now:\n\n```swift\nclass Settings: Object {\n   @Persisted(primaryKey: true) var _id: ObjectId\n   @Persisted var _partitionKey: String?\n   @Persisted var misc: Map<String, AnyRealmValue>\n}\n```\n\nUsage:\n\n```swift\nmisc[\"lastScreen\"] = .string(\"home\")\nmisc[\"lastOpened\"] = .date(.now)\n\n// To unwrap the values\n\nif case let .string(lastScreen) = misc[\"lastScreen\"] {\n    print(lastScreen) // \"home\"\n}\n```\n\nHere we can store different variants of the value, so depending on the need of your application, you may find it useful to be able to switch between different types.\n\n### Under the hood\n\nWe don't use any Foundation types for storing `AnyRealmValue`. Instead the `AnyRealmValue` enum is converted to the ObjectiveC representation of the stored type. This is any type that conforms to `RLMValue`. You can see how that works [here](https://github.com/realm/realm-cocoa/blob/c88e037f1507d76b0bb86a49cd38563d0221ef22/RealmSwift/Impl/BasicTypes.swift#L314).\n\n## Conclusion\n\nI hope you found this insightful and have some great ideas with what to do with these data types! All of these new data types are fully compatible with MongoDB Realm Sync too, and are available in Objective-C as well as Swift. We will follow up with another post and presentation on data modelling with Realm soon.\n\nLinks to documentation:\n\n- [MutableSet](https://docs.mongodb.com/realm/sdk/ios/data-types/mutablesets/)\n- [Map](https://docs.mongodb.com/realm/sdk/ios/data-types/map/)\n- [AnyRealmValue](https://docs.mongodb.com/realm/sdk/ios/data-types/anyrealmvalue/)","description":"In this blog post we will discover the new data types that Realm Cocoa has to offer.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt04671d607e7e92fb/644c466a1023738922966c6e/realm-logo.jpg?branch=prod","description":null}}]},"slug":"/realm-cocoa-data-types","title":"*New Realm Cocoa Data Types","original_publish_date":"2021-08-09T11:15:59.819Z","strapi_updated_at":"2023-03-06T17:56:50.535Z","expiry_date":"2022-08-06T23:43:31.252Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Mobile","calculated_slug":"/technologies/mobile"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:25.300Z","publish_details":{"time":"2023-04-28T22:52:53.296Z"}}},{"calculated_slug":"/products/realm/building-space-shooter-game-syncs-unity-mongodb-realm","content":"When developing a game, in most circumstances you're going to need to store some kind of data. It could be the score, it could be player inventory, it could be where they are located on a map. The possibilities are endless and it's more heavily dependent on the type of game.\n\nNeed to sync that data between devices and your remote infrastructure? That is a whole different scenario.\n\nIf you managed to catch MongoDB .Live 2021, you'll be familiar that the first stable release of the [Realm .NET SDK for Unity](https://www.mongodb.com/docs/realm/sdk/dotnet/unity/) was made available. This means that you can use Realm in your Unity game to store and sync data with only a few lines of code.\n\nIn this tutorial, we're going to build a nifty game that explores some storage and syncing use-cases.\n\nTo get a better idea of what we plan to accomplish, take a look at the following animated image:\n\n![MongoDB Realm Space Shooter Example](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/animated_space_shooter_example_d581baa64e.gif \"MongoDB Realm Space Shooter Example\")\n\nIn the above example, we have a space shooter style game. Waves of enemies are coming at you and as you defeat them your score increases. In addition to keeping track of score, the player has a set of enabled blasters. What you don't see in the above example is what's happening behind the scenes. The score is synced to and from the cloud and likewise are the blasters.\n\n## The Requirements\n\nThere are a lot of moving pieces for this particular gaming example. To be successful with this tutorial, you'll need to have the following ready to go:\n\n- Unity 2021.2.0b3 or newer\n- A MongoDB Atlas M0 cluster or better\n- A web application pointed at the Atlas cluster\n- Game media assets\n\nThis is heavily a Unity example. While older or newer versions of Unity might work, I was personally using 2021.2.0b3 when I developed it. You can check to see what version of Unity is available to you using the Unity Hub software.\n\nBecause we are going to be introducing a synchronization feature to the game, we're going to need an Atlas cluster as well as an Atlas App Services application. Both of these can be configured for free [here](https://cloud.mongodb.com). Don't worry about the finer details of the configuration because we'll get to those as we progress in the tutorial.\n\nAs much as I'd like to take credit for the space shooter assets used within this game, I can't. I actually downloaded them from the [Unity Asset Store](https://assetstore.unity.com/packages/2d/characters/warped-space-shooter-181590). Feel free to download what I used or create your own.\n\nIf you're looking for a basic getting started tutorial for Unity with Realm, check out my [previous tutorial](https://www.mongodb.com/developer/how-to/getting-started-realm-sdk-unity/) on the subject.\n\n## Designing the Scenes and Interfaces for the Unity Game\n\nThe game we're about to build is not a small and quick project. There will be many game objects and a few scenes that we have to configure, but none of it is particularly difficult.\n\nTo get an idea of what we need to create, make note of the following breakdown:\n\n- LoginScene\n    - Camera\n    - LoginController\n    - RealmController\n    - Canvas\n        - UsernameField\n        - PasswordField\n        - LoginButton\n- MainScene\n    - GameController\n    - RealmController\n    - Background\n    - Player\n    - Canvas\n        - HighScoreText\n        - ScoreText\n    - BlasterEnabled\n    - SparkBlasterEnabled\n    - CrossBlasterEnabled\n    - Blaster\n    - CrossBlast\n    - Enemy\n    - SparkBlast\n\nThe above list represents our two scenes with each of the components that live within the scene.\n\nLet's start by configuring the **LoginScene** with each of the components. Don't worry, we'll explore the logic side of things for this scene later.\n\nWithin the Unity IDE, create a **LoginScene** and within the **Hierarchy** choose to create a new **UI -> Input Field**. You'll need to do this twice because this is how we're going to create the **UsernameField** and the **PasswordField** that we defined in the list above. You're also going to want to create a **UI -> Button** which will represent our **LoginButton** to submit the form.\n\nFor each of the UI game objects, position them on the screen how you want them. Mine looks like the following:\n\n![Space Shooter Login Scene](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/space_shooter_login_ddb4b6dfa7.png \"Space Shooter Login Scene\")\n\nWithin the **Hierarchy** of your scene, create two empty game objects. The first game object, **LoginController**, will eventually hold a script for managing the user input and interactions with the UI components we had just created. The second game object, **RealmController**, will eventually have a script that contains any Realm interactions. For now, we're going to leave these as empty game objects and move on.\n\nNow let's move onto our next scene.\n\nCreate a **MainScene** if you haven't already and start adding **UI -> Text** to represent the current score and the high score.\n\nSince we probably don't want a solid blue background in our game, we should add a background image. Add an empty game object to the **Hierarch** and then add a **Sprite Renderer** component to that object using the inspector. Add whatever image you want to the **Sprite** field of the **Sprite Renderer** component.\n\nSince we're going to give the player a few different blasters to choose from, we want to show them which blasters they have at any given time. For this, we should add some simple sprites with blaster images on them.\n\nCreate three empty game objects and add a **Sprite Renderer** component to each of them. For each **Sprite** field, add the image that you want to use. Then position the sprites to a section on the screen that you're comfortable with.\n\nIf you've made it this far, you might have a scene that looks like the following:\n\n![Space Shooter Basic MainScene](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/space_shooter_basic_mainscene_82bc7938c2.png \"Space Shooter Basic MainScene\")\n\nThis might be hard to believe, but the visual side of things is almost complete. With just a few more game objects, we can move onto the more exciting logic things.\n\nLike with the **LoginScene**, the **GameController** and **RealmController** game objects will remain empty. There's a small change though. Even though the **RealmController** will eventually exist in the **MainScene**, we're not going to create it manually. Instead, just create an empty **GameController** game object.\n\nThis leaves us with the player, enemies, and various blasters.\n\nStarting with the player, create an empty game object and add a **Sprite Renderer**, **Rigidbody 2D**, and **Box Collider 2D** component to the game object. For the **Sprite Renderer**, add the graphic you want to use for your ship. The **Rigidbody 2D** and **Box Collider 2D** have to do with physics and collisions. We're not going to burden ourselves with gravity for this example, so make sure the **Body Type** for the **Rigidbody 2D** is **Kinematic** and the **Is Trigger** for the **Box Collider 2D** is enabled. Within the inspector, tag the player game object as \"Player.\"\n\nThe blasters and enemies will have the same setup as our player. Create new game objects for each, just like you did the player, only this time select a different graphic for them and give them the tags of \"Weapon\" or \"Enemy\" in the inspector.\n\nThis is where things get interesting.\n\nWe know that there will be more than one enemy in circulation and likewise with your blaster bullets. Rather than creating a bunch of each, take the game objects you used for the blasters and enemies and drag them into your **Assets** directory. This will convert the game objects into prefabs that can be recycled as many times as you want. Once the prefabs are created, the objects can be removed from the **Hierarchy** section of your scene. As we progress, we'll be instantiating these prefabs through code.\n\nWe're ready to start writing code to give our game life.\n\n## Configuring MongoDB Atlas and Atlas Device Sync for Data Synchronization\n\nFor this game, we're going to rely on a cloud and synchronization aspect, so there is some additional configuration that we'll need to take care of. However, before we worry about the cloud configurations, let's install the [Realm .NET SDK for Unity](https://docs.mongodb.com/realm/sdk/dotnet/unity/).\n\nWithin Unity, select **Window -> Package Manager** and then click the little cog icon to find the **Advanced Project Settings** area.\n\n![Install Realm SDK in Unity](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/realm_sdk_install_1_fbe9dde171.jpg \"Install Realm SDK in Unity\")\n\nHere you're going to want to add a new registry with the following information:\n\n```\nname: NPM\nurl: https://registry.npmjs.org\nscope(s): io.realm.unity\n```\n\nEven though we're working with Unity, the best way to get the Realm SDK is through NPM, hence the custom registry that we're going to use.\n\n![Install Realm SDK in Unity](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/realm_sdk_install_2_b18e1e052d.jpg \"Install Realm SDK in Unity\")\n\nWith the registry added, we can add an entry for Realm in the project's **Packages/manifest.json** file. Within the **manifest.json** file, add the following to the `dependencies` object:\n\n```\n\"io.realm.unity\": \"10.3.0\"\n```\n\nYou can swap the version of Realm with whatever you plan to use.\n\nFrom a Unity perspective, Realm is ready to be used. Now we just need to configure Device Sync and Atlas in the cloud.\n\nWithin [MongoDB Atlas](https://cloud.mongodb.com), assuming you already have a cluster to work with, click the **App Services** tab and then **Create a New App** to create a new application.\n\n![Create New Application](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/new_realm_app_9e4895c0b6.jpg \"Create New Realm Application\")\n\nName the application whatever you'd like. The MongoDB Atlas cluster requires no special configuration to work with App Services, only that such a cluster exists. App Services will create the necessary databases and collections when the time comes.\n\nBefore we start configuring your app, take note of your **App ID** in the top left corner of the screen:\n\n![Find Realm App Id](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/get_realm_app_id_c4c03d50c1.jpg \"Find Realm App Id\")\n\nThe **App ID** will be very important within the Unity project because it tells the SDK where to sync and authenticate with.\n\nNext you'll want to define what kind of authentication is allowed for your Unity game and the users that are allowed to authenticate. Within the dashboard, click the **Authentication** tab followed by the **Authentication Providers** tab. Enable **Email / Password** if it isn't already enabled. After email and password authentication is enabled for your application, click the **Users** tab and choose to **Add New User** with the email and password information of your choice.\n\nThe users can be added through an API request, but for this example we're just going to focus on adding them manually.\n\nWith the user information added, we need to define the collections and schemas to sync with our game. Click the **Schema** tab within the dashboard and choose to create a new database and collection if you don't already have a **space_shooter** database and a **PlayerProfile** collection.\n\nThe schema for the **PlayerProfile** collection should look like the following:\n\n```json\n{\n    \"title\": \"PlayerProfile\",\n    \"bsonType\": \"object\",\n    \"required\": [\n        \"high_score\",\n        \"spark_blaster_enabled\",\n        \"cross_blaster_enabled\",\n        \"score\",\n        \"_partition\"\n    ],\n    \"properties\": {\n        \"_id\": {\n            \"bsonType\": \"string\"\n        },\n        \"_partition\": {\n            \"bsonType\": \"string\"\n        },\n        \"high_score\": {\n            \"bsonType\": \"int\"\n        },\n        \"score\": {\n            \"bsonType\": \"int\"\n        },\n        \"spark_blaster_enabled\": {\n            \"bsonType\": \"bool\"\n        },\n        \"cross_blaster_enabled\": {\n            \"bsonType\": \"bool\"\n        }\n    }\n}\n```\n\nIn the above schema, we're saying that we are going to have five fields with the types defined. These fields will eventually be mapped to C# objects within the Unity game. The one field to pay the most attention to is the `_partition` field. The `_partition` field will be the most valuable when it comes to sync because it will represent which data is synchronized rather than attempting to synchronize the entire MongoDB Atlas collection.\n\nIn our example, the `_partition` field should hold user email addresses because they are unique and the user will provide them when they log in. With this we can specify that we only want to sync data based on the users email address.\n\nWith the schema defined, now we can enable Atlas Device Sync.\n\nWithin the dashboard, click on the **Sync** tab. Specify the cluster and the field to be used as the partition key. You should specify `_partition` as the partition key in this example, although the actual field name doesn't matter if you wanted to call it something else. Leaving the permissions as the default will give users read and write permissions.\n\n> Atlas Device Sync will only sync collections that have a defined schema. You could have other collections in your MongoDB Atlas cluster, but they won't sync automatically unless you have schemas defined for them.\n\nAt this point, we can now focus on the actual game development.\n\n## Defining the Data Model and Usage Logic\n\nWhen it comes to data, your Atlas App Services app is going to manage all of it. We need to create a data model that matches the schema that we had just created for synchronization and we need to create the logic for our **RealmController** game object.\n\nLet's start by creating the model to be used.\n\nWithin the **Assets** folder of your project, create a **Scripts** folder with a **PlayerProfile.cs** script in it. The **PlayerProfile.cs** script should contain the following C# code:\n\n```csharp\nusing Realms;\nusing Realms.Sync;\n\npublic class PlayerProfile : RealmObject {\n\n    [PrimaryKey]\n    [MapTo(\"_id\")]\n    public string UserId { get; set; }\n\n    [MapTo(\"high_score\")]\n    public int HighScore { get; set; }\n\n    [MapTo(\"score\")]\n    public int Score { get; set; }\n\n    [MapTo(\"spark_blaster_enabled\")]\n    public bool SparkBlasterEnabled { get; set; }\n\n    [MapTo(\"cross_blaster_enabled\")]\n    public bool CrossBlasterEnabled { get; set; }\n\n    public PlayerProfile() {}\n\n    public PlayerProfile(string userId) {\n        this.UserId = userId;\n        this.HighScore = 0;\n        this.Score = 0;\n        this.SparkBlasterEnabled = false;\n        this.CrossBlasterEnabled = false;\n    }\n\n}\n```\n\nWhat we're doing is we are defining object fields and how they map to a remote document in a MongoDB collection. While our C# object looks like the above, the BSON that we'll see in MongoDB Atlas will look like the following:\n\n```json\n{\n    \"_id\": \"12345\",\n    \"high_score\": 1337,\n    \"score\": 0,\n    \"spark_blaster_enabled\": false,\n    \"cross_blaster_enabled\": false\n}\n```\n\nIt's important to note that the documents in Atlas might have more fields than what we see in our game. We'll only be able to use the mapped fields in our game, so if we have for example an email address in our document, we won't see it in the game because it isn't mapped.\n\nWith the model in place, we can focus on syncing, querying, and writing our data.\n\nWithin the **Assets/Scripts** directory, add a **RealmController.cs** script. This script should contain the following C# code:\n\n```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Realms;\nusing Realms.Sync;\nusing Realms.Sync.Exceptions;\nusing System.Threading.Tasks;\n\npublic class RealmController : MonoBehaviour {\n\n    public static RealmController Instance;\n\n    public string RealmAppId = \"YOUR_REALM_APP_ID_HERE\";\n\n    private Realm _realm;\n    private App _realmApp;\n    private User _realmUser;\n\n    void Awake() {\n        DontDestroyOnLoad(gameObject);\n        Instance = this;\n    }\n\n    void OnDisable() {\n        if(_realm != null) {\n            _realm.Dispose();\n        }\n    }\n\n    public async Task<string> Login(string email, string password) {}\n\n    public PlayerProfile GetPlayerProfile() {}\n\n    public void IncreaseScore() {}\n\n    public void ResetScore() {}\n\n    public bool IsSparkBlasterEnabled() {}\n\n    public bool IsCrossBlasterEnabled() {}\n\n}\n```\n\nThe above code is incomplete, but it gives you an idea of where we are going.\n\nFirst, take notice of the `AppId` variable. You're going to want to use your App Services application so sync can happen based on how you've configured everything. This also applies to the authentication rules that are in place for your particular application.\n\nThe `RealmController` class is going to be used as a singleton object between scenes. The goal is to make sure it cannot be destroyed and everything we do is through a static instance of itself.\n\nIn the `Awake` method, we are saying that the game object that the script is attached to should not be destroyed and that we are setting the static variable to itself. In the `OnDisable`, we are doing cleanup which should really only happen when the game is closed.\n\nMost of the magic will happen in the `Login` function:\n\n```csharp\npublic async Task<string> Login(string email, string password) {\n    if(email != \"\" && password != \"\") {\n        _realmApp = App.Create(new AppConfiguration(RealmAppId) {\n            MetadataPersistenceMode = MetadataPersistenceMode.NotEncrypted\n        });\n        try {\n            if(_realmUser == null) {\n                _realmUser = await _realmApp.LogInAsync(Credentials.EmailPassword(email, password));\n                _realm = await Realm.GetInstanceAsync(new SyncConfiguration(email, _realmUser));\n            } else {\n                _realm = Realm.GetInstance(new SyncConfiguration(email, _realmUser));\n            }\n        } catch (ClientResetException clientResetEx) {\n            if(_realm != null) {\n                _realm.Dispose();\n            }\n            clientResetEx.InitiateClientReset();\n        }\n        return _realmUser.Id;\n    }\n    return \"\";\n}\n```\n\nIn the above code, we are defining our application based on the application ID. Next we are attempting to log into the application using email and password authentication, something we had previously configured in the web dashboard. If successful, we are getting an instance of our Realm to work with going forward. The data to be synchronized is based on our partition field which in this case is the email address. This means we're only synchronizing data for this particular email address.\n\nIf all goes smooth with the login, the ID for the user is returned.\n\nAt some point in time, we're going to need to load the player data. This is where the `GetPlayerProfile` function comes in:\n\n```csharp\npublic PlayerProfile GetPlayerProfile() {\n    PlayerProfile _playerProfile = _realm.Find<PlayerProfile>(_realmUser.Id);\n    if(_playerProfile == null) {\n        _realm.Write(() => {\n            _playerProfile = _realm.Add(new PlayerProfile(_realmUser.Id));\n        });\n    }\n    return _playerProfile;\n}\n```\n\nWhat we're doing is we're taking the current instance and we're finding a particular player profile based on the id. If one does not exist, then we create one using the current ID. In the end, we're returning a player profile, whether it be one that we had been using or a fresh one.\n\nWe know that we're going to be working with score data in our game. We need to be able to increase the score, reset the score, and calculate the high score for a player.\n\nStarting with the `IncreaseScore`, we have the following:\n\n```csharp\npublic void IncreaseScore() {\n    PlayerProfile _playerProfile = GetPlayerProfile();\n    if(_playerProfile != null) {\n        _realm.Write(() => {\n            _playerProfile.Score++;\n        });\n    }\n}\n```\n\nFirst we get the player profile and then we take whatever score is associated with it and increase it by one. With Realm we can work with our objects like native C# objects. The exception is that when we want to write, we have to wrap it in a `Write` block. Reads we don't have to.\n\nNext let's look at the `ResetScore` function:\n\n```csharp\npublic void ResetScore() {\n    PlayerProfile _playerProfile = GetPlayerProfile();\n    if(_playerProfile != null) {\n        _realm.Write(() => {\n            if(_playerProfile.Score > _playerProfile.HighScore) {\n                _playerProfile.HighScore = _playerProfile.Score;\n            }\n            _playerProfile.Score = 0;\n        });\n    }\n}\n```\n\nIn the end we want to zero out the score, but we also want to see if our current score is the highest score before we do. We can do all this within the `Write` block and it will synchronize to the server.\n\nFinally we have our two functions to tell us if a certain blaster is available to us:\n\n```csharp\npublic bool IsSparkBlasterEnabled() {\n    PlayerProfile _playerProfile = GetPlayerProfile();\n    return _playerProfile != null ? _playerProfile.SparkBlasterEnabled : false;\n}\n```\n\nThe reason our blasters are data dependent is because we may want to unlock them based on points or through a micro-transaction. In this case, maybe Realm Sync takes care of it.\n\nThe `IsCrossBlasterEnabled` function isn't much different:\n\n```csharp\npublic bool IsCrossBlasterEnabled() {\n    PlayerProfile _playerProfile = GetPlayerProfile();\n    return _playerProfile != null ? _playerProfile.CrossBlasterEnabled : false;\n}\n```\n\nThe difference is we are using a different field from our data model.\n\nWith the Realm logic in place for the game, we can focus on giving the other game objects life through scripts.\n\n## Developing the Game-Play Logic Scripts for the Space Shooter Game Objects\n\nAlmost every game object that we've created will be receiving a script with logic. To keep the flow appropriate, we're going to add logic in a natural progression. This means we're going to start with the **LoginScene** and each of the game objects that live in it.\n\nFor the **LoginScene**, only two game objects will be receiving scripts:\n\n- LoginController\n- RealmController\n\nSince we already have a **RealmController.cs** script file, go ahead and attach it to the **RealmController** game object as a component.\n\nNext up, we need to create an **Assets/Scripts/LoginController.cs** file with the following C# code:\n\n```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing UnityEngine.UI;\nusing UnityEngine.SceneManagement;\n\npublic class LoginController : MonoBehaviour {\n\n    public Button LoginButton;\n    public InputField UsernameInput;\n    public InputField PasswordInput;\n\n    void Start() {\n        UsernameInput.text = \"nic.raboy@mongodb.com\";\n        PasswordInput.text = \"password1234\";\n        LoginButton.onClick.AddListener(Login);\n    }\n\n    async void Login() {\n        if(await RealmController.Instance.Login(UsernameInput.text, PasswordInput.text) != \"\") {\n            SceneManager.LoadScene(\"MainScene\");\n        }\n    }\n\n    void Update() {\n        if(Input.GetKey(\"escape\")) {\n            Application.Quit();\n        }\n    }\n\n}\n```\n\nThere's not a whole lot going on since the backbone of this script is in the **RealmController.cs** file.\n\nWhat we're doing in the **LoginController.cs** file is we're defining the UI components which we'll link through the Unity IDE. When the script starts, we're going to default the values of our input fields and we're going to assign a click event listener to the button.\n\nWhen the button is clicked, the `Login` function from the **RealmController.cs** file is called and we pass the provided email and password. If we get an id back, we know we were successful so we can switch to the next scene.\n\nThe `Update` method isn't a complete necessity, but if you want to be able to quit the game with the escape key, that is what this particular piece of logic does.\n\nAttach the **LoginController.cs** script to the **LoginController** game object as a component and then drag each of the corresponding UI game objects into the script via the game object inspector. Remember, we defined public variables for each of the UI components. We just need to tell Unity what they are by linking them in the inspector.\n\nThe **LoginScene** logic is complete. Can you believe it? This is because the Realm .NET SDK for Unity is doing all the heavy lifting for us.\n\nThe **MainScene** has a lot more going on, but we'll break down what's happening.\n\nLet's start with something you don't actually see but that controls all of our prefab instances. I'm talking about the object pooling script.\n\nIn short, creating and destroying game objects on-demand is resource intensive. Instead, we should create a fixed amount of game objects when the game loads and hide them or show them based on when they are needed. This is what an object pool does.\n\nCreate an **Assets/Scripts/ObjectPool.cs** file with the following C# code:\n\n```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class ObjectPool : MonoBehaviour\n{\n\n    public static ObjectPool SharedInstance;\n\n    private List<GameObject> pooledEnemies;\n    private List<GameObject> pooledBlasters;\n    private List<GameObject> pooledCrossBlasts;\n    private List<GameObject> pooledSparkBlasts;\n    public GameObject enemyToPool;\n    public GameObject blasterToPool;\n    public GameObject crossBlastToPool;\n    public GameObject sparkBlastToPool;\n    public int amountOfEnemiesToPool;\n    public int amountOfBlastersToPool;\n    public int amountOfCrossBlastsToPool;\n    public int amountOfSparkBlastsToPool;\n\n    void Awake() {\n        SharedInstance = this;\n    }\n\n    void Start() {\n        pooledEnemies = new List<GameObject>();\n        pooledBlasters = new List<GameObject>();\n        pooledCrossBlasts = new List<GameObject>();\n        pooledSparkBlasts = new List<GameObject>();\n        GameObject tmpEnemy;\n        GameObject tmpBlaster;\n        GameObject tmpCrossBlast;\n        GameObject tmpSparkBlast;\n        for(int i = 0; i < amountOfEnemiesToPool; i++) {\n            tmpEnemy = Instantiate(enemyToPool);\n            tmpEnemy.SetActive(false);\n            pooledEnemies.Add(tmpEnemy);\n        }\n        for(int i = 0; i < amountOfBlastersToPool; i++) {\n            tmpBlaster = Instantiate(blasterToPool);\n            tmpBlaster.SetActive(false);\n            pooledBlasters.Add(tmpBlaster);\n        }\n        for(int i = 0; i < amountOfCrossBlastsToPool; i++) {\n            tmpCrossBlast = Instantiate(crossBlastToPool);\n            tmpCrossBlast.SetActive(false);\n            pooledCrossBlasts.Add(tmpCrossBlast);\n        }\n        for(int i = 0; i < amountOfSparkBlastsToPool; i++) {\n            tmpSparkBlast = Instantiate(sparkBlastToPool);\n            tmpSparkBlast.SetActive(false);\n            pooledSparkBlasts.Add(tmpSparkBlast);\n        }\n    }\n\n    public GameObject GetPooledEnemy() {\n        for(int i = 0; i < amountOfEnemiesToPool; i++) {\n            if(pooledEnemies[i].activeInHierarchy == false) {\n                return pooledEnemies[i];\n            }\n        }\n        return null;\n    }\n\n    public GameObject GetPooledBlaster() {\n        for(int i = 0; i < amountOfBlastersToPool; i++) {\n            if(pooledBlasters[i].activeInHierarchy == false) {\n                return pooledBlasters[i];\n            }\n        }\n        return null;\n    }\n\n    public GameObject GetPooledCrossBlast() {\n        for(int i = 0; i < amountOfCrossBlastsToPool; i++) {\n            if(pooledCrossBlasts[i].activeInHierarchy == false) {\n                return pooledCrossBlasts[i];\n            }\n        }\n        return null;\n    }\n\n    public GameObject GetPooledSparkBlast() {\n        for(int i = 0; i < amountOfSparkBlastsToPool; i++) {\n            if(pooledSparkBlasts[i].activeInHierarchy == false) {\n                return pooledSparkBlasts[i];\n            }\n        }\n        return null;\n    }\n    \n}\n```\n\nThe above object pooling logic is not code optimized because I wanted to keep it readable. If you want to see an optimized version, check out a [previous tutorial](https://www.thepolyglotdeveloper.com/2014/05/object-pooling-for-efficiency/) I wrote on the subject.\n\nSo let's break down what we're doing in this object pool.\n\nWe have four different game objects to pool:\n\n- Enemies\n- Spark Blasters\n- Cross Blasters\n- Regular Blasters\n\nThese need to be pooled because there could be more than one of the same object at any given time. We're using public variables for each of the game objects and quantities so that we can properly link them to actual game objects in the Unity IDE.\n\nLike with the **RealmController.cs** script, this script will also act as a singleton to be used as needed.\n\nIn the `Start` method, we are instantiating a game object, as per the quantities defined through the Unity IDE, and adding them to a list. Ideally the linked game object should be one of the prefabs that we previously defined. The list of instantiated game objects represent our pools. We have four object pools to pull from.\n\nPulling from the pool is as simple as creating a function for each pool and seeing what's available. Take the `GetPooledEnemy` function for example:\n\n```csharp\npublic GameObject GetPooledEnemy() {\n    for(int i = 0; i < amountOfEnemiesToPool; i++) {\n        if(pooledEnemies[i].activeInHierarchy == false) {\n            return pooledEnemies[i];\n        }\n    }\n    return null;\n}\n```\n\nIn the above code, we loop through each object in our pool, in this case enemies. If an object is inactive it means we can pull it and use it. If our pool is depleted, then we either defined too small of a pool or we need to wait until something is available.\n\nI like to pool about 50 of each game object even if I only ever plan to use 10. Doesn't hurt to have excess as it's still less resource-heavy than creating and destroying game objects as needed.\n\nThe **ObjectPool.cs** file should be attached as a component to the **GameController** game object. After attaching, make sure you assign your prefabs and the pooled quantities using the game object inspector within the Unity IDE.\n\nThe **ObjectPool.cs** script isn't the only script we're going to attach to the **GameController** game object. We need to create a script that will control the flow of our game. Create an **Assets/Scripts/GameController.cs** file with the following C# code:\n\n```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing UnityEngine.UI;\n\npublic class GameController : MonoBehaviour {\n\n    public float timeUntilEnemy = 1.0f;\n    public float minTimeUntilEnemy = 0.25f;\n    public float maxTimeUntilEnemy = 2.0f;\n\n    public GameObject SparkBlasterGraphic;\n    public GameObject CrossBlasterGraphic;\n\n    public Text highScoreText;\n    public Text scoreText;\n\n    private PlayerProfile _playerProfile;\n\n    void OnEnable() {\n        _playerProfile = RealmController.Instance.GetPlayerProfile();\n        highScoreText.text = \"HIGH SCORE: \" + _playerProfile.HighScore.ToString();\n        scoreText.text = \"SCORE: \" + _playerProfile.Score.ToString();\n    }\n\n    void Update() {\n        highScoreText.text = \"HIGH SCORE: \" + _playerProfile.HighScore.ToString();\n        scoreText.text = \"SCORE: \" + _playerProfile.Score.ToString();\n        timeUntilEnemy -= Time.deltaTime;\n        if(timeUntilEnemy <= 0) {\n            GameObject enemy = ObjectPool.SharedInstance.GetPooledEnemy();\n            if(enemy != null) {\n                enemy.SetActive(true);\n            }\n            timeUntilEnemy = Random.Range(minTimeUntilEnemy, maxTimeUntilEnemy);\n        }\n        if(_playerProfile != null) {\n            SparkBlasterGraphic.SetActive(_playerProfile.SparkBlasterEnabled);\n            CrossBlasterGraphic.SetActive(_playerProfile.CrossBlasterEnabled);\n        }\n        if(Input.GetKey(\"escape\")) {\n            Application.Quit();\n        }\n    }\n\n}\n```\n\nThere's a diverse set of things happening in the above script, so let's break them down.\n\nYou'll notice the following public variables:\n\n```csharp\npublic float timeUntilEnemy = 1.0f;\npublic float minTimeUntilEnemy = 0.25f;\npublic float maxTimeUntilEnemy = 2.0f;\n```\n\nWe're going to use these variables to define when a new enemy should be activated.\n\nThe `timeUntilEnemy` represents how much actual time from the current time until a new enemy should be pulled from the object pool. The `minTimeUntilEnemy` and `maxTimeUntilEnemy` will be used for randomizing what the `timeUntilEnemy` value should become after an enemy is pooled. It's boring to have all enemies appear after a fixed amount of time, so the minimum and maximum values keep things interesting.\n\n```csharp\npublic GameObject SparkBlasterGraphic;\npublic GameObject CrossBlasterGraphic;\n\npublic Text highScoreText;\npublic Text scoreText;\n```\n\nRemember those UI components and sprites to represent enabled blasters we had created earlier in the Unity IDE? When we attach this script to the **GameController** game object, you're going to want to assign the other components in the game object inspector.\n\nThis brings us to the `OnEnable` method:\n\n```csharp\nvoid OnEnable() {\n    _playerProfile = RealmController.Instance.GetPlayerProfile();\n    highScoreText.text = \"HIGH SCORE: \" + _playerProfile.HighScore.ToString();\n    scoreText.text = \"SCORE: \" + _playerProfile.Score.ToString();\n}\n```\n\nThe `OnEnable` method is where we're going to get our current player profile and then update the score values visually based on the data stored in the player profile. The `Update` method will continuously update those score values for as long as the scene is showing.\n\n```csharp\nvoid Update() {\n    highScoreText.text = \"HIGH SCORE: \" + _playerProfile.HighScore.ToString();\n    scoreText.text = \"SCORE: \" + _playerProfile.Score.ToString();\n    timeUntilEnemy -= Time.deltaTime;\n    if(timeUntilEnemy <= 0) {\n        GameObject enemy = ObjectPool.SharedInstance.GetPooledEnemy();\n        if(enemy != null) {\n            enemy.SetActive(true);\n        }\n        timeUntilEnemy = Random.Range(minTimeUntilEnemy, maxTimeUntilEnemy);\n    }\n    if(_playerProfile != null) {\n        SparkBlasterGraphic.SetActive(_playerProfile.SparkBlasterEnabled);\n        CrossBlasterGraphic.SetActive(_playerProfile.CrossBlasterEnabled);\n    }\n    if(Input.GetKey(\"escape\")) {\n        Application.Quit();\n    }\n}\n```\n\nIn the `Update` method, every time it's called, we subtract the delta time from our `timeUntilEnemy` variable. When the value is zero, we attempt to get a new enemy from the object pool and then reset the timer. Outside of the object pooling, we're also checking to see if the other blasters have become enabled. If they have been, we can update the game object status for our sprites. This will allow us to easily show and hide these sprites.\n\nIf you haven't already, attach the **GameController.cs** script to the **GameController** game object. Remember to update any values for the script within the game object inspector.\n\nIf we were to run the game, every enemy would have the same position and they would not be moving. We need to assign logic to the enemies.\n\nCreate an **Assets/Scripts/Enemy.cs** file with the following C# code:\n\n```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class Enemy : MonoBehaviour {\n\n    public float movementSpeed = 5.0f;\n\n    void OnEnable() {\n        float randomPositionY = Random.Range(-4.0f, 4.0f);\n        transform.position = new Vector3(10.0f, randomPositionY, 0);\n    }\n\n    void Update() {\n        transform.position += Vector3.left * movementSpeed * Time.deltaTime;\n        if(transform.position.x < -10.0f) {\n            gameObject.SetActive(false);\n        }\n    }\n\n    void OnTriggerEnter2D(Collider2D collider) {\n        if(collider.tag == \"Weapon\") {\n            gameObject.SetActive(false);\n            RealmController.Instance.IncreaseScore();\n        }\n    }\n\n}\n```\n\nWhen the enemy is pulled from the object pool, the game object becomes enabled. So the `OnEnable` method picks a random y-axis position for the game object. For every frame, the `Update` method will move the game object along the x-axis. If the game object goes off the screen, we can safely add it back into the object pool.\n\nThe `OnTriggerEnter2D` method is for our collision detection. We're not doing physics collisions so this method just tells us if the objects have touched. If the current game object, in this case the enemy, has collided with a game object tagged as a weapon, then add the enemy back into the queue and increase the score.\n\nAttach the **Enemy.cs** script to your enemy prefab.\n\nBy now, your game probably looks something like this, minus the animations:\n\n![Space Shooter Enemies](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/space_shooter_no_player_bd22b4816f.gif \"Space Shooter Enemies\")\n\nWe won't be worrying about animations in this tutorial. Consider that part of your extracurricular challenge after completing this tutorial.\n\nSo we have a functioning enemy pool. Let's look at the blaster logic since it is similar.\n\nCreate an **Assets/Scripts/Blaster.cs** file with the following C# logic:\n\n```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class Blaster : MonoBehaviour {\n\n    public float movementSpeed = 5.0f;\n    public float decayRate = 2.0f;\n\n    private float timeToDecay;\n\n    void OnEnable() {\n        timeToDecay = decayRate;\n    }\n\n    void Update() {\n        timeToDecay -= Time.deltaTime;\n        transform.position += Vector3.right * movementSpeed * Time.deltaTime;\n        if(transform.position.x > 10.0f || timeToDecay <= 0) {\n            gameObject.SetActive(false);\n        }\n    }\n\n    void OnTriggerEnter2D(Collider2D collider) {\n        if(collider.tag == \"Enemy\") {\n            gameObject.SetActive(false);\n        }\n    }\n\n}\n```\n\nLook mildly familiar to the enemy? It is similar.\n\nWe need to first define how fast each blaster should move and how quickly the blaster should disappear if it hasn't hit anything.\n\nIn the `Update` method will subtract the current time from our blaster decay time. The blaster will continue to move along the x-axis until it has either gone off screen or it has decayed. In this scenario, the blaster is added back into the object pool. If the blaster collides with a game object tagged as an enemy, the blaster is also added back into the pool. Remember, the blaster will likely be tagged as a weapon so the **Enemy.cs** script will take care of adding the enemy back into the object pool.\n\nAttach the **Blaster.cs** script to your blaster prefab and apply any value settings as necessary with the Unity IDE in the inspector.\n\nTo make the game interesting, we're going to add some very slight differences to the other blasters.\n\nCreate an **Assets/Scripts/CrossBlast.cs** script with the following C# code:\n\n```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class CrossBlast : MonoBehaviour {\n\n    public float movementSpeed = 5.0f;\n\n    void Update() {\n        transform.position += Vector3.right * movementSpeed * Time.deltaTime;\n        if(transform.position.x > 10.0f) {\n            gameObject.SetActive(false);\n        }\n    }\n\n    void OnTriggerEnter2D(Collider2D collider) { }\n\n}\n```\n\nAt a high level, this blaster behaves the same. However, if it collides with an enemy, it keeps going. It only goes back into the object pool when it goes off the screen. So there is no decay and it isn't a one enemy per blast weapon.\n\nLet's look at an **Assets/Scripts/SparkBlast.cs** script:\n\n```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class SparkBlast : MonoBehaviour {\n\n    public float movementSpeed = 5.0f;\n\n    void Update() {\n        transform.position += Vector3.right * movementSpeed * Time.deltaTime;\n        if(transform.position.x > 10.0f) {\n            gameObject.SetActive(false);\n        }\n    }\n\n    void OnTriggerEnter2D(Collider2D collider) {\n        if(collider.tag == \"Enemy\") {\n            gameObject.SetActive(false);\n        }\n    }\n\n}\n```\n\nThe minor difference in the above script is that it has no decay, but it can only ever destroy one enemy.\n\nMake sure you attach these scripts to the appropriate blaster prefabs.\n\nWe're almost done! We have one more script and that's for the actual player!\n\nCreate an **Assets/Scripts/Player.cs** file and add the following code:\n\n```csharp\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\n\npublic class Player : MonoBehaviour\n{\n\n    public float movementSpeed = 5.0f;\n    public float respawnSpeed = 8.0f;\n    public float weaponFireRate = 0.5f;\n\n    private float nextBlasterTime = 0.0f;\n    private bool isRespawn = true;\n\n    void Update() {\n        if(isRespawn == true) {\n            transform.position = Vector2.MoveTowards(transform.position, new Vector2(-6.0f, -0.25f), respawnSpeed * Time.deltaTime);\n            if(transform.position == new Vector3(-6.0f, -0.25f, 0.0f)) {\n                isRespawn = false;\n            }\n        } else {\n            if(Input.GetKey(KeyCode.UpArrow) && transform.position.y < 4.0f) {\n                transform.position += Vector3.up * movementSpeed * Time.deltaTime;\n            } else if(Input.GetKey(KeyCode.DownArrow) && transform.position.y > -4.0f) {\n                transform.position += Vector3.down * movementSpeed * Time.deltaTime;\n            }\n            if(Input.GetKey(KeyCode.Space) && Time.time > nextBlasterTime) {\n                nextBlasterTime = Time.time + weaponFireRate;\n                GameObject blaster = ObjectPool.SharedInstance.GetPooledBlaster();\n                if(blaster != null) {\n                    blaster.SetActive(true);\n                    blaster.transform.position = new Vector3(transform.position.x + 1, transform.position.y);\n                }\n            }\n            if(RealmController.Instance.IsCrossBlasterEnabled()) {\n                if(Input.GetKey(KeyCode.B) && Time.time > nextBlasterTime) {\n                    nextBlasterTime = Time.time + weaponFireRate;\n                    GameObject crossBlast = ObjectPool.SharedInstance.GetPooledCrossBlast();\n                    if(crossBlast != null) {\n                        crossBlast.SetActive(true);\n                        crossBlast.transform.position = new Vector3(transform.position.x + 1, transform.position.y);\n                    }\n                }\n            }\n            if(RealmController.Instance.IsSparkBlasterEnabled()) {\n                if(Input.GetKey(KeyCode.V) && Time.time > nextBlasterTime) {\n                    nextBlasterTime = Time.time + weaponFireRate;\n                    GameObject sparkBlast = ObjectPool.SharedInstance.GetPooledSparkBlast();\n                    if(sparkBlast != null) {\n                        sparkBlast.SetActive(true);\n                        sparkBlast.transform.position = new Vector3(transform.position.x + 1, transform.position.y);\n                    }\n                }\n            }\n        }\n    }\n\n    void OnTriggerEnter2D(Collider2D collider) {\n        if(collider.tag == \"Enemy\" && isRespawn == false) {\n            RealmController.Instance.ResetScore();\n            transform.position = new Vector3(-10.0f, -0.25f, 0.0f);\n            isRespawn = true;\n        }\n    }\n\n}\n```\n\nLooking at the above script, we have a few variables to keep track of:\n\n```csharp\npublic float movementSpeed = 5.0f;\npublic float respawnSpeed = 8.0f;\npublic float weaponFireRate = 0.5f;\n\nprivate float nextBlasterTime = 0.0f;\nprivate bool isRespawn = true;\n```\n\nWe want to define how fast the player can move, how long it takes for the respawn animation to happen, and how fast you're allowed to fire blasters.\n\nIn the `Update` method, we first check to see if we are currently respawning:\n\n```csharp\ntransform.position = Vector2.MoveTowards(transform.position, new Vector2(-6.0f, -0.25f), respawnSpeed * Time.deltaTime);\nif(transform.position == new Vector3(-6.0f, -0.25f, 0.0f)) {\n    isRespawn = false;\n}\n```\n\nIf we are respawning, then we need to smoothly move the player game object towards a particular coordinate position. When the game object has reached that new position, then we can disable the respawn indicator that prevents us from controlling the player.\n\nIf we're not respawning, we can check to see if the movement keys were pressed:\n\n```csharp\nif(Input.GetKey(KeyCode.UpArrow) && transform.position.y < 4.0f) {\n    transform.position += Vector3.up * movementSpeed * Time.deltaTime;\n} else if(Input.GetKey(KeyCode.DownArrow) && transform.position.y > -4.0f) {\n    transform.position += Vector3.down * movementSpeed * Time.deltaTime;\n}\n```\n\nWhen pressing a key, as long as we haven't moved outside our y-axis boundary, we can adjust the position of the player. Since this is in the `Update` method, the movement should be smooth for as long as you are holding a key.\n\nUsing a blaster isn't too different:\n\n```csharp\nif(Input.GetKey(KeyCode.Space) && Time.time > nextBlasterTime) {\n    nextBlasterTime = Time.time + weaponFireRate;\n    GameObject blaster = ObjectPool.SharedInstance.GetPooledBlaster();\n    if(blaster != null) {\n        blaster.SetActive(true);\n        blaster.transform.position = new Vector3(transform.position.x + 1, transform.position.y);\n    }\n}\n```\n\nIf the particular blaster key is pressed and our rate limit isn't exceeded, we can update our `nextBlasterTime` based on the rate limit, pull a blaster from the object pool, and let the blaster do its magic based on the **Blaster.cs** script. All we're doing in the **Player.cs** script is checking to see if we're allowed to fire and if we are pull from the pool.\n\nThe data dependent spark and cross blasters follow the same rules, the exception being that we first check to see if they are enabled in our player profile.\n\nFinally, we have our collisions:\n\n```csharp\nvoid OnTriggerEnter2D(Collider2D collider) {\n    if(collider.tag == \"Enemy\" && isRespawn == false) {\n        RealmController.Instance.ResetScore();\n        transform.position = new Vector3(-10.0f, -0.25f, 0.0f);\n        isRespawn = true;\n    }\n}\n```\n\nIf our player collides with a game object tagged as an enemy and we're not currently respawning, then we can reset the score and trigger the respawn.\n\nMake sure you attach this **Player.cs** script to your **Player** game object.\n\nIf everything worked out, the game should be functional at this point. If something isn't working correctly, double check the following:\n\n- Make sure each of your game objects is properly tagged.\n- Make sure the scripts are attached to the proper game object or prefab.\n- Make sure the values on the scripts have been defined through the Unity IDE inspector.\n\nPlay around with the game and setting values within MongoDB Atlas.\n\n## Conclusion\n\nYou just saw how to create a space shooter type game with Unity that syncs with MongoDB Atlas by using the Realm .NET SDK for Unity and Atlas Device Sync. Realm only played a small part in this game because that is the beauty of Realm. You can get data persistence and sync with only a few lines of code.\n\nWant to give this project a try? I've uploaded all of the source code to [GitHub](https://github.com/mongodb-developer/unity-space-shooter). You just need to clone the project, replace my App ID with yours, and build the project. Of course you'll still need to have properly configured Atlas and Device Sync in the cloud.\n\nIf you're looking for a slightly slower introduction to Realm with Unity, check out a [previous tutorial](https://www.mongodb.com/developer/how-to/getting-started-realm-sdk-unity/) that I wrote on the subject.\n\nIf you'd like to connect with us further, don't forget to [visit the community forums](https://www.mongodb.com/community/forums/).","description":"Learn how to build a space shooter game that synchronizes between clients and the cloud using MongoDB, Unity, and Atlas Device Sync.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltc8867785615ee977/644c46863c3aa60fa69de872/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/building-space-shooter-game-syncs-unity-mongodb-realm","title":"*Building a Space Shooter Game in Unity that Syncs with Realm and MongoDB Atlas","original_publish_date":"2021-08-12T16:17:04.768Z","strapi_updated_at":"2023-02-03T16:11:51.130Z","expiry_date":"2022-08-11T19:56:11.023Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*SDK","calculated_slug":"/products/realm/sdk"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*CSharp","calculated_slug":"/languages/csharp"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*Unity","calculated_slug":"/technologies/unity"}},{"node":{"title":"*.Net Framework","calculated_slug":"/technologies/dotnet-framework"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to build a space shooter game that synchronizes between clients and the cloud using MongoDB, Unity, and Atlas Device Sync.","og_description":"Learn how to build a space shooter game that synchronizes between clients and the cloud using MongoDB, Unity, and Atlas Device Sync.","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":"@nraboy"},"system":{"updated_at":"2023-04-28T22:52:24.860Z","publish_details":{"time":"2023-04-28T22:52:53.324Z"}}},{"calculated_slug":"/products/mongodb/triggers-tricks-preimage-cass","content":"In this blog series, we are trying to inspire you with some reactive [Realm trigger](https://docs.mongodb.com/realm/triggers) use cases. We hope these will help you bring your application pipelines to the next level.\n\nEssentially, triggers are components in our Atlas projects/Realm apps that allow a user to define a custom function to be invoked on a specific event.\n\n* **Database triggers:** We have triggers that can be triggered based on database events—like ``deletes``, ``inserts``, ``updates``, and ``replaces``—called [database triggers](https://docs.mongodb.com/realm/triggers/database-triggers/).\n* **Scheduled triggers**: We can schedule a trigger based on a ``cron`` expression via [scheduled triggers](https://docs.mongodb.com/realm/triggers/scheduled-triggers/).\n* **Authentication triggers**: These triggers are only relevant for Realm authentication. They are triggered by one of the Realm auth providers' authentication events and can be configured only via a Realm application.\n\nRelationships are an important part of any data design. Relational databases use primary and foreign key concepts to form those relationships when normalizing the data schema. Using those concepts, it allows a “cascading'' delete, which means a primary key parent delete will delete the related siblings.\n\nMongoDB allows you to form relationships in different ways—for example, by embedding documents or arrays inside a parent document. This allows the document to contain all of its relationships within itself and therefore it does the cascading delete out of the box. Consider the following example between a user and the assigned tasks of the user:\n\n``` js\n{\nuserId : \"abcd\",\nusername : \"user1@example.com\" \nTasks : [ \n         { taskId : 1, \n           Details : [\"write\",\"print\" , \"delete\"]\n          },\n       { taskId : 1, \n         Details : [\"clean\",\"cook\" , \"eat\"]\n  }\n}\n```\n\nDelete of this document will delete all the tasks.\n\nHowever, in some design cases, we will want to separate the data of the relationship into Parent and Sibling collections—for example, ``games`` collection holding data for a specific game including ids referencing a ``quests`` collection holding a per game quest. As amount of quest data per game can be large and complex, we’d rather not embed it in ``games`` but reference:\n\n**Games collection**\n\n``` js\n{\n    _id: ObjectId(\"60f950794a61939b6aac12a4\"),\n    userId: 'xxx',\n    gameId: 'abcd-wxyz',\n    gameName: 'Crash',\n    quests: [\n      {\n        startTime: ISODate(\"2021-01-01T22:00:00.000Z\"),\n        questId: ObjectId(\"60f94b7beb7f78709b97b5f3\")\n      },\n      {\n        questId: ObjectId(\"60f94bbfeb7f78709b97b5f4\"),\n        startTime: ISODate(\"2021-01-02T02:00:00.000Z\")\n      }\n    ]\n  }\n```\n\nEach game has a quest array with a start time of this quest and a reference to the quests collection where the quest data reside.\n\n**Quests collection**\n\n``` js\n{\n  _id: ObjectId(\"60f94bbfeb7f78709b97b5f4\"),\n  questName: 'War of fruits ',\n  userId: 'xxx',\n  details: {\n    lastModified: ISODate(\"2021-01-01T23:00:00.000Z\"),\n    currentState: 'in-progress'\n  },\n  progressRounds: [ 'failed', 'failed', 'in-progress' ]\n},\n{\n  _id: ObjectId(\"60f94b7beb7f78709b97b5f3\"),\n  questName: 'War of vegetable ',\n  userId: 'xxx',\n  details: {\n    lastModified: ISODate(\"2021-01-01T22:00:00.000Z\"),\n    currentState: 'failed'\n  },\n  progressRounds: [ 'failed', 'failed', 'failed' ]\n}\n```\n\nWhen a game gets deleted, we would like to purge the relevant quests in a cascading delete. This is where the **Preimage** trigger feature comes into play.\n\n## Preimage Trigger Option\n\nThe Preimage option allows the trigger function to receive a snapshot of the deleted/modified document just before the change that triggered the function. This feature is enabled by enriching the oplog of the underlying replica set to store this snapshot as part of the change.\nRead more on our [documentation](https://docs.mongodb.com/realm/mongodb/trigger-preimages/).\n\nIn our case, we will use this feature to capture the parent deleted document full snapshot (games) and delete the related relationship documents in the sibling collection (quests).\n\n## Building the Trigger\n\nWhen we define the database trigger, we will point it to the relevant cluster and parent namespace to monitor and trigger when a document is deleted—in our case, ``GamesDB.games``.\n\n![Trigger  Def](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Cascade_Trigger_Def_a8acf9292f.png)\n\nTo enable the “Preimage” feature, we will toggle Document Preimage to “ON” and specify our function to handle the cascade delete logic.\n\n![Preimage Toggle](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Pre_Imag_Toggle_a7a058ecd2.png)\n\n**deleteCascadingQuests - Function**\n\n``` js\nexports = async function(changeEvent) {\n\n  // Get deleted document preImage using \"fullDocumentBeforeChange\"\n  var deletedDocument = changeEvent.fullDocumentBeforeChange;\n\n\n  // Get sibling collection \"quests\"\n  const quests = context.services.get(\"mongodb-atlas\").db(\"GamesDB\").collection(\"quests\");\n\n  // Delete all relevant quest documents.\n  deletedDocument.quests.map( async (quest) => {\n     await quests.deleteOne({_id : quest.questId});\n  })\n};\n```\n\nAs you can see, the function gets the fully deleted “games” document present in “changeEvent.fullDocumentBeforeChange” and iterates over the “quests” array. For each of those array elements, the function runs a “deleteOne” on the “quests” collection to delete the relevant quests documents.\n\n## Deleting the Parent Document\n\nNow let's put our trigger to the test by deleting the game from the “games” collection:\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Delete_Game_df2f0c096c.png)\n\nOnce the document was deleted, our trigger was fired and now the “quests” collection is empty as it had only quests related to this deleted game:\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Odocs_d929e3c940.png)\nOur cascade delete works thanks to triggers “Preimages.”\n\n## Wrap Up\n\nThe ability to get a modified or deleted full document opens a new world of opportunities for trigger use cases and abilities. We showed here one option to use this new feature but this can be used for many other scenarios, like tracking complex document state changes for auditing or cleanup images storage using the deleted metadata documents.\n\nWe suggest that you try this new feature considering your use case and look forward to the next trick along this blog series.\n\nWant to keep going? Join the conversation over at our [community forums](https://www.mongodb.com/community/forums/)!","description":"In this article, we will show you how to use a preimage feature to perform cascading relationship deletes via a trigger - based on the deleted parent document.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt028fd4528f2bf9e9/644c468850ad364c03e3a4ad/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/triggers-tricks-preimage-cass","title":"*Triggers Treats and Tricks: Cascade Document Delete Using Triggers Preimage","original_publish_date":"2021-08-16T10:16:40.045Z","strapi_updated_at":"2022-05-13T15:01:19.299Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt2a9fc3cb5fa51b87/644c4689ba3aa5d350525cba/og-realm-triggers-tricks.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:24.475Z","publish_details":{"time":"2023-04-28T22:52:54.430Z"}}},{"calculated_slug":"/products/mongodb/real-time-data-architectures-with-mongodb-cloud-manager-and-verizon-5g-edge","content":"The network edge has been one of the most explosive cloud computing opportunities in recent years. As mobile contactless experiences become the norm and as businesses move ever-faster to digital platforms and services, edge computing is positioned as a faster, cheaper, and more reliable alternative for data processing and compute at scale.\n\nWhile mobile devices continue to increase their hardware capabilities with built-in GPUs, custom chipsets, and more storage, even the most cutting-edge devices will suffer the same fundamental problem: each device serves as a single point of failure and, thus, cannot effectively serve as a persistent data storage layer. Said differently, wouldn’t it be nice to have the high-availability of the cloud but with the topological distance to your end users of the smartphone?\n\nMobile edge computing promises to precisely address this problem—bringing low latency compute to the edge of networks with the high-availability and scale of cloud computing. Through Verizon 5G Edge with AWS Wavelength, we saw the opportunity to explore how to take existing compute-intensive workflows and overlay a data persistence layer with MongoDB, utilizing the MongoDB Atlas management platform, to enable ultra-immersive experiences with personalized experience—reliant on existing database structures in the parent region with the seamlessness to extend to the network edge. \n\n![About Verizon 5G Edge and MongoDB](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/mongodb_verizon_5g_edge_66e1e2d536.png)\n\nIn this article, learn how Verizon and MongoDB teamed up to deliver on this vision, a quick Getting Started guide to build your first MongoDB application at the edge, and advanced architectures for those proficient with MongoDB.\n\nLet’s get started!\n\n## About Verizon 5G Edge and MongoDB\n\nThrough Verizon 5G Edge, AWS developers can now deploy parts of their application that require low latency at the edge of 4G and 5G networks using the same AWS APIs, tools, and functionality they use today, while seamlessly connecting back to the rest of their application and the full range of cloud services running in an AWS Region. By embedding AWS compute and storage services at the edge of the network, use cases such as ML inference, real-time video streaming, remote video production, and game streaming can be rapidly accelerated.\n\nHowever, for many of these use cases, a persistent storage layer is required that extends beyond the native storage capabilities of AWS Wavelength—namely Elastic Block Storage (EBS) volumes. However, using MongoDB Enterprise, developers can leverage the underlying compute (i.e.,. EC2 instances) at the edge to deploy MongoDB clusters either a) as standalone clusters or b) highly available replica sets that can synchronize data seamlessly.\n\nMongoDB is a general purpose, document-based, distributed database built for modern application developers. With MongoDB Atlas, developers can get up and running even faster with fully managed MongoDB databases deployed across all major cloud providers.\n\nWhile [MongoDB Atlas](https://mongodb.com/atlas) today does not support deployments within Wavelength Zones, [MongoDB Cloud Manager](https://www.mongodb.com/cloud/cloud-manager) can automate, monitor, and back up your MongoDB infrastructure. Cloud Manager Automation enables you to configure and maintain MongoDB nodes and clusters, whereby MongoDB Agents running on each MongoDB host can maintain your MongoDB deployments. In this example, we’ll start with a fairly simple architecture highlighting the relationship between Wavelength Zones (the edge) and the Parent Region (core cloud):\n\nJust like any other architecture, we’ll begin with a VPC consisting of two subnets. Instead of one public subnet and one private subnet, we’ll have one public subnet and one carrier subnet —a new way to describe subnets exposed within Wavelength Zones to the mobile network only.\n\n![AWS Diagram](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/diagram_49b0c08be0.png)\n\n* **Public Subnet**: Within the us-west-2 Oregon region, we launched a subnet in us-west-2a availability zone consisting of a single EC2 instance with a public IP address. From a routing perspective, we attached an Internet Gateway to the VPC to provide outbound connectivity and attached the Internet Gateway as the default route (0.0.0.0/0) to the subnet’s associated route table.\n* **Carrier Subnet**: Also within the us-west-2 Oregon region, our second subnet is in the San Francisco Wavelength Zone (us-west-2-wl1-sfo-wlz-1) —an edge data center within the Verizon carrier network but part of the us-west-2 region. In this subnet, we also deploy a single EC2 instance, this time with a carrier IP address—a carrier network-facing IP address exposed to Verizon mobile devices. From a routing perspective, we attached a Carrier Gateway to the VPC to provide outbound connectivity and attached the Carrier Gateway as the default route (0.0.0.0/0) to the subnet’s associated route table.\n\nNext, let’s configure the EC2 instance in the parent region. Once you get the IP address (54.68.26.68) of the launched EC2 instance, SSH into the instance itself and begin to download the MongoDB agent.\n\n```bash\nssh -i \"mongovz.pem\" ec2-user@ec2-54-68-26-68.us-west-2.compute.amazonaws.com\n```\n\nOnce you are in, download and install the packages required for the MongoDB MMS Automation Agent. Run the following command:\n\n```bash\nsudo yum install cyrus-sasl cyrus-sasl-gssapi \\\n     cyrus-sasl-plain krb5-libs libcurl \\\n     lm_sensors-libs net-snmp net-snmp-agent-libs \\\n     openldap openssl tcp_wrappers-libs xz-libs\n```\n\nOnce within the instance, download the MongoDB MMS Automation Agent, and install the agent using the RPM package manager.\n\n```bash\ncurl -OL https://cloud.mongodb.com/download/agent/automation/mongodb-mms-automation-agent-manager-10.30.1.6889-1.x86_64.rhel7.rpm\n\nsudo rpm -U mongodb-mms-automation-agent-manager-10.30.1.6889-1.x86_64.rhel7.rpm\n```\n\nNext, navigate to the **/etc/mongodb-mms/** and edit the **automation-agent.config** file to include your MongoDB Cloud Manager API Key. To create a key, head over to MongoDB Atlas at [https://mongodb.com/atlas](https://mongodb.com/atlas) and either login to an existing account, or sign up for a new free account.\n\nOnce you are logged in, create a new organization, and for the cloud service, be sure to select Cloud Manager.\n\n![Cloud Manager Setup](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/cloud_manager_1_7118a3c187.png)\n\nWith your organization created, next we’ll create a new Project. When creating a new project, you may be asked to select a cloud service, and you’ll choose Cloud Manager again.\n\n![Selecting Cloud Manager on Atlas](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/cloud_manager_2_bdc0385fbc.png)\n\nNext, you’ll name your project. You can select any name you like, we’ll go with Verizon for our project name. After you give your project a name, you will be given a prompt to invite others to the project. You can skip this step for now as you can always add additional users in the future.\n\n![Creating a Cloud Manager Project](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/cloud_manager_3_0afe0790e2.png)\n\nFinally, you are ready to deploy MongoDB to your environment using Cloud Manager. With Cloud Manager, you can deploy both standalone instances as well as Replica Sets of MongoDB. Since we want high availability, we’ll deploy a replica set.\n\n![Creating a Replica Set on Cloud Manager](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/cloud_manager_4_0b43dada9f.png)\n\nClicking on the **New Replica Set** button will bring us to the user interface to configure our replica set. At this point, we’ll probably get a message saying that no servers were detected, and that’s fine since we haven’t started our MongoDB Agents yet. \n\n![Replica Set Configuration](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/cloud_manager_5_d24084d445.png)\n\nClick on the “see instructions” link to get more details on how to install the MongoDB Agent. On the modal that pops up, it will have familiar instructions that we’re already following, but it will also have two pieces of information that we’ll need. The **mmsApiKey** and **mmsGroupId** will be displayed here and you’ll likely have to click the Generate Key button to generate a new mmsAPIKey which will be automatically populated. Make note of these **mmsGroupId** and **mmsApiKey** values as we’ll need when configuring our MongoDB Agents next.\n\n![Replica Set Instructions](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/cloud_manager_6_6e86d6ddbd.png)\n\nHead back to your terminal for the EC2 instance and navigate to the **/etc/mongodb-mms/** and edit the **automation-agent.config** file to include your MongoDB Cloud Manager API Key. \n\n![Automation Agent Configuration](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/automation_agent_config_cf4383b9e7.png)\n\nIn this example, we edited the **mmsApiKey** and **mmsGroupId** variables. From there, we’ll create the data directory and start our MongoDB agent!\n\n```bash\nsudo mkdir -p /data\nsudo chown mongod:mongod /data\nsudo systemctl start mongodb-mms-automation-agent.service\n```\n\nOnce you’ve completed these configuration steps, go ahead and do the same for your Wavelength Zone instance. Note that you will not be able to SSH directly to the instance’s Carrier IP (155.146.16.178/). Instead, you must use the parent region instance as a bastion host to “jump” onto the edge instance itself. To do so, find the private IP address of the edge instance (10.0.0.54) and, from the parent region instance, SSH into the second instance using the same key pair you used.\n\n```bash\nssh -i \"mongovz.pem\" ec2-user@10.0.0.54\n```\n\nAfter completing configuration of the second instance, which follows the same instructions from above, it’s time for the fun part —launching the ReplicaSet on the Cloud Manager Console! The one thing to note for the replica set, since we’ll have three nodes, on the edge instance we’ll create a /data and /data2 directories to allow for two separate directories to host the individual nodes data. Head back over to [https://mongodb.com/atlas](https://mongodb.com/atlas) and the Cloud Manager to complete setup.\n\nRefresh the Create New Replica Set page and now since the MongoDB Agents are running you should see a lot of information pre-populated for you. Make sure that it matches what you’d expect and when you’re satisfied hit the Create Replica Set button.\n\n![Finalize Creating a Replica Set on Cloud Manager](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/cloud_manager_7_03e29fb2aa.png)\n\nClick on the “Create Replica Set” button to finalize the process.\n\nWithin a few minutes the replica set cluster will be deployed to the servers and your MongoDB cluster will be up and running. \n\n![Cloud Manager Replica Set Deployed](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/cloud_manager_8_debf956fbc.png)\n\nWith the replica set deployed, you should now be able to connect to your MongoDB cluster hosted on either the standard Us-West or Wavelength zone. To do this, you’ll need the public address for the cluster and the port as well as Authentication enabled in Cloud Manager. To enable Authentication, simply click on the Enabled/Disabled button underneath the Auth section of your replica set and you’ll be given a number of options to connect to the client. We’ll select Username/password.\n\n![Cloud Manager User Account Setup](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/cloud_manager_9_dcdd123ceb.png)\n\nClick Next, and the subsequent modal will have your username and password to connect to the cluster with.\n\n![Cloud Manager User Account](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/cloud_manager_10_38fbb3c3bf.png)\n\nYou are all set. Next, let’s see how the MongoDB performs at the edge. We’ll test this by reading data from both our standard US-West node as well as the Wavelength zone and compare our results.\n\n## Racing MongoDB at the Edge\n\nAfter laying out the architecture, we wanted to see the power of 5G Edge in action. To that end, we designed a very simple “race.” Over 1,000 trials we would read data from our MongoDB database, and timestamp each operation both from the client to the edge and to the parent region. \n\n```python\nfrom pymongo import MongoClient\nimport time\nclient = MongoClient('155.146.144.134', 27017)\nmydb = client[\"mydatabase\"]\nmycol = mydb[\"customers\"]\nmydict = { \"name\": \"John\", \"address\": \"Highway 37\" }\n\n# Load dataset\nfor i in range(1000):\n    x = mycol.insert(mydict)\n\n# Measure reads from Parent Region\nedge_latency=[]\nfor i in range(1000):\n    t1=time.time()\n    y = mycol.find_one({\"name\":\"John\"})\n    t2=time.time()\n    edge_latency.append(t2-t1)\n\nprint(sum(edge_latency)/len(edge_latency))\n\nclient = MongoClient('52.42.129.138', 27017)\nmydb = client[\"mydatabase\"]\nmycol = mydb[\"customers\"]\nmydict = { \"name\": \"John\", \"address\": \"Highway 37\" }\n\n# Measure reads from Wavelength Region\nedge_latency=[]\nfor i in range(1000):\n    t1=time.time()\n    y = mycol.find_one({\"name\":\"John\"})\n    t2=time.time()\n    edge_latency.append(t2-t1)\n\nprint(sum(edge_latency)/len(edge_latency))\n```\n\nAfter running this experiment, we found that our MongoDB node at the edge performed **over 40% faster** than the parent region! But why was that the case? \n\nGiven that the Wavelength Zone nodes were deployed within the mobile network, packets never had to leave the Verizon network and incur the latency penalty of traversing through the public internet—prone to incremental jitter, loss, and latency. In our example, our 5G Ultra Wideband connected device in San Francisco had two options: connect to a local endpoint within the San Francisco mobile network or travel 500+ miles to a data center in Oregon. Thus, we validated the significant performance savings of MongoDB on Verizon 5G Edge relative to the next best alternative: deploying the same architecture in the core cloud.\n\n## Getting started on 5G Edge with MongoDB\n\nWhile Verizon 5G Edge alone enables developers to build ultra-immersive applications, how can immersive applications become personalized and localized?\n\nEnter MongoDB. \n\nFrom real-time transaction processing, telemetry capture for your IoT application, or personalization using profile data for localized venue experiences, bringing MongoDB ReplicaSets to the edge allows you to maintain the low latency characteristics of your application without sacrificing access to user profile data, product catalogues, IoT telemetry, and more.\n\nThere’s no better time to start your edge enablement journey with Verizon 5G Edge and MongoDB. To learn more about Verizon 5G Edge, you can visit our [developer resources page](https://www.verizon.com/business/solutions/5g/edge-computing/developer-resources/). If you have any questions about this blog post, find us in the [MongoDB community](https://community.mongodb.com).\n\nIn our next post, we will demonstrate how to build your first image classifier on 5G Edge using MongoDB to identify VIPs at your next sporting event, developer conference, or large-scale event.","description":"From real-time transaction processing, telemetry capture for your IoT application, or personalization using profile data for localized venue experiences, bringing MongoDB to the edge allows you to maintain the low latency characteristics of your application without sacrificing access to data.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt028fd4528f2bf9e9/644c468850ad364c03e3a4ad/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/real-time-data-architectures-with-mongodb-cloud-manager-and-verizon-5g-edge","title":"*Real-time Data Architectures with MongoDB Cloud Manager and Verizon 5G Edge","original_publish_date":"2021-08-12T15:48:00.807Z","strapi_updated_at":"2022-05-12T20:57:20.224Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Cloud Manager","calculated_slug":"/products/mongodb/cloud-manager"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*AWS","calculated_slug":"/technologies/aws"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:24.084Z","publish_details":{"time":"2023-04-28T22:52:54.458Z"}}},{"calculated_slug":"/products/mongodb/time-series-candlestick","content":"## Introduction\n\nTechnical analysis is a methodology used in finance to provide price forecasts for financial assets based on historical market data. \n\nWhen it comes to analyzing market data, you need a better toolset. You will have a good amount of data, hence storing, accessing, and fast processing of this data becomes harder.\n\nThe financial assets price data is an example of time-series data. MongoDB 5.0 comes with a few important features to facilitate time-series data processing:\n\n- [Time Series Collections](https://docs.mongodb.com/manual/core/timeseries-collections/): This specialized MongoDB collection makes it incredibly simple to store and process time-series data with automatic bucketing capabilities.\n- [New Aggregation Framework Date Operators](https://docs.mongodb.com/manual/release-notes/5.0/#aggregation): `$dateTrunc`, `$dateAdd`, `$dateTrunc`, and `$dateDiff`.\n- [Window Functions](https://docs.mongodb.com/manual/reference/operator/aggregation/setWindowFields/#mongodb-pipeline-pipe.-setWindowFields): Performs operations on a specified span of documents in a collection, known as a window, and returns the results based on the chosen [window operator](https://docs.mongodb.com/manual/reference/operator/aggregation/setWindowFields/#window-operators).\n\nThis three-part series will explain how you can build a currency analysis platform where you can apply well-known financial analysis techniques such as [SMA](https://www.investopedia.com/terms/s/sma.asp), [EMA](https://www.investopedia.com/terms/e/ema.asp), [MACD](https://www.investopedia.com/terms/m/macd.asp), and [RSI](https://www.investopedia.com/terms/r/rsi.asp). While you can read through this article series and grasp the main concepts, you can also get your hands dirty and run the entire demo-toolkit by yourself. All the code is available in the [Github repository](https://github.com/afsungur/mongodb-currency-analysis).\n\n## Data Model\n\nWe want to save the last price of every currency in MongoDB, in close to real time. Depending on the currency data provider, it can be millisecond level to minute level. We insert the data as we get it from the provider with the following simple data model:\n\n```json\n{\n   \"time\": ISODate(\"20210701T13:00:01.343\"),\n   \"symbol\": \"BTC-USD\",\n   \"price\": 33451.33\n}\n```\n\nWe only have three fields in MongoDB:\n\n- `time` is the time information when the symbol information is received.\n- `symbol` is the currency symbol such as \"BTC-USD.\" There can be hundreds of different symbols. \n- `price` field is the numeric value which indicates the value of currency at the time.\n\n## Data Source\n\n[Coinbase](https://www.coinbase.com/), one of the biggest cryptocurrency exchange platforms, provides a [WebSocket API](https://docs.pro.coinbase.com/#websocket-feed) to consume real-time cryptocurrency price updates. We will connect to Coinbase through a WebSocket, retrieve the data in real-time, and insert it into MongoDB. In order to increase the efficiency of insert operations, we can apply bulk insert.\n\nEven though our data source in this post is a cryptocurrency exchange, this article and the demo toolkit are applicable to any exchange platform that has time, symbol, and price information.\n\n## Bucketing Design Pattern \n\nThe MongoDB document model provides a lot of flexibility in how you model data. That flexibility is incredibly powerful, but that power needs to be harnessed in terms of your application’s data access patterns; [schema design](https://www.mongodb.com/developer/article/mongodb-schema-design-best-practices/) in MongoDB has a tremendous impact on the performance of your application.\n\nThe [bucketing design pattern](https://www.mongodb.com/blog/post/building-with-patterns-the-bucket-pattern) is one MongoDB design pattern that groups raw data from multiple documents into one document rather than keeping separate documents for each and every raw piece of data. Therefore, we see performance benefits in terms of index size savings and read/write speed. Additionally, by grouping the data together with bucketing, we make it easier to organize specific groups of data, thus increasing the ability to discover historical trends or provide future forecasting. \n\nHowever, prior to MongoDB 5.0, in order to take advantage of bucketing, it required application code to be aware of bucketing and engineers to make conscious upfront schema decisions, which added overhead to developing efficient time series solutions within MongoDB. \n\n## Time Series Collections for Currency Analysis\n\nTime Series collections are a new collection type introduced in MongoDB 5.0. It automatically optimizes for the storage of time series data and makes it easier, faster, and less expensive to work with time series data in MongoDB. There is a great [blog post](https://developer.mongodb.com/how-to/new-time-series-collections/) that covers MongoDB’s newly introduced Time Series collections in more detail that you may want to read first or for additional information.\n\nFor our use case, we will create a Time Series collection as follows:\n\n```javascript\ndb.createCollection(\"ticker\", {\n timeseries: {\n   timeField: \"time\",\n   metaField: \"symbol\",\n },\n});\n\n```\n\nWhile defining the time series collection, we set the `timeField` of the time series collection as `time`, and the `metaField` of the time series collection as `symbol`. Therefore, a particular symbol’s data for a period will be stored together in the time series collection. \n\n### How the Currency Data is Stored in the Time Series Collection\n\nThe application code will make a simple insert operation as it does in a regular collection:\n\n```javascript\ndb.ticker.insertOne({\n time: ISODate(\"20210101T01:00:00\"),\n symbol: \"BTC-USD\",\n price: 34114.1145,\n});\n```\n\nWe read the data in the same way we would from any other MongoDB collection: \n\n```javascript\ndb.ticker.findOne({\"symbol\" : \"BTC-USD\"})\n\n{\n       \"time\": ISODate(\"20210101T01:00:00\"),\n       \"symbol\": \"BTC-USD\",\n       \"price\": 34114.1145,\n       \"_id\": ObjectId(\"611ea97417712c55f8d31651\")\n}\n```\n\nHowever, the underlying storage optimization specific to time series data will be done by MongoDB. For example, \"BTC-USD\" is a digital currency and every second you make an insert operation, it looks and feels like it’s stored as a separate document when you query it. However, the underlying optimization mechanism keeps the same symbols’ data together for faster and efficient processing. This allows us to automatically provide the advantages of the bucket pattern in terms of index size savings and read/write performance without sacrificing the way you work with your data.\n\n## Candlestick Charts\n\nWe have already inserted hours of data for different currencies. A particular currency’s data is stored together, thanks to the Time Series collection. Now it’s time to start analyzing the currency data.\n\nNow, instead of individually analyzing second level data, we will group the data by five-minute intervals, and then display the data on candlestick charts. Candlestick charts in technical analysis represent the movement in prices over a period of time. \n\nAs an example, consider the following candlestick. It represents one time interval, e.g. five minutes between `20210101-17:30:00` and `20210101-17:35:00`, and it’s labeled with the start date, `20210101-17:30:00.` It has four metrics: high, low, open, and close. High is the highest price, low is the lowest price, open is the first price, and close is the last price of the currency in this duration.  \n\n![Components of a candlestick chart](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/candlestick_chart_58903567ef.png)\n\nIn our currency dataset, we have to reach a stage where we need to have grouped the data by five-minute intervals like: `2021-01-01T01:00:00`, `2021-01-01T01:05:00`, etc. And every interval group needs to have four metrics: high, low, open, and close price. Examples of interval data are as follows:\n\n```json\n[{\n   \"time\": ISODate(\"20210101T01:00:00\"),\n   \"symbol\": \"BTC-USD\",\n   \"open\": 34111.12,\n   \"close\": 34192.23,\n   \"high\": 34513.28,\n   \"low\": 33981.17\n},\n{\n   \"time\": ISODate(\"20210101T01:05:00\"),\n   \"symbol\": \"BTC-USD\",\n   \"open\": 34192.23,\n   \"close\": 34244.16,\n   \"high\": 34717.90,\n   \"low\": 34001.13\n}]\n```\n\nHowever, we only currently have second-level data for each ticker stored in our Time Series collection as we push the data for every second. We need to group the data, but how can we do this?\n\nIn addition to Time Series collections, MongoDB 5.0 has introduced a new aggregation operator, [`$dateTrunc`](https://docs.mongodb.com/manual/reference/operator/aggregation/dateTrunc/). This powerful new aggregation operator can do many things, but essentially, its core functionality is to truncate the date information to the closest time or a specific datepart, by considering the given parameters. In our scenario, we want to group currency data for five-minute intervals. Therefore, we can set the `$dateTrunc` operator parameters accordingly:\n\n```json\n{\n   $dateTrunc: {\n     date: \"$time\",\n     unit: \"minute\",\n     binSize: 5\n   }\n}\n```\n\nIn order to set the high, low, open, and close prices for each group (each candlestick), we can use other MongoDB operators, which were already available before MongoDB 5.0:\n\n- high: [`$max`](https://docs.mongodb.com/manual/reference/operator/aggregation/max/)\n- low: [`$min`](https://docs.mongodb.com/manual/reference/operator/aggregation/min/)\n- open: [`$first`](https://docs.mongodb.com/manual/reference/operator/aggregation/first/)\n- close: [`$last`](https://docs.mongodb.com/manual/reference/operator/aggregation/last/)\n\nAfter grouping the data, we need to sort the data by time to analyze it properly. Therefore, recent data (represented by a candlestick) will be at the right-most of the chart.\n\nPutting this together, our entire aggregation query will look like this:\n\n```js\ndb.ticker.aggregate([\n {\n   $match: {\n     symbol: \"BTC-USD\",\n   },\n },\n {\n   $group: {\n     _id: {\n       symbol: \"$symbol\",\n       time: {\n         $dateTrunc: {\n           date: \"$time\",\n           unit: \"minute\",\n           binSize: 5\n         },\n       },\n     },\n     high: { $max: \"$price\" },\n     low: { $min: \"$price\" },\n     open: { $first: \"$price\" },\n     close: { $last: \"$price\" },\n   },\n },\n {\n   $sort: {\n     \"_id.time\": 1,\n   },\n },\n]);\n```\n\nAfter we grouped the data based on five-minute intervals, we can visualize it in a candlestick chart as follows:\n\n![Candlestick chart](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/candlestick_chart2_c814ff7558.png)\n\nWe are currently using an [open source visualization tool](https://apexcharts.com/) to display five-minute grouped data of BTC-USD currency. Every stick in the chart represents a five-minute interval and has four metrics: high, low, open, and close price. \n\n## Conclusion\n\nWith the introduction of Time Series collections and advanced aggregation operators for date calculations, MongoDB 5.0 makes currency analysing much easier. \n\nAfter you’ve grouped the data for the selected intervals, you can allow MongoDB to remove old data by setting the [`expireAfterSeconds`](https://docs.mongodb.com/manual/core/timeseries-collections/) parameter in the collection options. It will automatically remove the older data than the specified time in seconds.\n\nAnother option is to archive raw data to cold storage for further analysis. Fortunately, [MongoDB Atlas has automatic archiving capability](https://www.mongodb.com/developer/how-to/manage-data-at-scale-with-online-archive/) to offload the old data in a MongoDB Atlas cluster to cold object storage, such as S3. To do that, you can set your archiving rules on the time series collection and it will automatically offload the old data to the cold storage. Online Archive will be available for time-series collections very soon.\n\nIs the currency data already placed in Kafka topics? That’s perfectly fine. You can easily transfer the data in Kafka topics to MongoDB through [MongoDB Sink Connector for Kafka](https://docs.mongodb.com/kafka-connector/current/kafka-sink/). Please check out this [article](https://www.mongodb.com/blog/post/streaming-time-series-data-using-apache-kafka-mongodb) for further details on the integration of Kafka topics and the MongoDB Time Series collection.\n\nIn the following posts, we’ll discuss how well-known financial technical indicators can be calculated via windowing functions on time series collections.","description":"Time series collections part 1: generating data for a candlestick chart from time-series data","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt70d775c3de63aa73/644c468dc6aa71e2be7e7d23/aggregation-framework_ATF.png?branch=prod","description":null}}]},"slug":"/time-series-candlestick","title":"*Currency Analysis with Time Series Collections #1 — Generating Candlestick Charts Data","original_publish_date":"2021-08-27T07:05:10.184Z","strapi_updated_at":"2022-05-16T18:37:54.207Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Time series","calculated_slug":"/products/mongodb/time-series"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf56bbc20c158b9eb/644c4690716c5a78d72933e8/og-aggregation-framework.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:23.720Z","publish_details":{"time":"2023-04-28T22:52:54.487Z"}}},{"calculated_slug":"/products/mongodb/time-series-candlestick-sma-ema","content":"## Introduction\n\nIn the [previous post](https://www.mongodb.com/developer/article/time-series-candlestick/), we learned how to group currency data based on given time intervals to generate candlestick charts to perform trend analysis. In this article, we’ll learn how the moving average can be calculated on time-series data.\n\nMoving average is a well-known financial technical indicator that is commonly used either alone or in combination with other indicators. Additionally, the moving average is included as a parameter of other financial technical indicators like [MACD](https://www.investopedia.com/terms/m/macd.asp). The main reason for using this indicator is to smooth out the price updates to reflect recent price changes accordingly. There are many types of moving averages but here we’ll focus on two of them: Simple Moving Average ([SMA](https://www.investopedia.com/terms/s/sma.asp)) and Exponential Moving Average ([EMA](https://www.investopedia.com/terms/e/ema.asp)).\n\n## Simple Moving Average (SMA)\n\nThis is the average price value of a currency/stock within a given period. \n\nLet’s calculate the SMA for the BTC-USD currency over the last three data intervals, including the current data. Remember that each stick in the candlestick chart represents five-minute intervals. Therefore, for every interval, we would look for the previous three intervals.\n\nFirst we’ll group the BTC-USD currency data for five-minute intervals: \n\n\n```js\ndb.ticker.aggregate([\n {\n   $match: {\n     symbol: \"BTC-USD\",\n   },\n },\n {\n   $group: {\n     _id: {\n       symbol: \"$symbol\",\n       time: {\n         $dateTrunc: {\n           date: \"$time\",\n           unit: \"minute\",\n           binSize: 5\n         },\n       },\n     },\n     high: { $max: \"$price\" },\n     low: { $min: \"$price\" },\n     open: { $first: \"$price\" },\n     close: { $last: \"$price\" },\n   },\n },\n {\n   $sort: {\n     \"_id.time\": 1,\n   },\n },\n]);\n```\n\nAnd, we will have the following candlestick chart:\n\n![Candlestick chart](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/camdlestick3_e788070c14.png)\n\nWe have four metrics for each interval and we will choose the close price as the numeric value for our moving average calculation. We are only interested in `_id` (a nested field that includes the symbol and time information) and the close price. Therefore, since we are not interested in high, low, open prices for SMA calculation, we will exclude it from the aggregation pipeline with the [`$project`](https://docs.mongodb.com/manual/reference/operator/aggregation/project/) aggregation stage:\n\n```js\n{\n   $project: {\n     _id: 1,\n     price: \"$close\",\n   },\n}\n```\n\nAfter we grouped and trimmed, we will have the following dataset:\n\n```js\n{\"_id\": {\"time\": ISODate(\"20210101T17:00:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35050}\n{\"_id\": {\"time\": ISODate(\"20210101T17:05:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35170}\n{\"_id\": {\"time\": ISODate(\"20210101T17:10:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35280}\n{\"_id\": {\"time\": ISODate(\"20210101T17:15:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 34910}\n{\"_id\": {\"time\": ISODate(\"20210101T17:20:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35060}\n{\"_id\": {\"time\": ISODate(\"20210101T17:25:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35150}\n{\"_id\": {\"time\": ISODate(\"20210101T17:30:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35350}\n```\n\nOnce we have the above dataset, we want to enrich our data with the simple moving average indicator as shown below. Every interval in every symbol will have one more field (sma) to represent the SMA indicator by including the current and last three intervals:\n\n```js\n{\"_id\": {\"time\": ISODate(\"20210101T17:00:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35050, \"sma\": ?}\n{\"_id\": {\"time\": ISODate(\"20210101T17:05:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35170, \"sma\": ?}\n{\"_id\": {\"time\": ISODate(\"20210101T17:10:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35280, \"sma\": ?}\n{\"_id\": {\"time\": ISODate(\"20210101T17:15:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 34910, \"sma\": ?}\n{\"_id\": {\"time\": ISODate(\"20210101T17:20:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35060, \"sma\": ?}\n{\"_id\": {\"time\": ISODate(\"20210101T17:25:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35150, \"sma\": ?}\n{\"_id\": {\"time\": ISODate(\"20210101T17:30:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35350, \"sma\": ?}\n```\n\nHow is it calculated? For the time, `17:00:00`, the calculation of SMA is very simple. Since we don’t have the three previous data points, we can take the existing price (35050) at that time as average. If we don’t have three previous data points, we can get all the available possible price information and divide by the number of price data. \n\nThe harder part comes when we have more than three previous data points. If we have more than three previous data points, we need to remove the older ones. And, we have to keep doing this as we have more data for a single symbol. Therefore, we will calculate the average by considering only up to three previous data points. The below table represents the calculation step by step for every interval:\n\n| Time | SMA Calculation for the window (3 previous + current data points) |\n| --- | --- |\n| 17:00:00 | 35050/1 |\n| 17:05:00 | (35050+35170)/2 |\n| 17:10:00 | (35050+35170+35280)/3 |\n| 17:15:00 | (35050+35170+35280+34910)/4 |\n| 17:20:00 | (35170+35280+34910+35060)/4 <br/>*oldest price data (35050) discarded from the calculation |\n| 17:25:00 | (35280+34910+35060+35150)/4 <br/>*oldest price data (35170) discarded from the calculation |\n| 17:30:00 | (34190+35060+35150+35350)/4 <br/>*oldest price data (35280) discarded from the calculation |\n\nAs you see above, the window for the average calculation is moving as we have more data. \n\n## Window Functions\n\nUntil now, we learned the theory of moving average calculation. How can we use MongoDB to do this calculation for all of the currencies?\n\nMongoDB 5.0 introduced a new aggregation stage, [`$setWindowFields`](https://docs.mongodb.com/manual/reference/operator/aggregation/setWindowFields/), to perform operations on a specified range of documents (window) in the defined partitions. Because it also supports average calculation on a window through [`$avg`](https://docs.mongodb.com/manual/reference/operator/aggregation/avg/#use-in--setwindowfields-stage) operator, we can easily use it to calculate Simple Moving Average:\n\n```js\n{\n   $setWindowFields: {\n     partitionBy: \"_id.symbol\",\n     sortBy: { \"_id.time\": 1 },\n     output: {\n       sma: {\n         $avg: \"$price\",\n         window: { documents: [-3, 0] },\n       },\n     },\n   },\n}\n\n```\n\nWe chose the symbol field as partition key. For every currency, we have a partition, and each partition will have its own window to process that specific currency data. Therefore, when we’d like to process sequential data of a single currency, we will not mingle the other currency’s data.\n\nAfter we set the partition field, we apply sorting to process the data in an ordered way. The partition field provides processing of single currency data together. However, we want to process data as ordered by time. As we see in how SMA is calculated on the paper, the order of the data matters and therefore, we need to specify the field for ordering. \n\nAfter partitions are set and sorted, then we can process the data for each partition. We generate one more field, “`sma`”, and we define the calculation method of this derived field. Here we set three things:\n\n- The operator that is going to be executed (`$avg`).\n- The field (`$price`) where the operator is going to be executed on.\n- The boundaries of the window (`[-3,0]`).\n- `[-3`: “start from 3 previous data points”.\n- `0]`: “end up with including current data point”.\n    - We can also set the second parameter of the window as “`current`” to include the current data point rather than giving numeric value.\n\nMoving the window on the partitioned and sorted data will look like the following. For every symbol, we’ll have a partition, and all the records belonging to that partition will be sorted by the time information:\n\n![Calculation process](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/process_65fbc35406.gif)\n\nThen we will have the `sma` field calculated for every document in the input stream. You can apply [`$round`](https://docs.mongodb.com/manual/reference/operator/aggregation/round/) operator to trim to the specified decimal place in a [`$set`](https://docs.mongodb.com/manual/reference/operator/aggregation/set/) aggregation stage:\n\n```js\n{\n   $set: {\n     sma: { $round: [\"$sma\", 2] },\n   },\n}\n```\n\nIf we bring all the aggregation stages together, we will end-up with this aggregation pipeline:\n\n```js\ndb.ticker.aggregate([\n {\n   $match: {\n     symbol: \"BTC-USD\",\n   },\n },\n {\n   $group: {\n     _id: {\n       symbol: \"$symbol\",\n       time: {\n         $dateTrunc: {\n           date: \"$time\",\n           unit: \"minute\",\n           binSize: 5,\n         },\n       },\n     },\n     high: { $max: \"$price\" },\n     low: { $min: \"$price\" },\n     open: { $first: \"$price\" },\n     close: { $last: \"$price\" },\n   },\n },\n {\n   $sort: {\n     \"_id.time\": 1,\n   },\n },\n {\n   $project: {\n     _id: 1,\n     price: \"$close\",\n   },\n },\n {\n   $setWindowFields: {\n     partitionBy: \"_id.symbol\",\n     sortBy: { \"_id.time\": 1 },\n     output: {\n       sma: {\n         $avg: \"$price\",\n         window: { documents: [-3, 0] },\n       },\n     },\n   },\n },\n {\n   $set: {\n     sma: { $round: [\"$sma\", 2] },\n   },\n },\n]);\n```\n\nYou may want to add more calculated fields with different options. For example, you can have two SMA calculations with different parameters. One of them could include the last three points as we have done already, and the other one could include the last 10 points, and you may want to compare both. Find the query below:\n\n```js\n{\n   $setWindowFields: {\n     partitionBy: \"_id.symbol\",\n     sortBy: { \"_id.time\": 1 },\n     output: {\n       sma_3: {\n         $avg: \"$price\",\n         window: { documents: [-3, 0] },\n       },\n       sma_10: {\n         $avg: \"$price\",\n         window: { documents: [-10, 0] },\n       },\n     },\n   },\n }\n\n```\n\nHere in the above code, we set two derived fields. The `sma_3` field represents the moving average for the last three data points, and the `sma_10` field represents the moving average for the 10 last data points. Furthermore, you can compare these two moving averages to take a position on the currency or use it for a parameter for your own technical indicator.\n\nThe below chart shows two moving average calculations. The line with blue color represents the simple moving average with the window `[-3,0]`. The line with the turquoise color represents the simple moving average with the window `[-10,0]`. As you can see, when the window is bigger, reaction to price change gets slower:\n\n![Candlestick chart](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/camdlestick4_263a7049d5.png)\n\nYou can even enrich it further with the additional operations such as covariance, standard deviation, and so on. Check the full supported options [here](https://docs.mongodb.com/manual/reference/operator/aggregation/setWindowFields/#window-operators). We will cover the Exponential Moving Average here as an additional operation.\n\n## Exponential Moving Average (EMA)\n\nEMA is a kind of moving average. However, it weighs the recent data higher. In the calculation of the Simple Moving Average, we equally weight all the input parameters. However, in the Exponential Moving Average, based on the given parameter, recent data gets more important. Therefore, Exponential Moving Average reacts faster than Simple Moving Average to recent price updates within the similar size window.\n\n[`$expMovingAvg`](https://docs.mongodb.com/manual/reference/operator/aggregation/expMovingAvg/) has been introduced in MongoDB 5.0. It takes two parameters: the field name that includes numeric value for the calculation, and [`N`](https://docs.mongodb.com/manual/reference/operator/aggregation/expMovingAvg/#std-label-expMovingAvg-N) or [`alpha`](https://docs.mongodb.com/manual/reference/operator/aggregation/expMovingAvg/#std-label-expMovingAvg-alpha) value. We’ll set the parameter `N` to specify how many previous data points need to be evaluated while calculating the moving average and therefore, recent records within the `N` data points will have more weight than the older data. You can refer to the documentation for more information:\n\n```js\n{\n   $expMovingAvg: {\n      input: \"$price\",\n      N: 5\n   }\n}\n```\n\nIn the below diagram, SMA is represented with the blue line and EMA is represented with the red line, and both are calculated by five recent data points. You can see how the Simple Moving Average reacts slower to the recent price updates than the Exponential Moving Average even though they both have the same records in the calculation:\n\n![Candlestick chart](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/camdlestick5_c8e6c10e37.png)\n\n## Conclusion\n\nMongoDB 5.0, with the introduction of Windowing Function, makes calculations much easier over a window. There are many aggregation operators that can be executed over a window, and we have seen `$avg` and `$expMovingAvg` in this article. \n\nHere in the given examples, we set the window boundaries by including the positional documents. In other words, we start to include documents from three previous data points to current data point (`documents: [-3,0]`). You can also set a range of documents rather than defining position. \n\nFor example, if the window is sorted by time, you can include the last 30 minutes of data (whatever number of documents you have) by specifying the range option as follows: `range: [-30,0], unit: \"minute\". `Now, we may have hundreds of documents in the window but we know that we only include the documents that are not older than 30 minutes than the current data.\n\nYou can also materialize the query output into another collection through [`$out`](https://docs.mongodb.com/manual/reference/operator/aggregation/out/) or [`$merge`](https://docs.mongodb.com/manual/reference/operator/aggregation/merge/) aggregation stages. And furthermore, you can enable [change streams](https://docs.mongodb.com/manual/changeStreams/) or [Database Triggers](https://docs.atlas.mongodb.com/triggers/) on the materialized view to automatically trigger buy/sell actions based on the result of technical indicator changes.","description":"Time series collections part 2: How to calculate Simple Moving Average and Exponential Moving Average \n\n","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt5bee67225fcfa072/644c469506be8ef05af54a6f/TimeSeries_ATF2.png?branch=prod","description":null}}]},"slug":"/time-series-candlestick-sma-ema","title":"*Currency Analysis with Time Series Collections #2 — Simple Moving Average and Exponential Moving Average Calculation","original_publish_date":"2021-08-27T07:08:46.444Z","strapi_updated_at":"2022-05-16T18:37:54.207Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Time series","calculated_slug":"/products/mongodb/time-series"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf56bbc20c158b9eb/644c4690716c5a78d72933e8/og-aggregation-framework.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:23.329Z","publish_details":{"time":"2023-04-28T22:52:54.520Z"}}},{"calculated_slug":"/products/mongodb/time-series-macd-rsi","content":"In the [first post](https://www.mongodb.com/developer/article/time-series-candlestick/) of this series, we learned how to group currency data based on given time intervals to generate candlestick charts. In the [second article](https://www.mongodb.com/developer/article/time-series-candlestick-sma-ema), we learned how to calculate simple moving average and exponential moving average on the currencies based on a given time window. Now, in this post we’ll learn how to calculate more complex technical indicators.\n\n## MACD Indicator\n\n[MACD](https://www.investopedia.com/terms/m/macd.asp) (Moving Average Convergence Divergence) is another trading indicator and provides visibility of the trend and momentum of the currency/stock. MACD calculation fundamentally leverages multiple [EMA](https://www.investopedia.com/terms/e/ema.asp) calculations with different parameters.\n\nAs shown in the below diagram, MACD indicator has three main components: MACD Line, MACD Signal, and Histogram. (The blue line represents MACD Line, the red line represents MACD Signal, and green and red bars represent histogram):\n\n![MACD histogram](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/MACD_Histogram_c2dbe7f43d.png)\n\n- MACD Line is calculated by subtracting the 26-period (mostly, days are used for the period) exponential moving average from the 12-period exponential moving average. \n- After we get the MACD Line, we can calculate the MACD Signal. MACD Signal is calculated by getting the nine-period exponential moving average of MACD Line.\n- MACD Histogram is calculated by subtracting the MACD Signal from the MACD Line. \n\nWe can use the [MongoDB Aggregation Framework](https://docs.mongodb.com/manual/aggregation/) to calculate this complex indicator. \n\nIn the previous blog posts, we learned how we can group the second-level raw data into five-minutes intervals through the `$group` stage and `$dateTrunc` operator:\n\n```js\ndb.ticker.aggregate([\n {\n   $match: {\n     symbol: \"BTC-USD\",\n   },\n },\n {\n   $group: {\n     _id: {\n       symbol: \"$symbol\",\n       time: {\n         $dateTrunc: {\n           date: \"$time\",\n           unit: \"minute\",\n           binSize: 5,\n         },\n       },\n     },\n     high: { $max: \"$price\" },\n     low: { $min: \"$price\" },\n     open: { $first: \"$price\" },\n     close: { $last: \"$price\" },\n   },\n },\n {\n   $sort: {\n     \"_id.time\": 1,\n   },\n },\n {\n   $project: {\n     _id: 1,\n     price: \"$close\",\n   },\n }\n]);\n```\n\nAfter that, we need to calculate two exponential moving averages with different parameters:\n\n```js\n{\n   $setWindowFields: {\n     partitionBy: \"_id.symbol\",\n     sortBy: { \"_id.time\": 1 },\n     output: {\n       ema_12: {\n         $expMovingAvg: { input: \"$price\", N: 12 },\n       },\n       ema_26: {\n         $expMovingAvg: { input: \"$price\", N: 26 },\n       },\n     },\n   },\n}\n```\n\nAfter we calculate two separate exponential moving averages, we need to apply the `$subtract` operation in the next stage of the aggregation pipeline:\n\n```js\n{ $addFields : {\"macdLine\" : {\"$subtract\" : [\"$ema_12\", \"$ema_26\"]}}}\n```\n\nAfter we’ve obtained the `macdLine` field, then we can apply another exponential moving average to this newly generated field (`macdLine`) to obtain MACD signal value:\n\n```js\n{\n   $setWindowFields: {\n     partitionBy: \"_id.symbol\",\n     sortBy: { \"_id.time\": 1 },\n     output: {\n       macdSignal: {\n         $expMovingAvg: { input: \"$macdLine\", N: 9 },\n       },\n     },\n   },\n}\n```\n\nTherefore, we will have two more fields: `macdLine` and `macdSignal`. We can generate another field as `macdHistogram` that is calculated by subtracting the `macdSignal` from `macdLine` value:\n\n```js\n{ $addFields : {\"macdHistogram\" : {\"$subtract\" : [\"$macdLine\", \"$macdSignal\"]}}}\n```\n\nNow we have three derived fields: `macdLine`, `macdSignal`, and `macdHistogram`. Below, you can see how MACD is visualized together with Candlesticks:\n\n![Candlestick charts](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Candle_Stick_24919a4b2a.png)\n\nThis is the complete aggregation pipeline:\n\n```js\ndb.ticker.aggregate([\n {\n   $match: {\n     symbol: \"BTC-USD\",\n   },\n },\n {\n   $group: {\n     _id: {\n       symbol: \"$symbol\",\n       time: {\n         $dateTrunc: {\n           date: \"$time\",\n           unit: \"minute\",\n           binSize: 5,\n         },\n       },\n     },\n     high: { $max: \"$price\" },\n     low: { $min: \"$price\" },\n     open: { $first: \"$price\" },\n     close: { $last: \"$price\" },\n   },\n },\n {\n   $sort: {\n     \"_id.time\": 1,\n   },\n },\n {\n   $project: {\n     _id: 1,\n     price: \"$close\",\n   },\n },\n {\n   $setWindowFields: {\n     partitionBy: \"_id.symbol\",\n     sortBy: { \"_id.time\": 1 },\n     output: {\n       ema_12: {\n         $expMovingAvg: { input: \"$price\", N: 12 },\n       },\n       ema_26: {\n         $expMovingAvg: { input: \"$price\", N: 26 },\n       },\n     },\n   },\n },\n { $addFields: { macdLine: { $subtract: [\"$ema_12\", \"$ema_26\"] } } },\n {\n   $setWindowFields: {\n     partitionBy: \"_id.symbol\",\n     sortBy: { \"_id.time\": 1 },\n     output: {\n       macdSignal: {\n         $expMovingAvg: { input: \"$macdLine\", N: 9 },\n       },\n     },\n   },\n },\n {\n   $addFields: { macdHistogram: { $subtract: [\"$macdLine\", \"$macdSignal\"] } },\n },\n]);\n```\n\n## RSI Indicator\n\n[RSI](https://www.investopedia.com/terms/r/rsi.asp) (Relativity Strength Index) is another financial technical indicator that reveals whether the asset has been overbought or oversold. It usually uses a 14-period time frame window, and the value of RSI is measured on a scale of 0 to 100. If the value is closer to 100, then it indicates that the asset has been overbought within this time period. And if the value is closer to 0, then it indicates that the asset has been oversold within this time period. Mostly, 70 and 30 are used for upper and lower thresholds.\n\nCalculation of RSI is a bit more complicated than MACD:\n\n- For every data point, the gain and the loss values are set by comparing one previous data point.\n- After we set gain and loss values for every data point, then we can get a moving average of both gain and loss for a 14-period. (You don’t have to apply a 14-period. Whatever works for you, you can set accordingly.)\n- After we get the average gain and the average loss value, we can divide average gain by average loss.\n- After that, we can smooth the value to normalize it between 0 and 100.\n\n### Calculating Gain and Loss\n\nFirstly, we need to define the gain and the loss value for each interval. \n\nThe gain and loss value are calculated by subtracting one previous price information from the current price information:\n\n- If the difference is positive, it means there is a price increase and the value of the gain will be the difference between current price and previous price. The value of the loss will be 0.\n- If the difference is negative, it means there is a price decline and the value of the loss will be the difference between previous price and current price. The value of the gain will be 0.\n\nConsider the following input data set:\n\n```js\n{\"_id\": {\"time\": ISODate(\"20210101T17:00:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35050}\n{\"_id\": {\"time\": ISODate(\"20210101T17:05:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35150}\n{\"_id\": {\"time\": ISODate(\"20210101T17:10:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35280}\n{\"_id\": {\"time\": ISODate(\"20210101T17:15:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 34910}\n```\n\nOnce we calculate the Gain and Loss, we will have the following data:\n\n```js\n{\"_id\": {\"time\": ISODate(\"20210101T17:00:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35050, \"previousPrice\": null, \"gain\":0, \"loss\":0}\n{\"_id\": {\"time\": ISODate(\"20210101T17:05:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35150, \"previousPrice\": 35050, \"gain\":100, \"loss\":0}\n{\"_id\": {\"time\": ISODate(\"20210101T17:10:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 35280, \"previousPrice\": 35150, \"gain\":130, \"loss\":0}\n{\"_id\": {\"time\": ISODate(\"20210101T17:15:00\"), \"symbol\" : \"BTC-USD\"}, \"price\": 34910, \"previousPrice\": 35280, \"gain\":0, \"loss\":370}\n```\n\nBut in the MongoDB Aggregation Pipeline, how can we refer to the previous document from the current document? How can we derive the new field (`$previousPrice`) from the previous document in the sorted window?  \n\nMongoDB 5.0 introduced the [`$shift`](https://docs.mongodb.com/manual/reference/operator/aggregation/shift/#mongodb-group-grp.-shift) operator that includes data from another document in the same partition at the given location, e.g., you can refer to the document that is three documents before the current document or two documents after the current document in the sorted window.\n\nWe set our window with partitioning and introduce new field as previousPrice:\n\n```js\n{\n   $setWindowFields: {\n     partitionBy: \"$_id.symbol\",\n     sortBy: { \"_id.time\": 1 },\n     output: {\n       previousPrice: { $shift: { by: -1, output: \"$price\" } },\n     },\n   },\n}\n```\n\n`$shift` takes two parameters:\n\n- `by` specifies the location of the document which we’ll include. Since we want to include the previous document, then we set it  to `-1`. If we wanted to include one next document, then we would set it to `1`.\n- `output` specifies the field of the document that we want to include in the current document.\n\nAfter we set the `$previousPrice` information for the current document, then we need to subtract the previous value from current value. We will have another derived field “`diff`” that represents the difference value between current value and previous value:\n\n```js\n{\n   $addFields: {\n     diff: {\n       $subtract: [\"$price\", { $ifNull: [\"$previousPrice\", \"$price\"] }],\n     },\n   },\n}\n```\n\nWe’ve set the `diff` value and now we will set two more fields, `gain` and `loss,` to use in the further stages. We just apply the gain/loss logic here:\n\n```js\n{\n   $addFields: {\n     gain: { $cond: { if: { $gte: [\"$diff\", 0] }, then: \"$diff\", else: 0 } },\n     loss: {\n       $cond: { if: { $lte: [\"$diff\", 0] }, then: { $abs: \"$diff\" }, else: 0 },\n     },\n   },\n}\n```\n\nAfter we have enriched the symbol data with gain and loss information for every document, then we can apply further partitioning to get the moving average of gain and loss fields by considering the previous 14 data points:\n\n```js\n{\n   $setWindowFields: {\n     partitionBy: \"$_id.symbol\",\n     sortBy: { \"_id.time\": 1 },\n     output: {\n       avgGain: {\n         $avg: \"$gain\",\n         window: { documents: [-14, 0] },\n       },\n       avgLoss: {\n         $avg: \"$loss\",\n         window: { documents: [-14, 0] },\n       },\n       documentNumber: { $documentNumber: {} },\n     },\n   },\n}\n```\n\nHere we also used another newly introduced operator, [`$documentNumber`](https://docs.mongodb.com/manual/reference/operator/aggregation/documentNumber/). While we do calculations over the window, we give a sequential number for each document, because we will filter out the documents that have the document number less than or equal to 14. (RSI is calculated after at least 14 data points have been arrived.) We will do filtering out in the later stages. Here, we only set the number of the document.\n\nAfter we calculate the average gain and average loss for every symbol, then we will find the relative strength value. That is calculated by dividing average gain value by average loss value. Since we apply the divide operation, then we need to anticipate the “divide by 0” problem as well:\n\n```js\n{\n   $addFields: {\n     relativeStrength: {\n       $cond: {\n         if: {\n           $gt: [\"$avgLoss\", 0],\n         },\n         then: {\n           $divide: [\"$avgGain\", \"$avgLoss\"],\n         },\n         else: \"$avgGain\",\n       },\n     },\n   },\n}\n```\n\nRelative strength value has been calculated and now it’s time to smooth the Relative Strength value to normalize the data between 0 and 100:\n\n```js\n{\n   $addFields: {\n     rsi: {\n       $cond: {\n         if: { $gt: [\"$documentNumber\", 14] },\n         then: {\n           $subtract: [\n             100,\n             { $divide: [100, { $add: [1, \"$relativeStrength\"] }] },\n           ],\n         },\n         else: null,\n       },\n     },\n   },\n}\n```\n\nWe basically set `null` to the first 14 documents. And for the others, RSI value has been set.\n\nBelow, you can see a one-minute interval candlestick chart and RSI chart. After 14 data points, RSI starts to be calculated. For every interval, we calculated the RSI through aggregation queries by processing the previous data of that symbol:\n\n![Candlestick charts](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Candle_Stick2_dda548a743.png)\n\nThis is the complete aggregation pipeline:\n\n```js\ndb.ticker.aggregate([\n {\n   $match: {\n     symbol: \"BTC-USD\",\n   },\n },\n {\n   $group: {\n     _id: {\n       symbol: \"$symbol\",\n       time: {\n         $dateTrunc: {\n           date: \"$time\",\n           unit: \"minute\",\n           binSize: 5,\n         },\n       },\n     },\n     high: { $max: \"$price\" },\n     low: { $min: \"$price\" },\n     open: { $first: \"$price\" },\n     close: { $last: \"$price\" },\n   },\n },\n {\n   $sort: {\n     \"_id.time\": 1,\n   },\n },\n {\n   $project: {\n     _id: 1,\n     price: \"$close\",\n   },\n },\n {\n   $setWindowFields: {\n     partitionBy: \"$_id.symbol\",\n     sortBy: { \"_id.time\": 1 },\n     output: {\n       previousPrice: { $shift: { by: -1, output: \"$price\" } },\n     },\n   },\n },\n {\n   $addFields: {\n     diff: {\n       $subtract: [\"$price\", { $ifNull: [\"$previousPrice\", \"$price\"] }],\n     },\n   },\n },\n {\n   $addFields: {\n     gain: { $cond: { if: { $gte: [\"$diff\", 0] }, then: \"$diff\", else: 0 } },\n     loss: {\n       $cond: { if: { $lte: [\"$diff\", 0] }, then: { $abs: \"$diff\" }, else: 0 },\n     },\n   },\n },\n {\n   $setWindowFields: {\n     partitionBy: \"$_id.symbol\",\n     sortBy: { \"_id.time\": 1 },\n     output: {\n       avgGain: {\n         $avg: \"$gain\",\n         window: { documents: [-14, 0] },\n       },\n       avgLoss: {\n         $avg: \"$loss\",\n         window: { documents: [-14, 0] },\n       },\n       documentNumber: { $documentNumber: {} },\n     },\n   },\n },\n {\n   $addFields: {\n     relativeStrength: {\n       $cond: {\n         if: {\n           $gt: [\"$avgLoss\", 0],\n         },\n         then: {\n           $divide: [\"$avgGain\", \"$avgLoss\"],\n         },\n         else: \"$avgGain\",\n       },\n     },\n   },\n },\n {\n   $addFields: {\n     rsi: {\n       $cond: {\n         if: { $gt: [\"$documentNumber\", 14] },\n         then: {\n           $subtract: [\n             100,\n             { $divide: [100, { $add: [1, \"$relativeStrength\"] }] },\n           ],\n         },\n         else: null,\n       },\n     },\n   },\n },\n]);\n```\n\n\n## Conclusion\n\nMongoDB Aggregation Framework provides a great toolset to transform any shape of data into a desired format. As you see in the examples, we use a wide variety of aggregation pipeline [stages](https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/) and [operators](https://docs.mongodb.com/manual/reference/operator/aggregation/). As we discussed in the previous blog posts, time-series collections and window functions are great tools to process time-based data over a window.\n\nIn this post we've looked at the $shift and $documentNumber operators that have been introduced with MongoDB 5.0. The `$shift` operator includes another document in the same window into the current document to process positional data together with current data. In an RSI technical indicator calculation, it is commonly used to compare the current data point with the previous data points, and `$shift` makes it easier to refer to positional documents in a window. For example, price difference between current data point and previous data point.\n\nAnother newly introduced operator is `$documentNumber`. `$documentNumber` gives a sequential number for the sorted documents to be processed later in subsequent aggregation stages. In an RSI calculation, we need to skip calculating RSI value for the first 14 periods of data and $documentNumber helps us to identify and filter out these documents at later stages in the aggregation pipeline. ","description":"Time series collections part 3: calculating MACD & RSI values","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltdd22290c52198ad2/644c4698f3337d2a4f02cef1/TimeSeries_ATF3.png?branch=prod","description":null}}]},"slug":"/time-series-macd-rsi","title":"*Currency Analysis with Time Series Collections #3 — MACD and RSI Calculation","original_publish_date":"2021-08-27T07:09:04.915Z","strapi_updated_at":"2022-05-16T18:37:54.207Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Time series","calculated_slug":"/products/mongodb/time-series"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf56bbc20c158b9eb/644c4690716c5a78d72933e8/og-aggregation-framework.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:22.935Z","publish_details":{"time":"2023-04-28T22:52:54.554Z"}}},{"calculated_slug":"/products/realm/realm-schema-migration","content":"## Introduction\n\n[Murphy's law](https://en.wikipedia.org/wiki/Murphy%27s_law) dictates that as soon as your mobile app goes live, you'll receive a request to add a new feature. Then another. Then another.\n\nThis is fine if these features don't require any changes to your data schema. But, that isn't always the case.\n\nFortunately, Realm has built-in functionality to make schema migration easier.\n\nThis tutorial will step you through updating an existing mobile app to add some new features that require changes to the schema. In particular, we'll look at the Realm migration code that ensures that no existing data is lost when the new app versions are rolled out to your production users.\n\nWe'll use the [Scrumdinger app that I modified in a previous post to show how Apple's sample Swift app could be ported to Realm](https://www.mongodb.com/developer/how-to/realm-swiftui-scrumdinger-migration/). The starting point for the app can be found in [this branch of our Scrumdinger repo](https://github.com/realm/Scrumdinger/tree/realm) and the final version is in [this branch](https://github.com/realm/Scrumdinger/tree/new-schema).\n\nNote that the app we're using for this post doesn't use [Atlas Device Sync](https://www.mongodb.com/docs/atlas/app-services/sync/learn/overview/). If it did, then the schema migration process would be very different—that's covered in [Migrating Your iOS App's **Synced** Realm Schema in Production](https://www.mongodb.com/developer/how-to/realm-sync-migration/).\n\n## Prerequisites\n\nThis tutorial has a dependency on [Realm-Cocoa 10.13.0+](https://github.com/realm/realm-cocoa/releases).\n\n## Baseline App/Realm Schema\n\nAs a reminder, the starting point for this tutorial is the [\"realm\" branch of the Scrumdinger repo](https://github.com/realm/scrumdinger/tree/realm).\n\n![Animated gif of the original Scrumdinger app in action](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/scrumdinger_original_b6a15f10ea.gif)\n\nThere are two Realm model classes that we'll extend to add new features to Scrumdinger. The first, [DailyScrum](https://github.com/realm/Scrumdinger/blob/realm/Scrumdinger/Models/DailyScrum.swift), represents one scrum:\n\n``` swift\nclass DailyScrum: Object, ObjectKeyIdentifiable {\n   @Persisted var title = \"\"\n   @Persisted var attendeeList = RealmSwift.List<String>()\n   @Persisted var lengthInMinutes = 0\n   @Persisted var colorComponents: Components?\n   @Persisted var historyList = RealmSwift.List<History>()\n\n\n   var color: Color { Color(colorComponents ?? Components()) }\n   var attendees: [String] { Array(attendeeList) }\n   var history: [History] { Array(historyList) }\n   ...\n}\n```\n\nThe second, [History](https://github.com/realm/Scrumdinger/blob/realm/Scrumdinger/Models/History.swift), represents the minutes of a meeting from one of the user's scrums:\n\n``` swift\nclass History: EmbeddedObject, ObjectKeyIdentifiable {\n   @Persisted var date: Date?\n   @Persisted var attendeeList = List<String>()\n   @Persisted var lengthInMinutes: Int = 0\n   @Persisted var transcript: String?\n   var attendees: [String] { Array(attendeeList) }\n   ...\n}\n```\n\nWe can use [Realm Studio](https://docs.mongodb.com/realm/studio/) to examine the contents of our Realm database after the `DailyScrum` and `History` objects have been created:\n\n![DailyScrum data shown in RealmStudio](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Daily_Scrum_original_117e226072.png)\n\n![History data shown in RealmStudio](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/History_original_fcc4e0b78d.png)\n\n[Accessing Realm Data on iOS Using Realm Studio](https://www.mongodb.com/developer/how-to/realm-ios-database-access-using-realm-studio/) explains how to locate and open the Realm files from your iOS simulator.\n\n## Schema Change #1—Mark Scrums as Public/Private\n\nThe first new feature we've been asked to add is a flag to indicate whether each scrum is public or private:\n\n![Screen capture highlighting the new status - set to \"Private\"](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Scrumdinger_show_privacy_00eb2f407b.png)\n\n![Screen capture showing the user setting the meeting status to \"Public\"](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Scrumdinger_edit_privacy_8df1447d57.png)\n\nThis feature requires the addition of a new `Bool` named `isPublic` to [DailyScrum](https://github.com/realm/Scrumdinger/blob/new-schema/Scrumdinger/Models/DailyScrum.swift):\n\n``` swift\nclass DailyScrum: Object, ObjectKeyIdentifiable {\n   @Persisted var title = \"\"\n   @Persisted var attendeeList = RealmSwift.List<String>()\n   @Persisted var lengthInMinutes = 0\n   @Persisted var isPublic = false\n   @Persisted var colorComponents: Components?\n   @Persisted var historyList = RealmSwift.List<History>()\n\n\n   var color: Color { Color(colorComponents ?? Components()) }\n   var attendees: [String] { Array(attendeeList) }\n   var history: [History] { Array(historyList) }\n   ...\n}\n```\n\nRemember that our original version of Scrumdinger is already in production, and the embedded Realm database is storing instances of `DailyScrum`. We don't want to lose that data, and so we must migrate those objects to the new schema when the app is upgraded.\n\nFortunately, Realm has built-in functionality to automatically handle the addition and deletion of fields. When adding a field, Realm will use a default value (e.g., `0` for an `Int`, and `false` for a `Bool`).\n\nIf we simply upgrade the installed app with the one using the new schema, then we'll get a fatal error. That's because we need to tell Realm that we've updated the schema. We do that by setting the schema version to 1 (the version defaulted to 0 for the original schema):\n\n``` swift\n@main\nstruct ScrumdingerApp: SwiftUI.App {\n   var body: some Scene {\n       WindowGroup {\n           NavigationView {\n               ScrumsView()\n                   .environment(\\.realmConfiguration,\n                       Realm.Configuration(schemaVersion: 1))\n           }\n       }\n   }\n}\n```\n\nAfter upgrading the app, we can use [Realm Studio](https://docs.mongodb.com/realm/studio/) to confirm that our `DailyScrum` object has been updated to initialize `isPublic` to `false`:\n\n![RealmStudio showing that the new, isPublic field has been initialised to false](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Daily_Scrum_updated_19c30fabd6.png)\n\n## Schema Change #2—Store The Number of Attendees at Each Meeting\n\nThe second feature request is to show the number of attendees in the history from each meeting:\n\n![Screen capture showing that the number of attendees is now displayed in the meeting minutes](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Scrumdinger_History_Change_5218655494.png)\n\nWe could calculate the count every time that it's needed, but we've decided to calculate it just once and then store it in our [History](https://github.com/realm/Scrumdinger/blob/new-schema/Scrumdinger/Models/History.swift) object in a new field named `numberOfAttendees`:\n\n``` swift\nclass History: EmbeddedObject, ObjectKeyIdentifiable {\n   @Persisted var date: Date?\n   @Persisted var attendeeList = List<String>()\n   @Persisted var numberOfAttendees = 0\n   @Persisted var lengthInMinutes: Int = 0\n   @Persisted var transcript: String?\n   var attendees: [String] { Array(attendeeList) }\n   ...\n}\n```\n\nWe increment the schema version to 2. Note that the schema version applies to all Realm objects, and so we have to set the version to 2 even though this is the first time that we've changed the schema for `History`.\n\nIf we leave it to Realm to initialize `numberOfAttendees`, then it will set it to 0—which is not what we want. Instead, we provide a `migrationBlock` which initializes new fields based on the old schema version:\n\n``` swift\n@main\nstruct ScrumdingerApp: SwiftUI.App {\n   var body: some Scene {\n       WindowGroup {\n           NavigationView {\n               ScrumsView()\n                   .environment(\\.realmConfiguration, Realm.Configuration(\n                       schemaVersion: 2,\n                       migrationBlock: { migration, oldSchemaVersion in\n                            if oldSchemaVersion < 1 {\n                                // Could init the `DailyScrum.isPublic` field here, but the default behavior of setting\n                                // it to `false` is what we want.\n                            }\n                            if oldSchemaVersion < 2 {\n                                migration.enumerateObjects(ofType: History.className()) { oldObject, newObject in\n                                    let attendees = oldObject![\"attendeeList\"] as? RealmSwift.List<DynamicObject>\n                                    newObject![\"numberOfAttendees\"] = attendees?.count ?? 0\n                                }\n                            }\n                            if oldSchemaVersion < 3 {\n                                // TODO: This is where you'd add you're migration code to go from version\n                                // to version 3 when you next modify the schema\n                            }\n                        }\n                   ))\n           }\n       }\n   }\n}\n```\n\nNote that all other fields are migrated automatically.\n\nIt's up to you how you use data from the previous schema to populate fields in the new schema. E.g., if you wanted to combine `firstName` and `lastName` from the previous schema to populate a `fullName` field in the new schema, then you could do so like this:\n\n``` swift\nmigration.enumerateObjects(ofType: Person.className()) { oldObject, newObject in\n   let firstName = oldObject![\"firstName\"] as! String\n   let lastName = oldObject![\"lastName\"] as! String\n   newObject![\"fullName\"] = \"\\(firstName) \\(lastName)\"\n}\n```\n\nWe can't know what \"old version\" of the schema will be already installed on a user's device when it's upgraded to the latest version (some users may skip some versions,) and so the `migrationBlock` must handle all previous versions. Best practice is to process the incremental schema changes sequentially:\n\n* `oldSchemaVersion < 1` : Process the delta between v0 and v1\n* `oldSchemaVersion < 2` : Process the delta between v1 and v2\n* `oldSchemaVersion < 3` : Process the delta between v2 and v3\n* ...\n\nRealm Studio shows that our code has correctly initialized `numberOfAttendees`:\n\n![Realm Studio showing that the numberOfAttendees field has been set to 2 – matching the number of attendees in the meeting history](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/History_new_11564ec4b0.png)\n\n## Conclusion\n\nIt's almost inevitable that any successful mobile app will need some schema changes after it's gone into production. Realm makes adapting to those changes simple, ensuring that users don't lose any of their existing data when upgrading to new versions of the app.\n\nFor changes such as adding or removing fields, all you need to do as a developer is to increment the version with each new deployed schema. For more complex changes, you provide code that computes the values for fields in the new schema using data from the old schema.\n\nThis tutorial stepped you through adding two new features that both required schema changes. You can view the final app in the [new-schema branch of the Scrumdinger repo](https://github.com/realm/Scrumdinger/tree/new-schema).\n\n## Next Steps\n\nThis post focussed on schema migration for an iOS app. You can find some [more complex examples in the repo](https://github.com/realm/realm-cocoa/tree/master/examples/ios/swift/Migration).\n\nIf you're working with an app for a different platform, then you can find instructions in the docs:\n\n* [Node.js](https://docs.mongodb.com/realm/sdk/node/examples/modify-an-object-schema/)\n* [Android](https://docs.mongodb.com/realm/sdk/android/examples/modify-an-object-schema/)\n* [iOS](https://docs.mongodb.com/realm/sdk/android/examples/modify-an-object-schema/)\n* [.NET](https://docs.mongodb.com/realm/sdk/dotnet/examples/modify-an-object-schema/)\n* [React Native](https://docs.mongodb.com/realm/sdk/react-native/examples/modify-an-object-schema/)\n\nIf you've any questions about schema migration, or anything else related to Realm, then please post them to our [community forum](https://www.mongodb.com/community/forums/c/realm-sdks/58).","description":"Learn how to safely update your iOS app's Realm schema to support new functionality—without losing any existing data","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt7dea2808ae27e55e/644c469a50d2cb54eb93fda4/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/realm-schema-migration","title":"*Migrating Your iOS App's Realm Schema in Production","original_publish_date":"2021-08-27T07:21:37.409Z","strapi_updated_at":"2022-09-01T15:26:22.409Z","expiry_date":"2022-08-19T13:10:02.816Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*iOS","calculated_slug":"/technologies/ios"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt4f4deba53401e2e8/644c469bec6405b95cea3915/OG_1200x630.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:22.551Z","publish_details":{"time":"2023-04-28T22:52:54.583Z"}}},{"calculated_slug":"/products/realm/realm-asyncopen-autoopen","content":"## Introduction\n\nWe’re very happy to announce that v10.12.0 of the [Realm Cocoa SDK](https://github.com/realm/realm-cocoa) includes our two new property wrappers [`@AutoOpen`](https://docs.mongodb.com/realm-sdks/swift/latest/Structs/AutoOpen.html) and [`@AsyncOpen`](https://docs.mongodb.com/realm-sdks/swift/latest/Structs/AsyncOpen.html) for asynchronous opening of a realm for [Realm Sync](https://docs.mongodb.com/realm/sync/) users. This new feature, which is a response to your community feedback, aligns with our goal to make our developer experience better and more effortless, integrating it with SwiftUI, and removing boilerplate code.\n\nUp until now, the standard approach for opening a realm for any sync user is to call `Realm.asyncOpen()` using a user’s sync configuration, then publish the opened Realm to the view:\n\n``` swift\nenum AsyncOpenState {\n    case waiting\n    case inProgress(Progress)\n    case open(Realm)\n    case error(Error)\n}\n\nstruct AsyncView: View {\n    @State var asyncOpenState: AsyncOpenState = .waiting\n\n    var body: some View {\n        switch asyncOpenState {\n        case .waiting:\n            ProgressView()\n                .onAppear(perform: initAsyncOpen)\n        case .inProgress(let progress):\n            ProgressView(progress)\n        case .open(let realm):\n            ContactsListView()\n                .environment(\\.realm, realm)\n        case .error(let error):\n            ErrorView(error: error)\n        }\n    }\n\n    func initAsyncOpen() {\n        let app = App(id: \"appId\")\n        guard let currentUser = app.currentUser else { return }\n        let realmConfig = currentUser.configuration(partitionValue: \"myPartition\")\n        Realm.asyncOpen(configuration: realmConfig,\n                        callbackQueue: DispatchQueue.main) { result in\n            switch result {\n            case .success(let realm):\n                asyncOpenState = .open(realm)\n            case .failure(let error):\n                asyncOpenState = .error(error)\n            }\n        }.addProgressNotification { syncProgress in\n            let progress = Progress(totalUnitCount: Int64(syncProgress.transferredBytes))\n            progress.completedUnitCount = Int64(syncProgress.transferredBytes)\n            asyncOpenState = .inProgress(progress)\n        }\n    }\n}\n```\n\nWith `@AsyncOpen` and `@AutoOpen`, we are reducing development time and boilerplate, making it easier, faster, and cleaner to implement Realm.asyncOpen(). `@AsyncOpen` and `@AutoOpen` give the user the possibility to cover two common use cases in synced apps.\n\n> **Build better mobile apps with Atlas Device Sync**: Atlas Device Sync is a fully-managed mobile backend-as-a-service. Leverage out-of-the-box infrastructure, data synchronization capabilities, built-in network handling, and much more to quickly launch enterprise-grade mobile apps. [Get started now by build: Deploy Sample for Free](https://www.mongodb.com/realm/register?tck=devhub_opensyncedrealmsswiftui)!\n\n## Prerequisites\n\n- [Realm Cocoa 10.12.0+](https://github.com/realm/realm-cocoa/releases/tag/v10.12.0)\n\n## @AsyncOpen\n\nWith the `@AsyncOpen` property wrapper, we have the same behavior as using `Realm.asyncOpen()`, but with a much more natural API for SwiftUI developers. Using this property wrapper prevents your app from trying to fetch the Realm file if there is no network connection, and it will only return a realm when it's synced with MongoDB Realm data. If there is no internet connection, then @AsyncOpen< will throw an error.\n\nLet’s take, for example, a game app, which the user can play both on an iPhone and iPad. Having the data not updated would result in losing track of the current status of the player. In this case, it’s very important to have our data updated with any latest changes. This is the perfect use case for `@AsyncOpen`. \n\nThis property wrapper's API gives you the flexibility to optionally specify a MongoDB Realm AppId. If no AppId is provided, and you’ve only used one ID within your App, then that will be used. You can also provide a timeout for your asynchronous operation:\n\n```swift\n@AsyncOpen(appId: \"appId\",\n            partitionValue: \"myPartition\",\n            configuration: Realm.Configuration(objectTypes: [SwiftPerson.self])\n            timeout: 20000)\nvar asyncOpen\n```\n\nAdding it to your SwiftUI App is as simple as declaring it in your view and have your view react to the state of the sync operation:\n\n- Display a progress view while downloading or waiting for a user to be logged in.\n- Display an error view if there is a failure during sync.\n- Navigate to a new view after our realm is opened\n\nOnce the synced realm has been successfully opened, you can pass it to another view (embedded or via a navigation link):\n\n```swift\nstruct AsyncOpenView: View {\n    @AsyncOpen(appId: \"appId\",\n               partitionValue: \"myPartition\",\n               configuration: Realm.Configuration(objectTypes: [SwiftPerson.self])\n               timeout: 20000)\n    var asyncOpen\n\n    var body: some View {\n        VStack {\n            switch asyncOpen {\n            case .connecting:\n                ProgressView()\n            case .waitingForUser:\n                ProgressView(\"Waiting for user to logged in...\")\n            case .open(let realm):\n                ListView()\n                    .environment(\\.realm, realm)\n            case .error(let error):\n                ErrorView(error: error)\n            case .progress(let progress):\n                ProgressView(progress)\n            }\n        }\n    }\n}\n```\n\nIf you have been using Realm.asyncOpen() in your current SwiftUI App and want to maintain the same behavior, you may want to migrate to  `@AsyncOpen`. It will simplify your code and make it more intuitive.\n\n## @AutoOpen\n\n`@AutoOpen` should be used when you want to work with the synced realm file even when there is no internet connection.\n\nLet’s take, for example, Apple’s Notes app, which tries to sync your data if there is internet access and shows you all the notes synced from other devices. If there is no internet connection, then Notes shows you your local (possibly stale) data. This use case is perfect for the `@AutoOpen` property wrapper. When the user recovers a network connection, Realm will sync changes in the background, without the need to add any extra code.\n\nThe syntax for using `@AutoOpen` is the same as for `@AsyncOpen`:\n\n```swift\nstruct AutoOpenView: View {\n    @AutoOpen(appId: \"appId\",\n              partitionValue: \"myPartition\",\n              configuration: Realm.Configuration(objectTypes: [SwiftPerson.self])\n              timeout: 10000)\n    var autoOpen\n\n    var body: some View {\n        VStack {\n            switch autoOpen {\n            case .connecting:\n                ProgressView()\n            case .waitingForUser:\n                ProgressView(\"Waiting for user to logged in...\")\n            case .open(let realm):\n                ContactView()\n                    .environment(\\.realm, realm)\n            case .error(let error):\n                ErrorView(error: error)\n            case .progress(let progress):\n                ProgressView(progress)\n            }\n        }\n    }\n}\n```\n\n## One Last Thing… \n\nWe added a new key to our set of Environment Values: a “partition value” environment key which is used by our new property wrappers `@AsyncOpen` and `@AutoOpen` to dynamically inject a partition value when it's derived and not static. For example, in the case of using the user id as a partition value, you can pass this environment value to the view where `@AsyncOpen` or `@AutoOpen` are used:\n\n```swift\nAsyncView()\n    .environment(\\.partitionValue, user.id!)\n```\n\n## Conclusion\n\nWith these property wrappers, we continue to better integrate Realm into your SwiftUI apps. With the release of this feature, and more to come, we want to make it easier for you to incorporate our SDK and sync functionality into your apps, no matter whether you’re using UIKit or SwiftUI.\n\nWe are excited for our users to test these new features. Please share any feedback or ideas for new features in our [community forum](https://www.mongodb.com/community/forums/c/realm/9).\n\nDocumentation on both of these property wrappers can be found in our [docs](https://docs.mongodb.com/realm/sdk/ios/examples/sync-changes-between-devices/#open-a-synced-realm-with-swiftui). \n","description":"Learn how to use the new Realm @AutoOpen and @AsyncOpen property wrappers to open synced realms from your SwiftUI apps.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt7beb848d7d1786f6/644c469e33acf20d1eb3415f/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/realm-asyncopen-autoopen","title":"*Open Synced Realms in SwiftUI using @Auto/AsyncOpen","original_publish_date":"2021-08-25T15:23:12.532Z","strapi_updated_at":"2022-10-19T12:27:11.151Z","expiry_date":"2022-08-19T15:54:11.563Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*News & Announcements","calculated_slug":"/news"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf991c03ca885f8cd/644c46a02060d482712976e2/OG_1200x630.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:22.138Z","publish_details":{"time":"2023-04-28T22:52:54.614Z"}}},{"calculated_slug":"/code-examples/javascript/locator-app-code-example","content":"<h2>Introduction</h2>\n\nThis Summer, MongoDB hosted 112 interns, spread across departments such as MongoDB Cloud, Atlas, and Realm. These interns have worked on a vast array of projects using the MongoDB platform and technologies. One such project was created by two Software Engineering interns, José Pedro Martins and Linnea Jansson, on the MongoDB Realm team. \n\nUsing MongoDB Realm and React Native, they built an app to log and display the location and movement of a user’s devices in real-time on a map. Users can watch as their device’s position on the map updates in response to how its physical location changes in real life. Additionally, users can join groups and view the live location of devices owned by other group members. \n\nIn this article, I look forward to demonstrating the app’s features, discussing how it uses MongoDB Realm, and reviewing some noteworthy scenarios which arose during its development.\n\n\n<h2>App overview</h2>\n\nThe project, called *Find Our Devices*, is an app for iOS and Android which allows users to view the live location of their devices on a map. The demo video above demonstrates some key features and shows off the intuitive UI. Users can track multiple devices by installing the app, logging in with their email, and adding the current device to their account. \n\nFor each device, a new pin is added to the map to indicate the device’s location. This feature is perfect if one of your devices has been lost or stolen, as you can easily track the location of your iOS and Android devices from one app. Instead of using multiple apps to track devices on android and iOS, the user can focus on retrieving their device. Indeed, if you’re only interested in the location of one device, you can instantly find its location by selecting it from a dropdown menu. \n\nAdditionally, users can create groups with other users. In these groups, users can see both the location of their devices and the location of other group members' devices. Group members can also invite other users by inputting their email. If a user accepts an invitation, their devices' locations begin to sync to the map. They can also view the live location of other members’ devices on the group map. \n\nThis feature is fantastic for families or groups of friends travelling abroad. If somebody gets lost, their location is still visible to everyone in the group, provided they have network connectivity. Alternatively, logistics companies could use the app to track their fleets. If each driver installs the app, HQ could quickly find the location of any vehicle in the fleet and predict delays or suggest alternative routes to drivers. If users want privacy, they can disable location sharing at any time, or leave the group.\n\n<h2>Uses of Realm</h2>\n\nThis app was built using the [MongoDB RealmJS SDK](https://docs.mongodb.com/realm/sdk/react-native/) and React-Native and utilises many of Realm’s features. For example, the authentication process of registration, logging in, and logging out is handled using [Realm Email/Password authentication](https://docs.mongodb.com/realm/authentication/). Additionally, Realm enables a seamless data flow while updating device locations in groups, as demonstrated by the diagram below: \n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Find_Our_Devices_data_flow_updating_location_43a9d24b1f.png)\n\nAs a device moves, Realm writes the location to Atlas, provided the device has network connectivity. If the device doesn’t have network connectivity, Realm will sync the data into Atlas when the device is back online. Once the data is in Atlas, Realm will propagate the changes to the other users in the group. Upon receiving the new data, a change listener in the app is notified of this update in the device's location. As a result, the pin’s position on the map will update and users in the group can see the device’s new location.\n\nAnother feature of Realm used in this project is shared realms. In the Realm task tracker tutorial, available [here](https://docs.mongodb.com/realm/tutorial/), all users in a group have read/write permission to the group partition. The developers allowed this, as group members were trusted to change any data in the group’s shared resources. Indeed, this was encouraged, as it allowed team members to edit tasks created by other team members and mark them as completed. In this app, users couldn't have write permissions to the shared realm, as group members could modify other users' locations with write permission. The solution to this problem is shown in the diagram below. Group members only have read permissions for the shared realm, allowing them to read others' locations, but not edit them. You can learn more about Realm partitioning strategies [here](https://www.mongodb.com/developer/how-to/realm-partitioning-strategies/).\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Find_Our_Devices_comparison_of_permissions_for_shared_realms_072b1b6355.png)\n\n<h2>Fixing a security vulnerability</h2>\n\nSeveral difficult scenarios and edge cases came up during the development process. For example, in the initial version, users could write to the [*Group Membership*](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Demo_Video_2a492b355b.mov)(https://github.com/realm/FindOurDevices/blob/0b118053a3956d4415d40d9c059f6802960fc484/app/models/GroupMembership.js) class. The intention was that this permission would allow members to join new groups and write their new membership to Atlas from Realm. Unfortunately, this permission also created a security vulnerability, as the client could edit the *GroupMembership.groupId* value to anything they wanted. If they edited this value to another group’s ID value, this change would be synced to Atlas, as the user had write permission to this class. Malicious users could use this vulnerability to join a group without an invitation and snoop on the group members' locations.\n\nDue to the serious ethical issues posed by this vulnerability, a fix needed to be found. Ultimately, the solution was to split the Device partition from the User partition and retract write permissions from the User class, as shown in the diagram below. Thanks to this amendment, users could no longer edit their *GroupMembership.groupId* value. As such, malicious actors could no longer join groups for which they had no invitation. Additionally, each device is now responsible for updating its location, as the Device partition is now separate from the User partition, with write permissions.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Find_Our_Devices_solving_privacy_issue_e339dae9c0.png)\n\n<h2>Conclusion</h2>\n\nIn this blog post, we discussed a fascinating project built by two Realm interns this year. More specifically, we explored the functionality and use cases of the project, looked at how the project used MongoDB Realm, and examined a noteworthy security vulnerability that arose during development. \n\nIf you want to learn more about the project or dive into the code, you can check out the backend repository [here](https://github.com/realm/FindOurDevices-backend) and the frontend repository [here](https://github.com/realm/FindOurDevices). You can also build the project yourself by following the instructions in the ReadMe files in the two repositories. Alternatively, if you'd like to learn more about MongoDB, you can visit our [community forums](https://www.mongodb.com/community/forums/), sign up for [MongoDB University](https://university.mongodb.com/), or sign up for the [MongoDB newsletter](https://www.mongodb.com/newsletter)!","description":"Build an example mobile application using realm for iOS and Android","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltbcda524e555acc96/644c46a4bc3f0a5cf381511c/realm_devhub_logo.png?branch=prod","description":null}}]},"slug":"/locator-app-code-example","title":"*Find our Devices - A locator app built using Realm","original_publish_date":"2022-05-20T16:31:33.186Z","strapi_updated_at":"2022-06-15T21:13:03.977Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":"Full Application","content_typeConnection":{"edges":[{"node":{"title":"*Code Example","calculated_slug":"/code-examples"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":"https://github.com/realm/FindOurDevices","l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*iOS","calculated_slug":"/technologies/ios"}},{"node":{"title":"*Android","calculated_slug":"/technologies/android"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:21.739Z","publish_details":{"time":"2023-04-28T22:52:54.644Z"}}},{"calculated_slug":"/languages/csharp/use-azure-key-vault-mongodb-client-side-field-level-encryption","content":"When implementing MongoDB’s [client-side field level encryption](https://www.mongodb.com/client-side-encryption) (CSFLE), you’ll find yourself making an important decision: Where do I store my customer master key? In [another tutorial](https://www.mongodb.com/developer/how-to/client-side-field-level-encryption-mongodb-csharp/), I guided readers through the basics of CSFLE by using a locally-generated and stored master key. While this works for educational and local development purposes, it isn’t suitable for production! In this tutorial, we’ll see how to use Azure Key Vault to generate and securely store our master key.\n\n## Prerequisites\n\n* A [MongoDB Atlas cluster](https://www.mongodb.com/cloud/atlas) running MongoDB 4.2 (or later) OR [MongoDB 4.2 Enterprise Server](https://www.mongodb.com/try/download/enterprise) (or later)—required for automatic encryption\n* [MongoDB .NET Driver 2.13.0](https://www.nuget.org/packages/MongoDB.Driver/2.13.0) (or later)\n* [Mongocryptd](https://docs.mongodb.com/manual/reference/security-client-side-encryption-appendix/#installation)\n* An [Azure Account](https://azure.microsoft.com/en-us/free/) with an active subscription and the same permissions as those found in any of these Azure AD roles (only one is needed):\n    * [Application administrator](https://docs.microsoft.com/en-us/azure/active-directory/roles/permissions-reference#application-administrator)\n    * [Application developer](https://docs.microsoft.com/en-us/azure/active-directory/roles/permissions-reference#application-developer)\n    * [Cloud application administrator](https://docs.microsoft.com/en-us/azure/active-directory/roles/permissions-reference#cloud-application-administrator)\n* An [Azure AD tenant](https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-create-new-tenant#create-a-new-azure-ad-tenant) (you can use an existing one, assuming you have appropriate permissions)\n* [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)\n* [Cloned sample application](https://github.com/adriennetacke/mongodb-csfle-csharp-demo-azure)\n\n\n## Quick Jump\n\n**Prepare Azure Infrastructure**\n\n* [Register App in Azure Active Directory](#register-app-in-azure-active-directory)\n* [Create a Client Secret](#create-a-client-secret)\n* [Create an Azure Key Vault](#create-an-azure-key-vault)\n* [Create and Add a Key to your Key Vault](#create-and-add-a-key-to-your-key-vault)\n* [Grant Application Permissions to Key Vault](#grant-application-permissions-to-key-vault)\n\n**Configure your Client Application to use Azure Key Vault and CSFLE**\n\n* [Integrate Azure Key Vault into Your Client Application](#integrate-azure-key-vault-into-your-client-application)\n* [The Results - What You Get After Integrating Azure Key Vault with MongoDB CSFLE](#the-results--what-you-get-after-integrating-azure-key-vault-with-mongodb-csfle)\n\n- - -\n\n## Register App in Azure Active Directory\n\nIn order to establish a trust relationship between our application and the Microsoft identity platform, we first need to register it. \n\n1. Sign in to the [Azure portal](https://portal.azure.com/).\n2. If you have access to multiple tenants, in the top menu, use the “Directory + subscription filter” to select the tenant in which you want to register an application.\n3. In the main search bar, search for and select “Azure Active Directory.”\n4. On the left-hand navigation menu, find the Manage section and select “App registrations,” then “+ New registration.”\n5. Enter a display name for your application. You can change the display name at any time and multiple app registrations can share the same name. The app registration's automatically generated Application (client) ID, not its display name, uniquely identifies your app within the identity platform.\n6. Specify who can use the application, sometimes called its sign-in audience. For this tutorial, I’ve selected “Accounts in this organizational directory only (Default Directory only - Single tenant).” This only allows users that are in my current tenant access to my application.\n7. Click “Register.” Once the initial app registration is complete, copy the **Directory (tenant) ID** and **Application (client) ID** as we’ll need them later on.\n8. Find the linked application under “Managed application in local directory” and click on it. ![](https://lh4.googleusercontent.com/Xc6mphV7B5BnwojDmLEA2QnSG4xrqwfWOkOJvqG7uBbMNV_s6DTsKJFRyqaHryyziA5YBIZnc2aJ34DdCqalqpm8xCKdIrxAx2o2Pjx_whjiXM-rlmoV_l4NKZL3UmEpfP4W3ZVx=s0)\n9. Once brought to the “Properties” page, also copy the “**Object ID**” as we’ll need this too.![](https://lh4.googleusercontent.com/2KgAuRdeJI7K3e6DBJXKGr7K8SNC6bikQmZJkXNpy7LupwvXfuJxTWHKCjtAJnTaUA7vqDYMQiJB1zjiOAbczVKmTujWP820Zey8ZYVZaMvKtk3qdsrOXlxiLL19VpUELcBDn2SP=s0)\n\n## Create a Client Secret\n\nOnce your application is registered, we’ll need to create a [client secret](https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app#add-a-client-secret) for it. This will be required when authenticating to the Key Vault we’ll be creating soon.\n\n1. On the overview of your newly registered application, click on “Add a certificate or secret”:![](https://lh6.googleusercontent.com/GOPubpbAUfglFo6mcmG0TEd-ZXLkabsuY0S_p-3iMc1Fbn_Xv1QEArPZA-VoxZAVN3Mv_RzQK8o8DSNKPU9nRxmfNTwlxRQbRRESEUmtWjVFWDgSvvqhYzANy32hM9s1_TnATlOz=s0)\n2. Under “Client secrets,” click “+ New client secret.”\n3. Enter a short description for this client secret and leave the default “Expires” setting of 6 months.\n4. Click “Ad.\" Once the client secret is created, be sure to copy the secret’s “**Value**” as we’ll need it later. It’s also worth mentioning that once you leave this page, the secret value is never displayed again, so be sure to record it at least once!\n\n## Create an Azure Key Vault\n\nNext up, an [Azure Key Vault](https://azure.microsoft.com/en-us/services/key-vault/#product-overview)! We’ll create one so we can securely store our customer master key. We’ll be completing these steps via the Azure CLI, so open up your favorite terminal and follow along:\n\n1. Sign in to the Azure CLI using the `az login` command. Finish the authentication steps by following the steps displayed in your terminal.\n2. Create a resource group: \n\n    ``` bash\n    az group create --name \"YOUR-RESOURCE-GROUP-NAME\" --location <YOUR-AZURE-REGION>\n    ```\n\n3. Create a key vault: \n\n    ``` bash\n    az keyvault create --name \"YOUR-KEYVAULT-NAME\" --resource-group \"YOUR-RESOURCE-GROUP-NAME\" --location <YOUR-AZURE-REGION>\n    ```\n\n## Create and Add a Key to Your Key Vault\n\nWith a key vault, we can now create our customer master key! This will be stored, managed, and secured by Azure Key Vault.\n\nCreate a key and add to our key vault:\n\n``` bash\naz keyvault key create --vault-name \"YOUR-KEYVAULT-NAME\" --name \"YOUR-KEY-NAME\" --protection software\n```\n\nThe `--protection` parameter designates the [key protection type](https://docs.microsoft.com/en-us/azure/key-vault/keys/about-keys). For now, we'll use the `software` type. Once the command completes, take note of your key’s \"**name**\" as we‘ll need it later!\n\n## Grant Application Permissions to Key Vault\n\nTo enable our client application access to our key vault, some permissions need to be granted:\n\n1. Give your application the [wrapKey](https://docs.microsoft.com/en-us/rest/api/keyvault/wrap-key/wrap-key) and [unwrapKey](https://docs.microsoft.com/en-us/rest/api/keyvault/unwrap-key/unwrap-key) permissions to the keyvault. (For the `--object-id` parameter, paste in the Object ID of the application we registered earlier. This is the Object ID we copied in the last \"Register App in Azure Active Directory\" step.)\n\n    ``` bash\n    az keyvault set-policy --name \"YOUR-KEYVAULT-NAME\" --key-permissions wrapKey unwrapKey --object-id <YOUR-APP-OBJECT-ID>\n    ```\n\n2. Upon success, you’ll receive a JSON object. Find and copy the value for the “**vaultUri**” key. For example, mine is `https://csfle-mdb-demo-vault.vault.azure.net`.\n\n## Integrate Azure Key Vault into Your Client Application\n\nNow that our cloud infrastructure is configured, we can start integrating it into our application. We’ll be referencing the [sample repo](https://github.com/adriennetacke/mongodb-csfle-csharp-demo-azure) from our prerequisites for these steps, but feel free to use the portions you need in an existing application.\n<br>\n1. If you haven’t cloned the repo yet, do so now! \n\n    ``` shell\n    git clone https://github.com/adriennetacke/mongodb-csfle-csharp-demo-azure.git\n    ```\n\n2. Navigate to the root directory `mongodb-csfle-csharp-demo-azure` and open the `EnvoyMedSys` sample application in Visual Studio.\n3. In the Solution Explorer, find and open the `launchSettings.json` file (`Properties` > `launchSettings.json`).\n4. Here, you’ll see some scaffolding for some variables. Let’s quickly go over what those are:\n    * `MDB_ATLAS_URI`: The connection string to your MongoDB Atlas cluster. This enables us to store our data encryption key, encrypted by Azure Key Vault.\n    * `AZURE_TENANT_ID`: Identifies the organization of the Azure account.\n    * `AZURE_CLIENT_ID`: Identifies the `clientId` to authenticate your registered application.\n    * `AZURE_CLIENT_SECRET`: Used to authenticate your registered application.\n    * `AZURE_KEY_NAME`: Name of the Customer Master Key stored in Azure Key Vault.\n    * `AZURE_KEYVAULT_ENDPOINT`: URL of the Key Vault. E.g., `yourVaultName.vault.azure.net`.\n5. Replace all of the placeholders in the `launchSettings.json` file with **your own information**. Each variable corresponds to a value you were asked to copy and keep track of:\n    * `MDB_ATLAS_URI`: Your [**Atlas URI**](https://docs.atlas.mongodb.com/driver-connection/#connect-your-application)**.**\n    * `AZURE_TENANT_ID`: **Directory (tenant) ID**.\n    * `AZURE_CLIENT_ID`: **Application (client) ID.**\n    * `AZURE_CLIENT_SECRET`: Secret **Value** from our client secret.\n    * `AZURE_KEY_NAME`: Key **Name**.\n    * `AZURE_KEYVAULT_ENDPOINT`: Our Key Vault’s **vaultUri**.\n6. Save all your files!\n\nBefore we run the application, let’s go over what’s happening: When we run our main program, we set the connection to our Atlas cluster and our key vault’s collection namespace. We then instantiate two helper classes: a `KmsKeyHelper` and an `AutoEncryptHelper`. The `KmsKeyHelper`’s `CreateKeyWithAzureKmsProvider()` method is called to generate our encrypted data encryption key. This is then passed to the `AutoEncryptHelper`’s `EncryptedWriteAndReadAsync()` method to insert a sample document with encrypted fields and properly decrypt it when we need to fetch it. This is all in our `Program.cs` file:\n\n`Program.cs`\n\n``` cs\nusing System;\nusing MongoDB.Driver;\n\nnamespace EnvoyMedSys\n{\n    public enum KmsKeyLocation\n    {\n        Azure,\n    }\n\n    class Program\n    {\n        public static void Main(string[] args)\n        {\n            var connectionString = Environment.GetEnvironmentVariable(\"MDB_ATLAS_URI\");\n            var keyVaultNamespace = CollectionNamespace.FromFullName(\"encryption.__keyVaultTemp\");\n\n            var kmsKeyHelper = new KmsKeyHelper(\n                connectionString: connectionString,\n                keyVaultNamespace: keyVaultNamespace);\n            var autoEncryptHelper = new AutoEncryptHelper(\n                connectionString: connectionString,\n                keyVaultNamespace: keyVaultNamespace);\n\n            var kmsKeyIdBase64 = kmsKeyHelper.CreateKeyWithAzureKmsProvider().GetAwaiter().GetResult();\n\n            autoEncryptHelper.EncryptedWriteAndReadAsync(kmsKeyIdBase64, KmsKeyLocation.Azure).GetAwaiter().GetResult();\n\n            Console.ReadKey();\n        }\n    }\n}\n```\n\nTaking a look at the `KmsKeyHelper` class, there are a few important methods: the `CreateKeyWithAzureKmsProvider()` and `GetClientEncryption()` methods. I’ve opted to include comments in the code to make it easier to follow along:\n<br>\n`KmsKeyHelper.cs` / `CreateKeyWithAzureKmsProvider()`\n\n``` cs\npublic async Task<string> CreateKeyWithAzureKmsProvider()\n{\n    var kmsProviders = new Dictionary<string, IReadOnlyDictionary<string, object>>();\n\n    // Pull Azure Key Vault settings from environment variables\n    var azureTenantId = Environment.GetEnvironmentVariable(\"AZURE_TENANT_ID\");\n    var azureClientId = Environment.GetEnvironmentVariable(\"AZURE_CLIENT_ID\");\n    var azureClientSecret = Environment.GetEnvironmentVariable(\"AZURE_CLIENT_SECRET\");\n    var azureIdentityPlatformEndpoint = Environment.GetEnvironmentVariable(\"AZURE_IDENTIFY_PLATFORM_ENPDOINT\"); // Optional, only needed if user is using a non-commercial Azure instance\n\n   // Configure our registered application settings\n   var azureKmsOptions = new Dictionary<string, object>\n    {\n        { \"tenantId\", azureTenantId },\n        { \"clientId\", azureClientId },\n        { \"clientSecret\", azureClientSecret },\n    };\n\n    if (azureIdentityPlatformEndpoint != null)\n    {\n        azureKmsOptions.Add(\"identityPlatformEndpoint\", azureIdentityPlatformEndpoint);\n    }\n\t\n    // Specify remote key location; in this case, Azure\n    kmsProviders.Add(\"azure\", azureKmsOptions);\n\t\n    // Constructs our client encryption settings which\n    // specify which key vault client, key vault namespace,\n    // and KMS providers to use. \n    var clientEncryption = GetClientEncryption(kmsProviders);\n\n    // Set KMS Provider Settings\n    // Client uses these settings to discover the master key\n    var azureKeyName = Environment.GetEnvironmentVariable(\"AZURE_KEY_NAME\");\n    var azureKeyVaultEndpoint = Environment.GetEnvironmentVariable(\"AZURE_KEYVAULT_ENDPOINT\"); // typically <azureKeyName>.vault.azure.net\n    var azureKeyVersion = Environment.GetEnvironmentVariable(\"AZURE_KEY_VERSION\"); // Optional\n    var dataKeyOptions = new DataKeyOptions(\n        masterKey: new BsonDocument\n        {\n            { \"keyName\", azureKeyName },\n            { \"keyVaultEndpoint\", azureKeyVaultEndpoint },\n            { \"keyVersion\", () => azureKeyVersion, azureKeyVersion != null }\n        });\n\t\n    // Create Data Encryption Key\n    var dataKeyId = clientEncryption.CreateDataKey(\"azure\", dataKeyOptions, CancellationToken.None);\n    Console.WriteLine($\"Azure DataKeyId [UUID]: {dataKeyId}\");\n\n    var dataKeyIdBase64 = Convert.ToBase64String(GuidConverter.ToBytes(dataKeyId, GuidRepresentation.Standard));\n    Console.WriteLine($\"Azure DataKeyId [base64]: {dataKeyIdBase64}\");\n\t\n    // Optional validation; checks that key was created successfully\n    await ValidateKeyAsync(dataKeyId);\n\n    return dataKeyIdBase64;\n}\n```\n<br>\n\n`KmsKeyHelper.cs` / `GetClientEncryption()`\n\n``` cs\nprivate ClientEncryption GetClientEncryption(\n\tDictionary<string, IReadOnlyDictionary<string, object>> kmsProviders)\n{\n    // Construct a MongoClient using our Atlas connection string\n    var keyVaultClient = new MongoClient(_mdbConnectionString);\n\n    // Set MongoClient, key vault namespace, and Azure as KMS provider\n    var clientEncryptionOptions = new ClientEncryptionOptions(\n    \tkeyVaultClient: keyVaultClient,\n        keyVaultNamespace: _keyVaultNamespace,\n        kmsProviders: kmsProviders);\n\n    return new ClientEncryption(clientEncryptionOptions);\n}\n```\n\nWith our Azure Key Vault connected and data encryption key encrypted, we’re ready to insert some data into our Atlas cluster! This is where the `AutoEncryptHelper` class comes in. The important method to note here is the `EncryptedReadAndWrite()` method:\n<br>\n\n`AutoEncryptHelper.cs` / `EncryptedReadAndWrite()`\n\n``` cs\npublic async Task EncryptedWriteAndReadAsync(string keyIdBase64, KmsKeyLocation kmsKeyLocation)\n{\n    // Construct a JSON Schema\n    var schema = JsonSchemaCreator.CreateJsonSchema(keyIdBase64);\n\n    // Construct an auto-encrypting client\n    var autoEncryptingClient = CreateAutoEncryptingClient(\n        kmsKeyLocation,\n        _keyVaultNamespace,\n        schema);\n\n    // Set our working database and collection to medicalRecords.patientData\n    var collection = autoEncryptingClient\n        .GetDatabase(_medicalRecordsNamespace.DatabaseNamespace.DatabaseName)\n        .GetCollection<BsonDocument>(_medicalRecordsNamespace.CollectionName);\n\n    var ssnQuery = Builders<BsonDocument>.Filter.Eq(\"ssn\", __sampleSsnValue);\n\n    // Upsert (update if found, otherwise create it) a document into the collection\n    var medicalRecordUpdateResult = await collection\n        .UpdateOneAsync(ssnQuery, new BsonDocument(\"$set\", __sampleDocFields), new UpdateOptions() { IsUpsert = true });\n\n    if (!medicalRecordUpdateResult.UpsertedId.IsBsonNull)\n    {\n        Console.WriteLine(\"Successfully upserted the sample document!\");\n    }\n\n    // Query by SSN field with auto-encrypting client\n    var result = await collection.Find(ssnQuery).SingleAsync();\n\t\n    // Proper result in console should show decrypted, human-readable document\n    Console.WriteLine($\"Encrypted client query by the SSN (deterministically-encrypted) field:\\n {result}\\n\");\n}\n```\n\nNow that we know what’s going on, run your application!\n\n## The Results: What You Get After Integrating Azure Key Vault with MongoDB CSFLE\n\nIf all goes well, your console will print out two `DataKeyIds` (UUID and base64) and a document that resembles the following: \n<br>\n\n`Sample Result Document (using my information)`\n\n``` bash\n{\n    _id:UUID('ab382f3e-bc79-4086-8418-836a877efff3'),\nkeyMaterial:Binary('tvehP03XhUsztKr69lxlaGjiPhsNPjy6xLhNOLTpe4pYMeGjMIwvvZkzrwLRCHdaB3vqi9KKe6/P5xvjwlVHacQ1z9oFIwFbp9nk...', 0),\n    creationDate:2021-08-24T05:01:34.369+00:00,\n    updateDate:2021-08-24T05:01:34.369+00:00,\n    status:0,\n    masterKey:Object,\n    provider:\"azure\",\n    keyVaultEndpoint:\"csfle-mdb-demo-vault.vault.azure.net\",\n    keyName:\"MainKey\"\n}\n```\n\nHere’s what my console output looks like, for reference:\n\n![Screenshot of console output showing two Azure DatakeyIds and a non-formatted document](https://lh5.googleusercontent.com/qxOfV8p1E_aDSNAFcNj80r5tOIjAECArBQd-MBTNPsBYhc2XchPAU6UFTbn_koz4xGlm1CX216A9WXyn6eASJb0Gk3dJRFK84ZKYuiZNGwLuq0JtRP8gpRBRlyivDv4weT4A9Byi=s0)\n\nSeeing this is great news! A lot of things have just happened, and all of them are good:\n\n* Our application properly authenticated to our Azure Key Vault.\n* A properly generated data encryption key was created by our client application.\n* The data encryption key was properly encrypted by our customer master key that’s securely stored in Azure Key Vault.\n* The encrypted data encryption key was returned to our application and stored in our MongoDB Atlas cluster.\n\n<br>\nHere’s the same process in a workflow:\n\n![Workflow diagram of how a data encryption key is set up with a KMS Provider. Client application creates an unecrypted data encryption key which requests encryption from Azure Key Vault (the KMS provider). using the Customer Master Key, Azure Key Vault encrypts the data encryption key, sends it back to the client application which then stores the encrypted key in MongoDB's key vault collection.](https://lh3.googleusercontent.com/S0K7BCrMW-a0d6kdFN7jHndReRaLGt_GAcTnRgYCb7ZwDCrTHw59AfZlrQR4-MzsHReOEU4W0bnjzCbe0NMpDWm9b6ly_9EqfEDnEIH-g5ze86OFqat-gu-t2MACaqv7VEhNhaLE=s0)\n\nAfter a few more moments, and upon success, you’ll see a “Successfully upserted the sample document!” message, followed by the properly decrypted results of a test query. Again, here’s my console output for reference:\n\n![Screenshot of console output showing a \"Successfully upserted the sample document message!\" followed by a human-readable document of a sample patient's data, including properly decrypted SSN, policyNumber fields.](https://lh3.googleusercontent.com/kDVlD3Ws30LczJ2WR_zx1-6mTmk-fBujuMkRw5_KPFeAQWU-l7fdFZ07BK9XKMnb3kDD4TkjFtpLIfohS3D9jdtva_pv-JIWVCHJBpyvaW8xZMEIy5lxOjm2WEAFVUozFjyBCFlf=s0)\n\nThis means our sample document was properly encrypted, inserted into our `patientData` collection, queried with our auto-encrypting client by SSN, and had all relevant fields correctly decrypted before returning them to our console. How neat! \n\nAnd just because I’m a little paranoid, we can double-check that our data has actually been encrypted. If you log into your Atlas cluster and navigate to the `patientData` collection, you’ll see that our documents’ sensitive fields are all illegible:\n\n![Screenshot of encrypted document in Atlas cluster. Shows SSN, bloodType, polucyNumber, medicalRecord fields as asterisks and other fields as legible.](https://lh4.googleusercontent.com/PQuheTIdvzm0jNgNoINsgYgMi94gLsbmQRje7oeEvrSskFJDeoUDWCcs3Qh-VE2ZdJMtpW887hMoO_eEf-REN6huej4Twaw0NxYfY-vLfD1GuEqnsFfOH0eTkcIKxAq33V438G2L=s0)\n\n## Let's Summarize\n\nThat wasn’t so bad, right? Let's see what we've accomplished! This tutorial walked you through:\n\n* Registering an App in Azure Active Directory.\n* Creating a Client Secret.\n* Creating an Azure Key Vault.\n* Creating and Adding a Key to your Key Vault.\n* Granting Application Permissions to Key Vault.\n* Integrating Azure Key Vault into Your Client Application.\n* The Results: What You Get After Integrating Azure Key Vault with MongoDB CSFLE.\n\nBy using a remote key management system like Azure Key Vault, you gain access to many benefits over using a local filesystem. The most important of these is the secure storage of the key, reduced risk of access permission issues, and easier portability!\n\nFor more information, check out this helpful list of resources I used while preparing this tutorial:\n\n* [az keyvault command list](https://docs.microsoft.com/en-us/cli/azure/keyvault?view=azure-cli-latest)\n* [Registering an application with the Microsoft Identity Platform](https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app#register-an-application)\n* [MongoDB CSFLE and Azure Key Vault](https://docs.mongodb.com/manual/core/security-client-side-encryption-key-management/#azure-key-vault)\n\nAnd if you have any questions or need some additional help, be sure to check us out on the [MongoDB Community Forums](https://developer.mongodb.com/community/forums/) and start a topic!\n\nA whole community of MongoDB engineers (including the DevRel team) and fellow developers are sure to help!","description":"Learn how to use Azure Key Vault as your remote key management system with MongoDB's client-side field level encryption, step-by-step.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt54a5e766972c5a5f/644c46a5c605e46fe13512e4/security.png?branch=prod","description":null}}]},"slug":"/use-azure-key-vault-mongodb-client-side-field-level-encryption","title":"*Integrate Azure Key Vault with MongoDB Client-Side Field Level Encryption","original_publish_date":"2021-08-27T18:48:02.954Z","strapi_updated_at":"2022-05-24T18:13:05.873Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*CSharp","calculated_slug":"/languages/csharp"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Security","calculated_slug":"/products/mongodb/security"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*CSharp","calculated_slug":"/languages/csharp"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Azure","calculated_slug":"/technologies/azure"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:21.376Z","publish_details":{"time":"2023-04-28T22:52:54.669Z"}}},{"calculated_slug":"/languages/javascript/nextjs-with-mongodb","content":"Are you building your next amazing application with [Next.js](https://nextjs.org/)? Do you wish you could integrate MongoDB into your Next.js app effortlessly? Do you need this done before your coffee is done brewing? If you answered yes to these three questions, I have some good news for you. We have created an official [Next.js\\<\\>MongoDB](https://github.com/vercel/next.js/tree/canary/examples/with-mongodb) integration that will have you up and running in minutes, and you can consider this tutorial your official guide on how to use it.\n\nIn this tutorial, we'll take a look at how we can use the [with-mongodb example](https://github.com/vercel/next.js/tree/canary/examples/with-mongodb) to create a new Next.js application that follows MongoDB best practices for connectivity, connection pool monitoring, and querying. We'll also take a look at how to use MongoDB in our Next.js app with things like serverSideProps and APIs. Finally, we'll take a look at how we can easily deploy and host our application on [Vercel](https://vercel.com/), the official hosting platform for Next.js applications. If you already have an existing Next.js app, not to worry, simply drop in the [MongoDB utility file](https://github.com/vercel/next.js/blob/canary/examples/with-mongodb/lib/mongodb.ts) into your existing project and you are good to go. We have a lot of exciting stuff to cover, so let's dive right in!\n\n## Prerequisites\n\nFor this tutorial, you'll need:\n\n-   [MongoDB Atlas (sign up for free)](https://www.mongodb.com/cloud/atlas).\n-   [Vercel Account (sign up for free)](https://vercel.com/).\n-   NodeJS 12+.\n-   npm and npx.\n\nReact and Next.js familiarity is expected to get the most out of this tutorial, but I will try to cover unique features with enough depth to still be valuable to a newcomer.\n\n## What is Next.js?\n\nIf you're not already familiar, [Next.js](https://nextjs.org/) is a [React](https://reactjs.org/)-based framework for building modern web applications. The framework adds a lot of powerful features — such as server side rendering, automatic code splitting, and incremental static regeneration — that make it easy to build scalable and production-ready apps.\n\n![NextJS Homepage](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Next_JS_Homepage_625b7061dc.png)\n\n\n## Getting started with-mongodb\n\nNext.js has an extensive examples library that shows how you can integrate the framework with various features like [GraphQL servers](https://github.com/vercel/next.js/tree/canary/examples/with-graphql-react), [authentication libraries](https://github.com/vercel/next.js/tree/canary/examples/with-next-auth), or [CSS frameworks](https://github.com/vercel/next.js/tree/canary/examples/with-tailwindcss).  The example we'll use for this post is called [with-mongodb](https://github.com/vercel/next.js/tree/canary/examples/with-mongodb) and as you might expect, it'll come with everything needed to connect to a MongoDB database.\n\nTo create a new Next.js app with MongoDB integration built in, execute the following command in your terminal:\n\n``` shell\nnpx create-next-app --example with-mongodb mflix\n```\n\nWe are using the `npx create-next-app` command and are passing the `--example with-mongodb` parameter which will tell `create-next-app` to bootstrap our app with the MongoDB integration example. Finally, `mflix` is the name of our application. You can name your application something else if you'd prefer. Executing this command will take a couple of seconds to download and install all the npm dependencies, but once they're downloaded and installed, navigate to your project directory by running:\n\n``` shell\ncd mflix\n```\n\nRunning the above command in node version > 18 will throw an error as following: \n\n```\n? Could not download \"with-mongodb\" because of a connectivity issue between your machine and GitHub.\n✔ Could not download \"with-mongodb\" because of a connectivity issue between your machine and GitHub.\nDo you want to use the default template instead? (Y/n)\n```\n\nThe issue is in the `node-tar extract()`, which emits the close event. There is a [GitHub issue open](https://github.com/vercel/next.js/issues/39321) to address this, or you can follow these steps:\n\nUse `node version < 18` or type `Y` to get the default template downloaded:\n```shell\nDo you want to use the default template instead? (Y/n) Y\n```\nAfter that, navigate to the project directory by running:\n```shell\ncd mflix\n```\n\nAnd then install all the npm dependencies by running:\n```shell\nnpm install\n```\n\nIn this directory, let's start up our application and see what happens.  To start our Next.js app, in the `mflix` directory, execute:\n\n``` shell\nnpm run dev\n```\n\nOnce the app is built, let's see our app in action by navigating to `localhost:3000`. Uh-oh. We got an error.\n\n![NextJS MongoDB Not Configured](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Next_JS_Mongo_DB_Not_Configured_c9cd8dfab8.png)\n\nThe good news is that the error is fairly descriptive. The reason for this error is that we haven't provided our MongoDB connection string to the Next.js app. Let's do that next.\n\n## Connecting MongoDB to Next.js\n\nIf we look at our Next.js application directory, we'll find a `env.local.example` file. Let's rename this file to `env.local` and open it. This file will contain one properties that we'll need to fill out: `MONGODB_URI`.\n\nWe'll get this information from our [MongoDB Atlas Cluster](https://mongodb.com/atlas). You can use a local MongoDB installation if you have one, but if you're just getting started, MongoDB Atlas is a great way to get up and running without having to install or manage your MongoDB instance. MongoDB Atlas has a forever free tier that you can [sign up for](https://mongodb.com/atlas) as well as get the sample data that we'll be using for the rest of this tutorial.\n\nTo get our MongoDB URI, in our MongoDB Atlas dashboard, hit the **Connect** button. Then the **Connect to your application** button, and here you'll see a string that contains your URI that will look like this:\n\n``` \nmongodb+srv://<USERNAME>:<PASSWORD>@cluster0.tdm0q.mongodb.net/<DBNAME>?retryWrites=true&w=majority\n```\n\nIf you are new to MongoDB Atlas, you'll need to go to the **Database Access** section and create a username and password, as well as the **Network Access** tab to ensure your IP is allowed to connect to the database. However, if you already have a database user and network access enabled, you'll just need to replace the `<USERNAME>` and `<PASSWORD>` fields with your information.\n\nFor the `<DBNAME>`, we'll load the MongoDB Atlas sample datasets and use one of those databases. To load the sample datasets, in your MongoDB Atlas dashboard under your chosen Cluster, click the `...` button and hit **Load Sample Dataset** option. This will take a few minutes to load the data and create the various databases. The one we'll use for this tutorial is called `sample_mflix`, so you'll set your `<DBNAME>` value to that.\n\n![MongoDB Sample Dataset](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Mongo_DB_Sample_Dataset_e8b5295aa2.png)\n\nTo close out this section, our `env.local` file should look like this:\n\n``` \nMONGODB_URI=mongodb+srv://<USERNAME>:<PASSWORD>@cluster0.tdm0q.mongodb.net/sample_mflix?retryWrites=true&w=majority\n```\n\nTo make sure our configuration is correct, let's restart our Next.js app by going to the terminal and building the application again. Execute the following command in your terminal:\n\n``` shell\nnpm run dev\n```\n\nWhen the application is built, navigate to `localhost:3000` in your browser and you should see the following:\n\n![NextJS with MongoDB Connected](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Next_JS_with_Mongo_DB_Connected_509ed4c7c9.png)\n\nThis is the `with-mongodb` Next.js app welcome page. If you see the message **\"You are connected to MongoDB\"**, you are good to go. If you see the message **\"You are NOT connected to MongoDB\"**, then verify your connection string and make sure that the database user as well as network connection is properly set. If you run into any issues, head over to the [MongoDB community](https://developer.mongodb.com/community/forums/), and we'll help troubleshoot.\n\n## Querying MongoDB with Next.js\n\nNow that we are connected to MongoDB, let's discuss how we can query our MongoDB data and bring it into our Next.js application. Next.js supports multiple  ways to get data. We can create [API endpoints](https://nextjs.org/docs/api-routes/introduction), get data by running [server-side rendered functions](https://nextjs.org/docs/basic-features/data-fetching#getserversideprops-server-side-rendering) for a particular page, or even [generate static pages](https://nextjs.org/docs/basic-features/data-fetching#getstaticprops-static-generation) by getting our data at build-time. We'll look at all three examples.\n\n### Example 1: Next.js API endpoint with MongoDB\n\nThe first example we'll look at is building and exposing an API endpoint in our Next.js application. To create a new API endpoint route, we will first need to create an `api` directory in our `pages` directory, and then every file we create in this `api` directory will be treated as an individual API endpoint.\n\nLet's go ahead and create the `api` directory and a new file in this `directory` called `movies.js`. This endpoint will return a list of 20 movies from our MongoDB database. The implementation for this route is as follows:\n\n``` javascript\nimport clientPromise from \"../../lib/mongodb\";\n\nexport default async (req, res) => {\n   try {\n       const client = await clientPromise;\n       const db = client.db(\"sample_mflix\");\n\n       const movies = await db\n           .collection(\"movies\")\n           .find({})\n           .sort({ metacritic: -1 })\n           .limit(10)\n           .toArray();\n\n       res.json(movies);\n   } catch (e) {\n       console.error(e);\n   }\n};\n```\n\nTo explain what is going on here, we'll start with the import statement.  We are importing our `clientPromise` method from the `lib/mongodb` file. This file contains all the instructions on how to connect to our MongoDB Atlas cluster. Additionally, within this file we cache the instance of our connection so that subsequent requests do not have to reconnect to the cluster. They can use the existing connection. All of this is handled for you!\n\nNext, our API route handler has the signature of `export default async (req, res)`. If you're familiar with [Express.js](https://expressjs.com/), this should look very familiar.  This is the function that gets executed when the `localhost:3000/api/movies` route is called. We capture the request via `req` and return the response via the `res` object.\n\nOur handler function implementation calls the `clientPromise` function to get the instance of our MongoDB database. Next, we execute a MongoDB query using the [MongoDB Node.js Driver](https://docs.mongodb.com/drivers/node/) to get the top 20 movies out of our movies collection based on their **metacritic** rating sorted in descending order.\n\nFinally, we call the `res.json` method and pass in our array of movies.  This serves our movies in JSON format to our browser. If we navigate to `localhost:3000/api/movies`, we'll see a result that looks like this:\n\n![NextJS API Route with Movies](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Next_JS_API_Route_with_Movies_71ee0d123f.png)\n\nWe can add additional API routes by creating additional files in the `api` directory. As a homework exercise, why don't you create an API route that returns a single movie based on a user provided id?\n\nTo give you some pointers, you'll use [Next.js Dynamic API Routes](https://nextjs.org/docs/api-routes/dynamic-api-routes) to capture the `id`. So, if a user calls `http://localhost:3000/api/movies/573a1394f29313caabcdfa3e`, the movie that should be returned is **Seven Samurai**. Another tip, the `_id` property for the `sample_mflix` database in MongoDB is stored as an ObjectID, so you'll have to convert the string to an ObjectID. If you get stuck, create a thread on the [MongoDB Community forums](https://developer.mongodb.com/community/forums/) and we'll solve it together! Next, we'll take a look at how to access our MongoDB data within our Next.js pages.\n\n### Example 2: Next.js pages with MongoDB\n\nIn the last section, we saw how we can create an API endpoint and connect to MongoDB with it. In this section, we'll get our data directly into our Next.js pages. We'll do this using the [getServerSideProps() method](https://nextjs.org/docs/basic-features/data-fetching#getserversideprops-server-side-rendering) that is available to Next.js pages.\n\nThe `getServerSideProps()` method forces a Next.js page to load with server-side rendering. What this means is that every time this page is loaded, the `getServerSideProps()` method runs on the back end, gets data, and sends it into the React component via props. The code within `getServerSideProps()` is never sent to the client. This makes it a great place to implement our MongoDB queries.\n\nLet's see how this works in practice. Let's create a new file in the `pages` directory, and we'll call it `movies.js`. In this file, we'll add the following code:\n\n``` javascript\nimport clientPromise from \"../lib/mongodb\";\n\nexport default function Movies({ movies }) {\n    return (\n        <div>\n            <h1>Top 20 Movies of All Time</h1>\n            <p>\n                <small>(According to Metacritic)</small>\n            </p>\n            <ul>\n                {movies.map((movie) => (\n                    <li>\n                        <h2>{movie.title}</h2>\n                        <h3>{movie.metacritic}</h3>\n                        <p>{movie.plot}</p>\n                    </li>\n                ))}\n            </ul>\n        </div>\n    );\n}\n\nexport async function getServerSideProps() {\n    try {\n        const client = await clientPromise;\n        const db = client.db(\"sample_mflix\");\n\n        const movies = await db\n            .collection(\"movies\")\n            .find({})\n            .sort({ metacritic: -1 })\n            .limit(20)\n            .toArray();\n\n        return {\n            props: { movies: JSON.parse(JSON.stringify(movies)) },\n        };\n    } catch (e) {\n        console.error(e);\n    }\n}\n```\n\nAs you can see from the example above, we are importing the same `clientPromise` utility class, and our MongoDB query is exactly the same within the `getServerSideProps()` method. The only thing we really needed to change in our implementation is how we parse the response. The `getServerSideProps()` return method has some trouble serializing our data. There is a [GitHub issue open](https://github.com/vercel/next.js/issues/11993) to address this, but the current workaround is to stringify and then parse the data manually.\n\nOur page component called `Movies` gets the props from our `getServerSideProps()` method, and we use that data to render the page showing the top movie title, **metacritic** rating, and plot. Your result should look something like this:\n\n![NextJS Page with Movies](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Next_JS_Page_with_Movies_069338f2de.png)\n\nThis is great. We can directly query our MongoDB database and get all the data we need for a particular page. The contents of the `getServerSideProps()` method are never sent to the client, but the one downside to this is that this method runs every time we call the page.  Our data is pretty static and unlikely to change all that often. What if we pre-rendered this page and didn't have to call MongoDB on every refresh? We'll take a look at that next!\n\n### Example 3: Next.js static generation with MongoDB\n\nFor our final example, we'll take a look at how static page generation can work with MongoDB. Let's create a new file in the `pages` directory and call it `top.js`. For this page, what we'll want to do is render the top 1000 movies from our MongoDB database.\n\nTop 1000 movies? Are you out of your mind? That'll take a while, and the database round trip is not worth it. Well, what if we only called this method once when we built the application, so that even if that call takes a few seconds, it'll only ever happen once and our users won't be affected? They'll get the top 1000 movies delivered as quickly or even faster than the 20 using `serverSideProps()`. The magic lies in the `getStaticProps()` method, and our implementation looks like this:\n\n``` javascript\nimport clientPromise from \"../lib/mongodb\";\n\nexport default function Top({ movies }) {\n  return (\n    <div>\n      <h1>Top 1000 Movies of All Time</h1>\n      <p>\n        <small>(According to Metacritic)</small>\n      </p>\n      <ul>\n        {movies.map((movie) => (\n          <li>\n            <h2>{movie.title}</h2>\n            <h3>{movie.metacritic}</h3>\n            <p>{movie.plot}</p>\n          </li>\n        ))}\n      </ul>\n    </div>\n  );\n}\n\nexport async function getStaticProps() {\n    try {\n        const client = await clientPromise;\n        const db = client.db(\"sample_mflix\");\n\n        const movies = await db\n            .collection(\"movies\")\n            .find({})\n            .sort({ metacritic: -1 })\n            .limit(1000)\n            .toArray();\n\n        return {\n            props: { movies: JSON.parse(JSON.stringify(movies)) },\n        };\n    } catch (e) {\n        console.error(e);\n    }\n}\n        \n```\n\nAt a glance, this looks very similar to the `movies.js` file we created earlier. The only significant changes we made were changing our `limit` from `20` to `1000` and our `getServerSideProps()` method to `getStaticProps()`. If we navigate to `localhost:3000/top` in our browser, we'll see a long list of movies.\n\n![NextJS Page with 1000 Movies](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Next_JS_Page_with_1000_Movies_bc01a61d17.png)\n\nLook at how tiny that scrollbar is. Loading this page took about 3.5 seconds on my machine as opposed to 8.79 second response time for the `/movies` page. The reason it takes this long is because in development mode, the `getStaticProps()` method is called every single time (just like the `getServerSideProps()` method). But if we switch from development mode to production mode, we'll see the opposite. The `/top` page will be pre-rendered and will load almost immediately, while the `/movies` and `/api/movies` routes will execute the server-side code each time.\n\nLet's switch to production mode. In your terminal window, stop the current app from running. To run our Next.js app in production mode, we'll first need to build it, and then we can run the `start` command which will serve our built application. In your terminal window, execute the following commands:\n\n``` shell\nnpm run build\nnpm run start\n```\n\nWhen you run the `npm run start` command, your Next.js app is served in production mode. The `getStaticProps()` method will not be executed every time you hit the `/top` route as this page will now be served statically. We can even see the pre-rendered static page by navigating to the `.next/server/pages/top.html` file and seeing the 1,000 movies listed in plain HTML.\n\nNext.js can even update this static content without requiring a rebuild with a feature called [Incremental Static Regeneration](https://nextjs.org/docs/basic-features/data-fetching#incremental-static-regeneration), but that's outside of the scope of this tutorial. Next, we'll take a look at deploying our application on Vercel.\n\n## Deploying Your Next.js app on Vercel\n\nThe final step in our tutorial today is deploying our application. We'll deploy our Next.js with MongoDB app to Vercel. I have created a [GitHub repo](https://github.com/iamKushagra/nextjs-with-mongodb) that contains all of the code we have written today. Feel free to clone it, or create your own.\n\nNavigate to [Vercel](https://vercel.com) and log in. Once you are on your dashboard, click the **Import Project** button, and then **Import Git Repository**.\n\n![Vercel](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Vercel_Import_Project_8c4f666cf3.png)\n\n\nThe URL I'll use is the one provided above that has the application we built today. For reference, that URL is `https://github.com/mongodb-developer/nextjs-with-mongodb`. Add your projects GitHub URL and hit **Continue**. On the next screen, you'll have the option to add Environment Variables. Here, we'll want to add the two variables from our `env.local` file. The variables will be `MONGODB_URI`. Be sure to add both of these with their corresponding values before hitting **Deploy**.\n\n![Vercel Environmental Variables](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Vercel_Environmental_Variables_10163814e4.png)\n\nAfter hitting the **Deploy** button, your Next.js application will be automatically built and deployed. This process will take a few minutes, but once it's done, you will get a URL where you can access your Next.js application. In my case, that URL is [URL]().\n\n**NOTE**: Vercel uses dynamic IP addresses, so you'll need to add an exception to access from any IP address in your MongoDB Atlas dashboard. To do this, simplify navigate to the **Network Access** tab, hit the **Add IP Address** button, and then hit the **Allow Access From Anywhere** button or for the Access List Entry enter 0.0.0/0.\n\n![Next.js with MongoDB Deployed](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Next_js_with_Mongo_DB_Deployed_50e78ac27e.png)\n\nWe are live! Let's make sure everything works by navigating to the `https://nextjs-with-mongodb-ashen.vercel.app/movies`, `https://nextjs-with-mongodb-ashen.vercel.app/api/movies`, and `https://nextjs-with-mongodb-ashen.vercel.app/top` routes.\n\n## Next.js and MongoDB with one click\n\nOur app is now deployed and running in production. If you weren't following along with the tutorial and just want to quickly start your Next.js application with MongoDB, you could always use the with-mongodb starter found at [Github](https://github.com/vercel/next.js/tree/canary/examples/with-mongodb), but I’ve got an even better one for you.\n\n[Click this link](https://vercel.com/import/git?c=1&s=https://github.com/vercel/next.js/tree/canary/examples/with-mongodb&env=MONGODB_URI,MONGODB_DB&envDescription=Required%20to%20connect%20the%20app%20with%20MongoDB) and you'll be off to the races in creating and deploying the official Next.js with the MongoDB integration, and all you'll need to provide is your connection string.\n\n## Putting it all together\n\nIn this tutorial, we walked through the official Next.js with MongoDB example. I showed you how to connect your MongoDB database to your Next.js application and execute queries in multiple ways. Then, we deployed our application using Vercel.\n\nIf you have any questions or feedback, reach out through the[MongoDB Community forums](https://developer.mongodb.com/community/forums/) and let me know what you build with Next.js and MongoDB.","description":"Learn how to easily integrate MongoDB into your Next.js application with the official MongoDB package.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt84d3ace3a7d2a27c/644c46a643bec31a242f0b97/JavaScript_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/nextjs-with-mongodb/","title":"*How to Integrate MongoDB Into Your Next.js App","original_publish_date":"2022-05-12T16:09:49.411Z","strapi_updated_at":"2023-02-23T16:18:49.903Z","expiry_date":"2022-08-31T17:43:06.635Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*Nextjs","calculated_slug":"/technologies/nextjs"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:20.973Z","publish_details":{"time":"2023-04-28T22:52:54.703Z"}}},{"calculated_slug":"/products/realm/realm-sync-migration","content":"\n## Introduction\n\nIn the [previous post in this series](https://www.mongodb.com/developer/how-to/realm-schema-migration), we saw how to migrate your Realm data when you upgraded your iOS app with a new schema. But, that only handled the data in your local, standalone Realm database. What if you're using [MongoDB Realm Sync](https://docs.mongodb.com/realm/sync/) to replicate your local Realm data with other instances of your mobile app and with MongoDB Atlas? That's what this article will focus on.\n\nWe'll start with the original [RChat app](https://www.mongodb.com/developer/how-to/building-a-mobile-chat-app-using-realm-new-way/). We'll then extend the iOS app and backend Realm schema to add a new feature that allows chat messages to be tagged as high priority. The next (and perhaps surprisingly more complicated from a Realm perspective) upgrade is to make the `author` attribute of the existing `ChatMessage` object non-optional.\n\nYou can find all of the code for this post in the RChat repo under these branches:\n\n- [Starting point](https://github.com/realm/RChat)\n- [Upgrade #1](https://github.com/realm/RChat/tree/new-schema)\n- [Upgrade #2](https://github.com/realm/RChat/tree/v2-schema)\n\n## Prerequisites\n\n[Realm Cocoa 10.13.0](https://github.com/realm/realm-cocoa/releases) or later (for versions of the app that you're upgrading **to**)\n\n## Catch-Up — The RChat App\n\nRChat is a basic chat app:\n\n- Users can register and log in using their email address and a password.\n- Users can create chat rooms and include other users in those rooms.\n- Users can post messages to a chat room (optionally including their location and photos).\n- All members of a chatroom can see messages sent to the room by themselves or other users.\n\n:youtube[Existing RChat iOS app functionality]{vid=BlV9El_MJqk}\n\n## Upgrade #1: Add a High-Priority Flag to Chat Messages\n\nThe first update is to allow a user to tag a message as being high-priority as they post it to the chat room:\n\n![Screenshot showing the option to click a thermometer button to tag the message as urgent](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Set_Priority_a2c8f60550.png)\n\nThat message is then highlighted with bold text and a \"hot\" icon in the list of chat messages:\n\n![Screenshot showing that a high-priority message has bold text and a hot thermometer icon](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/View_Priority_1991c9d4bf.png)\n\n### Updating the Backend Realm Schema\n\nAdding a new field is an [additive change](https://docs.mongodb.com/realm/sync/synced-schema-overview/)—meaning that you don't need to restart sync (which would require every deployed instance of the RChat mobile app to recognize the change and start sync from scratch, potentially losing local changes).\n\nWe add the new `isHighPriority` bool to our Realm schema through the Realm UI:\n\n![Screenshot from the RealmUI showing that the isHighPriority bool has been added to the schema](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Update_Schema_v1_b9ff24b45c.png)\n\nWe also make `isHighPriority` a required (non-optional field).\n\nThe resulting schema looks like this:\n\n```js\n{\n   \"bsonType\": \"object\",\n   \"properties\": {\n     \"_id\": {\n       \"bsonType\": \"string\"\n     },\n     \"author\": {\n       \"bsonType\": \"string\"\n     },\n     \"image\": {\n       \"bsonType\": \"object\",\n       \"properties\": {\n         \"_id\": {\n           \"bsonType\": \"string\"\n         },\n         \"date\": {\n           \"bsonType\": \"date\"\n         },\n         \"picture\": {\n           \"bsonType\": \"binData\"\n         },\n         \"thumbNail\": {\n           \"bsonType\": \"binData\"\n         }\n       },\n       \"required\": [\n         \"_id\",\n         \"date\"\n       ],\n       \"title\": \"Photo\"\n     },\n     \"isHighPriority\": {\n       \"bsonType\": \"bool\"\n     },\n     \"location\": {\n       \"bsonType\": \"array\",\n       \"items\": {\n         \"bsonType\": \"double\"\n       }\n     },\n     \"partition\": {\n       \"bsonType\": \"string\"\n     },\n     \"text\": {\n       \"bsonType\": \"string\"\n     },\n     \"timestamp\": {\n       \"bsonType\": \"date\"\n     }\n   },\n   \"required\": [\n     \"_id\",\n     \"partition\",\n     \"text\",\n     \"timestamp\",\n     \"isHighPriority\"\n   ],\n   \"title\": \"ChatMessage\"\n }\n```\nNote that existing versions of our iOS RChat app can continue to work with our updated backend Realm app, even though their local `ChatMessage` Realm objects don't include the new field.\n\n### Updating the iOS RChat App\n\nWhile existing versions of the iOS RChat app can continue to work with the updated Realm backend app, they can't use the new `isHighPriority` field as it isn't part of the `ChatMessage` object.\n\nTo add the new feature, we need to update the mobile app after deploying the updated Realm backend application.\n\nThe first change is to add the `isHighPriority` field to the `ChatMessage` class:\n\n```swift\nclass ChatMessage: Object, ObjectKeyIdentifiable {\n   @Persisted(primaryKey: true) var _id = UUID().uuidString\n   @Persisted var partition = \"\" // \"conversation=<conversation-id>\"\n   @Persisted var author: String? // username\n   @Persisted var text = \"\"\n   @Persisted var image: Photo?\n   @Persisted var location = List<Double>()\n   @Persisted var timestamp = Date()\n   @Persisted var isHighPriority = false\n   ...\n}\n```\n\nAs seen in the [previous post in this series](https://www.mongodb.com/developer/how-to/realm-schema-migration), Realm can automatically update the local realm to include this new attribute and initialize it to `false`. Unlike with standalone realms, we **don't** need to signal to the Realm SDK that we've updated the schema by providing a schema version.\n\nThe new version of the app will happily exchange messages with instances of the original app on other devices (via our updated backend Realm app).\n\n## Upgrade #2: Make `author` a Non-Optional Chat Message field\n\nWhen the initial version of RChat was written, the `author` field of `ChatMessage` was declared as being optional. We've since realized that there are no scenarios where we wouldn't want the author included in a chat message. To make sure that no existing or future client apps neglect to include the author, we need to update our schema to make `author` a required field.\n\nUnfortunately, changing a field from optional to required (or vice versa) is a [destructive change](https://docs.mongodb.com/realm/sync/synced-schema-overview/#destructive-changes), and so would break sync for any deployed instances of the RChat app.\n\nOops!\n\nThis means that there's extra work needed to make the upgrade seamless for the end users. We'll go through the process now.\n\n### Updating the Backend Realm Schema\n\nThe change we need to make to the schema is destructive. This means that the new document schema is incompatible with the schema that's currently being used in our mobile app.\n\nIf RChat wasn't already deployed on the devices of hundreds of millions of users (we can dream!), then we could update the Realm schema for the `ChatMessage` collection and restart Realm Sync. During development, we can simply remove the original RChat mobile app and then install an updated version on our test devices.\n\nTo avoid that trauma for our end users, we leave the `ChatMessage` collection's schema as is and create a [partner collection](https://docs.mongodb.com/realm/sync/migrate-schema-partner-collection/#partner-collections). The partner collection (`ChatMessageV2`) will contain the same data as `ChatMessage`, except that its schema makes `author` a required field.\n\nThese are the steps we'll go through to create the partner collection:\n\n- Define a Realm schema for the `ChatMessageV2` collection.\n- Run an [aggregation](https://docs.mongodb.com/manual/aggregation/) to copy all of the documents from `ChatMessage` to `ChatMessageV2`. If `author` is missing from a `ChatMessage` document, then the aggregation will add it.\n- Add a trigger to the `ChatMessage` collection to propagate any changes to `ChatMessageV2` (adding `author` if needed).\n- Add a trigger to the `ChatMessageV2` collection to propagate any changes to `ChatMessage`.\n\n#### Define the Schema for the Partner Collection\n\nFrom the Realm UI, copy the schema from the `ChatMessage` collection. \n\nClick the button to create a new schema:\n\n![Showing a plus button in the Realm UI to add a new collection](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/add_collection_c16ec9a063.png)\n\nSet the database and collection name before clicking \"Add Collection\":\n\n![Setting the database and collection names in the Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/config_collection_324bc0182e.png)\n\nPaste in the schema copied from `ChatMessage`, add `author` to the `required` section, change the `title` to `ChatMessageV2`, and the click the \"SAVE\" button:\n\n![Adding \"author\" to the required attribute list and naming the class ChatMessageV2 in the Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/define_schema_be6ae7c555.png)\n\nThis is the resulting schema:\n\n```js\n{\n   \"bsonType\": \"object\",\n   \"properties\": {\n     \"_id\": {\n       \"bsonType\": \"string\"\n     },\n     \"author\": {\n       \"bsonType\": \"string\"\n     },\n     \"image\": {\n       \"bsonType\": \"object\",\n       \"properties\": {\n         \"_id\": {\n           \"bsonType\": \"string\"\n         },\n         \"date\": {\n           \"bsonType\": \"date\"\n         },\n         \"picture\": {\n           \"bsonType\": \"binData\"\n         },\n         \"thumbNail\": {\n           \"bsonType\": \"binData\"\n         }\n       },\n       \"required\": [\n         \"_id\",\n         \"date\"\n       ],\n       \"title\": \"Photo\"\n     },\n     \"isHighPriority\": {\n       \"bsonType\": \"bool\"\n     },\n     \"location\": {\n       \"bsonType\": \"array\",\n       \"items\": {\n         \"bsonType\": \"double\"\n       }\n     },\n     \"partition\": {\n       \"bsonType\": \"string\"\n     },\n     \"text\": {\n       \"bsonType\": \"string\"\n     },\n     \"timestamp\": {\n       \"bsonType\": \"date\"\n     }\n   },\n   \"required\": [\n     \"_id\",\n     \"partition\",\n     \"text\",\n     \"timestamp\",\n     \"isHighPriority\",\n     \"author\"\n   ],\n   \"title\": \"ChatMessageV2\"\n }\n```\n\n#### Copy Existing Data to the Partner Collection\n\nWe're going to use an [aggregation pipeline](https://docs.mongodb.com/manual/aggregation/) to copy and transform the existing data from the original collection (`ChatMessage`) to the partner collection (`ChatMessageV2`).\n\nYou may want to pause sync just before you run the aggregation, and then unpause it after you enable the trigger on the `ChatMessage` collection in the next step:\n\n![Pressing a button to pause Realm sync in the Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/pause_sync_707605a511.png)\n\nThe end users can continue to create new messages while sync is paused, but those messages won't be published to other users until sync is resumed. By pausing sync, you can ensure that all new messages will make it into the partner collection (and so be visible to users running the new version of the mobile app).\n\nIf pausing sync is too much of an inconvenience, then you could create a temporary trigger on the `ChatMessage` collection that will copy and transform document inserts to the `ChatMessageV2` collection (it's a subset of the `ChatMessageProp` trigger we'll define in the next section.).\n\nFrom the Atlas UI, select \"Collections\" -> \"ChatMessage\", \"New Pipeline From Text\":\n\n![Navigating through \"Atlas/ChatMessage/Collections/New Pipeline from Text\" in the Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/add_aggregation_2e84c4a8df.png)\n\nPaste in this aggregation pipeline and click the \"Create New\" button:\n\n```js\n[\n {\n   '$addFields': {\n     'author': {\n       '$convert': {\n         'input': '$author',\n         'to': 'string',\n         'onError': 'unknown',\n         'onNull': 'unknown'\n       }\n     }\n   }\n },\n {\n   '$merge': {\n     into: \"ChatMessageV2\",\n     on: \"_id\",\n     whenMatched: \"replace\",\n     whenNotMatched: \"insert\"\n   }\n }\n]\n```\n\nThis aggregation will take each `ChatMessage` document, set `author` to \"unknown\" if it's not already set, and then add it to the `ChatMessageV2` collection.\n\nClick \"MERGE DOCUMENTS\":\n\n![Clicking the \"Merge Documents\" button in the Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/merge_documents_8b40ae6ce6.png)\n\n`ChatMessageV2` now contains a (possibly transformed) copy of every document from `ChatMessage`. But, changes to one collection won't be propagated to the other. To address that, we add a database trigger to each collection…\n\n#### Add Database Triggers\n\nWe need to create two [Realm Functions](https://docs.mongodb.com/realm/functions/)—one to copy/transfer documents to `ChatMessageV2`, and one to copy documents to `ChatMessage`.\n\nFrom the \"Functions\" section of the Realm UI, click \"Create New Function\":\n\n![Clicking the \"Create New Function\" button in the Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/new_function_3fd3b90a11.png)\n\nName the function `copyToChatMessageV2`. Set the authentication method to \"System\"—this will circumvent any access permissions on the `ChatMessageV2` collection. Ensure that the \"Private\" switch is turned on—that means that the function can be called from a trigger, but not directly from a frontend app. Click \"Save\":\n\n![Set the name to \"copyToChatMessageV2\", authentication to \"System\" and \"Private\" to \"On\". Then click the \"Save\" button in the Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/configure_function_6acb29f42b.png)\n\nPaste this code into the function editor and save:\n\n```js\nexports = function (changeEvent) {\n   const db = context.services.get(\"mongodb-atlas\").db(\"RChat\");\n\n\n   if (changeEvent.operationType === \"delete\") {\n     return db.collection(\"ChatMessageV2\").deleteOne({ _id: changeEvent.documentKey._id });\n   }\n\n\n   const author = changeEvent.fullDocument.author ? changeEvent.fullDocument.author : \"Unknown\";\n   const pipeline = [\n     { $match: { _id: changeEvent.documentKey._id } },\n     {\n       $addFields: {\n         author: author,\n       }\n     },\n     { $merge: \"ChatMessageV2\" }];\n\n\n   return db.collection(\"ChatMessage\").aggregate(pipeline);\n};\n```\n\nThis function will receive a `ChatMessage` document from our trigger. If the operation that triggered the function is a delete, then this function deletes the matching document from `ChatMessageV2`. Otherwise, the function either copies `author` from the incoming document or sets it to \"Unknown\" before writing the transformed document to `ChatMessageV2`. We could initialize `author` to any string, but I've used \"Unknown\" to tell the user that we don't know who the author was.\n\nCreate the `copyToChatMessage` function in the same way:\n\n```js\nexports = function (changeEvent) {\n   const db = context.services.get(\"mongodb-atlas\").db(\"RChat\");\n\n\n   if (changeEvent.operationType === \"delete\") {\n     return db.collection(\"ChatMessage\").deleteOne({ _id: changeEvent.documentKey._id })\n   }\n    const pipeline = [\n     { $match: { _id: changeEvent.documentKey._id } },\n     { $merge: \"ChatMessage\" }]\n   return db.collection(\"ChatMessageV2\").aggregate(pipeline);\n};\n```\n\nThe final change needed to the backend Realm application is to add database triggers that invoke these functions.\n\nFrom the \"Triggers\" section of the Realm UI, click \"Add a Trigger\":\n\n![Click the \"Add a Trigger\" button in the Realm UI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/configure_trigger_558a629423.png)\n\nConfigure the `ChatMessageProp` trigger as shown:\n\n![In the Realm UI, set \"Trigger Type\" to \"Database\". Set \"Name\" to \"ChatMessageProp\". Enabled = On. Event Ordering = on. Cluster Name = Cluster 0. Database name = RChat. Collection name = ChatMessage. Check all operation types. Full Document = on. Document Preimage = off. Event Type = Function. Function = copyToChatMessageV2](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/trigger_settings_1601e02011.png)\n\nRepeat for `ChatMessageV2Change`:\n\n![In the Realm UI, set \"Trigger Type\" to \"Database\". Set \"Name\" to \"ChatMessageProp\". Enabled = On. Event Ordering = on. Cluster Name = Cluster 0. Database name = RChat. Collection name = ChatMessage. Check all operation types. Full Document = off. Document Preimage = off. Event Type = Function. Function = copyToChatMessage](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/trigger2_settings_c55de55738.png)\n\nIf you paused sync in the previous section, then you can now unpause it.\n\n### Updating the iOS RChat App\n\nWe want to ensure that users still running the old version of the app can continue to exchange messages with users running the latest version.\n\nExisting versions of RChat will continue to work. They will create `ChatMessage` objects which will get synced to the `ChatMessage` Atlas collection. The database triggers will then copy/transform the document to the `ChatMessageV2` collection.\n\nWe now need to create a new version of the app that works with documents from the `ChatMessageV2` collection. We'll cover that in this section.\n\nRecall that we set `title` to `ChatMessageV2` in the partner collection's schema. That means that to sync with that collection, we need to rename the `ChatMessage` class to `ChatMessageV2` in the iOS app. \n\nChanging the name of the class throughout the app is made trivial by Xcode.\n\nOpen `ChatMessage.swift` and right-click on the class name (`ChatMessage`), select \"Refactor\" and then \"Rename…\":\n\n![In Xcode, select the \"ChatMessage\" class name, right-click and select \"Refactor -> Rename...\"](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/refactor1_4c2d938e3d.png)\n\nOverride the class name with `ChatMessageV2` and click \"Rename\":\n\n![In Xcode, overwrite the class name with \"ChatMessageV2\" and then click the \"Rename\" button](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/refactor2_03837b2472.png)\n\nThe final step is to make the author field mandatory. Remove the ? from the author attribute to make it non-optional:\n\n```swift\nclass ChatMessageV2: Object, ObjectKeyIdentifiable {\n   @Persisted(primaryKey: true) var _id = UUID().uuidString\n   @Persisted var partition = \"\" // \"conversation=<conversation-id>\"\n   @Persisted var author: String\n   ...\n}\n```\n\n## Conclusion\n\nModifying a Realm schema is a little more complicated when you're using Realm Sync for a deployed app. You'll have end users who are using older versions of the schema, and those apps need to continue to work.\n\nFortunately, the most common schema changes (adding or removing fields) are additive. They simply require updates to the back end and iOS schema, together.\n\nThings get a little trickier for destructive changes, such as changing the type or optionality of an existing field. For these cases, you need to create and maintain a partner collection to avoid loss of data or service for your users.\n\nThis article has stepped through how to handle both additive and destructive schema changes, allowing you to add new features or fix issues in your apps without impacting users running older versions of your app.\n\nRemember, you can find all of the code for this post in the RChat repo under these branches:\n\n- [Starting point](https://github.com/realm/RChat)\n- [Upgrade #1](https://github.com/realm/RChat/tree/new-schema)\n- [Upgrade #2](https://github.com/realm/RChat/tree/v2-schema)\n\nIf you're looking to upgrade the Realm schema for an iOS app that **isn't** using Realm Sync, then refer to the [previous post in this series](https://www.mongodb.com/developer/how-to/realm-schema-migration).\n\nIf you have any questions or comments on this post (or anything else Realm-related), then please raise them on our [community forum](https://www.mongodb.com/community/forums/c/realm-sdks/58). To keep up with the latest Realm news, follow [@realm](https://twitter.com/realm) on Twitter and join the [Realm global community](https://live.mongodb.com/realm-global-community/).\n","description":"When you add features to your app, you may need to modify your Realm schema. Here, we step through how to migrate your synced schema and data.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltccaa0f3b9a286f02/644c46a73df9c33c60a92bf9/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/realm-sync-migration","title":"*Migrating Your iOS App's Synced Realm Schema in Production","original_publish_date":"2021-09-02T08:12:35.608Z","strapi_updated_at":"2023-03-06T17:56:18.624Z","expiry_date":"2022-09-01T13:07:10.185Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Sync","calculated_slug":"/products/realm/sync"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*iOS","calculated_slug":"/technologies/ios"}},{"node":{"title":"*Mobile","calculated_slug":"/technologies/mobile"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt7e393fbc59651a36/644c46a9421cf97701324d53/OG_1200x630.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:20.590Z","publish_details":{"time":"2023-04-28T22:52:55.826Z"}}},{"calculated_slug":"/code-examples/swift/realm-api-cache","content":"## Introduction\n\nWhen building a mobile app, there's a good chance that you want it to pull in data from a cloud service—whether from your own or from a third party. While other technologies are growing (e.g., [GraphQL](https://docs.mongodb.com/realm/graphql/) and [MongoDB Realm Sync](https://docs.mongodb.com/realm/sync/)), [REST APIs](https://en.wikipedia.org/wiki/Representational_state_transfer) are still prevalent.\n\nIt's easy to make a call to a REST API endpoint from your mobile app, but what happens when you lose network connectivity? What if you want to slice and dice that data after you've received it? How many times will your app have to fetch the same data (consuming data bandwidth and battery capacity each time)? How will your users react to a sluggish app that's forever fetching data over the internet?\n\nBy caching the data from API calls in Realm, the data is always available to your app. This leads to higher availability, faster response times, and reduced network and battery consumption.\n\nThis article shows how the [RCurrency](https://github.com/realm/RCurrency) mobile app fetches exchange rate data from a [public API](https://exchangerate.host/), and then caches it in Realm for always-on, local access.\n\n### Is Using the API from Your Mobile App the Best Approach?\n\nThis app only reads data through the API. Writing an offline-first app that needs to reliably update cloud data via an API is a **far** more complex affair. If you need to update cloud data when offline, then I'd strongly recommend you consider [MongoDB Realm Sync](https://docs.mongodb.com/realm/sync/).\n\nMany APIs throttle your request rate or charge per request. That can lead to issues as your user base grows. A more scalable approach is to have your backend Realm app fetch the data from the API and store it in Atlas. Realm Sync then makes that data available locally on every user's mobile device—without the need for any additional API calls.\n\n## Prerequisites\n\n- [Realm-Cocoa 10.13.0+](https://github.com/realm/realm-cocoa)\n- Xcode 13\n- iOS 15\n\n## The RCurrency Mobile App\n\nThe [RCurrency](https://github.com/realm/RCurrency) app is a simple exchange rate app. It's intended for uses such as converting currencies when traveling.\n\nYou choose a base currency and a list of other currencies you want to convert between. \n\nWhen opened for the first time, RCurrency uses a REST API to retrieve exchange rates, and stores the data in Realm. From that point on, the app uses the data that's stored in Realm. Even if you force-close the app and reopen it, it uses the local data.\n\nIf the stored rates are older than today, the app will fetch the latest rates from the API and replace the Realm data.\n\nThe app supports pull-to-refresh to fetch and store the latest exchange rates from the API.\n\nYou can alter the amount of any currency, and the amounts for all other currencies are instantly recalculated.\n\n![Animation of the RCurrency app running on an iPhone. Includes the user selecting currencies, changing amounts and observing the amounts changing for other currencies](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/R_Currency_demo_9820e35730.gif)\n\n## The REST API\n\nI'm using the API provided by [exchangerate.host](https://exchangerate.host/). The API is a free service that provides a simple API to fetch currency exchange rates. \n\nOne of the reasons I picked this API is that it doesn't require you to register and then manage access keys/tokens. It's not rocket science to handle that complexity, but I wanted this app to focus on when to fetch data, and what to do once you receive it.\n\nThe app uses a single endpoint (where you can replace `USD` and `EUR` with the currencies you want to convert between):\n\n```js\nhttps://api.exchangerate.host/convert?from=USD&to=EUR\n```\n\nYou can try [calling that endpoint directly from your browser](https://api.exchangerate.host/convert?from=USD&to=EUR).\n\nThe endpoint responds with a JSON document:\n\n```js\n{\n  \"motd\": {\n    \"msg\": \"If you or your company use this project or like what we doing, please consider backing us so we can continue maintaining and evolving this project.\",\n    \"url\": \"https://exchangerate.host/#/donate\"\n  },\n  \"success\": true,\n  \"query\": {\n    \"from\": \"USD\",\n    \"to\": \"EUR\",\n    \"amount\": 1\n  },\n  \"info\": {\n    \"rate\": 0.844542\n  },\n  \"historical\": false,\n  \"date\": \"2021-09-02\",\n  \"result\": 0.844542\n}\n```\n\nNote that the exchange rate for each currency is only updated once every 24 hours. That's fine for our app that's helping you decide whether you can afford that baseball cap when you're on vacation. If you're a currency day-trader, then you should look elsewhere.\n\n## The RCurrency App Implementation\n### Data Model\n\nJSON is the language of APIs. That's great news as most modern programming languages (including Swift) make it super easy to convert between JSON strings and native objects.\n\nThe app stores the results from the API query in objects of type `Rate`. To make it as simple as possible to receive and store the results, I made the [`Rate` class](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Model/Rate.swift) match the JSON format of the API results:\n\n```swift\nclass Rate: Object, ObjectKeyIdentifiable, Codable {\n    var motd = Motd()\n    var success = false\n    @Persisted var query: Query?\n    var info = Info()\n    @Persisted var date: String\n    @Persisted var result: Double\n}\n\nclass Motd: Codable {\n    var msg = \"\"\n    var url = \"\"\n}\n\nclass Query: EmbeddedObject, ObjectKeyIdentifiable, Codable {\n    @Persisted var from: String\n    @Persisted var to: String\n    var amount = 0\n}\n\nclass Info: Codable {\n    var rate = 0.0\n}\n```\n\nNote that only the fields annotated with `@Persisted` will be stored in Realm.\n\nSwift can automatically convert between `Rate` objects and the JSON strings returned by the API because we make the class comply with the [`Codable` protocol](https://developer.apple.com/documentation/swift/codable).\n\nThere are two other top-level classes used by the app. \n\n[`Symbols`](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Model/Symbols.swift) stores all of the supported currency symbols. In the app, the list is bootstrapped from a fixed list. For future-proofing, it would be better to fetch them from an API:\n\n```swift\nclass Symbols {\n    var symbols = Dictionary<String, String>()\n}\n\nextension Symbols {\n    static var data = Symbols()\n\n\n    static func loadData() {\n        data.symbols[\"AED\"] = \"United Arab Emirates Dirham\"\n        data.symbols[\"AFN\"] = \"Afghan Afghani\"\n        data.symbols[\"ALL\"] = \"Albanian Lek\"\n        ...\n    }\n}\n```\n\n`UserSymbols` is used to store the user's chosen base currency and the list of currencies they'd like to see exchange rates for:\n\n```swift\nclass UserSymbols: Object, ObjectKeyIdentifiable {\n    @Persisted var baseSymbol: String\n    @Persisted var symbols: List<String>\n}\n```\n\nAn instance of `UserSymbols` is stored in Realm so that the user gets the same list whenever they open the app.\n\n### `Rate` Data Lifecycle\n\nThis flowchart shows how the exchange rate for a single currency (represented by the `symbol` string) is managed when the `CurrencyRowContainerView` is used to render data for that currency:\n\n![Flowchart showing how the app fetches data from the API and stored in in Realm. The mobile app's UI always renders what's stored in MongoDB. The following sections will describe each block in the flow diagram.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Rendering_a_Currency_flowchart_c92433a8dc.png)\n\nNote that the actual behavior is a little more subtle than the diagram suggests. SwiftUI ties the Realm data to the UI. If stage #2 finds the data in Realm, then it will immediately get displayed in the view (stage #8). The code will then make the extra checks and refresh the Realm data if needed. If and when the Realm data is updated, SwiftUI will automatically refresh the UI to render it.\n\nLet's look at each of those steps in turn.\n\n#### #1 `CurrencyContainerView` loaded for currency represented by `symbol`\n\n[`CurrencyListContainerView`](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Views/CurrencyList/CurrencyListContainerView.swift) iterates over each of the currencies that the user has selected. For each currency, it creates a [`CurrencyRowContainerView`](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Views/CurrencyItem/CurrencyRowContainerView.swift) and passes in strings representing the base currency (`baseSymbol`) and the currency we want an exchange rate for (`symbol`):\n\n\n```swift\nList {\n   ForEach(userSymbols.symbols, id: \\.self) { symbol in\n       CurrencyRowContainerView(baseSymbol: userSymbols.baseSymbol,\n                                   baseAmount: $baseAmount,\n                                   symbol: symbol,\n                                   refreshNeeded: refreshNeeded)\n   }\n   .onDelete(perform: deleteSymbol)\n}\n```\n#### #2 `rate` = FetchFromRealm(`symbol`)\n\n[`CurrencyRowContainerView`](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Views/CurrencyItem/CurrencyRowContainerView.swift) then uses the `@ObservedResults` property wrapper to query all `Rate` objects that are already stored in Realm:\n\n```swift\nstruct CurrencyRowContainerView: View {\n   @ObservedResults(Rate.self) var rates\n   ...\n}\n```\n\nThe view then filters those results to find one for the requested `baseSymbol`/`symbol` pair:\n\n```swift\nvar rate: Rate? {\n   rates.filter(\n       NSPredicate(format: \"query.from = %@ AND query.to = %@\",\n                   baseSymbol, symbol)).first\n}\n```\n\n#### #3 `rate` found?\n\nThe view checks whether `rate` is set or not (i.e., whether a matching object was found in Realm). If `rate` is set, then it's passed to `CurrencyRowDataView` to render the details (step #8). If `rate` is `nil`, then a placeholder \"Loading Data...\" `TextView` is rendered, and `loadData` is called to fetch the data using the API (step #4-3):\n\n```swift\nvar body: some View {\n   if let rate = rate {\n       HStack {\n           CurrencyRowDataView(rate: rate, baseAmount: $baseAmount, action: action)\n           ...\n       }\n   } else {\n       Text(\"Loading Data...\")\n           .onAppear(perform: loadData)\n   }\n}\n```\n\n#### #4-3 Fetch `rate` from API — No matching object found in Realm\n\nThe API URL is formed by inserting the base currency (`baseSymbol`) and the target currency (`symbol`) into a template string. `loadData` then sends the request to the API endpoint and handles the response:\n\n```swift\nprivate func loadData() {\n   guard let url = URL(string: \"https://api.exchangerate.host/convert?from=\\(baseSymbol)&to=\\(symbol)\") else {\n       print(\"Invalid URL\")\n       return\n   }\n   let request = URLRequest(url: url)\n   print(\"Network request: \\(url.description)\")\n   URLSession.shared.dataTask(with: request) { data, response, error in\n       guard let data = data else {\n           print(\"Error fetching data: \\(error?.localizedDescription ?? \"Unknown error\")\")\n           return\n       }\n       if let decodedResponse = try? JSONDecoder().decode(Rate.self, from: data) {\n           // TODO: Step #5-3\n       } else {\n           print(\"No data received\")\n       }\n   }\n   .resume()\n}\n```\n\n#### #5-3 StoreInRealm(`rate`) — No matching object found in Realm\n\n`Rate` objects stored in Realm are displayed in our SwiftUI views. Any data changes that impact the UI must be done on the main thread. When the API endpoint sends back results, our code receives them in a callback thread, and so we must use `DispatchQueue` to run our closure in the main thread so that we can add the resulting `Rate` object to Realm:\n\n```swift\nif let decodedResponse = try? JSONDecoder().decode(Rate.self, from: data) {\n   DispatchQueue.main.async {\n       $rates.append(decodedResponse)\n   }\n} else {\n   print(\"No data received\")\n}\n```\n\nNotice how simple it is to convert the JSON response into a Realm `Rate` object and store it in our local realm!\n\n#### #6 Refresh Requested?\n\nRCurrency includes a [pull-to-refresh](https://www.hackingwithswift.com/quick-start/swiftui/how-to-enable-pull-to-refresh) feature which will fetch fresh exchange rate data for each of the user's currency symbols. We add the refresh functionality by appending the `.refreshable` modifier to the `List` of rates in [`CurrencyListContainerView`](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Views/CurrencyList/CurrencyListContainerView.swift):\n\n```swift\nList {\n   ...\n}\n.refreshable(action: refreshAll)\n```\n\n`refreshAll` sets the `refreshNeeded` variable to `true`, waits a second to allow SwiftUI to react to the change, and then sets it back to `false`: \n\n```swift\nprivate func refreshAll() {\n   refreshNeeded = true\n   DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {\n       refreshNeeded = false\n   }\n}\n```\n\n`refreshNeeded` is passed to each instance of `CurrencyRowContainerView`:\n\n```swift\nCurrencyRowContainerView(baseSymbol: userSymbols.baseSymbol,\n                        baseAmount: $baseAmount,\n                        symbol: symbol,\n                        refreshNeeded: refreshNeeded)\n```\n[`CurrencyRowContainerView`](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Views/CurrencyItem/CurrencyRowContainerView.swift) checks `refreshNeeded`. If `true`, it displays a temporary refresh image and invokes `refreshData` (step #4-6):\n\n```swift\nif refreshNeeded {\n   Image(systemName: \"arrow.clockwise.icloud\")\n       .onAppear(perform: refreshData)\n}\n```\n\n#### #4-6 Fetch `rate` from API — Refresh requested\n\n`refreshData` fetches the data in exactly the same way as `loadData` in step #4-3:\n\n```swift\nprivate func refreshData() {\n   guard let url = URL(string: \"https://api.exchangerate.host/convert?from=\\(baseSymbol)&to=\\(symbol)\") else {\n       print(\"Invalid URL\")\n       return\n   }\n   let request = URLRequest(url: url)\n   print(\"Network request: \\(url.description)\")\n   URLSession.shared.dataTask(with: request) { data, response, error in\n       guard let data = data else {\n           print(\"Error fetching data: \\(error?.localizedDescription ?? \"Unknown error\")\")\n           return\n       }\n       if let decodedResponse = try? JSONDecoder().decode(Rate.self, from: data) {\n           DispatchQueue.main.async {\n               // TODO: #5-5\n           }\n       } else {\n           print(\"No data received\")\n       }\n   }\n   .resume()\n}\n```\n\nThe difference is that in this case, there may already be a `Rate` object in Realm for this currency pair, and so the results are handled differently...\n\n#### #5-6 StoreInRealm(`rate`) — Refresh requested\n\nIf the `Rate` object for this currency pair had been found in Realm, then we reference it with `existingRate`. `existingRate` is then updated with the API results:\n\n```swift\nif let decodedResponse = try? JSONDecoder().decode(Rate.self, from: data) {\n   DispatchQueue.main.async {\n       if let existingRate = rate {\n           do {\n               let realm = try Realm()\n               try realm.write() {\n                   guard let thawedrate = existingRate.thaw() else {\n                       print(\"Couldn't thaw existingRate\")\n                       return\n                   }\n                   thawedrate.date = decodedResponse.date\n                   thawedrate.result = decodedResponse.result\n               }\n           } catch {\n               print(\"Unable to update existing rate in Realm\")\n           }\n       }\n   }\n}\n```\n\n#### #7 `rate` stale?\n\nThe exchange rates available through the API are updated daily. The date that the rate applies to is included in the API response, and it’s stored in the Realm `Rate` object. When displaying the exchange rate data, [`CurrencyRowDataView`](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Views/CurrencyItem/CurrencyRowDataView.swift) invokes `loadData`:\n\n```swift\nvar body: some View {\n   CurrencyRowView(value: (rate.result) * baseAmount,\n                   symbol: rate.query?.to ?? \"\",\n                   baseValue: $baseAmount,\n                   action: action)\n       .onAppear(perform: loadData)\n}\n```\n\n`loadData` checks that the existing Realm `Rate` object applies to today. If not, then it will refresh the data (stage 4-7):\n\n```swift\nprivate func loadData() {\n   if !rate.isToday {\n       // TODO: 4-7\n   }\n}\n```\n\n`isToday` is a [`Rate`](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Model/Rate.swift) method to check whether the stored data matches the current date:\n\n```swift\nextension Rate {\n   var isToday: Bool {\n       let today = Date().description.prefix(10)\n       return  date == today\n   }\n}\n```\n\n#### #4-7 Fetch `rate` from API — `rate` stale\n\nBy now, the code to fetch the data from the API should be familiar:\n\n```swift\nprivate func loadData() {\n   if !rate.isToday {\n       guard let query = rate.query else {\n           print(\"Query data is missing\")\n           return\n       }\n       guard let url = URL(string: \"https://api.exchangerate.host/convert?from=\\(query.from)&to=\\(query.to)\") else {\n           print(\"Invalid URL\")\n           return\n       }\n       let request = URLRequest(url: url)\n       URLSession.shared.dataTask(with: request) { data, response, error in\n           guard let data = data else {\n               print(\"Error fetching data: \\(error?.localizedDescription ?? \"Unknown error\")\")\n               return\n           }\n           if let decodedResponse = try? JSONDecoder().decode(Rate.self, from: data) {\n               DispatchQueue.main.async {\n                   // TODO: #5.7\n               }\n           } else {\n               print(\"No data received\")\n           }\n       }\n       .resume()\n   }\n}\n```\n\n#### #5-7 StoreInRealm(`rate`) — `rate` stale\n\n`loadData` copies the new `date` and exchange rate (`result`) to the stored Realm `Rate` object:\n\n```swift\nif let decodedResponse = try? JSONDecoder().decode(Rate.self, from: data) {\n   DispatchQueue.main.async {\n       $rate.date.wrappedValue = decodedResponse.date\n       $rate.result.wrappedValue = decodedResponse.result\n   }\n}\n```\n\n#### #8 View rendered with `rate`\n\n[`CurrencyRowView`](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Views/CurrencyItem/CurrencyRowView.swift) receives the raw exchange rate data, and the amount to convert. It’s responsible for calculating and rendering the results:\n\n![Screen capture showing the row for a single currency. In this case it shows the US flag, the \"USD\" symbol and the amount (3.23...)](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Single_Rate_Rendered_84c0c03e6a.png)\n\nThe number shown in this view is part of a `TextField`, which the user can overwrite:\n\n```swift\n@Binding var baseValue: Double\n...\nTextField(\"Amount\", text: $amount)\n   .keyboardType(.decimalPad)\n   .onChange(of: amount, perform: updateValue)\n   .font(.largeTitle)\n```\n\nWhen the user overwrites the number, the `onChange` function is called which recalculates `baseValue` (the value of the base currency that the user wants to convert):\n\n```swift\nprivate func updateValue(newAmount: String) {\n   guard let newValue = Double(newAmount) else {\n       print(\"\\(newAmount) cannot be converted to a Double\")\n       return\n   }\n   baseValue = newValue / rate\n}\n```\n\nAs `baseValue` was passed in as a binding, the new value percolates up the view hierarchy, and all of the currency values are updated. As the exchange rates are held in Realm, all of the currency values are recalculated without needing to use the API:\n\n![Animation showing the app running on an iPhone. When the user changes the amount for 1 currency, the amounts for all of the others changes immediately](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/change_values_fd69a8240b.gif)\n\n## Conclusion\n\nREST APIs let your mobile apps act on a vast variety of cloud data. The downside is that APIs can't help you when you don't have access to the internet. They can also make your app seem sluggish, and your users may get frustrated when they have to wait for data to be downloaded.\n\nA common solution is to use Realm to cache data from the API so that it's always available and can be accessed locally in an instant.\n\nThis article has shown you a typical data lifecycle that you can reuse in your own apps. You've also seen how easy it is to store the JSON results from an API call in your Realm database:\n\n```swift\nif let decodedResponse = try? JSONDecoder().decode(Rate.self, from: data) {\n   DispatchQueue.main.async {\n       $rates.append(decodedResponse)\n   }\n}\n```\n\nWe've focussed on using a read-only API. Things get complicated very quickly when your app starts modifying data through the API. What should your app do when your device is offline?\n\n- Don't allow users to do anything that requires an update?\n- Allow local updates and maintain a list of changes that you iterate through when back online?\n    - Will some changes you accept from the user have to be backed out once back online and you discover conflicting changes from other users?\n\nIf you need to modify data that's accessed by other users or devices, consider [MongoDB Realm Sync](https://docs.mongodb.com/realm/sync/) as an alternative to accessing APIs directly from your app. It will save you thousands of lines of tricky code!\n\nThe API you're using may throttle access or charge per request. You can create a backend MongoDB Realm app to fetch the data from the API just once, and then use Realm Sync to handle the fan-out to all instances of your mobile app.\n\nIf you have any questions or comments on this post (or anything else Realm-related), then please raise them on our [community forum](https://www.mongodb.com/community/forums/c/realm-sdks/58). To keep up with the latest Realm news, follow [@realm](https://twitter.com/realm) on Twitter and join the [Realm global community](https://live.mongodb.com/realm-global-community/).\n","description":"Learn how to make your mobile app always-on, even when you can't connect to your API.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltad43362177b8a88d/644c46acc3e5e73837d1c022/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/realm-api-cache","title":"*Build Offline-First Mobile Apps by Caching API Results in Realm","original_publish_date":"2021-09-13T12:30:00.291Z","strapi_updated_at":"2023-03-06T17:51:17.305Z","expiry_date":"2022-09-08T16:00:09.929Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Code Example","calculated_slug":"/code-examples"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":"ttps://github.com/realm/RCurrency","l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*iOS","calculated_slug":"/technologies/ios"}},{"node":{"title":"*Mobile","calculated_slug":"/technologies/mobile"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt223ac92ad5f5e6cd/644c46aed527e301a817fde9/OG_1200x630.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:20.197Z","publish_details":{"time":"2023-04-28T22:52:55.860Z"}}},{"calculated_slug":"/products/mongodb/mongodb-network-compression","content":"# MongoDB Network Compression: A Win-Win\n\nAn under-advertised feature of MongoDB is its ability to compress data between the client and the server. The CRM company Close has a really [nice article](https://making.close.com/posts/mongodb-network-compression) on how compression reduced their network traffic from about 140 Mbps to 65 Mpbs. As Close notes, with cloud data transfer costs ranging from $0.01 per GB and up, you can get a nice little savings with a simple configuration change. \n\n![mongodb-network-compression-chart](https://raw.githubusercontent.com/wbleonard/mongodb-network-compression/main/img/mongodb-network-compression-chart.webp)\n\nMongoDB supports the following compressors:\n\n* [snappy](https://docs.mongodb.com/manual/reference/glossary/#std-term-snappy)\n* [zlib](https://docs.mongodb.com/manual/reference/glossary/#std-term-zlib) (Available starting in MongoDB 3.6)\n* [zstd](https://docs.mongodb.com/manual/reference/glossary/#std-term-zlib) (Available starting in MongoDB 4.2)\n\nEnabling compression from the [client](https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html) simply involves installing the desired compression library and then passing the compressor as an argument when you connect to MongoDB. For example:\n\n```PYTHON\nclient = MongoClient('mongodb://localhost', compressors='zstd')\n```\n\n\nThis article provides two tuneable Python scripts, [read-from-mongo.py](https://github.com/wbleonard/mongodb-network-compression/blob/main/read-from-mongo.py) and [write-to-mongo.py](https://github.com/wbleonard/mongodb-network-compression/blob/main/write-to-mongo.py), that you can use to see the impact of network compression yourself. \n\n## Setup\n\n### Client Configuration\n\nEdit [params.py](https://github.com/wbleonard/mongodb-network-compression/blob/main/params.py) and at a minimum, set your connection string. Other tunables include the amount of bytes to read and insert (default 10 MB) and the batch size to read (100 records) and insert (1 MB):\n\n``` PYTHON\n# Read to Mongo\ntarget_read_database        = 'sample_airbnb'\ntarget_read_collection      = 'listingsAndReviews'\nmegabytes_to_read           = 10\nbatch_size                  = 100   # Batch size in records (for reads)\n\n# Write to Mongo\ndrop_collection             = True  # Drop collection on run\ntarget_write_database       = 'test'\ntarget_write_collection     = 'network-compression-test'\nmegabytes_to_insert         = 10\nbatch_size_mb               = 1     # Batch size of bulk insert in megabytes\n```\n### Compression Library\nThe [snappy](https://docs.mongodb.com/manual/reference/glossary/#std-term-snappy) compression in Python requires the `python-snappy` package.\n\n```pip3 install python-snappy```\n\nThe [zstd](https://docs.mongodb.com/manual/reference/glossary/#std-term-zlib) compression requires the zstandard package\n\n```pip3 install zstandard```\n\nThe [zlib](https://docs.mongodb.com/manual/reference/glossary/#std-term-zlib) compression is native to Python.\n\n### Sample Data\nMy  [read-from-mongo.py](https://github.com/wbleonard/mongodb-network-compression/blob/main/read-from-mongo.py) script uses the [Sample AirBnB Listings Dataset](https://docs.atlas.mongodb.com/sample-data/sample-airbnb/#std-label-sample-airbnb) but ANY dataset will suffice for this test. \n\nThe [write-to-mongo.py](https://github.com/wbleonard/mongodb-network-compression/blob/main/write-to-mongo.py) script generates sample data using the Python package \n[Faker](https://faker.readthedocs.io/en/master/).\n\n```pip3 install faker ```\n\n## Execution\n### Read from Mongo\nThe cloud providers notably charge for data egress, so anything that reduces network traffic out is a win. \n\nLet's first run the script without network compression (the default):\n\n```ZSH\n✗ python3 read-from-mongo.py\n\nMongoDB Network Compression Test\nNetwork Compression: Off\nNow: 2021-11-03 12:24:00.904843\n\nCollection to read from: sample_airbnb.listingsAndReviews\nBytes to read: 10 MB\nBulk read size: 100 records\n\n1 megabytes read at 307.7 kilobytes/second\n2 megabytes read at 317.6 kilobytes/second\n3 megabytes read at 323.5 kilobytes/second\n4 megabytes read at 318.0 kilobytes/second\n5 megabytes read at 327.1 kilobytes/second\n6 megabytes read at 325.3 kilobytes/second\n7 megabytes read at 326.0 kilobytes/second\n8 megabytes read at 324.0 kilobytes/second\n9 megabytes read at 322.7 kilobytes/second\n10 megabytes read at 321.0 kilobytes/second\n\n 8600 records read in 31 seconds (276.0 records/second)\n\n MongoDB Server Reported Megabytes Out: 188.278 MB\n ```\n\n_You've obviously noticed the reported Megabytes out (188 MB) are more than 18 times our test size of 10 MBs. There are several reasons for this, including other workloads running on the server, data replication to secondary nodes, and the TCP packet being larger than just the data. Focus on the delta between the other tests runs._\n\nThe script accepts an optional compression argument, that must be either `snappy`, `zlib` or `zstd`. Let's run the test again using `snappy`, which is known to be fast, while sacrificing some compression:\n\n```ZSH\n✗ python3 read-from-mongo.py -c \"snappy\"\n\nMongoDB Network Compression Test\nNetwork Compression: snappy\nNow: 2021-11-03 12:24:41.602969\n\nCollection to read from: sample_airbnb.listingsAndReviews\nBytes to read: 10 MB\nBulk read size: 100 records\n\n1 megabytes read at 500.8 kilobytes/second\n2 megabytes read at 493.8 kilobytes/second\n3 megabytes read at 486.7 kilobytes/second\n4 megabytes read at 480.7 kilobytes/second\n5 megabytes read at 480.1 kilobytes/second\n6 megabytes read at 477.6 kilobytes/second\n7 megabytes read at 488.4 kilobytes/second\n8 megabytes read at 482.3 kilobytes/second\n9 megabytes read at 482.4 kilobytes/second\n10 megabytes read at 477.6 kilobytes/second\n\n 8600 records read in 21 seconds (410.7 records/second)\n\n MongoDB Server Reported Megabytes Out: 126.55 MB\n```\nWith `snappy` compression, our reported bytes out were about `62 MBs` fewer. That's a `33%` savings. But wait, the `10 MBs` of data was read in `10` fewer seconds. That's also a `33%` performance boost!\n\nLet's try this again using `zlib`, which can achieve better compression, but at the expense of performance.  \n\n_[zlib](https://docs.mongodb.com/manual/reference/glossary/#std-term-zlib) compression supports an optional compression level. For this test I've set it to `9` (max compression)._\n\n```ZSH\n✗ python3 read-from-mongo.py -c \"zlib\"\n\nMongoDB Network Compression Test\nNetwork Compression: zlib\nNow: 2021-11-03 12:25:07.493369\n\nCollection to read from: sample_airbnb.listingsAndReviews\nBytes to read: 10 MB\nBulk read size: 100 records\n\n1 megabytes read at 362.0 kilobytes/second\n2 megabytes read at 373.4 kilobytes/second\n3 megabytes read at 394.8 kilobytes/second\n4 megabytes read at 393.3 kilobytes/second\n5 megabytes read at 398.1 kilobytes/second\n6 megabytes read at 397.4 kilobytes/second\n7 megabytes read at 402.9 kilobytes/second\n8 megabytes read at 397.7 kilobytes/second\n9 megabytes read at 402.7 kilobytes/second\n10 megabytes read at 401.6 kilobytes/second\n\n 8600 records read in 25 seconds (345.4 records/second)\n\n MongoDB Server Reported Megabytes Out: 67.705 MB\n ```\n With `zlib` compression configured at its maximum compression level, we were able to achieve a `64%` reduction in network egress, although it took 4 seconds longer. However, that's still a `19%` performance improvement over using no compression at all.\n\n Let's run a final test using `zstd`, which is advertised to bring together the speed of `snappy` with the compression efficiency of `zlib`:\n\n ```ZSH\n ✗ python3 read-from-mongo.py -c \"zstd\"\n\nMongoDB Network Compression Test\nNetwork Compression: zstd\nNow: 2021-11-03 12:25:40.075553\n\nCollection to read from: sample_airbnb.listingsAndReviews\nBytes to read: 10 MB\nBulk read size: 100 records\n\n1 megabytes read at 886.1 kilobytes/second\n2 megabytes read at 798.1 kilobytes/second\n3 megabytes read at 772.2 kilobytes/second\n4 megabytes read at 735.7 kilobytes/second\n5 megabytes read at 734.4 kilobytes/second\n6 megabytes read at 714.8 kilobytes/second\n7 megabytes read at 709.4 kilobytes/second\n8 megabytes read at 698.5 kilobytes/second\n9 megabytes read at 701.9 kilobytes/second\n10 megabytes read at 693.9 kilobytes/second\n\n 8600 records read in 14 seconds (596.6 records/second)\n\n MongoDB Server Reported Megabytes Out: 61.254 MB\n ```\nAnd sure enough, `zstd` lives up to its reputation, achieving `68%` percent improvement in compression along with a `55%` improvement in performance!\n\n### Write to Mongo\n\nThe cloud providers often don't charge us for data ingress. However, given the substantial performance improvements with read workloads, what can be expected from write workloads?\n\nThe [write-to-mongo.py](https://github.com/wbleonard/mongodb-network-compression/blob/main/write-to-mongo.py) script writes a randomly generated document to the database and collection configured in [params.py](https://github.com/wbleonard/mongodb-network-compression/blob/main/params.py), the default being `test.network_compression_test`.\n\nAs before, let's run the test without compression:\n\n```ZSH\npython3 write-to-mongo.py\n\nMongoDB Network Compression Test\nNetwork Compression: Off\nNow: 2021-11-03 12:47:03.658036\n\nBytes to insert: 10 MB\nBulk insert batch size: 1 MB\n\n1 megabytes inserted at 614.3 kilobytes/second\n2 megabytes inserted at 639.3 kilobytes/second\n3 megabytes inserted at 652.0 kilobytes/second\n4 megabytes inserted at 631.0 kilobytes/second\n5 megabytes inserted at 640.4 kilobytes/second\n6 megabytes inserted at 645.3 kilobytes/second\n7 megabytes inserted at 649.9 kilobytes/second\n8 megabytes inserted at 652.7 kilobytes/second\n9 megabytes inserted at 654.9 kilobytes/second\n10 megabytes inserted at 657.2 kilobytes/second\n\n 27778 records inserted in 15.0 seconds\n\n MongoDB Server Reported Megabytes In: 21.647 MB\n```\n\nSo it took `15` seconds to write `27,778` records. Let's run the same test with `zstd` compression:\n\n```ZSH\n✗ python3 write-to-mongo.py -c 'zstd'\n\nMongoDB Network Compression Test\nNetwork Compression: zstd\nNow: 2021-11-03 12:48:16.485174\n\nBytes to insert: 10 MB\nBulk insert batch size: 1 MB\n\n1 megabytes inserted at 599.4 kilobytes/second\n2 megabytes inserted at 645.4 kilobytes/second\n3 megabytes inserted at 645.8 kilobytes/second\n4 megabytes inserted at 660.1 kilobytes/second\n5 megabytes inserted at 669.5 kilobytes/second\n6 megabytes inserted at 665.3 kilobytes/second\n7 megabytes inserted at 671.0 kilobytes/second\n8 megabytes inserted at 675.2 kilobytes/second\n9 megabytes inserted at 675.8 kilobytes/second\n10 megabytes inserted at 676.7 kilobytes/second\n\n 27778 records inserted in 15.0 seconds\n\n MongoDB Server Reported Megabytes In: 8.179 MB\n ```\nOur reported megabytes in are reduced by `62%`. However, our write performance remained identical. Personally, I think most of this is due to the time it takes the [Faker](https://faker.readthedocs.io/en/master/) library to generate the sample data. But having gained compression without a performance impact it is still a win.\n## Measurement\n\nThere are a couple of options for measuring network traffic. This script is using the [db.serverStatus()](https://docs.mongodb.com/manual/reference/method/db.serverStatus/) `physicalBytesOut` and `physicalBytesIn`, reporting on the delta between the reading at the start and end of the test run. As mentioned previously, our measurements are corrupted by other network traffic occuring on the server, but my tests have shown a consistent improvement when run. Visually, my results achieved appear as follows:\n\n![megabytes out](https://github.com/wbleonard/mongodb-network-compression/raw/main/img/megabytes_read_graph.png)\n![seconds to read](https://github.com/wbleonard/mongodb-network-compression/raw/main/img/seconds_to_read_graph.png)\n\n\n\nAnother option would be using a network analysis tool like [Wireshark](https://www.wireshark.org/). But that's beyond the scope of this article for now.\n\nBottom line, compression reduces network traffic by more than 60%, which is in line with the improvement seen by Close. More importantly, compression also had a dramatic improvement on read performance. That's a Win-Win.\n\n\n","description":"An under advertised feature of MongoDB is its ability to compress data between the client and the server. This blog will show you exactly how to enable network compression along with a script you can run to see concrete results. Not only will you save some $, but your performance will also likely improve - a true win-win.\n","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltb1437d43a74d831e/644c46b02992e6f5332143e8/mongodb-network-compression-chart_(1).webp?branch=prod","description":null}}]},"slug":"/mongodb-network-compression","title":"*MongoDB Network Compression: A Win-Win","original_publish_date":"2021-11-03T18:31:01.020Z","strapi_updated_at":"2022-10-12T15:08:01.585Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:19.831Z","publish_details":{"time":"2023-04-28T22:52:55.894Z"}}},{"calculated_slug":"/languages/kotlin/realm-startactivityforresult-registerforactivityresult-deprecated-android-kotlin","content":"## Introduction\n\nAndroid has been on the edge of evolution for a while recently, with updates to `androidx.activity:activity-ktx` to `1.2.0`. It has deprecated `startActivityForResult` in favour of `registerForActivityResult`.\n\nIt was one of the first fundamentals that any Android developer has learned, and the backbone of Android's way of communicating between two components. API design was simple enough to get started quickly but had its cons, like how it’s hard to find the caller in real-world applications (except for cmd+F in the project 😂), getting results on the fragment, results missed if the component is recreated, conflicts with the same request code, etc.\n\nLet’s try to understand how to use the new API with a few examples.\n\n## Example 1: Activity A calls Activity B for the result\n\nOld School:\n\n```kotlin\n// Caller \nval intent = Intent(context, Activity1::class.java)\nstartActivityForResult(intent, REQUEST_CODE)\n\n// Receiver \noverride fun onActivityResult(requestCode: Int, resultCode: Int, data: Intent?) {\n    super.onActivityResult(requestCode, resultCode, data)\n    if (resultCode == Activity.RESULT_OK && requestCode == REQUEST_CODE) {\n        val value = data?.getStringExtra(\"input\")\n    }\n}\n```\n\nNew Way:\n\n```kotlin\n\n// Caller \nval intent = Intent(context, Activity1::class.java)\ngetResult.launch(intent)\n\n// Receiver \nprivate val getResult =\n    registerForActivityResult(\n        ActivityResultContracts.StartActivityForResult()\n    ) {\n        if (it.resultCode == Activity.RESULT_OK) {\n            val value = it.data?.getStringExtra(\"input\")\n        }\n    }\n```\n\nAs you would have noticed, `registerForActivityResult` takes two parameters. The first defines the type of action/interaction needed (`ActivityResultContracts`) and the second is a callback function where we receive the result.\n\nNothing much has changed, right? Let’s check another example.\n\n## Example 2: Start external component like the camera to get the image:\n\n```kotlin\n//Caller\ngetPreviewImage.launch(null)\n\n//Receiver \nprivate val getPreviewImage = registerForActivityResult(ActivityResultContracts.TakePicture { bitmap ->\n    // we get bitmap as result directly\n})\n```\n\nThe above snippet is the complete code getting a preview image from the camera. No need for permission request code, as this is taken care of automatically for us!\n\nAnother benefit of using the new API is that it forces developers to use the right contract. For example, with `ActivityResultContracts.TakePicture()` — which returns the full image — you need to pass a `URI` as a parameter to `launch`, which reduces the development time and chance of errors.\n\nOther default contracts available can be found [here](https://developer.android.com/reference/androidx/activity/result/contract/package-summary).\n\n---\n\n## Example 3: Fragment A calls Activity B for the result\n\nThis has been another issue with the old system, with no clean implementation available, but the new API works consistently across activities and fragments. Therefore, we refer and add the snippet from example 1 to our fragments.\n\n---\n\n## Example 4: Receive the result in a non-Android class\n\nOld Way: 😄\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/giphy_f99f638e0c.webp)\n\nWith the new API, this is possible using `ActivityResultRegistry` directly.\n\n```kotlin\nclass MyLifecycleObserver(private val registry: ActivityResultRegistry) : DefaultLifecycleObserver {\n\n    lateinit var getContent: ActivityResultLauncher<String>\n\n    override fun onCreate(owner: LifecycleOwner) {\n        getContent = registry.register(\"key\", owner, GetContent()) { uri ->\n            // Handle the returned Uri\n        }\n    }\n\n    fun selectImage() {\n        getContent.launch(\"image/*\")\n    }\n}\n\nclass MyFragment : Fragment() {\n    lateinit var observer: MyLifecycleObserver\n\n    override fun onCreate(savedInstanceState: Bundle?) {\n        // ...\n\n        observer = MyLifecycleObserver(requireActivity().activityResultRegistry)\n        lifecycle.addObserver(observer)\n    }\n\n    override fun onViewCreated(view: View, savedInstanceState: Bundle?) {\n        val selectButton = view.findViewById<Button>(R.id.select_button)\n\n        selectButton.setOnClickListener {\n            // Open the activity to select an image\n            observer.selectImage()\n        }\n    }\n}\n```\n\n## Summary\n\nI have found the registerForActivityResult useful and clean. Some of the pros, in my opinion, are:\n\n1. Improve the code readability, no need to remember to jump to `onActivityResult()` after `startActivityForResult`.\n\n2. `ActivityResultLauncher` returned from `registerForActivityResult` used to launch components, clearly defining the input parameter for desired results.\n\n3. Removed the boilerplate code for requesting permission from the user. \n\nHope this was informative and enjoyed reading it.\n","description":"Learn the benefits and usage of registerForActivityResult for Android in Kotlin.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt301be517264b838e/644c46b3bad93af30c396f8c/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/realm-startactivityforresult-registerForActivityResult-deprecated-android-kotlin","title":"*StartActivityForResult is Deprecated!","original_publish_date":"2021-09-14T10:27:31.801Z","strapi_updated_at":"2022-05-12T18:04:11.918Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Kotlin","calculated_slug":"/languages/kotlin"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Kotlin","calculated_slug":"/languages/kotlin"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"startActivityForResult Deprecated, Android, Kotlin, Example, StartActivityForResult,","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt06d4fa1952d9d9c9/644c46b53c3aa679a49de876/OG_1200x630.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:19.463Z","publish_details":{"time":"2023-04-28T22:52:55.930Z"}}},{"calculated_slug":"/products/realm/document-swift-powered-frameworks-using-docc","content":"## Introduction\n\nIn the [previous post of this series](https://developer.mongodb.com/how-to/adding-realm-as-dependency-ios-framework/) we added Realm to a really simple [Binary Tree library](https://github.com/mongodb-developer/realm-binary-tree). The idea was to create a Package that, using Realm, allowed you to define binary trees and store them locally.\n\nNow we have that library, but how will anyone know how to use it? We can write a manual, a blog post or FAQs, but luckily Xcode allowed us to add Documentation Comments since forever. And in WWDC 21 Apple announced the new Documentation Compiler, DocC, which takes all our documentation comments and creates a nice, powerful documentation site for our libraries and frameworks.\n\nLet’s try it documenting our library!\n\n\n## Documentation Comments\n\n[Comments](https://docs.swift.org/swift-book/LanguageGuide/TheBasics.html) are part of any language. You use regular comments to explain some specially complicated piece of code, to leave a reason about why some code was constructed in a certain way or just to organise a really big file or function (By the way, if this happens, split it, it’s better). These comments start with `//` for single line comments or with `/*` for block comments.\n\nAnd please, please please don't use comments for things like\n\n```swift\n\n// i is now 0\n\ni = 0\n```\n\nFor example, these are line comments from `Package.swift`:\n\n```swift\n\n// swift-tools-version:5.5\n\n// The swift-tools-version declares the minimum version of Swift required to build this package.\n\n```\n\nWhile this is a regular block comment:\n\n```swift\n\n/*\n  File.swift\n  \n  Created by The Realm Team on 16/6/21.\n*/\n\n```\n\nWe’ve had Documentation Comments in Swift (and Objective C) since forever. This is how they look:\n\n```swift\n/// Single-line documentation comment starts with three /\n```\n\n```swift\n/**\n  Multi-line documentation\n  Comment\n  Block starts with /**\n*/\n```\n\nThese are similar in syntax, although have two major differences:\n\n\n\n* You can write [Markup](https://developer.apple.com/library/archive/documentation/Xcode/Reference/xcode_markup_formatting_ref/index.html#//apple_ref/doc/uid/TP40016497-CH2-SW1) in documentation comments and Xcode will render it\n* These comments explain _what_ something is and _how it’s used_, not how it’s coded.\n\nDocumentation comments are perfect to explain what that class does, or how to use this function. If you can’t put it in plain words, probably you don’t understand what they do and need to think about it a bit more. Also, having clearly stated what a function receives as parameters, what returns, edge cases and possible side effects helps you a lot while writing unit tests. Is simple to test something that you’ve just written how it should be used, what behaviour will exhibit and which values should return.  \n\n\n## DocC\n\nApple announced the Documentation Compiler, DocC, during WWDC21. This new tool, integrated with Xcode 13 allows us to generate a Documentation bundle that can be shared, with beautiful web pages containing all our symbols (classes, structs, functions, etc.)\n\nWith DocC we can generate documentation for our libraries and frameworks. It won’t work for Apps, as the idea of these comments is to explain how to use a piece of code and that works perfectly with libraries.\n\nDocC allows for much more than just generating a web site from our code. It can host tutorials, and any pages we want to add. Let’s try it!\n\n\n## Generating Documentation with DocC\n\nFirst, grab the code for the Realm Binary Tree library from [this repository](https://github.com/mongodb-developer/realm-binary-tree). In order to do that, run the following commands from a Terminal:\n\n```bash\n$ git clone https://github.com/mongodb-developer/realm-binary-tree\n$ cd realm-binary-tree\n```\n\nIf you want to follow along and make these changes, just checkout the tag `initial-state` with `git checkout initial-state`.\n\nThen open the project by double clicking on the `Package.swift` file. Once Xcode ends getting all necessary dependencies (`Realm-Swift` is the main one) we can generate the documentation clicking in the menu option `Product > Build Documentation` or the associated keyboard shortcut `⌃⇧⌘D`. This will open the Documentation Browser with our library’s documentation in it.\n\n![Xcode’s Documentation Browser showing the BinaryTree Framework documentation.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/docc_post_first_doc_generated_b798191fb4.png)\n\nAs we can see, all of our public symbols (in this case the `BinaryTree` class and `TreeTraversable` protocol are there, with their documentation comments nicely showing. This is how it looks for `TreeTraversable::mapInOrder(tree:closure:)`\n\n![Xcode Documentation Browser showing details for the mapInOrder function](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/docc_post_tree_traversable_b8f6c1ef83.png)\n\n## Adding an About Section and Articles\n\nThis is nice, but Xcode 13 now allows us to create a new type of file: a **Documentation Catalog**. This can host Articles, Tutorials and Images. Let’s start by selecting the `Sources > BinaryTree` folder and typing ⌘N to add a new File. Then scroll down to the Documentation section and select `Documentation Catalog`. Give it the name `BinaryTree.docc`. We can rename this resource later as any other file/group in Xcode. We want a name that identifies it clearly when we create an exported documentation package.\n\n![Xcode “Choose a template for your new file” window opened with “Documentation Catalog” selected](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/docc_post_adding_doc_catalog_e97b3b0b2b.png)\n\nLet’s start by renaming the `Documentation.md` file into `BinaryTree.md`. As this has the same name as our Doc Package, everything we put inside this file will appear in the Documentation node of the Framework itself.\n\nWe can add images to our Documentation Catalog simply by dragging them into `Resources`. Then, we can reference those images using the usual Markdown syntax `![](/image-name.png)`. This is how our framework’s main page look like now:\n\n![Documentation Browser showing the BinaryTree main page with an image of a tree, in pixel art style](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/docc_post_main_page_f1e7d0f21e.png)\n\nInside this documentation package we can add Articles. Articles are just Markdown pages where we can explain a subject in written longform. Select the Documentation Package `BinaryTree.docc` and add a new file, using ⌘N. Choose `Article File` from `Documentation`. A new Markdown file will be created. Now write your awesome content to explain how your library works, some concepts you need to know before using it, etc.\n\n\n## Tutorials\n\nTutorials are step by step instructions on how to use your library or framework. Here you can explain, for example, how to initialize a class that needs several parameters injected when calling the `init` method, or how a certain threading problem can be handled. \n\nIn our case, we want to explain how we can create a Tree, and how we can traverse it.\n\n![Tutorial main page showing a header with a pixel art tree image and two Articles at the bottom.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/docc_post_tutorial_index_28b12d8b55.png)\n\nSo first we need a Tutorial File. Go to your `Tutorials` folder and create a new File. Select Documentation > Tutorial File. A Tutorial file describes the steps in a tutorial, so while you scroll through it related code appears, as you can see here in action.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/docc_post_tutorial_b4d558e04a.gif\" alt=\"Animated GIF showing how as you scroll down steps, new code appears to the right of the Article\" style=\"width:100%\"/>\n\nWe need two things: our code snippets and the tutorial file. The tutorial file looks like this:\n\n```swift\n@Tutorial(time: 5) {\n    @Intro(title: \"Creating Trees\") {\n        How to create Trees\n\n        @Image(source: seed-tree.jpg, alt: \"This is an image of a Tree\")\n    }\n\n    @Section(title: \"Creating trees\") {\n        @ContentAndMedia() {\n            Let's create some trees\n        }\n\n        @Steps {\n            @Step {\n                Import `BinaryTree`                \n                @Code(name: \"CreateTree.swift\", file: 01-create-tree.swift)\n            }\n\n            @Step {\n                Create an empty Tree object\n                @Code(name: \"CreateTree.swift\", file: 02-create-tree.swift)\n            }\n\n            @Step {\n                Add left and right children. These children are also of type `RealmBinaryTree`\n                @Code(name: \"CreateTree.swift\", file: 03-create-tree.swift)\n            }\n        }\n    }\n}\n``` \n\nAs you can see, we have a first `@Tutorial(time: 5)` line where we put the estimated time to complete this tutorial. Then some introduction text and images, and one `@Section`. We can create as many sections as we need. When the documentation is rendered they’ll correspond to a new page of the tutorial and can be selected from a dropdown picker. As a tutorial is a step-by-step explanation, we now add each and every step, that will have some text that will tell you what the code will do and the code itself you need to enter. \n\nThat code is stored in Resources > code as regular Swift files. So if you have 5 steps you’ll need five files. Each step will show what’s in the associated snippet file, so to make it appear as you advance one step should include the previous step’s code. My approach to code snippets is to do it backwards: first I write the final snippet with the complete sample code, then I duplicate it as many times as steps I have in this tutorial, finally I delete code in each file as needed.\n\n\n## Recap\n\nIn this post we’ve seen how to add developer documentation to our code, how to generate a DocC package including sample code and tutorials.\n\nThis will help us explain to others how to use our code, how to test it, its limitations and a better understanding of our own code. Explaining how something works is the quickest way to master it.\n\nIn the next post we’ll have a look at how we can host this package online!\n\nIf you have any questions or comments on this post (or anything else Realm-related), then please raise them on our [community forum\\(https://www.mongodb.com/community/forums/c/realm-sdks/58). To keep up with the latest Realm news, follow [@realm](https://twitter.com/realm) on Twitter and join the [Realm global community](https://live.mongodb.com/realm-global-community/).\n\n## Reference Materials\n\n\n### Sample Repo\n\nSource code repo: [https://github.com/mongodb-developer/realm-binary-tree](https://github.com/mongodb-developer/realm-binary-tree) \n\n\n### Apple DocC documentation\n\n[Documentation about DocC](https://developer.apple.com/documentation/xcode#Overview)\n\n\n### WWDC21 Videos\n\n* [Meet DocC documentation in Xcode](https://developer.apple.com/videos/play/wwdc2021/10166/)\n* [Build interactive tutorials using DocC](https://developer.apple.com/videos/play/wwdc2021/10235/)\n* [Elevate your DocC documentation in Xcode](https://developer.apple.com/videos/play/wwdc2021/10167)\n* [Host and automate your DocC documentation](https://developer.apple.com/videos/play/wwdc2021/10236)\n","description":"Learn how to use the new Documentation Compiler from Apple, DocC, to create outstanding tutorials, how-tos and explain how your Frameworks work.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt7b81196ffe8cec6a/644c46bc118e7560f3ebb63e/docc-post-main-page.png?branch=prod","description":null}}]},"slug":"/document-swift-powered-frameworks-using-docc","title":"*Document our Realm-Powered Swift Frameworks using DocC","original_publish_date":"2021-09-22T09:54:16.482Z","strapi_updated_at":"2022-05-13T13:06:43.077Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:18.699Z","publish_details":{"time":"2023-04-28T22:52:55.959Z"}}},{"calculated_slug":"/code-examples/kotlin/splash-screen-android","content":"> In this article, we will explore and learn how to build a splash screen with SplashScreen API, which was introduced in Android 12.\n\n<br>\n\n## What is a Splash Screen?\n\nIt is the first view that is shown to a user as soon as you tap on the app icon. If you notice a blank white screen (for\na short moment) after tapping on your favourite app, it means it doesn't have a splash screen.\n\n## Why/When Do I Need It?\n\nOften, the splash screen is seen as a differentiator between normal and professional apps. Some use cases where a splash\nscreen fits perfectly are:\n\n* When we want to download data before users start using the app.\n* If we want to promote app branding and display your logo for a longer period of time, or just have a more immersive\n  experience that smoothly takes you from the moment you tap on the icon to whatever the app has to offer.\n\nUntil now, creating a splash screen was never straightforward and always required some amount of boilerplate code added\nto the application, like creating SplashActivity with no view, adding a timer for branding promotion purposes, etc. With\nSplashScreen API, all of this is set to go.\n\n## Show Me the Code\n\n### Step 1: Creating a Theme\n\nEven for the new `SplashScreen` API, we need to create a theme but in the `value-v31` folder as a few parameters are\nsupported only in **Android 12**. Therefore, create a folder named `value-v31` under `res` folder and add `theme.xml`\nto it.\n\nAnd before that, let’s break our splash screen into pieces for simplification.\n\n![Splash Screen placeholder](https://developer.android.com/about/versions/12/images/splash-screen-composition.png)\n\n* Point 1 represents the icon of the screen.\n* Point 2 represents the background colour of the splash screen icon.\n* Point 3 represents the background colour of the splash screen.\n* Point 4 represents the space for branding logo if needed.\n\nNow, let's assign some values to the corresponding keys that describe the different pieces of the splash screen.\n\n```xml\n\n<style name=\"SplashActivity\" parent=\"Theme.AppCompat.NoActionBar\">\n\n    <!-- Point 3-->\n    <item name=\"android:windowSplashScreenBackground\">#FFFFFF</item>\n\n    <!-- Point 2-->\n    <item name=\"android:windowSplashScreenIconBackgroundColor\">#000000</item>\n\n    <!-- Point 1-->\n    <item name=\"android:windowSplashScreenAnimatedIcon\">@drawable/ic_realm_logo_250</item>\n\n    <!-- Point 4-->\n    <item name=\"android:windowSplashScreenBrandingImage\">@drawable/relam_horizontal</item>\n</style>\n```\n\n<br/>\nIn case you want to use an app icon (or don't have a separate icon) as `windowSplashScreenAnimatedIcon`, you ignore this\nparameter and by default, it will take your app icon.\n\n> **Tips & Tricks**: If your drawable icon is getting cropped on the splash screen, create an app icon from the image\n> and then replace the content of `windowSplashScreenAnimatedIcon` drawable with the `ic_launcher_foreground.xml`.\n>\n> For `windowSplashScreenBrandingImage`, I couldn't find any alternative. Do share in the comments if you find one.\n\n### Step 2: Add the Theme to Activity\n\nOpen AndroidManifest file and add a theme to the activity.\n\n``` xml\n<activity\n android:theme=\"@style/SplashActivity\">\n</activity>\n```\n\nIn my view, there is no need for a new `activity` class for the splash screen, which traditionally was required. And now\nwe are all set for the new **Android 12** splash screen.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screenshot_1632115543_4547ee45af.png\" alt=\"drawing\" width=\"200\"/>\n\nAdding animation to the splash screen is also a piece of cake. Just update the icon drawable with\n`AnimationDrawable` and `AnimatedVectorDrawable` drawable and custom parameters for the duration of the animation.\n\n```xml\n\n<item name=\"android:windowSplashScreenAnimationDuration\">1000</item>\n```\n\nEarlier, I mentioned that the new API helps with the initial app data download use case, so let's see that in action.\n\nIn the splash screen activity, we can register for `addOnPreDrawListener` listener which will help to hold off the first\ndraw on the screen, until data is ready.\n\n``` Kotlin\n private val viewModel: MainViewModel by viewModels()\n\n override fun onCreate(savedInstanceState: Bundle?) {\n     super.onCreate(savedInstanceState)\n     addInitialDataListener()\n     loadAppView()\n }\n\n private fun addInitialDataListener() {\n     val content: View = findViewById(android.R.id.content)\n     // This would be called until true is not returned from the condition\n     content.viewTreeObserver.addOnPreDrawListener {\n         return@addOnPreDrawListener viewModel.isAppReady.value ?: false\n     }\n }\n\n private fun loadAppView() {\n     binding = ActivityMainBinding.inflate(layoutInflater)\n     setContentView(binding.root)\n```\n\n<br/>\n\n> **Tips & Tricks**: While developing Splash screen you can return `false` for `addOnPreDrawListener`, so the next screen is not rendered and you can validate the splash screen easily.\n\n### Summary\n\nI really like the new `SplashScreen` API, which is very clean and easy to use, getting rid of SplashScreen activity\naltogether. There are a few things I disliked, though.\n\n1. The splash screen background supports only single colour. We're waiting for support of vector drawable backgrounds.\n2. There is no design spec available for icon and branding images, which makes for more of a hit and trial game. I still\n   couldn't fix the banding image, in my example.\n3. Last but not least, SplashScreen UI side feature(`theme.xml`) is only supported from Android 12 and above, so we\n   can't get rid of the old code for now.\n\nYou can also check out the complete working example from my GitHub repo. Note: Just running code on the device will show\nyou white. To see the example, close the app recent tray and then click on the app icon again.\n\n[Github Repo link](https://github.com/mongodb-developer/SplashScreen-Android)\n\nHope this was informative and enjoyed reading it.\n\n","description":"In this article, we will explore and learn how to build a splash screen with SplashScreen API, which was introduced in Android 12.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt66e06eba9656a31c/644c46becc5633821e147e18/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/splash-screen-android","title":"*Building Splash Screen Natively, Android 12, Kotlin","original_publish_date":"2021-09-23T13:13:44.644Z","strapi_updated_at":"2022-05-12T18:03:22.042Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Kotlin","calculated_slug":"/languages/kotlin"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":"Snippet","content_typeConnection":{"edges":[{"node":{"title":"*Code Example","calculated_slug":"/code-examples"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":"https://github.com/mongodb-developer/SplashScreen-Android","l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Kotlin","calculated_slug":"/languages/kotlin"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Android","calculated_slug":"/technologies/android"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt6403bc24380175ee/644c46c027dc2b6735cc9133/OG_1200x630.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:18.228Z","publish_details":{"time":"2023-04-28T22:52:55.994Z"}}},{"calculated_slug":"/products/realm/realm-ios15-swiftui","content":"## Introduction\n\nI'm all-in on using SwiftUI to build iOS apps. I find it so much simpler than wrangling with storyboards and UIKit. Unfortunately, there are still occasions when SwiftUI doesn't let you do what you need—forcing you to break out into UIKit.\n\nThat's why I always focus on Apple's SwiftUI enhancements at each year's WWDC. And, each year I'm rewarded with a few more enhancements that make SwiftUI more powerful and easy to work with. For example, iOS14 made it much [easier to work with Apple Maps](https://www.mongodb.com/developer/how-to/realm-swiftui-maps-location/).\n\nWWDC 2021 was no exception, introducing a raft of [SwiftUI enhancements that were coming in iOS 15/ SwiftUI 3 / Xcode 13](https://developer.apple.com/videos/play/wwdc2021/10018/). As iOS 15 has now been released, it feels like a good time to cover the features that I've found the most useful.\n\nI've revisited some of my existing iOS apps to see how I could exploit the new iOS 15 SwiftUI features to improve the user experience and/or simplify my code base. This article steps through the features I found most interesting/useful, and how I tested them out on my apps. These are the apps/branches that I worked with:\n\n- [RCurrency](https://github.com/realm/RCurrency/)\n- [RChat](https://github.com/realm/RChat/)\n- [LiveTutorial2021](https://github.com/mongodb-developer/LiveTutorial2021/)\n- [task-tracker-swiftui](https://github.com/realm/task-tracker-swiftui/)\n\n## Prerequisites\n\n- Xcode 13\n- iOS 15\n- [Realm-Cocoa](https://github.com/realm/realm-cocoa/releases) (varies by app, but 10.13.0+ is safe for them all)\n\n## Lists\n\nSwiftUI `List`s are pretty critical to data-based apps. I use `List`s in almost every iOS app I build, typically to represent objects stored in Realm. That's why I always go there first when seeing what's new.\n\n### Custom Swipe Options\n\nWe've all used mobile apps where you swipe an item to the left for one action, and to the right for another. SwiftUI had a glaring omission—the only supported action was to swipe left to delete an item.\n\nThis was a massive pain.\n\nThis limitation meant that my [task-tracker-swiftui](https://github.com/realm/task-tracker-swiftui/) app had a cumbersome UI. You had to click on a task to expose a sheet that let you click on your preferred action.\n\nWith iOS 15, I can replace that popup sheet with swipe actions:\n\n![iOS app showing that action buttons are revealed when swiping a list item to the left or right](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/swipe_action_b291b24533.gif)\n\nThe swipe actions are implemented in [`TasksView`](https://github.com/realm/task-tracker-swiftui/blob/main/task-tracker-swiftui/Views/Projects%20%26%20Tasks/TasksView.swift):\n\n```swift\nList {\n   ForEach(tasks) { task in\n       TaskView(task: task)\n           .swipeActions(edge: .leading) {\n               if task.statusEnum == .Open || task.statusEnum == .InProgress {\n                   CompleteButton(task: task)\n               }\n               if task.statusEnum == .Open || task.statusEnum == .Complete {\n                   InProgressButton(task: task)\n               }\n               if task.statusEnum == .InProgress || task.statusEnum == .Complete {\n                   NotStartedButton(task: task)\n               }\n           }\n           .swipeActions(edge: .trailing) {\n               Button(role: .destructive, action: { $tasks.remove(task) }) {\n                   Label(\"Delete\", systemImage: \"trash\")\n               }\n           }\n   }\n}\n```\n\nThe role of the delete button is set to `.destructive` which automatically sets the color to red.\n\nFor the other actions, I created custom buttons. For example, this is the code for [`CompleteButton`](https://github.com/realm/task-tracker-swiftui/blob/main/task-tracker-swiftui/Views/Projects%20%26%20Tasks/Buttons/CompleteButton.swift):\n\n\n```swift\nstruct CompleteButton: View {\n   @ObservedRealmObject var task: Task\n\n   var body: some View {\n       Button(action: { $task.statusEnum.wrappedValue = .Complete }) {\n           Label(\"Complete\", systemImage: \"checkmark\")\n       }\n       .tint(.green)\n   }\n}\n```\n\n### Searchable Lists\n\nWhen you're presented with a long list of options, it helps the user if you offer a way to filter the results.\n\n[RCurrency](https://github.com/realm/RCurrency/) lets the user choose between 150 different currencies. Forcing the user to scroll through the whole list wouldn't make for a good experience. A search bar lets them quickly jump to the items they care about:\n\n![Animation showing currencies being filtered as a user types into the search box](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/search_d5cdac1c86.gif)\n\nThe selection of the currency is implemented in the [`SymbolPickerView`](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Views/SymbolPickerView.swift) view.\n\nThe view includes a state variable to store the `searchText` (the characters that the user has typed) and a `searchResults` computed value that uses it to filter the full list of symbols:\n\n```swift\nstruct SymbolPickerView: View {\n   ...\n   @State private var searchText = \"\"\n   ...\n   var searchResults: Dictionary<String, String> {\n       if searchText.isEmpty {\n           return Symbols.data.symbols\n       } else {\n           return Symbols.data.symbols.filter {\n               $0.key.contains(searchText.uppercased()) || $0.value.contains(searchText)}\n       }\n   }\n}\n```\n\nThe `List` then loops over those `searchResults`. We add the `.searchable` modifier to add the search bar, and bind it to the `searchText` state variable:\n\n```swift\nList {\n   ForEach(searchResults.sorted(by: <), id: \\.key) { symbol in\n       ...\n   }\n}\n.searchable(text: $searchText)\n```\n\nThis is the full view:\n\n```swift\nstruct SymbolPickerView: View {\n   @Environment(\\.presentationMode) var presentationMode\n\n   var action: (String) -> Void\n   let existingSymbols: [String]\n\n   @State private var searchText = \"\"\n\n   var body: some View {\n       List {\n           ForEach(searchResults.sorted(by: <), id: \\.key) { symbol in\n               Button(action: {\n                   pickedSymbol(symbol.key)\n               }) {\n                   HStack {\n                       Image(symbol.key.lowercased())\n                       Text(\"\\(symbol.key): \\(symbol.value)\")\n                   }\n                   .foregroundColor(existingSymbols.contains(symbol.key) ? .secondary : .primary)\n               }\n               .disabled(existingSymbols.contains(symbol.key))\n           }\n       }\n       .searchable(text: $searchText)\n       .navigationBarTitle(\"Pick Currency\", displayMode: .inline)\n   }\n\n   private func pickedSymbol(_ symbol: String) {\n       action(symbol)\n       presentationMode.wrappedValue.dismiss()\n   }\n\n   var searchResults: Dictionary<String, String> {\n       if searchText.isEmpty {\n           return Symbols.data.symbols\n       } else {\n           return Symbols.data.symbols.filter {\n               $0.key.contains(searchText.uppercased()) || $0.value.contains(searchText)}\n       }\n   }\n}\n```\n\n## Pull to Refresh\n\nWe've all used this feature in iOS apps. You're impatiently waiting on an important email, and so you drag your thumb down the page to get the app to check the server.\n\nThis feature isn't always helpful for apps that use Realm and Atlas Device Sync. When the Atlas cloud data changes, the local realm is updated, and your SwiftUI view automatically refreshes to show the new data.\n\nHowever, the feature **is** useful for the RCurrency app. I can use it to refresh all of the locally-stored exchange rates with fresh data from the API:\n\n![Animation showing currencies being refreshed when the screen is dragged dowm](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/pull_to_refresh_b37e5a81b9.gif)\n\nWe allow the user to trigger the refresh by adding a `.refreshable` modifier and action (`refreshAll`) to the list of currencies in [`CurrencyListContainerView`](https://github.com/realm/RCurrency/blob/main/iOS-SwiftUI/RCurrency/RCurrency/Views/CurrencyList/CurrencyListContainerView.swift):\n\n```swift\nList {\n   ForEach(userSymbols.symbols, id: \\.self) { symbol in\n       CurrencyRowContainerView(baseSymbol: userSymbols.baseSymbol,\n                                   baseAmount: $baseAmount,\n                                   symbol: symbol,\n                                   refreshNeeded: refreshNeeded)\n           .listRowSeparator(.hidden)\n   }\n   .onDelete(perform: deleteSymbol)\n}\n.refreshable{ refreshAll() }\n```\n\nIn that code snippet, you can see that I added the `.listRowSeparator(.hidden)` modifier to the `List`. This is another iOS 15 feature that hides the line that would otherwise be displayed between each `List` item. Not a big feature, but every little bit helps in letting us use native SwiftUI to get the exact design we want.\n\n## Text\n### Markdown\n\nI'm a big fan of [Markdown](https://daringfireball.net/projects/markdown/). Markdown lets you write formatted text (including tables, links, and images) without taking your hands off the keyboard. I added this post to our CMS in markdown.\n\niOS 15 allows you to render markdown text within a `Text` view. If you pass a literal link to a `Text` view, then it's automatically rendered correctly:\n\n```swift\nstruct MarkDownTest: View {\n   var body: some View {\n       Text(\"Let's see some **bold**, *italics* and some ***bold italic text***. ~~Strike that~~. We can even include a [link](https://realm.io).\")\n   }\n}\n```\n\n![Text formatted. Included bold, italics and a link](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/markdown1_718e4edcae.png)\n\nBut, it doesn't work out of the box for string constants or variables (e.g., data read from Realm):\n\n```swift\nstruct MarkDownTest: View {\n   let myString = \"Let's see some **bold**, *italics* and some ***bold italic text***. ~~Strike that~~. We can even include a [link](https://realm.io).\"\n\n   var body: some View {\n       Text(myString)\n   }\n}\n```\n\n![Raw Markdown source code, rather than rendered text](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/markdown2_be6d3e6123.png)\n\nThe issue is that the version of `Text` that renders markdown expects to be passed an `AttributedString`. I created this simple [`Markdown`](https://github.com/realm/RChat/blob/main/RChat-iOS/RChat/Views/Components/MarkDown.swift) view to handle this for us:\n\n```swift\nstruct MarkDown: View {\n   let text: String\n\n   @State private var formattedText: AttributedString?\n\n   var body: some View {\n       Group {\n           if let formattedText = formattedText {\n               Text(formattedText)\n           } else {\n               Text(text)\n           }\n       }\n       .onAppear(perform: formatText)\n   }\n\n   private func formatText() {\n       do {\n           try formattedText = AttributedString(markdown: text)\n       } catch {\n           print(\"Couldn't convert this from markdown: \\(text)\")\n       }\n   }\n}\n```\n\nI updated the [`ChatBubbleView`](https://github.com/realm/RChat/blob/main/RChat-iOS/RChat/Views/Chat%20Messages/ChatBubbleView.swift) in RChat to use the `Markdown` view:\n\n```swift\nif chatMessage.text != \"\" {\n   MarkDown(text: chatMessage.text)\n   .padding(Dimensions.padding)\n}\n```\n\nRChat now supports markdown in user messages:\n\n![Animation showing that Markdown source is converted to formated text in the RChat app](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/markdown_in_R_Chat_b6d3a01409.gif)\n\n### Dates\n\nWe all know that working with dates can be a pain. At least in iOS 15 we get some nice new functionality to control how we display dates and times. We use the new `Date.formatted` syntax. \n\nIn RChat, I want the date/time information included in a chat bubble to depend on how recently the message was sent. If a message was sent less than a minute ago, then I care about the time to the nearest second. If it were sent a day ago, then I want to see the day of the week plus the hour and minutes. And so on.\n\nI created a [`TextDate`](https://github.com/realm/RChat/blob/main/RChat-iOS/RChat/Views/Components/TextDate.swift) view to perform this conditional formatting:\n\n```swift\nstruct TextDate: View {\n   let date: Date\n\n   private var isLessThanOneMinute: Bool { date.timeIntervalSinceNow > -60 }\n   private var isLessThanOneDay: Bool { date.timeIntervalSinceNow > -60 * 60 * 24 }\n   private var isLessThanOneWeek: Bool { date.timeIntervalSinceNow > -60 * 60 * 24 * 7}\n   private var isLessThanOneYear: Bool { date.timeIntervalSinceNow > -60 * 60 * 24 * 365}\n\n   var body: some View {\n       if isLessThanOneMinute {\n           Text(date.formatted(.dateTime.hour().minute().second()))\n       } else {\n           if isLessThanOneDay {\n               Text(date.formatted(.dateTime.hour().minute()))\n           } else {\n               if isLessThanOneWeek {\n                   Text(date.formatted(.dateTime.weekday(.wide).hour().minute()))\n               } else {\n                   if isLessThanOneYear {\n                       Text(date.formatted(.dateTime.month().day()))\n                   } else {\n                       Text(date.formatted(.dateTime.year().month().day()))\n                   }\n               }\n           }\n       }\n   }\n}\n```\n\nThis preview code lets me test it's working in the Xcode Canvas preview:\n\n```swift\nstruct TextDate_Previews: PreviewProvider {\n   static var previews: some View {\n       VStack {\n           TextDate(date: Date(timeIntervalSinceNow: -60 * 60 * 24 * 365)) // 1 year ago\n           TextDate(date: Date(timeIntervalSinceNow: -60 * 60 * 24 * 7))   // 1 week ago\n           TextDate(date: Date(timeIntervalSinceNow: -60 * 60 * 24))       // 1 day ago\n           TextDate(date: Date(timeIntervalSinceNow: -60 * 60))            // 1 hour ago\n           TextDate(date: Date(timeIntervalSinceNow: -60))                 // 1 minute ago\n           TextDate(date: Date())                                          // Now\n       }\n   }\n}\n```\n\n![Screen capture of dates rendered in various formatt](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dates_9f872c74b7.png)\n\nWe can then use `TextDate` in RChat's [`ChatBubbleView`](https://github.com/realm/RChat/blob/main/RChat-iOS/RChat/Views/Chat%20Messages/ChatBubbleView.swift) to add context-sensitive date and time information:\n\n```swift\nTextDate(date: chatMessage.timestamp)\n   .font(.caption)\n```\n\n![Screen capture of properly formatted dates against each chat message in the RChat app](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/rchat_dates_034fae5600.png)\n\n## Keyboards\n\nCustomizing keyboards and form input was a real pain in the early days of SwiftUI—take a look at the work we did for the [WildAid O-FISH](https://github.com/WildAid/o-fish-ios/tree/main/o-fish-ios/Views/Components/KeyboardControlling) app if you don't believe me. Thankfully, iOS 15 has shown some love in this area. There are a couple of features that I could see an immediate use for...\n\n### Submit Labels\n\nIt's now trivial to rename the on-screen keyboard's \"return\" key. It sounds trivial, but it can give the user a big hint about what will happen if they press it.\n\nTo rename the return key, add a [`.submitLabel`](https://developer.apple.com/documentation/swiftui/form/submitlabel(_:)) modifier to the input field. You pass the modifier one of these values:\n\n- `done`\n- `go`\n- `send`\n- `join`\n- `route`\n- `search`\n- `return`\n- `next`\n- `continue`\n\nI decided to use these labels to improve the login flow for the [LiveTutorial2021](https://github.com/mongodb-developer/LiveTutorial2021) app. In [`LoginView`](https://github.com/mongodb-developer/LiveTutorial2021/blob/main/iOS/LiveChat/LiveChat/Views/LoginView.swift), I added a `submitLabel` to both the \"email address\" and \"password\" `TextFields`:\n\n```swift\nTextField(\"email address\", text: $email)\n   .submitLabel(.next)\nSecureField(\"password\", text: $password)\n   .onSubmit(userAction)\n   .submitLabel(.go)\n```\n\n![Screen capture showing that the \"return\" key is replaced with \"next\" when editing the email/username field](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/email_field_6245860cb5.png)\n\n![Screen capture showing that the \"return\" key is replaced with \"go\" when editing the password field](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/password_field_924329af36.png)\n\nNote the `.onSubmit(userAction)` modifier on the password field. If the user taps \"go\" (or hits return on an external keyboard), then the `userAction` function is called. `userAction` either registers or logs in the user, depending on whether \"Register new user” is checked.\n\n### Focus\n\nIt can be tedious to have to click between different fields on a form. iOS 15 makes it simple to automate that shifting focus.\n\nSticking with [LiveTutorial2021](https://github.com/mongodb-developer/LiveTutorial2021/), I want the \"email address\" field to be selected when the view opens. When the user types their address and hits ~~\"return\"~~ \"next\", focus should move to the \"password\" field. When the user taps \"go,\" the app logs them in.\n\nYou can use the new [`FocusState`](https://developer.apple.com/documentation/swiftui/focusstate) SwiftUI property wrapper to create variables to represent the placement of focus in the view. It can be a boolean to flag whether the associated field is in focus. In our login view, we have two fields that we need to switch focus between and so we use the `enum` option instead.\n\nIn [`LoginView`](https://github.com/mongodb-developer/LiveTutorial2021/blob/main/iOS/LiveChat/LiveChat/Views/LoginView.swift), I define the `Field` enumeration type to represent whether the username (email address) or password is in focus. I then create the `focussedField` `@FocusState` variable to store the value using the `Field` type:\n\n```swift\nenum Field: Hashable {\n   case username\n   case password\n}\n\n@FocusState private var focussedField: Field?\n```\n\nI use the `.focussed` modifier to bind `focussedField` to the two fields:\n\n```swift\nTextField(\"email address\", text: $email)\n   .focused($focussedField, equals: .username)\n   ...\nSecureField(\"password\", text: $password)\n    .focused($focussedField, equals: .password)\n   ...\n```\n\nIt's a two-way binding. If the user selects the email field, then `focussedField` is set to `.username`. If the code sets `focussedField` to `.password`, then focus switches to the password field.\n\nThis next step feels like a hack, but I've not found a better solution yet. When the view is loaded, the code waits half a second before setting focus to the username field. Without the delay, the focus isn't set:\n\n```swift\nVStack(spacing: 16) {\n   ...\n}\n.onAppear {\n   DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {\n       focussedField = .username\n\t...\n   }\n}\n```\n\nThe final step is to shift focus to the password field when the user hits the \"next\" key in the username field:\n\n```swift\nTextField(\"email address\", text: $email)\n   .onSubmit { focussedField = .password }\n   ...\n```\n\nThis is the complete body from [`LoginView`](https://github.com/mongodb-developer/LiveTutorial2021/blob/main/iOS/LiveChat/LiveChat/Views/LoginView.swift):\n\n```swift\nvar body: some View {\n   VStack(spacing: 16) {\n       Spacer()\n       TextField(\"email address\", text: $email)\n           .focused($focussedField, equals: .username)\n           .submitLabel(.next)\n           .onSubmit { focussedField = .password }\n       SecureField(\"password\", text: $password)\n           .focused($focussedField, equals: .password)\n           .onSubmit(userAction)\n           .submitLabel(.go)\n       Button(action: { newUser.toggle() }) {\n           HStack {\n               Image(systemName: newUser ? \"checkmark.square\" : \"square\")\n               Text(\"Register new user\")\n               Spacer()\n           }\n       }\n       Button(action: userAction) {\n           Text(newUser ? \"Register new user\" : \"Log in\")\n       }\n       .buttonStyle(.borderedProminent)\n       .controlSize(.large)\n       Spacer()\n   }\n   .onAppear {\n       DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {\n           focussedField = .username\n       }\n   }\n   .padding()\n}\n```\n\n## Buttons\n### Formatting\n\nPreviously, I've created custom SwiftUI views to make buttons look like…. buttons.\n\nThings get simpler in iOS 15.\n\nIn [`LoginView`](https://github.com/mongodb-developer/LiveTutorial2021/blob/main/iOS/LiveChat/LiveChat/Views/LoginView.swift), I added two new modifiers to my register/login button:\n\n```swift\nButton(action: userAction) {\n   Text(newUser ? \"Register new user\" : \"Log in\")\n}\n.buttonStyle(.borderedProminent)\n.controlSize(.large)\n```\n\nBefore making this change, I experimented with other button styles:\n\n![Xcode. Showing button source code and the associated previews](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/button_styles_7882dc43b7.png)\n\n### Confirmation\n\nIt's very easy to accidentally tap the \"Logout\" button, and so I wanted to add this confirmation dialog:\n\n![Dialog for the user to confirm that they wish to log out](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/confirm_logout_e913c93674.png)\n\nAgain, iOS 15 makes this simple.\n\nThis is the modified version of the `LogoutButton` view:\n\n```swift\nstruct LogoutButton: View {\n   ...\n   @State private var isConfirming = false\n\n   var body: some View {\n       Button(\"Logout\") { isConfirming = true }\n       .confirmationDialog(\"Are you sure want to logout\",\n                           isPresented: $isConfirming) {\n           Button(action: logout) {\n               Text(\"Confirm Logout\")\n           }\n           Button(\"Cancel\", role: .cancel) {}\n       }\n   }\n   ...\n}\n```\n\nThese are the changes I made:\n\n- Added a new state variable (`isConfirming`)\n- Changed the logout button's action from calling the `logout` function to setting `isConfirming` to `true`\n- Added the [`confirmationDialog`](https://developer.apple.com/documentation/swiftui/view/confirmationdialog(_:ispresented:titlevisibility:presenting:actions:)-9ibgk) modifier to the button, providing three things:\n    - The dialog title (I didn't override the `titleVisibility` option and so the system decides whether this should be shown)\n    - A binding to `isConfirming` that controls whether the dialog is shown or not\n    - A view containing the contents of the dialog:\n        - A button to logout the user\n        - A cancel button\n\n## Material\n\nI'm no designer, and this is _blurring_ the edges of what changes I consider worth adding.  \n\nThe [RChat](https://github.com/realm/RChat/) app may have to wait a moment while the backend MongoDB Atlas App Services application confirms that the user has been authenticated and logged in. I superimpose a progress view while that's happening:\n\n![A semi-transparrent overlay to indicate that the apps is working on something](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/busy_62dee69adb.png)\n\nTo make it look a bit more professional, I can update [`OpaqueProgressView`](https://github.com/realm/RChat/blob/ios15/RChat-iOS/RChat/Views/Components/OpaqueProgressView.swift) to use [Material](https://developer.apple.com/documentation/swiftui/material) to blur the content that's behind the overlay. To get this effect, I update the background modifier for the `VStack`:\n\n```swift\nvar body: some View {\n   VStack {\n       if let message = message {\n           ProgressView(message)\n       } else {\n           ProgressView()\n       }\n   }\n   .padding(Dimensions.padding)\n   .background(.ultraThinMaterial,\n               in: RoundedRectangle(cornerRadius: Dimensions.cornerRadius))\n}\n```\n\nThe result looks like this:\n\n![A semi-transparrent overlay, with the background blurred, to indicate that the apps is working on something](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/busy_material_35e6fedf53.png)\n\n## Developer Tools\n\nFinally, there are a couple of enhancements that are helpful during your development phase.\n\n### Landscape Previews\n\nI'm a big fan of Xcode's \"Canvas\" previews. Previews let you see what your view will look like. Previews update in more or less real time as you make code changes. You can even display multiple previews at once for example:\n\n- For different devices: `.previewDevice(PreviewDevice(rawValue: \"iPhone 12 Pro Max\"))`\n- For dark mode: `.preferredColorScheme(.dark)`\n\nA glaring omission was that there was no way to preview landscape mode. That's fixed in iOS 15 with the addition of the [`.previewInterfaceOrientation`](https://developer.apple.com/documentation/SwiftUI/View/previewInterfaceOrientation(_:)) modifier. \n\nFor example, this code will show two devices in the preview. The first will be in portrait mode. The second will be in landscape and dark mode:\n\n```swift\nstruct CurrencyRow_Previews: PreviewProvider {\n   static var previews: some View {\n       Group {\n           List {\n               CurrencyRowView(value: 3.23, symbol: \"USD\", baseValue: .constant(1.0))\n               CurrencyRowView(value: 1.0, symbol: \"GBP\", baseValue: .constant(10.0))\n           }\n           List {\n               CurrencyRowView(value: 3.23, symbol: \"USD\", baseValue: .constant(1.0))\n               CurrencyRowView(value: 1.0, symbol: \"GBP\", baseValue: .constant(10.0))\n           }\n           .preferredColorScheme(.dark)\n           .previewInterfaceOrientation(.landscapeLeft)\n       }\n   }\n}\n```\n\n![Animation of Xcode preview. Shows that the preview updates in real time as the code is changed. There are previews for both landscape and portrait modes](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/preview_demo_12b5116b98.gif)\n\n### Self._printChanges\n\nSwiftUI is very smart at automatically refreshing views when associated state changes. But sometimes, it can be hard to figure out exactly why a view is or isn't being updated.\n\niOS 15 adds a way to print out what pieces of state data have triggered each refresh for a view. Simply call `Self._printChanges()` from the body of your view. For example, I updated [`ContentView`](https://github.com/mongodb-developer/LiveTutorial2021/blob/main/iOS/LiveChat/LiveChat/Views/ContentView.swift) for the LiveChat app:\n\n```swift\nstruct ContentView: View {\n   @State private var username = \"\"\n\n   var body: some View {\n       print(Self._printChanges())\n       return NavigationView {\n           Group {\n               if app.currentUser == nil {\n                   LoginView(username: $username)\n               } else {\n                   ChatRoomsView(username: username)\n               }\n           }\n           .navigationBarTitle(username, displayMode: .inline)\n           .navigationBarItems(trailing: app.currentUser != nil ? LogoutButton(username: $username) : nil) }\n    }\n}\n```\n\nIf I log in and check the Xcode console, I can see that it's the update to `username` that triggered the refresh (rather than `app.currentUser`):\n\n\n```swift\nContentView: _username changed.\n```\n\nThere can be a lot of these messages, and so remember to turn them off before going into production.\n\n## Conclusion\n\nSwiftUI is developing at pace. With each iOS release, there is less and less reason to not use it for all/some of your mobile app.\n\nThis post describes how to use some of the iOS 15 SwiftUI features that caught my attention. I focussed on the features that I could see would instantly benefit my most recent mobile apps. In this article, I've shown how those apps could be updated to use these features.\n\nThere are lots of features that I didn't include here. A couple of notable omissions are:\n\n- [`AsyncImage`](https://developer.apple.com/documentation/swiftui/asyncimage) is going to make it far easier to work with images that are stored in the cloud. I didn't need it for any of my current apps, but I've no doubt that I'll be using it in a project soon.\n- The [`task`](https://developer.apple.com/documentation/swiftui/anyview/task(priority:_:)/) view modifier is going to have a significant effect on how people run asynchronous code when a view is loaded. I plan to cover this in a future article that takes a more general look at how to handle concurrency with Realm.\n- Adding a toolbar to your keyboards (e.g., to let the user switch between input fields).\n\nIf you have any questions or comments on this post (or anything else Realm-related), then please raise them on our [community forum](https://www.mongodb.com/community/forums/c/realm-sdks/58). To keep up with the latest Realm news, follow [@realm](https://twitter.com/realm) on Twitter.\n","description":"See how to use some of the most useful new iOS 15 SwiftUI features in your mobile apps","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltdceba9d0e8c0f2aa/644c46c445f0985cd79389c6/SwiftUI_Features_ATF.png?branch=prod","description":null}}]},"slug":"/realm-ios15-swiftui","title":"*Most Useful iOS 15 SwiftUI Features","original_publish_date":"2021-09-27T13:00:00.280Z","strapi_updated_at":"2022-09-01T14:54:53.206Z","expiry_date":"2022-09-24T16:34:57.953Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*iOS","calculated_slug":"/technologies/ios"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"See how to use some of the most useful new iOS 15 SwiftUI features in your mobile apps","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt095b8993502a2af3/644c46c6e554e9214df7a679/SwiftUI_Features_OG.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:17.853Z","publish_details":{"time":"2023-04-28T22:52:56.019Z"}}},{"calculated_slug":"/products/atlas/automate-automation-mongodb-atlas","content":"MongoDB Atlas is an awesome Cloud Data Platform providing an immense amount of automation to set up your databases, data lakes, charts, full-text search indexes, and more across all major cloud providers around the globe. Through the MongoDB Atlas GUI, you can easily deploy a fully scalable global cluster across several regions and even across different cloud providers in a matter of minutes. That's what I call automation. Using the MongoDB GUI is super intuitive and great, but how can I manage all these features in my own way?\n\nThe answer is simple and you probably already know it….**APIs**!\n\n![Did I hear API?](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/did_i_hear_api_4069df724d.png)\n\nMongoDB Atlas has a full featured API which allows users to programmatically manage all Atlas has to offer.\n\nThe main idea is to enable users to integrate Atlas with all other aspects of your Software Development Life Cycle (SDLC), giving the ability for your DevOps team to create automation on their current processes across all their environments (Dev, Test/QA, UAT, Prod).\n\nOne example would be the DevOps teams leveraging APIs on the creation of ephemeral databases to run their CI/CD processes in lower environments for test purposes. Once it is done, you would just terminate the database deployment.\n\nAnother example we have seen DevOps teams using is to incorporate the creation of the databases needed into their Developers Portals. The idea is to give developers a self-service experience, where they can start a project by using a portal to provide all project characteristics (tech stack according to their coding language, app templates, etc.), and the portal will create all the automation to provide all aspects needed, such as a new code repo, CI/CD job template, Dev Application Servers, and a MongoDB database. So, they can start coding as soon as possible!\n\nEven though the [MongoDB Atlas API Resources documentation](https://docs.atlas.mongodb.com/api/) is great with lots of examples using cURL, we thought developers would appreciate it if they could also have all these in one of their favorite tools to work with APIs. I am talking about [Postman](https://www.postman.com/downloads/), an API platform for building and using APIs. So, we did it! Below you will find step-by-step instructions on how to use it.\n\n### Step 1: Configure your workstation/laptop\n\n* [Download](https://www.postman.com/downloads/) and install Postman on your workstation/laptop.\n* [Training](https://learning.postman.com/) on Postman is available if you need a refresher on how to use it.\n\n### Step 2: Configure MongoDB Atlas\n\n* Create a free [MongoDB Atlas account](https://account.mongodb.com/account/register) to have access to a free cluster to play around in. Make sure you create an organization and a project. Don't skip that step. Here is a coupon code—**GOATLAS10**—for some credits to explore more features (valid as of August 2021). Watch this [video](https://www.youtube.com/watch?v=FxeQ5eck3tw) to learn how to add these credits to your account.\n* Create an [API key](https://docs.atlas.mongodb.com/configure-api-access/) with Organization Owner privileges and save the public/private key to use when calling APIs. Also, don't forget to add your laptop/workstation IP to the API access list.\n* Create a [database deployment](https://docs.atlas.mongodb.com/create-database-deployment/#create-a-new-database-deployment) (cluster) via the Atlas UI or the MongoDB CLI (check out the [MongoDB CLI Atlas Quick Start](https://www.mongodb.com/blog/post/introducing-mongodb-cli-atlas-quick-start) for detailed instructions). Note that a free database deployment will allow you to run most of the API calls. Use an M10 database deployment or higher if you want to have full access to all of the APIs. Feel free to explore all of the other database deployment options, but the default options should be fine for this example.\n* Navigate to your Project Settings and retrieve your Project ID so it can be used in one of our examples below.\n\n![Finding your Project ID on MongoDB Atlas](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/mongodb_atlas_project_id_7e84ef1db2.png)\n\n### Step 3: Configure and use Postman\n\n* Fork or Import the MongoDB Atlas Collection to your Postman Workspace:  \n    [![Run in Postman](https://run.pstmn.io/button.svg)](https://god.gw.postman.com/run-collection/17637161-25049d75-bcbc-467b-aba0-82a5c440ee02?action=collection%2Ffork&collection-url=entityId%3D17637161-25049d75-bcbc-467b-aba0-82a5c440ee02%26entityType%3Dcollection%26workspaceId%3D8355a86e-dec2-425c-9db0-cb5e0c3cec02#?env%5BAtlas%5D=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)\n* Click on the MongoDB Atlas Collection. Under the Authorization tab, choose the Digest Auth Type and use the *public key* as the *user* and the *private key* as your *password*.\n\n![Configuring Authorization on Postman using MongoDB Atlas API key](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/postman_auth_config_d644e181dc.png)\n\n* Open up the **Get All Clusters** API call under the cluster folder.\n\n![Accessing Postman API Calls to MongoDB Atlas](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/postman_api_calls_e5317b661f.png)\n\n* Make sure you select the Atlas environment variables and [update the Postman variable](https://learning.postman.com/docs/sending-requests/variables/) ProjectID value to your **Project ID** captured in the previous steps.\n\n![Setting Postman variables](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/postman_variables_8c0bb2b3d7.png)\n\n![Setting Postman variables in Atlas Environment variables](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/postman_variables_atlas_env_83abc36dc1.png)\n\n* Execute the API call by hitting the Send button and you should get a response containing a list of all your clusters (database deployments) alongside the cluster details, like whether backup is enabled or the cluster is running.\n\n![Executing API call to list all MongoDB Atlas cluster in your project.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/executing_api_call_bf5e1fd674.png)\n\nNow explore all the APIs available to create your own automation.\n\nOne last tip: Once you have tested all your API calls to build your automation, Postman allows you to export that in code snippets in your favorite programming language.\n\n![Generating Code snippet in Postman](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/generating_code_snippet_in_postman_9431e3320f.png)\n\n![Generating Python Code snippet in Postman](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/generating_code_snippet_postman_6b6c83aae6.png)\n\nPlease always refer to the online [documentation](https://docs.atlas.mongodb.com/reference/api-resources/) for any changes or new resources. Also, feel free to make pull requests to update the project with new API resources, fixes, and enhancements.\n\nHope you enjoyed it! Please share this with your team and community. It might be really helpful for everyone!\n\nHere are some other great posts related to this subject:\n\n* [Programmatic API Management of Your MongoDB Atlas Database Clusters](https://www.mongodb.com/blog/post/programmatic-api-management-of-your-mongodb-atlas-database-clusters)\n* [Programmatic API Management of Your MongoDB Atlas Database Clusters - Part II](https://www.mongodb.com/blog/post/programmatic-api-management-of-your-mongodb-atlas-database-clusters-part-ii)\n* [Calling the MongoDB Atlas API - How to Do it from Node, Python, and Ruby](https://www.mongodb.com/developer/how-to/nodejs-python-ruby-atlas-api/)\n\n\\**A subset of API endpoints are supported in (free) M0, M2, and M5 clusters.*\n\nPublic Repo - [https://github.com/cassianobein/mongodb-atlas-api-resources](https://github.com/cassianobein/mongodb-atlas-api-resources)  \nAtlas API Documentation - [https://docs.atlas.mongodb.com/api/](https://docs.atlas.mongodb.com/api/)  \nPostman MongoDB Public Workspace - [https://www.postman.com/mongodb-devrel/workspace/mongodb-public/overview](https://www.postman.com/mongodb-devrel/workspace/mongodb-public/overview)  ","description":"Build your own automation with MongoDB Atlas API resources.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt2d8dcba41f6872ca/644c46c945d2d735c7b6fa40/docs.png?branch=prod","description":null}}]},"slug":"/automate-automation-mongodb-atlas","title":"*Automate the Automation on MongoDB Atlas","original_publish_date":"2021-10-12T22:21:03.315Z","strapi_updated_at":"2022-05-13T13:06:01.205Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Postman API","calculated_slug":"/technologies/postman-api"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:17.456Z","publish_details":{"time":"2023-04-28T22:52:56.080Z"}}},{"calculated_slug":"/products/realm/realm-how-to-add-realm-to-your-unity-project","content":"When creating a game with Unity, we often reach the point where we need to save data that we need at a later point in time. This could be something simple, like a table of high scores, or a lot more complex, like the state of the game that got paused and now needs to be resumed exactly the way the user left it when they quit it earlier. Maybe you have tried this before using `PlayerPrefs` but your data was too complex to save it in there. Or you have tried SQL only to find it to be very complicated and cumbersome to use.\n\nRealm can help you achieve this easily and quickly with just some minor adjustments to your code.\n\nThe goal of this article is to show you how to add Realm to your Unity game and make sure your data is persisted. The Realm Unity SDK is part of our [Realm .NET SDK](https://github.com/realm/realm-dotnet). The [documentation for the Realm .NET SDK](https://docs.mongodb.com/realm/sdk/dotnet) will help you get started easily.\n\nThe first part of this tutorial will describe the example itself. If you are already familiar with Unity or really just want to see Realm in action, you can also skip it and [jump straight to the second part](#add-realm).\n\n## Example game\n\nWe will be using a simple 3D chess game for demonstration purposes. Creating this game itself will not be part of this tutorial. However, this section will provide you with an overview so that you can follow along and add Realm to the game. This example can be found in [our Unity examples repository](https://github.com/realm/unity-examples/tree/3d-chess/example-template).\n\nThe [final implementation of the game](https://github.com/realm/unity-examples/tree/3d-chess/local-realm) including the usage of Realm is also part of the example repository.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/00_chess_board_8154eab7f2.png)\n\nTo make it easy to find your way around this example, here are some notes to get you started:\n\nThe interesting part in the `MainScene` to look at is the `Board` which is made up of `Squares` and `Pieces`. The `Squares` are just slightly scaled and colored default `Cube` objects which we utilize to visualize the `Board` but also detect clicks for moving `Pieces` by using its already attached [`Box Collider`](https://docs.unity3d.com/Manual/class-BoxCollider.html) component.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/01_square_game_object_0f42872b3d.jpg)\n\nThe `Pieces` have to be activated first, which happens by making them clickable as well. `Pieces` are not initially added to the `Board` but instead will be spawned by the `PieceSpawner`. You can find them in the `Prefabs` folder in the `Project` hierarchy.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/02_piece_prefab_c413189c24.jpg)\n\nThe important part to look for here is the `Piece` script which detects clicks on this `Piece` (3) and offers a color change via `Select()` (1) and `Deselect()` (2) to visualize if a `Piece` is active or not.\n\n```cs\nusing UnityEngine;\n\npublic class Piece : MonoBehaviour\n{\n    private Events events = default;\n    private readonly Color selectedColor = new Color(1, 0, 0, 1);\n    private readonly Color deselectedColor = new Color(1, 1, 1, 1);\n\n    // 1\n    public void Select()\n    {\n        gameObject.GetComponent<Renderer>().material.color = selectedColor;\n    }\n\n    // 2\n    public void Deselect()\n    {\n        gameObject.GetComponent<Renderer>().material.color = deselectedColor;\n    }\n\n    // 3\n    private void OnMouseDown()\n    {\n        events.PieceClickedEvent.Invoke(this);\n    }\n\n    private void Awake()\n    {\n        events = FindObjectOfType<Events>();\n    }\n}\n\n```\n\nWe use two events to actually track the click on a `Piece` (1) or a `Square` (2):\n\n```cs\nusing UnityEngine;\nusing UnityEngine.Events;\n\npublic class PieceClickedEvent : UnityEvent<Piece> { }\npublic class SquareClickedEvent : UnityEvent<Vector3> { }\n\npublic class Events : MonoBehaviour\n{\n    // 1\n    public readonly PieceClickedEvent PieceClickedEvent = new PieceClickedEvent();\n    // 2\n    public readonly SquareClickedEvent SquareClickedEvent = new SquareClickedEvent();\n}\n```\n\nThe `InputListener` waits for those events to be invoked and will then notify other parts of our game about those updates. Pieces need to be selected when clicked (1) and deselected if another one was clicked (2).\n\nClicking a `Square` while a `Piece` is selected will send a message (3) to the `GameState` to update the position of this `Piece`.\n\n```cs\nusing UnityEngine;\n\npublic class InputListener : MonoBehaviour\n{\n    [SerializeField] private Events events = default;\n    [SerializeField] private GameState gameState = default;\n\n    private Piece activePiece = default;\n\n    private void OnEnable()\n    {\n        events.PieceClickedEvent.AddListener(OnPieceClicked);\n        events.SquareClickedEvent.AddListener(OnSquareClicked);\n    }\n\n    private void OnDisable()\n    {\n        events.PieceClickedEvent.RemoveListener(OnPieceClicked);\n        events.SquareClickedEvent.RemoveListener(OnSquareClicked);\n    }\n\n    private void OnPieceClicked(Piece piece)\n    {\n        if (activePiece != null)\n        {\n            // 2\n            activePiece.Deselect();\n        }\n        // 1\n        activePiece = piece;\n        activePiece.Select();\n    }\n\n    private void OnSquareClicked(Vector3 position)\n    {\n        if (activePiece != null)\n        {\n            // 3\n            gameState.MovePiece(activePiece, position);\n            activePiece.Deselect();\n            activePiece = null;\n        }\n    }\n}\n\n```\n\nThe actual movement as well as controlling the spawning and destroying of pieces is done by the `GameState`, in which all the above information eventually comes together to update `Piece` positions and possibly destroy other `Piece` objects. Whenever we move a `Piece` (1), we not only update its position (2) but also need to check if there is a `Piece` in that position already (3) and if so, destroy it (4).\n\nIn addition to updating the game while it is running, the `GameState` offers two more functionalities:\n- set up the initial board (5)\n- reset the board to its initial state (6)\n\n```cs\nusing System.Linq;\nusing UnityEngine;\n\npublic class GameState : MonoBehaviour\n{\n    [SerializeField] private PieceSpawner pieceSpawner = default;\n    [SerializeField] private GameObject pieces = default;\n\n    // 1\n    public void MovePiece(Piece movedPiece, Vector3 newPosition)\n    {\n        // 3\n        // Check if there is already a piece at the new position and if so, destroy it.\n        var attackedPiece = FindPiece(newPosition);\n        if (attackedPiece != null)\n        {\n            // 4\n            Destroy(attackedPiece.gameObject);\n        }\n\n        // 2\n        // Update the movedPiece's GameObject.\n        movedPiece.transform.position = newPosition;\n    }\n\n    // 6\n    public void ResetGame()\n    {\n        // Destroy all GameObjects.\n        foreach (var piece in pieces.GetComponentsInChildren<Piece>())\n        {\n            Destroy(piece.gameObject);\n        }\n\n        // Recreate the GameObjects.\n        pieceSpawner.CreateGameObjects(pieces);\n    }\n\n    private void Awake()\n    {\n        // 5\n        pieceSpawner.CreateGameObjects(pieces);\n    }\n\n    private Piece FindPiece(Vector3 position)\n    {\n        return pieces.GetComponentsInChildren<Piece>()\n            .FirstOrDefault(piece => piece.transform.position == position);\n    }\n}\n```\n\nGo ahead and try it out yourself if you like. You can play around with the board and pieces and reset if you want to start all over again.\n\nTo make sure the example is not overly complex and easy to follow, there are no rules implemented. You can move the pieces however you want. Also, the game is purely local for now and will be expanded using our Sync component in a later article to be playable online with others.\n\nIn the following section, I will explain how to make sure that the current game state gets saved and the players can resume the game at any state.\n\n## <a name=\"add-realm\"></a>Adding Realm to your project\n\nThe first thing we need to do is to import the Realm framework into Unity.\nThe easiest way to do this is by using NPM.\n\nYou'll find it via `Windows` → `Package Manager` → cogwheel in the top right corner → `Advanced Project Settings`:\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/03_package_manager_5cd4f7f4a0.jpg)\n\nWithin the `Scoped Registries`, you can add the `Name`, `URL`, and `Scope` as follows:\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/04_project_settings_6b6709dd5b.jpg)\n\nThis adds `NPM` as a source for libraries. The final step is to tell the project which dependencies to actually integrate into the project. This is done in the `manifest.json` file which is located in the `Packages` folder of your project.\n\nHere you need to add the following line to the `dependencies`:\n\n```json\n\"io.realm.unity\": \"<version-number>\"\n```\n\nReplace `<version-number>` with the most recent Realm version found in https://github.com/realm/realm-dotnet/releases and you're all set.\n\nThe final `manifest.json` should look something like this:\n\n```json\n{\n \"dependencies\": {\n   ...\n   \"io.realm.unity\": \"10.3.0\"\n },\n \"scopedRegistries\": [\n   {\n     \"name\": \"NPM\",\n     \"url\": \"https://registry.npmjs.org/\",\n     \"scopes\": [\n       \"io.realm.unity\"\n     ]\n   }\n ]\n}\n```\n\nWhen you switch back to Unity, it will reload the dependencies. If you then open the `Package Manager` again, you should see `Realm` as a new entry in the list on the left:\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/05_realm_in_project_manager_3b7fca2a66.jpg)\n\nWe can now start using Realm in our Unity project.\n\n## Top-down or bottom-up?\n\nBefore we actually start adding Realm to our code, we need to think about how we want to achieve this and how the UI and database will interact with each other.\n\nThere are basically two options we can choose from: top-down or bottom-up.\n\nThe top-down approach would be to have the UI drive the changes. The `Piece` would know about its database object and whenever a `Piece` is moved, it would also update the database with its new position.\n\nThe preferred approach would be bottom-up, though. Changes will be applied to the Realm and it will then take care of whatever implications this has on the UI by sending notifications.\n\nLet's first look into the initial setup of the board.\n\n## Setting up the board\n\nThe first thing we want to do is to define a Realm representation of our piece since we cannot save the `MonoBehaviour` directly in Realm. Classes that are supposed to be saved in Realm need to subclass `RealmObject`. The class `PieceEntity` will represent such an object. Note that we cannot just duplicate the types from `Piece` since [not all of them can be saved in Realm](https://docs.mongodb.com/realm/sdk/dotnet/data-types/field-types/), like `Vector3` and `enum`.\n\nAdd the following scripts to the project:\n\n```cs\nusing Realms;\nusing UnityEngine;\n\npublic class PieceEntity : RealmObject\n{\n    // 1\n    public PieceType PieceType\n    {\n        get => (PieceType)Type;\n        private set => Type = (int)value;\n    }\n\n    // 2\n    public Vector3 Position\n    {\n        get => PositionEntity.ToVector3();\n        set => PositionEntity = new Vector3Entity(value);\n    }\n\n    // 3\n    private int Type { get; set; }\n    private Vector3Entity PositionEntity { get; set; }\n\n    // 4\n    public PieceEntity(PieceType type, Vector3 position)\n    {\n        PieceType = type;\n        Position = position;\n    }\n\n    // 5\n    protected override void OnPropertyChanged(string propertyName)\n    {\n        if (propertyName == nameof(PositionEntity))\n        {\n            RaisePropertyChanged(nameof(Position));\n        }\n    }\n\n    // 6\n    private PieceEntity()\n    {\n    }\n}\n```\n\n```cs\nusing Realms;\nusing UnityEngine;\n\npublic class Vector3Entity : EmbeddedObject // 7\n{\n    public float X { get; private set; }\n    public float Y { get; private set; }\n    public float Z { get; private set; }\n\n    public Vector3Entity(Vector3 vector) // 8\n    {\n        X = vector.x;\n        Y = vector.y;\n        Z = vector.z;\n    }\n\n    public Vector3 ToVector3() => new Vector3(X, Y, Z); // 9\n\n    private Vector3Entity() // 10\n    {\n    }\n}\n```\n\nEven though we cannot save the `PieceType` (1) and the position (2) directly in the Realm, we can still expose them using backing variables (3) to make working with this class easier while still fulfilling the requirements for saving data in Realm.\n\nAdditionally, we provide a convenience constructor (4) for setting those two properties. A default constructor (6) also has to be provided for every `RealmObject`. Since we are not going to use it here, though, we can set it to `private`.\n\nNote that one of these backing variables is a `RealmObject` itself, or rather a subclass of it: `EmbeddedObject` (7). By extracting the position to a separate class `Vector3Entity` the `PieceEntity` is more readable. Another plus is that we can use the `EmbeddedObject` to represent a 1:1 relationship. Every `PieceEntity` can only have one `Vector3Entity` and even more importantly, every `Vector3Entity` can only belong to one `PieceEntity` because there can only ever be one `Piece` on any given `Square`.\n\nThe `Vector3Entity`, like the `PieceEntity`, has some convenience functionality like a constructor that takes a `Vector3` (8), the `ToVector3()` function (9) and the private, mandatory default constructor (10) like `PieceEntity`.\n\nLooking back at the `PieceEntity`, you will notice one more function: `OnPropertyChanged` (5). Realm sends notifications for changes to fields saved in the database. Since we expose those fields using `PieceType` and `Position`, we need to make sure those notifications are passed on. This is achieved by calling `RaisePropertyChanged(nameof(Position));` whenever `PositionEntity` changes.\n\nThe next step is to add some way to actually add `Pieces` to the `Realm`. The current database state will always represent the current state of the board. When we create a new `PieceEntity`—for example, when setting up the board—the `GameObject` for it (`Piece`) will be created. If a `Piece` gets moved, the `PieceEntity` will be updated by the `GameState` which then leads to the `Piece`'s `GameObject` being updated using above mentioned notifications.\n\nFirst, we will need to set up the board. To achieve this using the bottom-up approach, we adjust the `PieceSpawner` as follows:\n\n```cs\nusing Realms;\nusing UnityEngine;\n\npublic class PieceSpawner : MonoBehaviour\n{\n    [SerializeField] private Piece prefabBlackBishop = default;\n    [SerializeField] private Piece prefabBlackKing = default;\n    [SerializeField] private Piece prefabBlackKnight = default;\n    [SerializeField] private Piece prefabBlackPawn = default;\n    [SerializeField] private Piece prefabBlackQueen = default;\n    [SerializeField] private Piece prefabBlackRook = default;\n\n    [SerializeField] private Piece prefabWhiteBishop = default;\n    [SerializeField] private Piece prefabWhiteKing = default;\n    [SerializeField] private Piece prefabWhiteKnight = default;\n    [SerializeField] private Piece prefabWhitePawn = default;\n    [SerializeField] private Piece prefabWhiteQueen = default;\n    [SerializeField] private Piece prefabWhiteRook = default;\n\n    public void CreateNewBoard(Realm realm)\n    {\n        realm.Write(() =>\n        {\n            // 1\n            realm.RemoveAll<PieceEntity>();\n\n            // 2\n            realm.Add(new PieceEntity(PieceType.WhiteRook, new Vector3(1, 0, 1)));\n            realm.Add(new PieceEntity(PieceType.WhiteKnight, new Vector3(2, 0, 1)));\n            realm.Add(new PieceEntity(PieceType.WhiteBishop, new Vector3(3, 0, 1)));\n            realm.Add(new PieceEntity(PieceType.WhiteQueen, new Vector3(4, 0, 1)));\n            realm.Add(new PieceEntity(PieceType.WhiteKing, new Vector3(5, 0, 1)));\n            realm.Add(new PieceEntity(PieceType.WhiteBishop, new Vector3(6, 0, 1)));\n            realm.Add(new PieceEntity(PieceType.WhiteKnight, new Vector3(7, 0, 1)));\n            realm.Add(new PieceEntity(PieceType.WhiteRook, new Vector3(8, 0, 1)));\n\n            realm.Add(new PieceEntity(PieceType.WhitePawn, new Vector3(1, 0, 2)));\n            realm.Add(new PieceEntity(PieceType.WhitePawn, new Vector3(2, 0, 2)));\n            realm.Add(new PieceEntity(PieceType.WhitePawn, new Vector3(3, 0, 2)));\n            realm.Add(new PieceEntity(PieceType.WhitePawn, new Vector3(4, 0, 2)));\n            realm.Add(new PieceEntity(PieceType.WhitePawn, new Vector3(5, 0, 2)));\n            realm.Add(new PieceEntity(PieceType.WhitePawn, new Vector3(6, 0, 2)));\n            realm.Add(new PieceEntity(PieceType.WhitePawn, new Vector3(7, 0, 2)));\n            realm.Add(new PieceEntity(PieceType.WhitePawn, new Vector3(8, 0, 2)));\n\n            realm.Add(new PieceEntity(PieceType.BlackPawn, new Vector3(1, 0, 7)));\n            realm.Add(new PieceEntity(PieceType.BlackPawn, new Vector3(2, 0, 7)));\n            realm.Add(new PieceEntity(PieceType.BlackPawn, new Vector3(3, 0, 7)));\n            realm.Add(new PieceEntity(PieceType.BlackPawn, new Vector3(4, 0, 7)));\n            realm.Add(new PieceEntity(PieceType.BlackPawn, new Vector3(5, 0, 7)));\n            realm.Add(new PieceEntity(PieceType.BlackPawn, new Vector3(6, 0, 7)));\n            realm.Add(new PieceEntity(PieceType.BlackPawn, new Vector3(7, 0, 7)));\n            realm.Add(new PieceEntity(PieceType.BlackPawn, new Vector3(8, 0, 7)));\n\n            realm.Add(new PieceEntity(PieceType.BlackRook, new Vector3(1, 0, 8)));\n            realm.Add(new PieceEntity(PieceType.BlackKnight, new Vector3(2, 0, 8)));\n            realm.Add(new PieceEntity(PieceType.BlackBishop, new Vector3(3, 0, 8)));\n            realm.Add(new PieceEntity(PieceType.BlackQueen, new Vector3(4, 0, 8)));\n            realm.Add(new PieceEntity(PieceType.BlackKing, new Vector3(5, 0, 8)));\n            realm.Add(new PieceEntity(PieceType.BlackBishop, new Vector3(6, 0, 8)));\n            realm.Add(new PieceEntity(PieceType.BlackKnight, new Vector3(7, 0, 8)));\n            realm.Add(new PieceEntity(PieceType.BlackRook, new Vector3(8, 0, 8)));\n        });\n    }\n\n    public void SpawnPiece(PieceEntity pieceEntity, GameObject parent)\n    {\n        var piecePrefab = pieceEntity.PieceType switch\n        {\n            PieceType.BlackBishop => prefabBlackBishop,\n            PieceType.BlackKing => prefabBlackKing,\n            PieceType.BlackKnight => prefabBlackKnight,\n            PieceType.BlackPawn => prefabBlackPawn,\n            PieceType.BlackQueen => prefabBlackQueen,\n            PieceType.BlackRook => prefabBlackRook,\n            PieceType.WhiteBishop => prefabWhiteBishop,\n            PieceType.WhiteKing => prefabWhiteKing,\n            PieceType.WhiteKnight => prefabWhiteKnight,\n            PieceType.WhitePawn => prefabWhitePawn,\n            PieceType.WhiteQueen => prefabWhiteQueen,\n            PieceType.WhiteRook => prefabWhiteRook,\n            _ => throw new System.Exception(\"Invalid piece type.\")\n        };\n\n        var piece = Instantiate(piecePrefab, pieceEntity.Position, Quaternion.identity, parent.transform);\n        piece.Entity = pieceEntity;\n    }\n}\n```\n\nThe important change here is `CreateNewBoard`. Instead of spawning the `Piece`s, we now add `PieceEntity` objects to the Realm. When we look at the changes in `GameState`, we will see how this actually creates a `Piece` per `PieceEntity`.\n\nHere we just wipe the database (1) and then add new `PieceEntity` objects (2). Note that this is wrapped by a `realm.write` block. Whenever we want to change the database, we need to enclose it in a write transaction. This makes sure that no other piece of code can change the database at the same time since transactions block each other.\n\nThe last step to create a new board is to update the `GameState` to make use of the new `PieceSpawner` and the `PieceEntity` that we just created.\n\nWe'll go through these changes step by step. First we also need to import Realm here as well:\n\n```cs\nusing Realms;\n```\n\nThen we add a private field to save our `Realm` instance to avoid creating it over and over again. We also create another private field to save the collection of pieces that are on the board and a notification token which we need for above mentioned notifications:\n\n```cs\nprivate Realm realm;\nprivate IQueryable<PieceEntity> pieceEntities;\nprivate IDisposable notificationToken;\n```\n\nIn `Awake`, we do need to get access to the `Realm`. This is achieved by opening an instance of it (1) and then asking it for all `PieceEntity` objects currently saved using `realm.All` (2) and assigning them to our `pieceEntities` field:\n\n```cs\nprivate void Awake()\n{\n    realm = Realm.GetInstance(); // 1\n    pieceEntities = realm.All<PieceEntity>(); // 2\n\n    // 3\n    notificationToken = pieceEntities.SubscribeForNotifications((sender, changes, error) =>\n    {\n        // 4\n        if (error != null)\n        {\n            Debug.Log(error.ToString());\n            return;\n        }\n\n        // 5\n        // Initial notification\n        if (changes == null)\n        {\n            // Check if we actually have `PieceEntity` objects in our Realm (which means we resume a game).\n            if (sender.Count > 0)\n            {\n                // 6\n                // Each `RealmObject` needs a corresponding `GameObject` to represent it.\n                foreach (PieceEntity pieceEntity in sender)\n                {\n                    pieceSpawner.SpawnPiece(pieceEntity, pieces);\n                }\n            }\n            else\n            {\n                // 7\n                // No game was saved, create a new board.\n                pieceSpawner.CreateNewBoard(realm);\n            }\n            return;\n        }\n\n        // 8\n        foreach (var index in changes.InsertedIndices)\n        {\n            var pieceEntity = sender[index];\n            pieceSpawner.SpawnPiece(pieceEntity, pieces);\n        }\n    });\n}\n```\n\nNote that collections are live objects. This has two positive implications: Every access to the object reference always returns an updated representation of said object. Because of this, every subsequent change to the object will be visible any time the object is accessed again. We also get notifications for those changes if we subscribed to them. This can be done by calling `SubscribeForNotifications` on a collection (3).\n\nApart from an error object that we need to check (4), we also receive the `changes` and the `sender` (the updated collection itself) with every notification. For every new collection of objects, an initial notification is sent that does not include any `changes` but gives us the opportunity to do some initial setup work (5).\n\nIn case we resume a game, we'll already see `PieceEntity` objects in the database even for the initial notification. We need to spawn one `Piece` per `PieceEntity` to represent it (6). We make use of the `SpawnPiece` function in `PieceSpawner` to achieve this. In case the database does not have any objects yet, we need to create the board from scratch (7). Here we use the `CreateNewBoard` function we added earlier to the `PieceSpawner`.\n\nOn top of the initial notification, we also expect to receive a notification every time a `PieceEntity` is inserted into the Realm. This is where we continue the `CreateNewBoard` functionality we started in the `PieceSpawner` by adding new objects to the database. After those changes happen, we end up with `changes` (8) inside the notifications. Now we need to iterate over all new `PieceEntity` objects in the `sender` (which represents the `pieceEntities` collection) and add a `Piece` for each new `PieceEntity` to the board.\n\nApart from inserting new pieces when the board gets set up, we also need to take care of movement and pieces attacking each other. This will be explained in the next section.\n\n## Updating the position of a PieceEntity\n\nWhenever we receive a click on a `Square` and therefore call `MovePiece` in `GameState`, we need to update the `PieceEntity` instead of directly moving the corresponding `GameObject`. The movement of the `Piece` will then happen via the `PropertyChanged` notifications as we saw earlier.\n\n```cs\npublic void MovePiece(Vector3 oldPosition, Vector3 newPosition)\n{\n    realm.Write(() =>\n    {\n        // 1\n        var attackedPiece = FindPieceEntity(newPosition);\n        if (attackedPiece != null)\n        {\n            realm.Remove(attackedPiece);\n        }\n\n        // 2\n        var movedPieceEntity = FindPieceEntity(oldPosition);\n        movedPieceEntity.Position = newPosition;\n    });\n}\n\n// 3\nprivate PieceEntity FindPieceEntity(Vector3 position)\n{\n    return pieceEntities\n             .Filter(\"PositionEntity.X == $0 && PositionEntity.Y == $1 && PositionEntity.Z == $2\",\n                     position.x, position.y, position.z)\n             .FirstOrDefault();\n}\n```\n\nBefore actually moving the `PieceEntity`, we do need to check if there is already a `PieceEntity` at the desired position and if so, destroy it. To find a `PieceEntity` at the `newPosition` and also to find the `PieceEntity` that needs to be moved from `oldPosition` to `newPosition`, we can use queries on the `pieceEntities` collection (3).\n\nBy querying the collection (calling `Filter`), we can look for one or multiple `RealmObject`s with specific characteristics. In this case, we're interested in the `RealmObject` that represents the `Piece` we are looking for. Note that when using a `Filter` we can only filter using the Realm properties saved in the database, not the exposed properties (`Position` and `PieceType`) exposed for convenience by the `PieceEntity`.\n\nIf there is an `attackedPiece` at the target position, we need to delete the corresponding `PieceEntity` for this `GameObject` (1). After the `attackedPiece` is updated, we can then also update the `movedPiece` (2).\n\nLike the initial setup of the board, this has to be called within a write transaction to make sure no other code is changing the database at the same time.\n\nThis is all we had to do to update and persist the position. Go ahead and start the game. Stop and start it again and you should now see the state being persisted.\n\n## Resetting the board\n\nThe final step will be to also update our `ResetGame` button to update (or rather, wipe) the `Realm`. At the moment, it does not update the state in the database and just recreates the `GameObject`s.\n\nResetting works similar to what we do in `Awake` in case there were no entries in the database—for example, when starting the game for the first time.\n\nWe can reuse the `CreateNewBoard` functionality here since it includes wiping the database before actually re-creating it:\n\n```cs\npublic void ResetGame()\n{\n    pieceSpawner.CreateNewBoard(realm);\n}\n```\n\nWith this change, our game is finished and fully functional using a local `Realm` to save the game's state.\n\n## Recap and conclusion\n\nIn this tutorial, we have seen that saving your game and resuming it later can be easily achieved by using `Realm`.\n\nThe steps we needed to take:\n\n- Add `Realm` via NPM as a dependency.\n- Import `Realm` in any class that wants to use it by calling `using Realms;`.\n- Create a new `Realm` instance via `Realm.GetInstance()` to get access to the database.\n- Define entites by subclassing `RealmObject` (or any of its subclasses):\n  - Fields need to be public and primitive values or lists.\n  - A default constructor is mandatory.\n  - A convenience constructor and additional functions can be defined.\n- Write to a `Realm` using `realm.Write()` to avoid data corruption.\n- CRUD operations (need to use a `write` transaction):\n  - Use `realm.Add()` to `Create` a new object.\n  - Use `realm.Remove()` to `Delete` an object.\n  - `Read` and `Update` can be achieved by simply `getting` and `setting` the `public fields`.\n\nWith this, you should be ready to use Realm in your games.\n\nIf you have questions, please head to our [developer community website](https://community.mongodb.com) where the MongoDB engineers and the MongoDB community will help you build your next big idea with MongoDB and Realm.","description":"This article shows how to integrate the Realm Unity SDK into your Unity game. We will cover everything you need to know to get started: installing the SDK, defining your models, and connecting the database to your GameObjects.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltccaa0f3b9a286f02/644c46a73df9c33c60a92bf9/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/realm-how-to-add-realm-to-your-unity-project","title":"*Persistence in Unity Using Realm","original_publish_date":"2021-10-14T16:27:28.108Z","strapi_updated_at":"2022-09-07T10:16:23.015Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*SDK","calculated_slug":"/products/realm/sdk"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*CSharp","calculated_slug":"/languages/csharp"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*Unity","calculated_slug":"/technologies/unity"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:17.044Z","publish_details":{"time":"2023-04-28T22:52:56.199Z"}}},{"calculated_slug":"/products/realm/ionic-realm-web-app-convert-to-mobile-app","content":"[Realm](https://realm.io/) is an open-source, easy-to-use local database that helps mobile developers to build better apps, faster. It offers a data synchronization service—MongoDB Realm Sync—that makes it simple to move data between the client and MongoDB Atlas on the back end. Using Realm can save you from writing thousands of lines of code, and offers an intuitive way to work with your data.\n\nThe Ionic team posted a fantastic article on how you can [use Ionic with Realm to build a React Web app](https://ionicframework.com/blog/ionic-react-and-realm/) quickly, taking advantage of Realm to easily persist your data in a MongoDB Atlas Database.\n\nAfter [cloning the repo](https://github.com/mhartington/ionic-realm-demo) and running `ionic serve`, you'll have a really simple task management web application. You can register (using any user/password combination, Realm takes care of your onboarding needs). You can log in, have a look at your tasks, and add new tasks.\n\n|   Login in the Web App      |   Browsing Tasks   | \n|--------------|-----------|\n| ![Login window asking for user and password running in a browser. There’s a Login and Register button.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ionic_native_login_web_44095db30a.png)|  ![Tasks already entered in the app, one for “Do the dishes”, another for “Finish post about Ionic / Realm” and last one “Install Ionic app running Realm in iOS”](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ionic_native_tasks_web_98bd9784b2.png)      | \n\n\nLet’s build on what the Ionic team created for the web, and expand it by building a mobile app for iOS and Android using one of the best features Ionic has: the _“Write Once, Run Anywhere”_ approach to coding. I’ll start with an iOS app.\n\n\n## Prerequisites\n\nTo follow along this post, you’ll need five things:\n\n* A macOS-powered computer running Xcode (to develop for iOS). I’m using Xcode 13 Beta. You don’t have to risk your sanity.\n* Ionic installed. You can follow the instructions [here](https://ionicframework.com/docs/intro/cli), but TL;DR it’s `npm install -g @ionic/cli`\n* Clone the [repo with the Ionic React Web App](https://docs.mongodb.com/realm/tutorial/realm-app/) that we’ll turn into mobile.\n* As we need an Atlas Database to store our data in the cloud, and a Realm app to make it easy to work with Atlas from mobile, set up a Free Forever MongoDB cluster and [create and import a Realm app schema](https://docs.mongodb.com/realm/tutorial/realm-app/) so everything is ready server-side.\n* Once you have your Realm app created, copy the Realm app ID from the MongoDB admin interface for Realm, and paste it into `src/App.tsx`, in the line:\n\n`export const APP_ID = '<Put your Realm App Id here>';`\n\nOnce your `APP_ID` is set, run:\n\n```\n$ npm run build\n```\n\n\n## The iOS app\n\nTo add iOS capabilities to our existing app, we need to open a terminal and run:\n\n```bash\n$ ionic cap add ios\n``` \n\nThis will create the iOS Xcode Project native developers know and love, with the code from our Ionic app. I ran into a problem doing that and it was that the version of Capacitor used in the repo was 3.1.2, but for iOS, I needed at least 3.2.0. So, I just changed `package.json` and ran `npm install` to update Capacitor.\n\n`package.json` fragment:\n\n```\n...\n\"dependencies\": {\n\n    \"@apollo/client\": \"^3.4.5\",\n    \"@capacitor/android\": \"3.2.2\",\n    \"@capacitor/app\": \"1.0.2\",\n    \"@capacitor/core\": \"3.2.0\",\n    \"@capacitor/haptics\": \"1.0.2\",\n    \"@capacitor/ios\": \"3.2.2\",\n...\n```\n\nNow we have a new `ios` directory. If we enter that folder, we’ll see an `App` directory that has a CocoaPods-powered iOS app. To run this iOS app, we need to:\n\n\n\n* Change to that directory with `cd ios`. You’ll find an `App` directory. `cd App`\n* Install all CocoaPods with `pod repo update && pod install`, as usual in a native iOS project. This updates all libraries’ caches for CocoaPods and then installs the required libraries and dependencies in your project.\n* Open the generated `App.xcworkspace` file with Xcode. From Terminal, you can just type `open App.xcworkspace`.\n* Run the app from Xcode.\n\n|   Login in the iOS App      |   Browsing Tasks   | \n|--------------|-----------|\n|![Login screen in iOS asking for user and password running in a browser. There’s a Login and Register button.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ionic_native_login_ios_21fa1f7446.png)|  ![Tasks already entered in the iOS app, one for “Do the dishes”, another for “Finish post about Ionic / Realm” and last one “Install Ionic app running Realm in iOS”](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ionic_native_tasks_ios_04d29ff898.png)  |\n\n\n\nThat’s it. Apart from updating Capacitor, we only needed to run one command to get our Ionic web project running on iOS!\n\n\n## The Android App\n\nHow hard can it be to build our Ionic app for Android now that we have done it for iOS? Well, it turns out to be super-simple. Just `cd` back to the root of the project and type in a terminal:\n\n```\n ionic cap android\n```\n\nThis will create the Android project. Once has finished, launch your app using:\n\n```\nionic capacitor run android -l --host=10.0.1.81\n```\n\nIn this case, `10.0.1.81` is my own IP address. As you can see, if you have more than one Emulator or even a plugged-in Android phone, you can select where you want to run the Ionic app.\n\n![before running the Android App we can choose the Emulator where it will run.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ionic_native_terminal_f3e08bace5.png)\n\nOnce running, you can register, log in, and add tasks in Android, just like you can do in the web and iOS apps.\n\n\n|  Adding a task in Android      |   Browsing Tasks in Android   | \n|--------------|-----------|\n|![Adding a new task screen from Android, with “New Task from Android” typed in.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ionic_native_android_add_7040b32898.png)|![Tasks already entered in the Android app, one for “Do the dishes”, another for “Finish post about Ionic / Realm”, “Install Ionic app running Realm in iOS” and “New Task from Android”](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ionic_native_android_tasks_7ee2c9c1bc.png)|\n\nThe best part is that thanks to the synchronization happening in the MongoDB Realm app, every time we add a new task locally, it gets uploaded to the cloud to a MongoDB Atlas database behind the scenes. And **all other apps accessing the same MongoDB Realm app can show that data**! \n\n\n![thanks to Ionic and Realm, Web, iOS and Android access the same data using the same code base.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ionic_native_three_apps_1a7dbfbd05.png)\n\n\n## Automatically refreshing tasks\n\n[Realm SDKs](https://docs.mongodb.com/realm/sdk/) are well known for their syncing capabilities. You change something in the server, or in one app, and other users with access to the same data will see the changes almost immediately. You don’t have to worry about invalidating caches, writing complex networking/multithreading code that runs in the background, listening to silent push notifications, etc. MongoDB Realm takes care of all that for you.\n\nBut in this example, we access data using the [Apollo GraphQL Client](https://docs.mongodb.com/realm/web/graphql-apollo-react/) for React. Using this client, we can log into our Realm app and run GraphQL Queries—although as designed for the web, we don’t have access to the hard drive to store a .realm file. It’s just a simpler way to use the otherwise awesome [Apollo GraphQL Client](https://www.apollographql.com/docs/react/) with Realm, so we don’t have synchronization implemented. But luckily, Apollo GraphQL queries can automatically refresh themselves just passing a `pollInterval` argument. I told you it was awesome. You set the time interval in milliseconds to refresh the data.\n\nSo, in `useTasks.ts`, our function to get all tasks will look like this, auto-refreshing our data every half second.\n\n```typescript\nfunction useAllTasksInProject(project: any) {\n const { data, loading, error } = useQuery(\n   gql`\n     query GetAllTasksForProject($partition: String!) {\n       tasks(query: { _partition: $partition }) {\n         _id\n         name\n         status\n       }\n     }\n   `,\n   { variables: { partition: project.partition }, pollInterval: 500 }\n );\n if (error) {\n   throw new Error(`Failed to fetch tasks: ${error.message}`);\n }\n\n // If the query has finished, return the tasks from the result data\n // Otherwise, return an empty list\n const tasks = data?.tasks ?? [];\n return { tasks, loading };\n}\n```\n\n![Now we can sync our actions. Adding a task in the Android Emulator gets propagated to the iOS and Web versions](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ionic_native_sync_add_task_cd8ec70ccc.gif)\n\n## Pull to refresh\n\nAdding automatic refresh is nice, but in mobile apps, we’re used to also refreshing lists of data just by pulling them. To get this, we’ll need to add the Ionic component `IonRefresher` to our Home component:\n\n```html\n<IonPage>\n     <IonHeader>\n       <IonToolbar>\n         <IonTitle>Tasks</IonTitle>\n         <IonButtons slot=\"end\">\n           <IonButton onClick={addNewTask}>\n             <IonIcon slot=\"icon-only\" icon={add}></IonIcon>\n           </IonButton>\n         </IonButtons>\n       </IonToolbar>\n     </IonHeader>\n     <IonContent fullscreen>\n       <IonRefresher slot=\"fixed\" onIonRefresh={doRefresh}>\n         <IonRefresherContent></IonRefresherContent>\n       </IonRefresher>\n       <IonHeader collapse=\"condense\">\n         <IonToolbar>\n           <IonTitle size=\"large\">Tasks</IonTitle>\n         </IonToolbar>\n       </IonHeader>\n       <IonList>\n         {loading ? <IonLoading isOpen={loading} /> : null}\n         {tasks.map((task: any) => (\n           <TaskItem key={parseInt(task._id)} {...task}></TaskItem>\n         ))}\n       </IonList>\n     </IonContent>\n   </IonPage>\n```\n\nAs we can see, an `IonRefresher` component will add the pull-to-refresh functionality with an included loading indicator tailored for each platform.\n\n\n```html\n<IonRefresher slot=\"fixed\" onIonRefresh={doRefresh}>\n   <IonRefresherContent></IonRefresherContent>\n</IonRefresher>\n```\n\nTo refresh, we call `doRefresh` and there, we just reload the whole page.\n\n```typescript\n const doRefresh = (event: CustomEvent<RefresherEventDetail>) => {\n   window.location.reload(); // reload the whole page\n   event.detail.complete();  // we signal the loading indicator to hide\n };\n```\n\n\n## Deleting tasks\n\nRight now, we can swipe tasks from right to left to change the status of our tasks. But I wanted to also add a left to right swipe so we can delete tasks. We just need to add the swiping control to the already existing `IonItemSliding` control. In this case, we want a swipe from the _start_ of the control. This way, we avoid any ambiguities with right-to-left vs. left-to-right languages. When the user taps on the new “Delete” button (which will appear red as we’re using the _danger_ color), `deleteTaskSelected` is called.\n\n```html\n<IonItemSliding ref={slidingRef} className={'status-' + task.status}>\n     <IonItem>\n       <IonLabel>{task.name}</IonLabel>\n     </IonItem>\n     <IonItemOptions side=\"end\">\n       <IonItemOption onClick={toggleStatus}>Status</IonItemOption>\n     </IonItemOptions>\n     <IonItemOptions side=\"start\">\n       <IonItemOption onClick={deleteTaskSelected} color=\"danger\">Delete</IonItemOption>\n     </IonItemOptions>\n   </IonItemSliding>\n```\n\nTo delete the task, we use a GraphQL mutation defined in `useTaskMutations.ts`:\n\n```typescript\nconst deleteTaskSelected = () => {\n   slidingRef.current?.close();  // close sliding menu\n   deleteTask(task);             // delete task\n };\n```\n\n\n\n![Deleting can be done with a Swipe. Here we show how it's been done in the Android Emulator and changes appear in the iOS Simulator and Web page](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/ionic_native_delete_184d02206d.gif)\n\n## Recap\n\nIn this post, we’ve seen how easy it is to start with an Ionic React web application and, with only a few lines of code, turn it into a mobile app running on iOS and Android. Then, we easily added some functionality to the three apps at the same time. Ionic makes it super simple to run your Realm-powered apps everywhere!\n\nYou can check out the code from this post [in this branch](https://github.com/mongodb-developer/ionic-realm-demo/tree/observe-changes) of the [repo](https://github.com/mongodb-developer/ionic-realm-demo), just by typing:\n\n```\n$ git clone https://github.com/mongodb-developer/ionic-realm-demo\n$ git checkout observe-changes\n```\n\nBut this is not the only way to integrate Realm in your Ionic apps. Using Capacitor and our native SDKs, we’ll show you how to use Realm from Ionic in a future follow-up post. \n\n\n","description":"We can convert a existing Ionic React Web App that saves data in MongoDB Realm using Apollo GraphQL into an iOS and Android app using a couple commands, and the three apps will share the same MongoDB Realm backend. Also, we can easily add functionality to all three apps, just modifying one code base.\n","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt5c1d523c37102487/644c46ca6988f46f01d764fb/realm_ionic.png?branch=prod","description":null}}]},"slug":"/ionic-realm-web-app-convert-to-mobile-app","title":"*Let’s Give Your Realm-Powered Ionic Web App the Native Treatment on iOS and Android!","original_publish_date":"2021-11-03T08:10:04.802Z","strapi_updated_at":"2022-09-16T10:48:56.133Z","expiry_date":"2022-10-04T16:17:32.918Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*GraphQL","calculated_slug":"/technologies/graphql"}},{"node":{"title":"*React","calculated_slug":"/technologies/react"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:16.656Z","publish_details":{"time":"2023-04-28T22:52:56.238Z"}}},{"calculated_slug":"/products/mongodb/securing-mongodb-with-tls","content":"Hi! I'm [Carl](http://twitter.com/tashian) from [Smallstep](http://smallstep.com). We make it easier to use TLS everywhere. In this post, I’m going to make a case for using TLS/SSL certificates to secure your self-managed MongoDB deployment, and I’ll take you through the steps to enable various TLS features in MongoDB.\n\nMongoDB has very strong support for TLS that can be granularly controlled. At minimum, TLS will let you validate and encrypt connections into your database or between your cluster member nodes. But MongoDB can also be configured to authenticate users using TLS client certificates instead of a password. This opens up the possibility for more client security using short-lived (16-hour) certificates. The addition of Smallstep step-ca, an open source certificate authority, makes it easy to create and manage MongoDB TLS certificates.\n\n## The Case for Certificates\n\nTLS certificates come with a lot of benefits:\n\n* Most importantly, TLS makes it possible to require *authenticated encryption* for every database connection—just like SSH connections.\n* Unlike SSH keys, certificates expire. You can issue ephemeral (e.g., five-minute) certificates to people whenever they need to access your database, and avoid having long-lived key material (like SSH keys) sitting around on people's laptops.\n* Certificates allow you to create a trust domain around your database. MongoDB can be configured to refuse connections from clients who don’t have a certificate issued by your trusted Certificate Authority (CA).\n* Certificates can act as user login credentials in MongoDB, replacing passwords. This lets you delegate MongoDB authentication to a CA. This opens the door to further delegation via OpenID Connect, so you can have Single Sign-On MongoDB access.\n\nWhen applied together, these benefits offer a level of security comparable to an SSH tunnel—without the need for SSH.\n\n## MongoDB TLS \n\nHere’s an overview of TLS features that can be enabled in MongoDB:\n\n* **Channel encryption**: The traffic between clients and MongoDB is encrypted. You can enable channel encryption using self-signed TLS certificates. Self-signed certificates are easy to create, but they will not offer any client or server identity validation, so you will be vulnerable to man-in-the-middle attacks. This option only makes sense within a trusted network.\n* **Identity validation**: To enable identity validation on MongoDB, you’ll need to run an X.509 CA that can issue certificates for your MongoDB hosts and clients. Identity validation happens on both sides of a MongoDB connection:\n    * **Client identity validation**: Client identity validation means that the database can ensure all client connections are coming from *your* authorized clients. In this scenario, the client has a certificate and uses it to authenticate itself to the database when connecting.\n    * **Server identity validation**: Server identity validation means that MongoDB clients can ensure that they are talking to your MongoDB database. The server has an identity certificate that all clients can validate when connecting to the database.\n* **Cluster member validation**: MongoDB can require all members of a cluster to present valid certificates when they join the cluster. This encrypts the traffic between cluster members.\n* **X.509 User Authentication**: Instead of passwords, you can use X.509 certificates as login credentials for MongoDB users.\n* **Online certificate rotation**: Use short-lived certificates and MongoDB online certificate rotation to automate operations.\n\nTo get the most value from TLS with your self-managed MongoDB deployment, you need to run a CA (the fully-managed [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) comes with TLS features enabled by default).\n\nSetting up a CA used to be a difficult, time-consuming hassle requiring deep domain knowledge. Thanks to emerging protocols and tools, it has become a lot easier for any developer to create and manage a simple private CA in 2021. At Smallstep, we’ve created an [open source online CA called step-ca](https://smallstep.com/certificates/) that’s secure and easy to use, either online or offline.\n\n## TLS Deployment with MongoDB and Smallstep step-ca\n\nHere are the main steps required to secure MongoDB with TLS. If you’d like to try it yourself, you can find a [series of blog posts](https://smallstep.com/blog/securing-mongodb-with-tls-part-one/) on the Smallstep website detailing the steps:\n\n* Set up a CA. A single step-ca instance is sufficient. When you run your own CA and use short-lived certificates, you can avoid the complexity of managing CRL and OCSP endpoints by using passive revocation. With passive revocation, if a key is compromised, you simply block the renewal of its certificate in the CA.\n* For server validation, issue a certificate and private key to your MongoDB server and configure server TLS.\n* For client validation, issue certificates and private keys to your clients and configure client-side TLS.\n* For cluster member validation, issue certificates and keys to your MongoDB cluster members and configure cluster TLS.\n* Deploy renewal mechanisms for your certificates. For example, certificates used by humans could be renewed manually when a database connection is needed. Certificates used by client programs or service accounts can be renewed with a scheduled job.\n* To enable X.509 user authentication, you’ll need to add X.509-authenticated users to your database, and configure your clients to attempt X.509 user authentication when connecting to MongoDB.\n* Here’s the icing on the cake: Once you’ve set all of this up, you can configure step-ca to allow users to get MongoDB certificates via an identity provider, using OpenID Connect. This is a straightforward way to enable Single Sign-on for MongoDB.\n\nFinally, it’s important to note that it’s possible to stage the migration of an existing MongoDB cluster to TLS: You can make TLS connections to MongoDB optional at first, and only require client validation once you’ve migrated all of your clients.\n\nReady to get started? In this Smallstep [series of tutorials](https://smallstep.com/blog/securing-mongodb-with-tls-part-one/), we’ll take you through this process step-by-step.","description":"Learn how to secure your self-managed MongoDB TLS deployment with certificates using the Smallstep open source online certificate authority.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltfdb8bf506fb97938/644c46cb1d9ecce690e8633c/security.png?branch=prod","description":null}}]},"slug":"/securing-mongodb-with-tls","title":"*Securing MongoDB with TLS","original_publish_date":"2021-10-06T13:00:00.215Z","strapi_updated_at":"2022-05-09T19:23:40.398Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*TLS","calculated_slug":"/technologies/tls"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:16.278Z","publish_details":{"time":"2023-04-28T22:52:57.210Z"}}},{"calculated_slug":"/products/mongodb/paginations-time-series-collections-in-five-minutes","content":"# Paginations 1.0: Time-Series Collections in 5 Minutes\n# \n\nAs someone who loves to constantly measure myself and everything around me, I was excited to see MongoDB add dedicated time-series collections in MongoDB 5.0. Previously, MongoDB had been great for handling time-series data, but only if you were prepared to write some fairly complicated insert and update code and use a complex schema. In 5.0, all the hard work is done for you, including lots of behind-the-scenes optimization.\n\nWorking with time-series data brings some interesting technical challenges for databases. Let me explain.\n\n## What is time-series data?\n\nTime-series data is where we have multiple related data points that have a time, a source, and one or more values. For example, I might be recording my speed on my bike and the gradient of the road, so I have the time, the source (me on that bike), and two data values (speed and gradient). The source would change if it was a different bike or another person riding it.\n\nTime-series data is not simply any data that has a date component, but specifically data where we want to look at how values change over a period of time and so need to compare data for a given time window or windows. On my bike, am I slowing down over time on a ride? Or does my speed vary with the road gradient?\n\nThis means when we store time-series data, we usually want to retrieve or work with all data points for a time period, or all data points for a time period for one or more specific sources.\n\nThese data points tend to be small. A time is usually eight bytes, an identifier is normally only (at most) a dozen bytes, and a data point is more often than not one or more eight-byte floating point numbers. So, each \"record\" we need to store and access is perhaps 50 or 100 bytes in length.\n## \n## Why time-series data needs special handling\n## \nThis is where dealing with time-series data gets interesting—at least, I think it's interesting. Most databases, MongoDB included, store data on disks, and those are read and written by the underlying hardware in blocks of typically 4, 8, or 32 KB at a time.  Because of these disk blocks, the layers on top of the physical disks—virtual memory, file systems, operating systems, and databases—work in blocks of data too. MongoDB, like all databases, uses blocks of records when reading,writing, and caching. Unfortunately, this can make reading and writing these tiny little time-series records much less efficient.\n\nThis animation shows what happens when these records are simply inserted into a general purpose database such as MongoDB or an RDBMS.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Simple_TS_0780f7e57f.gif)\n\n\n\nAs each record is received, it is stored sequentially in a block on the disk. To allow us to access them, we use two indexes: one with the unique record identifier, which is required for replication, and the other with the source and timestamp to let us find everything for a specific device over a time period.\n\nThis is fine for writing data. We have quick sequential writing and we can amortise disk flushes of blocks to get a very high write speed.\n\nThe issue arises when we read. In order to find the data about one device over a time period, we need to fetch many of these small records. Due to the way they were stored, the records we want are spread over multiple database blocks and disk blocks. For each block we have to read, we pay a penalty of having to read and process the whole block, using database cache space equivalent to the block size. This is a lot of wasted compute resources.\n\n## Time-series specific collections\n\nMongoDB 5.0 has specialized time-series collections optimized for this type of data, which we can use simply by adding two parameters when creating a collection.\n\n\n```\n   db.createCollection(\"readings\",\n            \"time-series\" :{ \"timeField\" : \"timestamp\",\n                            \"metaField\" : \"deviceId\"}})\n```\n \nWe don't need to change the code we use for reading or writing at all. MongoDB takes care of everything for us behind the scenes. This second animation shows how.\n\n![Small Documents being loaded into a Time-Series collection.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Real_TS_e80004822f.gif)\n\n\nWith a time-series collection, MongoDB organizes the writes so that data for the same source is stored in the same block, alongside other data points from a similar point in time. The blocks are limited in size (because so are disk blocks)  and once we have enough data in a block, we will automatically create another one. The important point is that each block will cover one source and one span of time, and we have an index for each block to help us find that span.\n\nDoing this means we can have much smaller indexes as we only have one unique identifier  per block. We also only have one index per block, typically for the source and time range. This results in an overall  reduction in index size of hundreds of times.\n\nNot only that but by storing data like this, MongoDB is better able to apply compression. Over time, data for a source will not change randomly, so we can compress the changes in values that are co-located. This makes for a data size improvement of at least three to five times.\n\nAnd when we come to read it, we can read it several times faster as we no longer need to read data, which is not relevant to our query just to get to the data we want.\n\n## Summing up time-series collections\n\nAnd that, in a nutshell, is MongoDB time-series collections. I can just specify the time and source fields when creating a collection and MongoDB will reorganise my cycling data to make it three to five times smaller, as well as faster, to read and analyze.\n\n\n\n\n\n\n\n\n\n\n\n\n\n","description":"A brief, animated  introduction to what Time-Series data is, why is challenging for traditional database structures and how MongoDB Time-Series Collections are specially adapted to managing this sort of data.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltb14b657d702285e8/644c46cc27e0fb60639417cb/docs_bb8ada8243.png?branch=prod","description":null}}]},"slug":"/Paginations-Time-Series-Collections-in-five-minutes","title":"*Paginations 1.0: Time Series Collections in five minutes","original_publish_date":"2021-10-21T10:48:35.291Z","strapi_updated_at":"2022-05-19T22:15:47.876Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Time series","calculated_slug":"/products/mongodb/time-series"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:15.911Z","publish_details":{"time":"2023-04-28T22:52:57.240Z"}}},{"calculated_slug":"/products/mongodb/paginations-why-choose-mongodb","content":"# Paginations 2.0: Why I Would Choose MongoDB\n\n\nI've been writing and designing large scale, multi-user, applications with database backends since 1995, as lead architect for intelligence management systems, text mining, and analytics platforms, and as a consultant working in retail and investment banking, mobile games, connected-car IoT projects, and country scale document management. It's fair to say I've seen how a lot of applications are put together.\n\nNow it's also reasonable to assume that as I work for MongoDB, I have some bias, but MongoDB isn't my first database, or even my first document database, and so I do have a fairly broad perspective. I'd like to share with you three features of MongoDB that would make it my first choice for almost all large, multi-user database applications.\n\n## The Document Model\n\nThe Document model is a fundamental aspect of MongoDB. All databases store records—information about things that have named attributes and values for those attributes. Some attributes might have multiple values. In a tabular database, we break the record into multiple rows with a single scalar value for each attribute and have a way to relate those rows together to access the record.\n\nThe difference in a Document database is when we have multiple values for an attribute, we can retain those as part of a single record, storing access and manipulating them together. We can also group attributes together to compare and refer to them as a group. For example, all the parts of an address can be accessed as a single address field or individually.\n\nWhy does this matter? Well, being able to store an entire record co-located on disk and in memory has some huge advantages.\n\nBy having these larger, atomic objects to work with, there are some oft quoted benefits like making it easier for OO developers and reducing the computational overheads of accessing the whole record, but this misses a third, even more important benefit.\n\nWith the correct schema, documents reduce each database write operation to single atomic changes of one piece of data. This has two huge and related benefits.\n\nBy only requiring one piece of data to be examined for its current state and changed to a new state at a time, the period of time where the database state is unresolved is reduced to almost nothing. Effectively, there is no interaction between multiple writes to the database and none have to wait for another to complete, at least not beyond a single change to a single document.\n\nIf we have to use traditional transactions, whether in an RDBMS or MongoDB, to perform a change then all records concerned remain effectively locked until the transaction is complete. This  greatly widens the window for contention and delay. Using the document model instead, you can remove all contention in your database and achieve far higher 'transactional' throughput in a multi-user system.\n\nThe second part of this is that when each write to the database can be treated as an independent operation, it makes it easy to horizontally scale the database to support large workloads as the state of a document on one server has no impact on your ability to change a document on another. Every operation can be parallelised.\n\nDoing this does require you to design your schema correctly, though. Document databases are far from schemaless (a term MongoDB has not used for many years). In truth, it makes schema design even more important than in an RDBMS.\n\n## Highly Available as standard\n\nThe second reason I would choose to use MongoDB is that high-availability is at the heart of the database. MongoDB is designed so that a server can be taken offline instantly, at any time and there is no loss of service or data. This is absolutely fundamental to how all of MongoDB is designed. It doesn't rely on specialist hardware, third-party software, or add-ons. It allows for replacement of servers, operating systems, and even database versions invisibly to the end user, and even mostly to the developer. This goes equally for Atlas, where MongoDB can provide a multi-cloud database service at any scale that is resilient to the loss of an entire cloud provider, whether it’s Azure, Google, or Amazon. This level of uptime is unprecedented.\n\nSo, if I plan to develop a large, multi-user application I just want to know the database will always be there, zero downtime, zero data loss, end of story. \n\n\n## Smart Update Capability\n\nThe third reason I would choose MongoDB is possibly the most surprising. Not all document databases are the same, and  allow you to realise all the benefits of a document versus relational model, some are simply JSON stores or Key/Value stores where the value is some form of document.\n\nMongoDB has the powerful, specialised update operators capable of doing more than simply replacing a document or a value in the database. With MongoDB, you can, as part of a single atomic operation, verify the state of values in the document, compute the new value for any field based on it and any other fields, sort and truncate arrays when adding to them and, should you require it automatically, create a new document rather than modify an existing one.\n\nIt is this \"smart\" update capability that makes MongoDB capable of being a principal, \"transactional\" database in large, multi-user systems versus a simple store of document shaped data.\n\nThese three features, at the heart of an end-to-end data platform, are what genuinely make MongoDB my personal first choice when I want to build a system to support many users with a snappy user experience, 24 hours a day, 365 days a year.\n","description":"Distinguished Engineer and 25 year NoSQL veteran John Page explains in 5 minutes why MongoDB would be his first choice for building a multi-user application.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltb14b657d702285e8/644c46cc27e0fb60639417cb/docs_bb8ada8243.png?branch=prod","description":null}}]},"slug":"/Paginations-Why-Choose-MongoDB","title":"*Paginations 2.0: Why I Would Choose MongoDB","original_publish_date":"2022-05-13T14:59:52.753Z","strapi_updated_at":"2022-05-19T22:15:23.625Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Schema","calculated_slug":"/products/mongodb/schema"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:15.541Z","publish_details":{"time":"2023-04-28T22:52:57.266Z"}}},{"calculated_slug":"/products/atlas/mongodb-charts-embedding-sdk-react","content":"## Introduction\n\nIn the [previous blog post](https://www.mongodb.com/developer/how-to/react-query-rest-api-realm/) of this series, we created a React website that was retrieving a list of countries using Axios and a REST API hosted in [MongoDB Realm](https://www.mongodb.com/realm).\n\nIn this blog post, we will continue to build on this foundation and create a dashboard with COVID-19 charts, built with [MongoDB Charts](https://www.mongodb.com/products/charts) and embedded in a React website with the [MongoDB Charts Embedding SDK](https://docs.mongodb.com/charts/saas/embedding-charts-sdk/).\n\nTo add some spice in the mix, we will use our list of countries to create a dynamic filter so we can filter all the COVID-19 charts by country.\n\nYou can see the **final result [here](https://react-charts-npwaa-qynhm.mongodbstitch.com/)** that I hosted in a [MongoDB Realm](https://www.mongodb.com/realm) application using the [static hosting feature](https://docs.mongodb.com/realm/hosting/) available.\n\n![Final website](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/website_2255a0a81c.png)\n\n## Prerequisites\n\nThe code of this project is available on GitHub in [this repository](https://github.com/mongodb-developer/mongodb-charts-embedded-react).\n\n```shell\ngit clone git@github.com:mongodb-developer/mongodb-charts-embedded-react.git\n```\n\nTo run this project, you will need `node` and `npm` in a recent version. Here is what I'm currently using:\n\n```shell\n$ node -v \nv14.17.1\n$ npm -v\n8.0.0\n```\n\nYou can run the project locally like so:\n\n```sh\n$ cd mongodb-realm-react-charts\n$ npm install\n$ npm start\n```\n\nIn the next sections of this blog post, I will explain what we need to do to make this project work.\n\n## Create a MongoDB Charts Dashboard\n\nBefore we can actually embed our charts in our custom React website, we need to create them in MongoDB Charts.\n\nHere is the [link to the dashboard I created](https://charts.mongodb.com/charts-open-data-covid-19-zddgb/public/dashboards/1b47162b-ad48-4611-8c1b-a23df8844fdc) for this website. It looks like this.\n\n![Dashboard in MongoDB Charts](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dashboard_dc3427c223.png)\n\nIf you want to use the same data as me, check out [this blog post about the Open Data COVID-19 Project](https://www.mongodb.com/developer/article/johns-hopkins-university-covid-19-data-atlas/) and especially [this section](https://www.mongodb.com/developer/article/johns-hopkins-university-covid-19-data-atlas/#take-a-copy-of-the-data) to duplicate the data in your own cluster in MongoDB Atlas.\n\nAs you can see in [the dashboard](https://charts.mongodb.com/charts-open-data-covid-19-zddgb/public/dashboards/1b47162b-ad48-4611-8c1b-a23df8844fdc), my charts are not filtered by country here. You can find the data of all the countries in the four charts I created.\n\n## Enable the Filtering and the Embedding\n\nTo enable the filtering when I'm embedding my charts in my website, I must tell MongoDB Charts which field(s) I will be able to filter by, based on the fields available in my collection. Here, I chose to filter by a single field, `country`, and I chose to enable the unauthenticated access for this public blog post (see below).\n\n![Embed menu in MongoDB Charts](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/embed_menu_55208eb666.png)\n\nIn the `User Specified Filters` field, I added `country` and chose to use the JavaScript SDK option instead of the iFrame alternative that is less convenient to use for a React website with dynamic filters.\n\n![Embedding Menu in my Charts](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/embed_chart_7b5fe5d479.png)\n\nFor each of the four charts, I need to retrieve the `Charts Base URL` (unique for a dashboard) and the `Charts IDs`.\n\nNow that we have everything we need, we can go into the React code.\n\n## React Website\n\n### MongoDB Charts Embedding SDK\n\nFirst things first: We need to install the [MongoDB Charts Embedding SDK](https://www.npmjs.com/package/@mongodb-js/charts-embed-dom) in our project.\n\n```shell\nnpm i @mongodb-js/charts-embed-dom\n```\n\nIt's already done in the project I provided above but it's not if you are following from the [first blog post](https://www.mongodb.com/developer/how-to/react-query-rest-api-realm/).\n\n### React Project\n\nMy React project is made with just two [function components](https://reactjs.org/docs/components-and-props.html#function-and-class-components): `Dashboard` and `Chart`.\n\nThe `index.js` root of the project is just calling the `Dashboard` function component.\n\n```js\nimport React from 'react';\nimport ReactDOM from 'react-dom';\nimport Dashboard from \"./Dashboard\";\n\nReactDOM.render(<React.StrictMode>\n  <Dashboard/>\n</React.StrictMode>, document.getElementById('root'));\n```\n\nThe `Dashboard` is the central piece of the project: \n\n```js\nimport './Dashboard.css';\nimport {useEffect, useState} from \"react\";\nimport axios from \"axios\";\nimport Chart from \"./Chart\";\n\nconst Dashboard = () => {\n  const url = 'https://webhooks.mongodb-stitch.com/api/client/v2.0/app/covid-19-qppza/service/REST-API/incoming_webhook/metadata';\n  const [countries, setCountries] = useState([]);\n  const [selectedCountry, setSelectedCountry] = useState(\"\");\n  const [filterCountry, setFilterCountry] = useState({});\n\n  function getRandomInt(max) {\n    return Math.floor(Math.random() * max);\n  }\n\n  useEffect(() => {\n    axios.get(url).then(res => {\n      setCountries(res.data.countries);\n      const randomCountryNumber = getRandomInt(res.data.countries.length);\n      let randomCountry = res.data.countries[randomCountryNumber];\n      setSelectedCountry(randomCountry);\n      setFilterCountry({\"country\": randomCountry});\n    })\n  }, [])\n\n  useEffect(() => {\n    if (selectedCountry !== \"\") {\n      setFilterCountry({\"country\": selectedCountry});\n    }\n  }, [selectedCountry])\n\n  return <div className=\"App\">\n    <h1 className=\"title\">MongoDB Charts</h1>\n    <h2 className=\"title\">COVID-19 Dashboard with Filters</h2>\n    <div className=\"form\">\n      {countries.map(c => <div className=\"elem\" key={c}>\n        <input type=\"radio\" name=\"country\" value={c} onChange={() => setSelectedCountry(c)} checked={c === selectedCountry}/>\n        <label htmlFor={c} title={c}>{c}</label>\n      </div>)}\n    </div>\n    <div className=\"charts\">\n      <Chart height={'600px'} width={'800px'} filter={filterCountry} chartId={'6e3cc5ef-2be2-421a-b913-512c80f492b3'}/>\n      <Chart height={'600px'} width={'800px'} filter={filterCountry} chartId={'be3faa53-220c-438f-aed8-3708203b0a67'}/>\n      <Chart height={'600px'} width={'800px'} filter={filterCountry} chartId={'7ebbba33-a92a-46a5-9e80-ba7e8f3b13de'}/>\n      <Chart height={'600px'} width={'800px'} filter={filterCountry} chartId={'64f3435e-3b83-4478-8bbc-02a695c1a8e2'}/>\n    </div>\n  </div>\n};\n\nexport default Dashboard;\n```\n\nIt's responsible for a few things:\n\n- Line 17 - Retrieve the list of countries from the REST API using Axios (cf [previous blog post](https://www.mongodb.com/developer/how-to/react-query-rest-api-realm/)).\n- Lines 18-22 - Select a random country in the list for the initial value.\n- Lines 22 & 26 - Update the filter when a new value is selected (randomly or manually).\n- Line 32 `counties.map(...)` - Use the list of countries to build a list of radio buttons to update the filter.\n- Line 32 `<Charts .../> x4` - Call the `Chart` component one time for each chart with the appropriate props, including the filter and the Chart ID.\n\nAs you may have noticed here, I'm using the same filter `fitlerCountry` for all the Charts, but nothing prevents me from using a custom filter for each Chart.\n\nYou may also have noticed a very minimalistic CSS file `Dashboard.css`. Here it is: \n\n```css\n.title {\n  text-align: center;\n}\n\n.form {\n  border: solid black 1px;\n}\n\n.elem {\n  overflow: hidden;\n  display: inline-block;\n  width: 150px;\n  height: 20px;\n}\n\n.charts {\n  text-align: center;\n}\n\n.chart {\n  border: solid #589636 1px;\n  margin: 5px;\n  display: inline-block;\n}\n```\n\nThe `Chart` component looks like this:\n\n```js\nimport React, {useEffect, useRef, useState} from 'react';\nimport ChartsEmbedSDK from \"@mongodb-js/charts-embed-dom\";\n\nconst Chart = ({filter, chartId, height, width}) => {\n  const sdk = new ChartsEmbedSDK({baseUrl: 'https://charts.mongodb.com/charts-open-data-covid-19-zddgb'});\n  const chartDiv = useRef(null);\n  const [rendered, setRendered] = useState(false);\n  const [chart] = useState(sdk.createChart({chartId: chartId, height: height, width: width, theme: \"dark\"}));\n\n  useEffect(() => {\n    chart.render(chartDiv.current).then(() => setRendered(true)).catch(err => console.log(\"Error during Charts rendering.\", err));\n  }, [chart]);\n\n  useEffect(() => {\n    if (rendered) {\n      chart.setFilter(filter).catch(err => console.log(\"Error while filtering.\", err));\n    }\n  }, [chart, filter, rendered]);\n\n  return <div className=\"chart\" ref={chartDiv}/>;\n};\n\nexport default Chart;\n```\n\nThe `Chart` component isn't doing much. It's just responsible for rendering the Chart **once** when the page is loaded and reloading the chart if the filter is updated to display the correct data (thanks to React).\n\nNote that the second useEffect (with the `chart.setFilter(filter)` call) shouldn't be executed if the chart isn't done rendering. So it's protected by the `rendered` state that is only set to `true` once the chart is rendered on the screen.\n\nAnd voilà! If everything went as planned, you should end up with a (not very) beautiful website like [this one](https://react-charts-npwaa-qynhm.mongodbstitch.com/).\n\n## Conclusion\n\nIn this blog post, your learned how to embed [MongoDB Charts](https://www.mongodb.com/products/charts) into a [React](https://reactjs.org/) website using the [MongoDB Charts Embedding SDK](https://docs.mongodb.com/charts/saas/embedding-charts-sdk/).\n\nWe also learned how to create dynamic filters for the charts using `useEffect()`.\n\nWe didn't learn how to secure the Charts with an authentication token, but you can learn how to do that in [this documentation](https://docs.mongodb.com/charts/saas/embed-chart-stitch-auth/). \n\nIf you have questions, please head to our [developer community website](https://www.mongodb.com/community/forums/) where the MongoDB engineers and the MongoDB community will help you build your next big idea with MongoDB.","description":"In this blog post, we are creating a dynamic dashboard using React and the MongoDB Charts Embedding SDK with filters.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt9c9fda560f0e4689/644c46cd3fa0e77905598489/realm-functions.jpg?branch=prod","description":null}}]},"slug":"/mongodb-charts-embedding-sdk-react/","title":"*MongoDB Charts Embedding SDK with React","original_publish_date":"2021-10-20T13:58:48.892Z","strapi_updated_at":"2023-01-26T16:56:03.124Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Charts","calculated_slug":"/products/atlas/charts"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*React","calculated_slug":"/technologies/react"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to use the MongoDB Charts Embedding SDK with React to create a dashboard with filters.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt129e431d7164cdb1/644c46cf05c6a326dc511e4c/og-realm-functions.jpg?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:15.165Z","publish_details":{"time":"2023-04-28T22:52:57.296Z"}}},{"calculated_slug":"/products/realm/realm-using-realm-sync-in-unity","content":"Playing a game locally can be fun at times. But there is nothing more exciting than playing with or against the whole world. Using Realm Sync you can easily synchronize data between multiple instances and turn your local game into an online experience.\n\nIn [a previous tutorial](https://www.mongodb.com/developer/how-to/realm-how-to-add-realm-to-your-unity-project/) we showed how to use Realm locally to persist your game's data. We will build on the local Realm to show how to easily transition to Realm Sync.\n\nIf you have not used local Realms before we recommend working through the previous tutorial first so you can easily follow along here when we build on them.\n\nYou can find the local Realm example that this tutorial is based on in [our example repository at Github](https://github.com/realm/unity-examples/tree/3d-chess/local-realm) and use it to follow along.\n\nThe [final of result of this tutorial](https://github.com/realm/unity-examples/tree/3d-chess/synced-realm) can also be found in the examples reposity.\n\n## MongoDB Realm Sync and MongoDB Atlas\n\nThe local Realm database we have seen in the previous tutorial is one of three components we need to synchronize data between multiple instances of our game. The other two are [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) and [MongoDB Realm Sync](https://www.mongodb.com/realm).\n\nWe will use Atlas as our backend and cloud-based database. Realm Sync on the other side enables sync between your local Realm database and Atlas, seamlessly stitching together the two components into an application layer for your game. To support these services, MongoDB Realm also provides [components to fulfill several common application requirements](https://docs.mongodb.com/realm/get-started/introduction-mobile/#mongodb-realm) from which we will be using the [Realm Users and Authentication](https://docs.mongodb.com/realm/authentication/) feature to register and login the user.\n\nThere are a couple of things we need to prepare in order to enable synchronisation in our app. You can find an overview on [how to get started with MongoDB Realm Sync](https://docs.mongodb.com/realm/sync/get-started/#std-label-enable-sync) in the documentation. Here are the steps we need to take:\n\n- [Create an Atlas account](https://www.mongodb.com/cloud/atlas/register?tck=docs_realm)\n- [Create a Realm App](https://docs.mongodb.com/realm/manage-apps/create/create-with-realm-ui/#std-label-create-a-realm-app)\n- [Enable Sync](https://docs.mongodb.com/realm/sync/rules/#std-label-sync-rules)\n- [Enable Developer Mode](https://docs.mongodb.com/realm/sync/enable-development-mode/#std-label-enable-development-mode)\n- [Enable email registration](https://docs.mongodb.com/realm/authentication/email-password/) and choose `Automatically confirm users` under `User Confirmation Method`\n\n## Example\n\nWe will build on the local Realm example we created in the previous tutorial using the 3D chess game. To get you started easily you can find the final result in our [examples reposity](https://github.com/realm/unity-examples/tree/3d-chess/local-realm) (branch: `local-realm`).\n\nThe local Realm is based on four building blocks:\n\n- `PieceEntity`\n- `Vector3Entity`\n- `PieceSpawner`\n- `GameState`\n\nThe `PieceEntity` along with the `Vector3Entity` represents our model which include the two properties that make up a chess piece: type and position.\n\n```cs\n...\n\npublic class PieceEntity : RealmObject\n{\n    public PieceType PieceType\n    {\n        ...\n    }\n\n    public Vector3 Position\n    {\n        ...\n    }\n    ...\n}\n```\n\nIn the previous tutorial we have also added functionality to persist changes in position to the Realm and react to changes in the database that have to be reflected in the model. This was done by implementing `OnPropertyChanged` in the `Piece` and `PieceEntity` respectively.\n\nThe `PieceSpawner` is responsible for spawning new `Piece` objects when the game starts via `public void CreateNewBoard(Realm realm)`. Here we can see some of the important functions that we need when working with Realm:\n\n- `Write`: Starts a new write transaction which is necessary to change the state of the database.\n- `Add`: Adds a new `RealmObject` to the database that has not been there before.\n- `RemoveAll`: Removes all objects of a specified type from the database.\n\nAll of this comes together in the central part of the game that manages the flow of it: `GameState`. The `GameState` open the Realm using `Realm.GetInstance()` in `Awake` and offers an option to move pieces via `public void MovePiece(Vector3 oldPosition, Vector3 newPosition)` which also checks if a `Piece` already exists at the target location. Furthermore we subscribe for notifications to set up the initial board. One of the things we will be doing in this tutorial is to expand on this subscription mechanic to also react to changes that come in through Realm Sync.\n\n## Extending the model\n\nThe first thing we need to change to get the local Realm example ready for Sync is to [add a PrimaryKey to the PieceType](https://docs.mongodb.com/realm/sdk/dotnet/quick-start-with-sync/#define-your-object-model). This is a mandatory requirement for Sync to make sure objects can be distinguished from each other. We will be using the field `Id` here. Note that you can add a `MapTo` attribute in case the name of the field in the `RealmObject` differs from the name set in Atlas. By default the primary key is named `_id` in Atlas which would conflict with the .NET coding guidelines. By adding `[MapTo(\"_id\")]` we can address this fact.\n\n```cs\nusing MongoDB.Bson;\n```\n\n```cs\n[PrimaryKey]\n[MapTo(\"_id\")]\npublic ObjectId Id { get; set; } = ObjectId.GenerateNewId();\n```\n\n## Who am I playing with?\n\nThe local Realm tutorial showed you how to create a persisted game locally. While you could play with someone else using the same game client, there was only ever one game running at a time since every game is accessing the same table in the database and therefore the same objects.\n\nThis would still be the same when using Realm Sync if we do not separate those games. Everyone accessing the game from wherever they are would see the same state. We need a way to create multiple games and identify which one we are playing. Realm Sync offers a feature that let's us achieve exactly this: [partitions](https://docs.mongodb.com/realm/sync/partitions/).\n\n> A partition represents a subset of the documents in a synced cluster that are related in some way and have the same read/write permissions for a given user. Realm directly maps partitions to individual synced .realm files so each object in a synced realm has a corresponding document in the partition.\n\nWhat does this mean for our game? If we use one partition per match we can make sure that only players using the same partition will actually play the same game. Furthermore, we can start as many games as we want. Using the same partition simply means using the same `partiton key` when opening a synced Realm. Partition keys are restricted to the following types: `String`, `ObjectID`, `Guid`, `Long`.\n\nFor our game we will use a string that we ask the user for when they start the game. We will do this by adding a new scene to the game which also acts as a welcome and loading scene.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/00_welcome_screen_9315707515.png\" alt=\"Welcome Screen\" width=\"125\"/>\n\nGo to `Assets -> Create -> Scene` to create a new scene and name it `WelcomeScene`. Double click it to activate it.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/01_add_scene_6b483d6635.png)\n\nUsing `GameObject -> UI` we then add `Text`, `Input Field` and `Button` to the new scene. The input will be our partition key. To make it easier to understand for the player we will call its placeholder `game id`. The `Text` object can be set to `Your Game ID:` and the button's text to `Start Game`. Make sure to reposition them to your liking.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/02_adding_ui_b2879af9c7.png\" alt=\"Adding UI\" width=\"500\"/>\n\n## Getting everything in Sync\n\nAdd a script to the button called `StartGameButton` by clicking `Add Component` in the Inspector with the start button selected. Then select `script` and type in its name.\n\n```cs\nusing Realms;\nusing Realms.Sync;\nusing System;\nusing System.IO;\nusing System.Threading.Tasks;\nusing UnityEngine;\nusing UnityEngine.SceneManagement;\nusing UnityEngine.UI;\n\npublic class StartGameButton : MonoBehaviour\n{\n    [SerializeField] private GameObject loadingIndicator = default; // 1\n    [SerializeField] private InputField gameIdInputField = default; // 2\n\n    public async void OnStartButtonClicked() // 3\n    {\n        loadingIndicator.SetActive(true); // 4\n\n        // 5\n        var gameId = gameIdInputField.text;\n        PlayerPrefs.SetString(Constants.PlayerPrefsKeys.GameId, gameId);\n\n        await CreateRealmAsync(gameId); // 5\n\n        SceneManager.LoadScene(Constants.SceneNames.Main); // 13\n    }\n    \n    private async Task CreateRealmAsync(string gameId)\n    {\n        var app = App.Create(Constants.Realm.AppId); // 6\n        var user = app.CurrentUser; // 7\n\n        if (user == null) // 8\n        {\n            // This example focuses on an introduction to Sync.\n            // We will keep the registration simple for now by just creating a random email and password.\n            // We'll also not create a separate registration dialog here and instead just register a new user every time.\n            // In a different example we will focus on authentication methods, login / registration dialogs, etc.\n            var email = Guid.NewGuid().ToString();\n            var password = Guid.NewGuid().ToString();\n            await app.EmailPasswordAuth.RegisterUserAsync(email, password); // 9\n            user = await app.LogInAsync(Credentials.EmailPassword(email, password)); // 10\n        }\n\n        RealmConfiguration.DefaultConfiguration = new SyncConfiguration(gameId, user);\n\n        if (!File.Exists(RealmConfiguration.DefaultConfiguration.DatabasePath)) // 11\n        {\n            // If this is the first time we start the game, we need to create a new Realm and sync it.\n            // This is done by `GetInstanceAsync`. There is nothing further we need to do here.\n            // The Realm is then used by `GameState` in it's `Awake` method.\n            using var realm = await Realm.GetInstanceAsync(); // 12\n        }\n    }\n}\n```\n\nThe `StartGameButton` knows two other game objects: the `gameIdInputField` (1) that we created above and a `loadingIndicator` (2) that we will be creating in a moment. If offers one action that will be executed when the button is clicked: `OnStartButtonClicked` (3).\n\nFirst, we want to show a loading indicator (4) in case loading the game takes a moment. Next we grab the `gameId` from the `InputField` and save it using the [`PlayerPrefs`](https://docs.unity3d.com/ScriptReference/PlayerPrefs.html). Saving data using the `PlayerPrefs` is acceptable if it is user input that does not need to be saved safely and only has a simple structure since `PlayerPrefs` can only take a limited set of data types: `string`, `float`, `int`.\n\nNext, we need to create a Realm (5). Note that this is done asynchrounously using `await`. There are a couple of components necessary for opening a synced Realm:\n\n- `app`: An instance of `App` (6) represents your Realm App that you created in Atlas. Therefore we need to pass the `app id` in here.\n- `user`: If a user has been logged in before, we can access them by using `app.CurrentUser` (7). In case there has not been a successful login before this variable will be null (8) and we need to register a new user.\n\nThe actual values for `email` and `password` are not really relevant for this example. In your game you would use more `Input Field` objects to ask the user for this data. Here we can just use `Guid` to generate random values. Using `EmailPasswordAuth.RegisterUserAsync` offered by the `App` class we can then register the user (9) and finally log them in (10) using these credentials. Note that we need to await this asynchrounous call again.\n\nWhen we are done with the login, all we need to do is to create a new `SyncConfiguration` with the `gameId` (which acts as our partition key) and the `user` and save it as the `RealmConfiguration.DefaultConfiguration`. This will make sure whenever we open a new Realm, we will be using this `user` and `partitionKey`.\n\nFinally we want to open the Realm and synchronize it to get it ready for the game. We can detect if this is the first start of the game simply by checking if a Realm file for the given coonfiguration already exists or not (11). If there is no such file we open a Realm using `Realm.GetInstanceAsync()` (12) which automatically uses the `DefaultConfiguration` that we set before.\n\nWhen this is done, we can load the main scene (13) using the `SceneManager`. Note that the name of the main scene was extracted into a file called `Constants` in which we also added the app id and the key we use to save the `game id` in the `PlayerPrefs`. You can either add another class in your IDE or in Unity (using `Assets -> Create -> C# Script`).\n\n```cs\nsealed class Constants\n{\n    public sealed class Realm\n    {\n        public const string AppId = \"insert your Realm App ID here\";\n    }\n\n    public sealed class PlayerPrefsKeys\n    {\n        public const string GameId = \"GAME_ID_KEY\";\n    }\n\n    public sealed class SceneNames\n    {\n        public const string Main = \"MainScene\";\n    }\n}\n```\n\nOne more thing we need to do is adding the main scene in the build settings, otherwise the `SceneManager` will not be able to find it. Go to `File -> Build Settings ...` and click `Add Open Scenes` while the `MainScene` is open.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/03_build_settings_3da2b07e8b.png\" alt=\"Build Settings\" width=\"500\"/>\n\nWith these adjustments we are ready to synchronize data. Let's add the loading indicator to improve the user experience before we start and test our game.\n\n## Loading Indicator\n\nAs mentioned before we want to add a loading indicator while the game is starting up. Don't worry, we will keep it simple since it is not the focus of this tutorial. We will just be using a simple `Text` and an `Image` which can both be found in the same `UI` sub menu we used above.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/04_loading_indicator_ui_6805a0236b.png\" alt=\"Loading Indicator UI\" width=\"100\"/>\n\nThe make sure things are a bit more organised, embed both of them into another `GameObject` using `GameObject -> Create Empty`.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/05_loading_indicator_hierarchy_0dc6a27f68.png\" alt=\"Loading Indicator Hierarchy\" width=\"200\"/>\n\nYou can arrange and style the UI elements to your liking and when you're done just add a script to the `LoadingIndicatorImage`:\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/06_loading_indicator_script_4359052d19.png\" alt=\"Loading Indicator Script\" width=\"500\"/>\n\nThe script itself should look like this:\n\n```cs\nusing UnityEngine;\n\npublic class LoadingIndicator : MonoBehaviour\n{\n    // 1\n    [SerializeField] private float maxLeft = -150;\n    [SerializeField] private float maxRight = 150;\n    [SerializeField] private float speed = 100;\n\n    // 2\n    private enum MovementDirection { None, Left, Right }\n    private MovementDirection movementDirection = MovementDirection.Left;\n\n    private void Update()\n    {\n        switch (movementDirection) // 3\n        {\n            case MovementDirection.None:\n                break;\n            case MovementDirection.Left:\n                transform.Translate(speed * Time.deltaTime * Vector3.left);\n                if (transform.localPosition.x <= maxLeft) // 4\n                {\n                    transform.localPosition = new Vector3(maxLeft, transform.localPosition.y, transform.localPosition.z); // 5\n                    movementDirection = MovementDirection.Right; // 6\n                }\n                break;\n            case MovementDirection.Right:\n                transform.Translate(speed * Time.deltaTime * Vector3.right);\n                if (transform.localPosition.x >= maxRight) // 4\n                {\n                    transform.localPosition = new Vector3(maxRight, transform.localPosition.y, transform.localPosition.z); // 5\n                    movementDirection = MovementDirection.Left; // 6\n                }\n                break;\n        }\n    }\n}\n```\n\nThe loading indicator that we will be using for this example is just a simple square moving sideways to indicate progress. There are two fields (1) we are going to expose to the Unity Editor by using `SerializeField` so that you can adjust these values while seing the indicator move. `maxMovement` will tell the indicator how far to move to the left and right from the original position. `speed` - as the name indicates - will determine how fast the indicator moves. The initial movement direction (2) is set to left, with `Vector3.Left` and `Vector3.Right` being the options given here.\n\nThe movement itself will be calculated in `Update()` which is run every frame. We basically just want to do one of two things:\n\n- Move the loading indicator to the left until it reaches the left boundary, then swap the movement direction.\n- Move the loading indicator to the right until it reaches the right boundary, then swap the movement direction.\n\nUsing the [`transform`](https://docs.unity3d.com/ScriptReference/Transform.html) component of the `GameObject` we can move it by calling `Translate`. The movement consists of the direction (`Vector3.left` or `Vector3.right`), the speed (set via the Unity Editor) and `Time.deltaTime` which represents the time since the last frame. The latter makes sure we see a smooth movement no matter what the frame time is. After moving the square we check (3) if we have reached the boundary and if so, set the position to this boundary (4). This is just to make sure the indicator does not visibly slip out of bounds in case we see a low frame rate. Finally the position is swapped (5).\n\nThe loading indicator will only be shown when the start button is clicked. The script above takes care of showing it. We need to disable it so that it does not show up before. This can be done by clicking the checkbox next to the name of the `LoadingIndicator` parent object in the Inspector.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/07_disable_loading_indicator_b2c5d78c33.png\" alt=\"Disabling the loading indicator\" width=\"500\"/>\n\n## Connecting UI and code\n\nThe scripts we have written above are finished but still need to be connected to the UI so that it can act on it.\n\nFirst, let's assign the action to the button. With the `StartGameButton` selected in the `Hierarchy` open the `Inspector` and scroll down to the `On Click ()` area. Click the plus icon in the lower right to add a new on click action.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/08_button_add_onclick_91effae9e2.png\" alt=\"Add OnClick to StartGameButton\" width=\"500\"/>\n\nNext, drag and drop the `StartGameButton` from the `Hierarchy` onto the new action. This tells Unity which `GameObject` to use to look for actions that can be executed (which are functions that we implement like `OnStartButtonClicked()`).\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/09_button_add_script_c665be4bfe.png\" alt=\"Add script to StartGameButton\" width=\"800\"/>\n\nFinally, we can choose the action that should be assigned to the `On Click ()` event by opening the drop down. Choose the `StartGameButton` and then `OnStartButtonClicked ()`.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/10_button_choose_action_7dd1eb5e16.png\" alt=\"Choose action\" width=\"500\"/>\n\nWe also need to connect the input field and the loading indicator to the `StartGameButton` script so that it can access those. This is done via drag&drop again as before.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/11_button_connecting_UI_bc0218eac4.png\" alt=\"Connect UI\" width=\"800\"/>\n\n## Let's play!\n\nNow that the loading indicator is added the game is finished and we can start and run it. Go ahead and try it!\n\nYou will notice the experience when using one local Unity instance with Sync is the same as it was in the local Realm version. To actually test multiple game instances you can open the project on another computer. An easier way to test multiple Unity instances is [ParallelSync](https://github.com/VeriorPies/ParrelSync). After following the installation instruction you will find a new menu item `ParallelSync` which offers a `Clones Manager`.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/12_parallel_sync_menu_a6ef41a263.png\" alt=\"Parallel Sync Menu\" width=\"500\"/>\n\nWithin the `Clones Manager` you add and open a new clone by clicking `Add new clone` and `Open in New Editor`.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/13_parallel_sync_clones_manager_d1112343b5.png\" alt=\"Parallel Sync Clones Manager\" width=\"500\"/>\n\nUsing both instances you can then test the game and Realm Sync.\n\nRemember that you need to use the same `game id` / `partition key` to join the same game with both instances.\n\nHave fun!\n\n## Recap and Conclusion\n\nIn this tutorial we have learned how to turn a game with a local Realm into a multiplayer experience using MongoDB Realm Sync. Let's summarise what needed to be done:\n\n- Create an Atlas account and a Realm App therein\n- Enable Sync, an authentication method and development mode\n- Make sure every `RealmObject` has an `_id` field\n- Choose a partition strategy (in this case: use the `partition key` to identify the match)\n- Open a Realm using the `SyncConfiguration` (which incorporates the `App` and `User`)\n\nThe code for all of this can be found in [our example repository](https://github.com/realm/unity-examples/tree/3d-chess/synced-realm).\n\nIf you have questions, please head to our [developer community website](https://community.mongodb.com) where the MongoDB engineers and the MongoDB community will help you build your next big idea with MongoDB and Realm.","description":"This article shows how to migrate from using a local Realm to MongoDB Realm Sync. We will cover everything you need to know to transform your game into a multiplayer experience.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltccaa0f3b9a286f02/644c46a73df9c33c60a92bf9/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/realm-using-realm-sync-in-unity","title":"*Turning Your Local Game into an Online Experience with MongoDB Realm Sync","original_publish_date":"2021-11-03T17:05:16.588Z","strapi_updated_at":"2023-03-06T18:01:16.188Z","expiry_date":"2022-10-12T16:25:56.371Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Sync","calculated_slug":"/products/realm/sync"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Mobile","calculated_slug":"/technologies/mobile"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:14.786Z","publish_details":{"time":"2023-04-28T22:52:57.328Z"}}},{"calculated_slug":"/products/atlas/improve-your-apps-search-results-with-auto-tuning","content":"Historically, the only way to improve your app’s search query relevance is through manual intervention. For example, you can introduce [score boosting](https://github.com/esteininger/atlas-search-guide/tree/master/patterns/3-advanced-scoring) to multiply a base relevance score in the presence of particular fields. This ensures that searches where a key present in some fields weigh higher than others. This is, however, fixed by nature. The results are dynamic but the logic itself doesn’t change.\n\nThe following project will showcase how to leverage synonyms to create a feedback loop that is self-tuning, in order to deliver incrementally more relevant search results to your users—*all without complex machine learning models!*\n\n## Example\n\nWe have a food search application where a user searches for “Romanian Food.” Assuming that we’re logging every user's clickstream data (their step-by-step interaction with our application), we can take a look at this “sequence” and compare it to other results that have yielded a strong CTA (call-to-action): a successful checkout.\n\nAnother user searched for “German Cuisine” and that had a very similar clickstream sequence. Well, we can build a script that analyzes both these users’ (and other users’) clickstreams, identify similarities, we can tell the script to append it to a [synonyms document](https://github.com/esteininger/atlas-search-guide/tree/master/patterns/5-synonyms) that contains “German,” “Romanian,” and other more common cuisines, like “Hungarian.”\n\nHere’s a workflow of what we’re looking to accomplish:\n\n![diagram of what we're looking to accomplish](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/process_diagram_416a9bf4fc.png)\n\n## Tutorial\n\n### Step 1: Log user’s clickstream activity\n\nIn our app tier, as events are fired, we log them to a clickstreams collection, like:\n\n```\n[{\n\t\t\"session_id\": \"1\",\n\t\t\"event_id\": \"search_query\",\n\t\t\"metadata\": {\n\t\t\t\"search_value\": \"romanian food\"\n\t\t},\n\t\t\"timestamp\": \"1\"\n\t},\n\t{\n\t\t\"session_id\": \"1\",\n\t\t\"event_id\": \"add_to_cart\",\n\t\t\"product_category\":\"eastern european cuisine\",\n\t\t\"timestamp\": \"2\"\n\t},\n\t{\n\t\t\"session_id\": \"1\",\n\t\t\"event_id\": \"checkout\",\n\t\t\"timestamp\": \"3\"\n\t},\n\t{\n\t\t\"session_id\": \"1\",\n\t\t\"event_id\": \"payment_success\",\n\t\t\"timestamp\": \"4\"\n\t},\n\t{\n\t\t\"session_id\": \"2\",\n\t\t\"event_id\": \"search_query\",\n\t\t\"metadata\": {\n\t\t\t\"search_value\": \"hungarian food\"\n\t\t},\n\t\t\"timestamp\": \"1\"\n\t},\n\t{\n\t\t\"session_id\": \"2\",\n\t\t\"event_id\": \"add_to_cart\",\n\t\t\"product_category\":\"eastern european cuisine\",\n\t\t\"timestamp\": \"2\"\n\t}\n]\n\n```\n\nIn this simplified list of events, we can conclude that {\"session_id\":\"1\"} searched for “romanian food,” which led to a higher conversion rate, payment_success, compared to {\"session_id\":\"2\"}, who searched “hungarian food” and stalled after the add_to_cart event.\nYou can import this data yourself using [sample_data.json](https://github.com/esteininger/atlas-search-guide/blob/master/patterns/9-automated-search-tuning/sample_data.json).\n\nLet’s prepare the data for our search_tuner script.\n\n### Step 2: Create a view that groups by session_id, then filters on the presence of searches\n\nBy the way, it’s no problem that only some documents have a metadata field. Our $group operator can intelligently identify the ones that do vs don’t.\n\n```\n[\n    # first we sort by timestamp to get everything in the correct sequence of events,\n    # as that is what we'll be using to draw logical correlations\n    {\n        '$sort': {\n            'timestamp': 1\n        }\n    },\n    # next, we'll group by a unique session_id, include all the corresponding events, and begin\n    # the filter for determining if a search_query exists\n    {\n        '$group': {\n            '_id': '$session_id',\n            'events': {\n                '$push': '$$ROOT'\n            },\n            'isSearchQueryPresent': {\n                '$sum': {\n                    '$cond': [\n                        {\n                            '$eq': [\n                                '$event_id', 'search_query'\n                            ]\n                        }, 1, 0\n                    ]\n                }\n            }\n        }\n    },\n    # we hide session_ids where there is no search query\n    # then create a new field, an array called searchQuery, which we'll use to parse\n    {\n        '$match': {\n            'isSearchQueryPresent': {\n                '$gte': 1\n            }\n        }\n    },\n    {\n        '$unset': 'isSearchQueryPresent'\n    },\n    {\n        '$set': {\n            'searchQuery': '$events.metadata.search_value'\n        }\n    }\n]\n\n```\n\nLet’s create the view by building the query, then going into Compass and adding it as a new collection called group_by_session_id_and_search_query:\n\n![screenshot of creating a view in compass](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/compass_view_474651ee64.png)\n\n![screenshot of the view in compass](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/compass_view_output_e82d97d1bd.png)\n\nHere’s what it will look like:\n\n```\n[\n  {\n    \"session_id\": \"1\",\n    \"events\": [\n      {\n        \"event_id\": \"search_query\",\n        \"search_value\": \"romanian food\"\n      },\n      {\n        \"event_id\": \"add_to_cart\",\n        \"context\": {\n          \"cuisine\": \"eastern european cuisine\"\n        }\n      },\n      {\n        \"event_id\": \"checkout\"\n      },\n      {\n        \"event_id\": \"payment_success\"\n      }\n    ],\n    \"searchQuery\": \"romanian food\"\n  }, {\n    \"session_id\": \"2\",\n    \"events\": [\n      {\n        \"event_id\": \"search_query\",\n        \"search_value\": \"hungarian food\"\n      },\n      {\n        \"event_id\": \"add_to_cart\",\n        \"context\": {\n          \"cuisine\": \"eastern european cuisine\"\n        }\n      },\n      {\n        \"event_id\": \"checkout\"\n      }\n    ],\n    \"searchQuery\": \"hungarian food\"\n  },\n  {\n    \"session_id\": \"3\",\n    \"events\": [\n      {\n        \"event_id\": \"search_query\",\n        \"search_value\": \"italian food\"\n      },\n      {\n        \"event_id\": \"add_to_cart\",\n        \"context\": {\n          \"cuisine\": \"western european cuisine\"\n        }\n      }\n    ],\n    \"searchQuery\": \"sad food\"\n  }\n]\n\n```\n\n### Step 3: Build a scheduled job that compares similar clickstreams and pushes the resulting synonyms to the synonyms collection\n\n```\n// Provide a success indicator to determine which session we want to\n// compare any incomplete sessions with\nconst successIndicator = \"payment_success\"\n\n//  what percentage similarity between two sets of click/event streams\n//  we'd accept to be determined as similar enough to produce a synonym\n//  relationship\nconst acceptedConfidence = .9\n\n//  boost the confidence score when the following values are present\n//  in the eventstream\nconst eventBoosts = {\n  successIndicator: .1\n}\n\n/**\n * Enrich sessions with a flattened event list to make comparison easier.\n * Determine if the session is to be considered successful based on the success indicator.\n * @param {*} eventList List of events in a session.\n * @returns {any} Calculated values used to determine if an incomplete session is considered to\n * be related to a successful session.\n */\nconst enrichEvents = (eventList) => {\n  return {\n    eventSequence: eventList.map(event => { return event.event_id }).join(';'),\n    isSuccessful: eventList.some(event => { return event.event_id === successIndicator })\n  }\n}\n\n/**\n * De-duplicate common tokens in two strings\n * @param {*} str1\n * @param {*} str2\n * @returns Returns an array with the provided strings with the common tokens removed\n */\nconst dedupTokens = (str1, str2) => {\n  const splitToken = ' '\n  const tokens1 = str1.split(splitToken)\n  const tokens2 = str2.split(splitToken)\n  const dupedTokens = tokens1.filter(token => { return tokens2.includes(token)});\n  const dedupedStr1 = tokens1.filter(token => { return !dupedTokens.includes(token)});\n  const dedupedStr2 = tokens2.filter(token => { return !dupedTokens.includes(token)});\n\n  return [ dedupedStr1.join(splitToken), dedupedStr2.join(splitToken) ]\n}\n\nconst findMatchingIndex = (synonyms, results) => {\n  let matchIndex = -1\n  for(let i = 0; i < results.length; i++) {\n    for(const synonym of synonyms) {\n      if(results[i].synonyms.includes(synonym)){\n        matchIndex = i;\n        break;\n      }\n    }\n  }\n  return matchIndex;\n}\n/**\n * Inspect the context of two matching sessions.\n * @param {*} successfulSession\n * @param {*} incompleteSession\n */\nconst processMatch = (successfulSession, incompleteSession, results) => {\n  console.log(`=====\\nINSPECTING POTENTIAL MATCH: ${ successfulSession.searchQuery} = ${incompleteSession.searchQuery}`);\n  let contextMatch = true;\n\n  // At this point we can assume that the sequence of events is the same, so we can\n  // use the same index when comparing events\n  for(let i = 0; i < incompleteSession.events.length; i++) {\n    // if we have a context, let's compare the kv pairs in the context of\n    // the incomplete session with the successful session\n    if(incompleteSession.events[i].context){\n      const eventWithContext = incompleteSession.events[i]\n      const contextKeys = Object.keys(eventWithContext.context)\n\n      try {\n        for(const key of contextKeys) {\n          if(successfulSession.events[i].context[key] !== eventWithContext.context[key]){\n            // context is not the same, not a match, let's get out of here\n            contextMatch = false\n            break;\n          }\n         }\n      } catch (error) {\n        contextMatch = false;\n        console.log(`Something happened, probably successful session didn't have a context for an event.`);\n      }\n    }\n  }\n\n  // Update results\n  if(contextMatch){\n    console.log(`VALIDATED`);\n    const synonyms = dedupTokens(successfulSession.searchQuery, incompleteSession.searchQuery, true)\n    const existingMatchingResultIndex = findMatchingIndex(synonyms, results)\n    if(existingMatchingResultIndex >= 0){\n      const synonymSet = new Set([...synonyms, ...results[existingMatchingResultIndex].synonyms])\n      results[existingMatchingResultIndex].synonyms = Array.from(synonymSet)\n    }\n    else{\n      const result = {\n        \"mappingType\": \"equivalent\",\n        \"synonyms\": synonyms\n      }\n      results.push(result)\n    }\n\n  }\n  else{\n    console.log(`NOT A MATCH`);\n  }\n\n  return results;\n}\n\n/**\n * Compare the event sequence of incomplete and successful sessions\n * @param {*} successfulSessions\n * @param {*} incompleteSessions\n * @returns\n */\nconst compareLists = (successfulSessions, incompleteSessions) => {\n  let results = []\n  for(const successfulSession of successfulSessions) {\n    for(const incompleteSession of incompleteSessions) {\n      // if the event sequence is the same, let's inspect these sessions\n      // to validate that they are a match\n      if(successfulSession.enrichments.eventSequence.includes(incompleteSession.enrichments.eventSequence)){\n        processMatch(successfulSession, incompleteSession, results)\n      }\n    }\n  }\n  return results\n}\n\nconst processSessions = (sessions) => {\n  // console.log(`Processing the following list:`, JSON.stringify(sessions, null, 2));\n  // enrich sessions for processing\n  const enrichedSessions = sessions.map(session => {\n    return { ...session, enrichments: enrichEvents(session.events)}\n  })\n  // separate successful and incomplete sessions\n  const successfulEvents = enrichedSessions.filter(session => { return session.enrichments.isSuccessful})\n  const incompleteEvents = enrichedSessions.filter(session => { return !session.enrichments.isSuccessful})\n\n  return compareLists(successfulEvents, incompleteEvents);\n}\n\n/**\n * Main Entry Point\n */\nconst main = () => {\n  const results = processSessions(eventsBySession);\n  console.log(`Results:`, results);\n}\n\nmain();\n\nmodule.exports = processSessions;\n\n```\n\nRun [the script](https://github.com/esteininger/atlas-search-guide/blob/master/patterns/9-automated-search-tuning/realm/index.js) yourself.\n\n### Step 4: Enhance our search query with the newly appended synonyms\n\n```\n[\n    {\n        '$search': {\n            'index': 'synonym-search',\n            'text': {\n                'query': 'hungarian',\n                'path': 'cuisine-type'\n            },\n            'synonyms': 'similarCuisines'\n        }\n    }\n]\n\n```\n\nSee [the synonyms tutorial](https://github.com/esteininger/atlas-search-guide/tree/master/patterns/5-synonyms).\n\n## Next Steps\n\nThere you have it, folks. We’ve taken raw data recorded from our application server and put it to use by building a feedback that encourages positive user behavior.\n\nBy measuring this feedback loop against your KPIs, you can build a simple A/B test against certain synonyms and user patterns to optimize your application!","description":"This blog will cover how to leverage synonyms to create a feedback loop that is self-tuning, in order to deliver incrementally more relevant search results to your users.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt3f362db314ca755a/644c46d1efd5713ae8fe6f6d/atlas-search-indexes.png?branch=prod","description":null}}]},"slug":"/improve-your-apps-search-results-with-auto-tuning","title":"*Improve Your App's Search Results with Auto-Tuning","original_publish_date":"2021-10-20T13:00:00.204Z","strapi_updated_at":"2022-09-07T20:44:19.759Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Search","calculated_slug":"/products/atlas/search"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:14.387Z","publish_details":{"time":"2023-04-28T22:52:57.362Z"}}},{"calculated_slug":"/languages/python/pymongoarrow-and-data-analysis","content":"## Overview\n\nMongoDB has always been a great database for data science and data analysis, and that's because you can:\n\n* Import data without a fixed schema.\n* Clean it up within the database.\n* Listen in real-time for updates (a very handy feature that's used by our [MongoDB Kafka Connector](https://docs.mongodb.com/kafka-connector/current/)).\n* Query your data with the super-powerful and intuitive [Aggregation Framework](https://docs.mongodb.com/manual/aggregation/).\n\nBut MongoDB is a general-purpose database, and not a data analysis tool, so a common pattern when analysing data that's stored within MongoDB is to extract the results of a query into a Numpy array, or Pandas dataframe, and to run complex and potentially long running analyses using the toolkit those frameworks provide. Until recently, the performance hit of converting large amounts of BSON data, as provided by MongoDB into these data structures, has been slower than we'd like.\n\nFortunately, MongoDB recently released [PyMongoArrow](https://mongo-arrow.readthedocs.io/en/pymongoarrow-0.1.1/), a Python library for efficiently converting the result of a MongoDB query into the [Apache Arrow](https://arrow.apache.org/) data model. If you're not aware of Arrow, you may now be thinking, \"Mark, how does converting to Apache Arrow help me with my Numpy or Pandas analysis?\" The answer is: Conversion between Arrow, Numpy, and Pandas is super efficient, so it provides a useful intermediate format for your tabular data. This way, we get to focus on building a powerful tool for mapping between MongoDB and Arrow, and leverage the existing [PyArrow](https://arrow.apache.org/docs/python/) library for integration with Numpy and MongoDB\n\n## Prerequisites\n\nYou'll need a recent version of Python (I'm using 3.8) with pip available. You can use [conda](https://docs.conda.io/projects/conda/en/latest/index.html) if you like, but PyMongoArrow is released on PyPI, so you'll still need to use pip to install it into your conda Python environment.\n\nThis tutorial was written for PyMongoArrow v0.1.1.\n\n## Getting Started\n\nIn this tutorial, I'm going to be using a sample database you can install when creating a cluster hosted on MongoDB Atlas. The database I'll be using is the \"sample\\_weatherdata\" database. You'll access this with a `mongodb+srv` URI, so you'll need to install PyMongo with the \"srv\" extra, like this:\n\n``` shell\n$ python -m pip install jupyter pymongoarrow 'pymongo[srv]' pandas\n```\n\n> **Useful Tip**: If you just run `pip`, you may end up using a copy of `pip` that was installed for a different version of `python` than the one you're using. For some reason, the `PATH` getting messed up this way happens more often than you'd think. A solution to this is to run pip via Python, with the command `python -m pip`. That way, it'll always run the version of `pip` that's associated with the version of `python` in your `PATH`. This is now the [officially recommended](https://pip.pypa.io/en/stable/getting-started/#ensure-you-have-a-working-pip) way to run `pip`!\n\nYou'll also need a MongoDB cluster set up with the sample datasets imported. Follow these instructions to import them into your MongoDB cluster and then set an environment variable, `MDB_URI`, pointing to your database. It should look like the line below, but with the URI you copy out of the Atlas web interface. (Click the \"Connect\" button for your cluster.)\n\n``` shell\nexport MDB_URI=mongodb+srv://USERNAME:PASSWORD@CLUSTERID.azure.mongodb.net/sample_weatherdata?retryWrites=true&w=majority\n```\n\nA sample document from the \"data\" collection looks like this:\n\n``` json\n{'_id': ObjectId('5553a998e4b02cf7151190bf'),\n  'st': 'x+49700-055900',\n  'ts': datetime.datetime(1984, 3, 5, 15, 0),\n  'position': {'type': 'Point', 'coordinates': [-55.9, 49.7]},\n  'elevation': 9999,\n  'callLetters': 'SCGB',\n  'qualityControlProcess': 'V020',\n  'dataSource': '4',\n  'type': 'FM-13',\n  'airTemperature': {'value': -5.1, 'quality': '1'},\n  'dewPoint': {'value': 999.9, 'quality': '9'},\n  'pressure': {'value': 1020.8, 'quality': '1'},\n  'wind': {'direction': {'angle': 100, 'quality': '1'},\n   'type': 'N',\n   'speed': {'rate': 3.1, 'quality': '1'}},\n  'visibility': {'distance': {'value': 20000, 'quality': '1'},\n   'variability': {'value': 'N', 'quality': '9'}},\n  'skyCondition': {'ceilingHeight': {'value': 22000,\n    'quality': '1',\n    'determination': 'C'},\n   'cavok': 'N'},\n  'sections': ['AG1', 'AY1', 'GF1', 'MD1', 'MW1'],\n  'precipitationEstimatedObservation': {'discrepancy': '2',\n   'estimatedWaterDepth': 0},\n  'pastWeatherObservationManual': [{'atmosphericCondition': {'value': '0',\n     'quality': '1'},\n    'period': {'value': 3, 'quality': '1'}}],\n  'skyConditionObservation': {'totalCoverage': {'value': '01',\n    'opaque': '99',\n    'quality': '1'},\n   'lowestCloudCoverage': {'value': '01', 'quality': '1'},\n   'lowCloudGenus': {'value': '01', 'quality': '1'},\n   'lowestCloudBaseHeight': {'value': 800, 'quality': '1'},\n   'midCloudGenus': {'value': '00', 'quality': '1'},\n   'highCloudGenus': {'value': '00', 'quality': '1'}},\n  'atmosphericPressureChange': {'tendency': {'code': '8', 'quality': '1'},\n   'quantity3Hours': {'value': 0.5, 'quality': '1'},\n   'quantity24Hours': {'value': 99.9, 'quality': '9'}},\n  'presentWeatherObservationManual': [{'condition': '02', 'quality': '1'}]}\n```\n\nTo keep things simpler in this tutorial, I'll ignore all the fields except for \"ts,\" \"wind,\" and the \"\\_id\" field.\n\nI set the `MDB_URI` environment variable, installed the dependencies above, and then fired up a new Python 3 Jupyter Notebook. I've put the notebook [on GitHub](https://github.com/mongodb-developer/pymongoarrow-sample-code/blob/main/PyMongoArrow%20Demo.ipynb), if you want to follow along, or run it yourself.\n\nI added the following code to a cell at the top of the file to import the necessary modules, and to connect to my database:\n\n``` python\nimport os\nimport pyarrow\nimport pymongo\nimport bson\nimport pymongoarrow.monkey\nfrom pymongoarrow.api import Schema\n\nMDB_URI = os.environ['MDB_URI']\n\n\n# Add extra find_* methods to pymongo collection objects:\npymongoarrow.monkey.patch_all()\n\nclient = pymongo.MongoClient(MDB_URI)\ndatabase = client.get_default_database()\ncollection = database.get_collection(\"data\")\n```\n\n## Working With Flat Data\n\nIf the data you wish to convert to Arrow, Pandas, or Numpy data tables is already flat—i.e., the fields are all at the top level of your documents—you can use the methods `find\\_arrow\\_all`, `find\\_pandas\\_all`, and `find\\_numpy\\_all` to query your collection and return the appropriate data structure.\n\n``` python\ncollection.find_pandas_all(\n    {},\n    schema=Schema({\n        'ts': pyarrow.timestamp('ms'),\n    })\n)\n```\n\n|  | ts |\n| --- | ---: |\n| 0 | 1984-03-05 15:00:00 |\n| 1 | 1984-03-05 18:00:00 |\n| 2 | 1984-03-05 18:00:00 |\n| 3 | 1984-03-05 18:00:00 |\n| 4 | 1984-03-05 18:00:00 |\n| ... | ... |\n| 9995 | 1984-03-13 06:00:00 |\n| 9996 | 1984-03-13 06:00:00 |\n| 9997 | 1984-03-13 06:00:00 |\n| 9998 | 1984-03-12 09:00:00 |\n| 9999 | 1984-03-12 12:00:00 |\n\n10000 rows × 1 columns\n\nThe first argument to find\\_pandas\\_all is the `filter` argument. I'm interested in all the documents in the collection, so I've left it empty. The documents in the data collection are quite nested, so the only real value I can access with a find query is the timestamp of when the data was recorded, the \"ts\" field. Don't worry—I'll show you how to access the rest of the data in a moment!\n\nBecause Arrow tables (and the other data types) are strongly typed, you'll also need to provide a Schema to map from MongoDB's permissive dynamic schema into the types you want to handle in your in-memory data structure.\n\nThe `Schema` is a mapping of the field name, to the appropriate type to be used by Arrow, Pandas, or Numpy. At the current time, these types are 64-bit ints, 64-bit floating point numbers, and datetimes. The easiest way to specify these is with the native python types `int` and `float`, and with `pyarrow.datetime`. Any fields in the document that aren't listed in the schema will be ignored.\n\nPyMongoArrow currently hijacks the `projection` parameter to the `find_*_all` methods, so unfortunately, you can't write a projection to flatten the structure at the moment.\n\n## Convert Your Documents to Tabular Data\nMongoDB documents are very flexible, and can support nested arrays and documents. Although Apache Arrow also supports nested lists, structs, and dictionaries, Numpy arrays and Pandas dataframes, in contrast, are tabular or columnar data structures. There are plans to support mapping to the nested Arrow data types in future, but at the moment, only scalar values are supported with all three libraries. So in all these cases, it will be necessary to flatten the data you are exporting from your documents.\n\nTo project your documents into a flat structure, you'll need to use the more powerful `aggregate_*_all` methods that PyMongoArrow adds to your PyMongo Collection objects.\n\nIn an aggregation pipeline, you can add a `$project` stage to your query to project the nested fields you want in your table to top level fields in the aggregation result.\n\nIn order to test my `$project` stage, I first ran it with the standard PyMongo aggregate function. I converted it to a `list` so that Jupyter would display the results.\n\n``` python\nlist(collection.aggregate([\n    {'$match': {'_id': bson.ObjectId(\"5553a998e4b02cf7151190bf\")}},\n    {'$project': {\n        'windDirection': '$wind.direction.angle',\n        'windSpeed': '$wind.speed.rate',\n    }}\n]))\n\n[{'_id': ObjectId('5553a998e4b02cf7151190bf'),\n  'windDirection': 100,\n  'windSpeed': 3.1}]\n```\n\nBecause I've matched a single document by \"\\_id,\" only one document is returned, but you can see that the `$project` stage has mapped `$wind.direction.angle` to the top-level \"windDirection\" field in the result, and the same with `$wind.speed.rate` and \"windSpeed\" in the result.\n\nI can take this `$project` stage and use it to flatten all the results from an aggregation query, and then provide a schema to identify \"windDirection\" as an integer value, and \"windSpeed\" as a floating point number, like this:\n\n``` python\ncollection.aggregate_pandas_all([\n        {'$project': {\n            'windDirection': '$wind.direction.angle',\n            'windSpeed': '$wind.speed.rate',\n        }}\n    ],\n    schema=Schema({'windDirection': int, 'windSpeed': float})\n)\n```\n\n| A | B | C |\n| --- | --- | --- |\n|  | windDirection | windSpeed |\n| 0 | 100 | 3.1 |\n| 1 | 50 | 9.0 |\n| 2 | 30 | 7.7 |\n| 3 | 270 | 19.0 |\n| 4 | 50 | 8.2 |\n| ... | ... | ... |\n| 9995 | 10 | 7.0 |\n| 9996 | 60 | 5.7 |\n| 9997 | 330 | 3.0 |\n| 9998 | 140 | 7.7 |\n| 9999 | 80 | 8.2 |\n\n10000 rows × 2 columns\n\nThere are only 10000 documents in this collection, but some basic benchmarks I wrote show this to be around 20% faster than working directly with `DataFrame.from_records` and `PyMongo`. With larger datasets, I'd expect the difference in performance to be more significant. It's early days for the PyMongoArrow library, and so there are some limitations at the moment, such as the ones I've mentioned above, but the future looks bright for this library in providing fast mappings between your rich, flexible MongoDB collections and any in-memory analysis requirements you might have with Arrow, Pandas, or Numpy.\n\n## Next Steps\n\nIf you're planning to do lots of analysis of data that's stored in MongoDB, then make sure you're up on the latest features of MongoDB's powerful [aggregation framework](https://docs.mongodb.com/manual/aggregation/). You can do many things within the database so you may not need to export your data at all. You can connect to secondary servers in your cluster to reduce load on the primary for analytics queries, or even have [dedicated analytics nodes](https://www.mongodb.com/blog/post/atlas-mapped-analytics-nodes-to-power-your-bi-are-now-available) for running these kinds of queries.\nCheck out MongoDB 5.0's new [window functions](https://docs.mongodb.com/manual/reference/operator/aggregation/setWindowFields/) and if you're working with time series data, you'll definitely want to know about MongoDB 5.0's new [time-series collections](https://docs.mongodb.com/manual/core/timeseries-collections/).","description":"MongoDB has always been a great database for data science and data analysis, and now with PyMongoArrow, it integrates optimally with Apache Arrow, Python's Numpy, and Pandas libraries.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt597bdb2eb2829c8d/644c46d347e9cc95a2cd0937/Python_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/pymongoarrow-and-data-analysis","title":"*PyMongoArrow: Bridging the Gap Between MongoDB and Your Data Analysis App","original_publish_date":"2021-10-15T15:52:14.156Z","strapi_updated_at":"2022-05-20T18:12:33.039Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Python","calculated_slug":"/languages/python"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Python","calculated_slug":"/languages/python"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Pandas","calculated_slug":"/technologies/pandas"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:13.994Z","publish_details":{"time":"2023-04-28T22:52:57.410Z"}}},{"calculated_slug":"/products/realm/realm-sdk-schema-migration-android","content":"> This is a follow-up article in the **[Getting Started Series](https://www.mongodb.com/developer/how-to/introduction-realm-sdk-android/)**.\n> In this article, we learn how to modify/migrate Realm **local** database schema.\n\n## Introduction\n\nAs you add and change application features, you need to modify database schema, and the need for migrations arises, which is very important for a seamless user experience.\n\nBy the end of this article, you will learn:\n\n1. How to update database schema post-production release on play store.\n2. How to migrate user data from one schema to another.\n\nBefore we get down to business, let's quickly recap how we set `Realm` in our application.\n\n```kotlin\nconst val REALM_SCHEMA_VERSION: Long = 1\nconst val REALM_DB_NAME = \"rMigrationSample.db\"\n\nfun setupRealm(context: Context) {\n    Realm.init(context)\n\n    val config = RealmConfiguration.Builder()\n        .name(REALM_DB_NAME)\n        .schemaVersion(REALM_SCHEMA_VERSION)\n        .build()\n\n    Realm.setDefaultConfiguration(config)\n}\n```\n\nDoing migration in Realm is very straightforward and simple. The high-level steps for the successful migration of any database are:\n\n1. Update the database version.\n2. Make changes to the database schema.\n3. Migrate user data from old schema to new.\n\n## Update the Database Version\n\nThis is the simplest step, which can be done by incrementing the version of\n`REALM_SCHEMA_VERSION`, which notifies `Relam` about database changes. This, in turn, runs triggers migration, if provided.\n\nTo add migration, we use the `migration` function available in `RealmConfiguration.Builder`, which takes an argument of `RealmMigration`, which we will review in the next step.\n\n```kotlin\nval config = RealmConfiguration.Builder()\n    .name(REALM_DB_NAME)\n    .schemaVersion(REALM_SCHEMA_VERSION)\n    .migration(DBMigrationHelper())\n    .build()\n```\n\n## Make Changes to the Database Schema\n\nIn `Realm`, all the migration-related operation has to be performed within the scope\nof `RealmMigration`.\n\n```kotlin\nclass DBMigrationHelper : RealmMigration {\n\n    override fun migrate(realm: DynamicRealm, oldVersion: Long, newVersion: Long) {\n        migration1to2(realm.schema)\n        migration2to3(realm.schema)\n        migration3to4(realm.schema)\n    }\n\n    private fun migration3to4(schema: RealmSchema?) {\n        TODO(\"Not yet implemented\")\n    }\n\n    private fun migration2to3(schema: RealmSchema?) {\n        TODO(\"Not yet implemented\")\n    }\n\n    private fun migration1to2(schema: RealmSchema) {\n        TODO(\"Not yet implemented\")\n    }\n}\n```\n\nTo add/update/rename any field:\n\n```kotlin\n\nprivate fun migration1to2(schema: RealmSchema) {\n    val userSchema = schema.get(UserInfo::class.java.simpleName)\n    userSchema?.run {\n        addField(\"phoneNumber\", String::class.java, FieldAttribute.REQUIRED)\n        renameField(\"phoneNumber\", \"phoneNo\")\n        removeField(\"phoneNo\")\n    }\n}\n```\n\n## Migrate User Data from Old Schema to New\n\nAll the data transformation during migration can be done with `transform` function with the help of `set` and `get` methods.\n\n```kotlin\n\nprivate fun migration2to3(schema: RealmSchema) {\n    val userSchema = schema.get(UserInfo::class.java.simpleName)\n    userSchema?.run {\n        addField(\"fullName\", String::class.java, FieldAttribute.REQUIRED)\n        transform {\n            it.set(\"fullName\", it.get<String>(\"firstName\") + it.get<String>(\"lastName\"))\n        }\n    }\n}\n```\n\nIn the above snippet, we are setting the default value of **fullName** by extracting the value from old data, like **firstName** and **lastName**.\n\nWe can also use `transform` to update the data type.\n\n```kotlin\n\nval personSchema = schema!![\"Person\"]\n\n// Change type from String to int\nschema[\"Pet\"]?.run {\n    addField(\"type_tmp\", Int::class.java)\n    transform { obj ->\n        val oldType = obj.getString(\"type\")\n        if (oldType == \"dog\") {\n            obj.setLong(\"type_tmp\", 1)\n        } else if (oldType == \"cat\") {\n            obj.setInt(\"type_tmp\", 2)\n        } else if (oldType == \"hamster\") {\n            obj.setInt(\"type_tmp\", 3)\n        }\n    }\n    removeField(\"type\")\n    renameField(\"type_tmp\", \"type\")\n}\n```\n\nIn case you want to delete the complete `Realm`, you can use `deleteRealmIfMigrationNeeded` with `RealmConfiguration.Builder`, but that should be considered as the last resort.\n\nThank you for reading! If you have any queries or comments, you can share them on\nthe [Realm forum](https://www.mongodb.com/community/forums/c/realm/9) or tweet\nme [@codeWithMohit](http://twitter.com/codeWithMohit).\n\nIn the next article, we will discuss how to migrate the Realm database with Atlas Device Sync.\n\nIf you have an iOS app, do check out the iOS tutorial\non [Realm iOS Migration](https://www.mongodb.com/developer/how-to/realm-schema-migration/). ","description":"In this article, we explore and learn how to make Realm SDK database schema changes. ","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt711bc5bcc3577a57/644c46d5c3e5e71f2cd1c026/ATF_720x720.png?branch=prod","description":null}}]},"slug":"/realm-sdk-schema-migration-android","title":"*How to Update Realm SDK Database Schema for Android","original_publish_date":"2021-10-21T09:19:57.621Z","strapi_updated_at":"2022-09-02T14:29:50.495Z","expiry_date":"2022-10-19T12:59:16.070Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*SDK","calculated_slug":"/products/realm/sdk"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Kotlin","calculated_slug":"/languages/kotlin"}},{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Android","calculated_slug":"/technologies/android"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf22d7f4e3e14599d/644c46d7a2d3bcbb57e53e69/OG_1200x630.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:13.587Z","publish_details":{"time":"2023-04-28T22:52:57.445Z"}}},{"calculated_slug":"/products/realm/swiftui-previews","content":"## Introduction\n\nCanvas previews are an in-your-face feature of SwiftUI. When you create a new view, half of the boilerplate code is for the preview. A third of your Xcode real estate is taken up by the preview.\n\nDespite the prominence of the feature, many developers simply delete the preview code from their views and rely on the simulator.\n\nIn past releases of Xcode (including the Xcode 13 betas), a reluctance to use previews was understandable. They'd fail for no apparent reason, and the error messages were beyond cryptic.\n\nI've stuck with previews from the start, but at times, they've felt like more effort than they're worth. But, with Xcode 13, I think we should all be using them for all views. In particular, I've noticed:\n\n- They're more reliable.\n- The error messages finally make sense.\n- Landscape mode is supported.\n\n![Xcode showing a blackjack app with multiple previews for different devices, orientations, and color schemes](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/preview_multiple_devices_8cfe4cccd4.png)\n\nI consider previews a little like UI unit tests for your views. Like with unit tests, there's some extra upfront effort required, but you get a big payback in terms of productivity and quality.\n\nIn this article, I'm going to cover:\n\n- What you can check in your previews (think light/dark mode, different devices, landscape mode, etc.) and how to do it.\n- Reducing the amount of boilerplate code you need in your previews.\n- Writing previews for stateful apps. (I'll be using [Realm](https://realm.io/), but the same approach can be used with Core Data.)\n- Troubleshooting your previews.\n\nOne feature I won't cover is using previews as a graphical way to edit views. One of the big draws of SwiftUI is writing everything in code rather than needing storyboards and XML files. Using a drag-and-drop view builder for SwiftUI doesn't appeal to me.\n\n95% of the examples I use in this article are based on a BlackJack training app. You can find the final version in the [repo](https://github.com/mongodb-developer/BlackJackTrainer).\n\n> **Build better mobile apps with Atlas Device Sync**: Atlas Device Sync is a fully-managed mobile backend-as-a-service. Leverage out-of-the-box infrastructure, data synchronization capabilities, built-in network handling, and much more to quickly launch enterprise-grade mobile apps. [Get started now by build: Deploy Sample for Free](https://www.mongodb.com/realm/register?tck=devhub_makingswiftuipreviewswork)!\n\n## Prerequisites\n\n- Xcode 13+\n- iOS 15+\n- Realm-Cocoa 10.17.0+\n\nNote: \n\n- I've used Xcode 13 and iOS 15, but most of the examples in this post will work with older versions.\n- Previewing in landscape mode is new in Xcode 13.\n- The `buttonStyle` modifier is only available in iOS 15.\n- I used Realm-Cocoa 10.17.0, but earlier 10.X versions are likely to work. \n\n## Working with previews\n\nPreviews let you see what your view looks like without running it in a simulator or physical device. When you edit the code for your view, its preview updates in real time.\n\nThis section shows what aspects you can preview, and how it's done.\n\n### A super-simple preview\n\nWhen you create a new Xcode project or SwiftUI view, Xcode adds the code for the preview automatically. All you need to do is press the \"Resume\" button (or CMD-Alt-P).\n\n![A simple Xcode preview which updates as the code is edited](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/simple_preview_a0d9cef7ab.gif)\n\nThe preview code always has the same structure, with the `View` that needs previewing (in this case, `ContentView`) within the `previews` `View`:\n\n```swift\nstruct ContentView_Previews: PreviewProvider {\n   static var previews: some View {\n       ContentView()\n   }\n}\n```\n\n### Views that require parameters\n\nMost of your views will require that the enclosing view pass in parameters. Your preview must do the same—you'll get a build error if you forget.\n\nMy [`ResetButton`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Views/Decision%20Matrix/ResetButton.swift) view requires that the caller provides two values—`label` and `resetType`:\n\n```swift\nstruct ResetButton: View {\n   var label: String\n   var resetType: ResetType\n   ...\n}\n```\n\nThe preview code needs to pass in those values, just like any embedding view:\n\n```swift\nstruct ResetButton_Previews: PreviewProvider {\n   static var previews: some View {\n       ResetButton(label: \"Reset All Matrices\",\n                   resetType: .all)\n   }\n}\n```\n\n### Views that require `Binding`s\n\nIn a chat app, I have a [`LoginView`](https://github.com/mongodb-developer/LiveTutorial2021/blob/main/iOS/LiveChat/LiveChat/Views/LoginView.swift) that updates the `username` binding that's past from the enclosing view:\n\n```swift\nstruct LoginView: View {  \n   @Binding var username: String\n   ...\n}\n```\n\nThe simplest way to create a binding in your preview is to use the `constant` function:\n\n```swift\nstruct LoginView_Previews: PreviewProvider {\n   static var previews: some View {\n       LoginView(username: .constant(\"Billy\"))\n   }\n}\n```\n\n### `NavigationView`s\n\nIn your view hierarchy, you only add a `NavigationView` at a single level. That `NavigationView` then wraps all subviews.\n\nWhen previewing those subviews, you may or may not care about the `NavigationView` functionality. For example, you'll only see titles and buttons in the top nav bar if your preview wraps the view in a `NavigationView`.\n\nIf I preview my [`PracticeView`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Views/PracticeView.swift) without adding a `NavigationView`, then I don't see the title:\n\n![Preview where there is no title on the screen](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/no_title_171f797127.png)\n\nTo preview the title, my preview code needs to wrap `PracticeView` in a `NavigationView`:\n\n```swift\nstruct PracticeView_Previews: PreviewProvider {\n   static var previews: some View {\n       NavigationView {\n           PracticeView()\n       }\n   }\n}\n```\n\n![Preview with title](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/with_title_bf15560aac.png)\n\n### Smaller views\n\nSometimes, you don't need to preview your view in the context of a full device screen. My [`CardView`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Views/CardView.swift) displays a single playing card. Previewing it in a full device screen just wastes desk space: \n\n![Lots of preview space taken up for the iPhone, but the view we're testing only takes up a tiny proportion of the space](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/full_device_preview_c8ffd5925f.png)\n\nWe can add the `previewLayout` modifier to indicate that we only want to preview an area large enough for the view. It often makes sense to add some `padding` as well:\n\n```swift\nstruct CardView_Previews: PreviewProvider {\n   static var previews: some View {\n       CardView(card: Card(suit: .heart))\n           .previewLayout(.sizeThatFits)\n           .padding()\n   }\n}\n```\n\n![Lots of screen space freed up by only previewing the view (rather than the entire iPhone screen](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/no_device_bfc4332e8f.png)\n\n### Light and dark modes\n\nIt can be quite a shock when you finally get around to testing your app in dark mode. If you've not thought about light/dark mode when implementing each of your views, then the result can be ugly, or even unusable.\n\nPreviews to the rescue!\n\nReturning to [`CardView`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Views/CardView.swift), I can preview a card in dark mode using the `preferredColorScheme` view modifier:\n\n```swift\nstruct CardView_Previews: PreviewProvider {\n   static var previews: some View {\n       CardView(card: Card(suit: .heart))\n           .preferredColorScheme(.dark)\n           .previewLayout(.sizeThatFits)\n           .padding()\n   }\n}\n```\n\n![10 Hearts displaying correctly in front of a dark background](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dark_heart_bbecd98b30.png)\n\nThat seems fine, but what if I previewed a spade instead?\n\n![Black spade isn't visible in front of dark background](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dark_spade_fe9e8efa1d.png)\n\nThat could be a problem.\n\nAdding a white background to the view fixes it:\n\n![King of spades now shows up as it sits in a white box](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dark_spade_2_d61601c19e.png)\n\n### Preview multiple view instances\n\nSometimes, previewing a single instance of your view doesn't paint the full picture. Just look at the surprise I got when enabling dark mode for my card view. Wouldn't it be better to simultaneously preview both hearts and spades in both dark and light modes?\n\nYou can create multiple previews for the same view using the `Group` view:\n\n```swift\nstruct CardView_Previews: PreviewProvider {\n   static var previews: some View {\n       Group {\n           CardView(card: Card(suit: .heart))\n           CardView(card: Card(suit: .spade))\n           CardView(card: Card(suit: .heart))\n               .preferredColorScheme(.dark)\n           CardView(card: Card(suit: .spade))\n               .preferredColorScheme(.dark)\n       }\n       .previewLayout(.sizeThatFits)\n       .padding()\n   }\n}\n```\n\n![Multiple cards, each with their own preview](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/multiple_cards_835ef6cf03.png)\n\n### Composing views in a preview\n\nA preview of a single view in isolation might look fine, but what will they look like within a broader context?\n\nPreviewing a single [`DecisionCell`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Views/Decision%20Matrix/DecisionCell.swift) view looks great:\n\n```swift\nstruct DecisionCell_Previews: PreviewProvider {\n   static var previews: some View {\n       DecisionCell(\n           decision: Decision(handValue: 6, dealerCardValue: .nine, action: .hit), myHandValue: 8, dealerCardValue: .five)\n           .previewLayout(.sizeThatFits)\n           .padding()\n   }\n}\n```\n\n![A single, yellow Hit cell](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/single_decision_cell_ea54c12fee.png)\n\nBut, the app will never display a single `DecisionCell`. They'll always be in a grid. Also, the text, background color, and border vary according to state. To create a more realistic preview, I created some sample data within the view and then composed multiple `DecisionCell`s using vertical and horizontal stacks:\n\n```swift\nstruct DecisionCell_Previews: PreviewProvider {\n   static var previews: some View {\n       let decisions: [Decision] = [\n           Decision(handValue: 6, dealerCardValue: .nine, action: .split),\n           Decision(handValue: 6, dealerCardValue: .nine, action: .stand),\n           Decision(handValue: 6, dealerCardValue: .nine, action: .double),\n           Decision(handValue: 6, dealerCardValue: .nine, action: .hit)\n       ]\n       return Group {\n           VStack(spacing: 0) {\n               ForEach(decisions) { decision in\n                   HStack (spacing: 0) {\n                       DecisionCell(decision: decision, myHandValue: 8, dealerCardValue: .three)\n                       DecisionCell(decision: decision, myHandValue: 6, dealerCardValue: .three)\n                       DecisionCell(decision: decision, myHandValue: 8, dealerCardValue: .nine)\n                       DecisionCell(decision: decision, myHandValue: 6, dealerCardValue: .nine)\n                   }\n               }\n           }\n           VStack(spacing: 0) {\n               ForEach(decisions) { decision in\n                   HStack (spacing: 0) {\n                       DecisionCell(decision: decision, myHandValue: 8, dealerCardValue: .three)\n                       DecisionCell(decision: decision, myHandValue: 6, dealerCardValue: .three)\n                       DecisionCell(decision: decision, myHandValue: 8, dealerCardValue: .nine)\n                       DecisionCell(decision: decision, myHandValue: 6, dealerCardValue: .nine)\n                   }\n               }\n           }\n           .preferredColorScheme(.dark)\n       }\n       .previewLayout(.sizeThatFits)\n       .padding()\n   }\n```\n\nI could then see that the black border didn't work too well in dark mode:\n\n![Dark border around selected cells is lost in front of the dark background](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/stacked_decisions_47d742bf14.png)\n\nSwitching the border color from `black` to `primary` quickly fixed the issue:\n\n![White frame around selected cell is visible in front of dark background](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/stacked_decisions2_ecf23e1695.png)\n\n### Landscape mode\n\nPreviews default to portrait mode. Use the `previewInterfaceOrientation` modifier to preview in landscape mode instead:\n\n```swift\nstruct ContentView_Previews: PreviewProvider {\n   static var previews: some View {\n       ContentView()\n           .previewInterfaceOrientation(.landscapeRight)\n   }\n}\n```\n\n![iPhone app previewed in lanscape mode](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/landscape_80d4442055.png)\n\n### Device type\n\nPreviews default to the simulator device that you've selected in Xcode. Chances are that you want your app to work well on multiple devices. Typically, I find that there's extra work needed to make an app I designed for the iPhone work well on an iPad.\n\nThe `previewDevice` modifier lets us specify the device type to use in the preview:\n\n```swift\nstruct ContentView_Previews: PreviewProvider {\n   static var previews: some View {\n       ContentView()\n           .previewDevice(PreviewDevice(rawValue: \"iPad (9th generation)\"))\n   }\n}\n```\n\n![App previewed on an iPad](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/i_Pad_27efe03ad8.png)\n\nYou can find the names of the available devices from Xcode's simulator menu, or from the terminal using `xcrun simctl list devices`.\n\n### Pinning views\n\nIn the bottom-left corner of the preview area, there's a pin button. Pressing this \"pins\" the current preview so that it's still shown when you browse to the code for other views:\n\n![Pin icon in Xcode](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/pin_icon_82889deb40.png)\n\nThis is useful to observe how a parent view changes as you edit the code for the child view:\n\n![Animation of the pinned view continuing to be shown and updates as the developer switches to other views in Xcode](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/pin_2ac0418850.gif)\n\n### Live previews\n\nAt the start of this article, I made a comparison between previews and unit testing. Live previews mean that you really can test your views in isolation (to be accurate, the view you're testing plus all of the views it embeds or links to).\n\nPress the play button above the preview to enter live mode:\n\n![Play button above Xcode preview](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/play_button_4c5824b927.png)\n\nYou can now interact with your view:\n\n![Preview responds to user actions](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/live_mode_6bcaa486c9.gif)\n\n## Getting rid of excess boilerplate preview code\n\nAs you may have noticed, some of my previews now have more code than the actual views. This isn't necessarily a problem, but there's a lot of repeated boilerplate code used by multiple views. Not only that, but you'll be embedding the same boilerplate code into previews in other projects.\n\nTo streamline my preview code, I've created several view builders. They all follow the same pattern—receive a `View` and return a new `View` that's built from that `View`.\n\nI start the name of each view builder with `_Preview` to make it easy to take advantage of Xcode's code completion feature.\n\n### Light/dark mode\n\n[`_PreviewColorScheme`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Preview%20Content/Custom%20Previews/_PreviewColorScheme.swift) returns a `Group` of copies of the view. One is in light mode, the other dark:\n\n```swift\nstruct _PreviewColorScheme<Value: View>: View {\n   private let viewToPreview: Value\n\n   init(_ viewToPreview: Value) {\n       self.viewToPreview = viewToPreview\n   }\n\n   var body: some View {\n       Group {\n           viewToPreview\n           viewToPreview.preferredColorScheme(.dark)\n       }\n   }\n}\n```\n\nTo use this view builder in a preview, simply pass in the `View` you're previewing:\n\n```swift\nstruct CardView_Previews: PreviewProvider {\n   static var previews: some View {\n       _PreviewColorScheme(\n           VStack {\n               ForEach(Suit.allCases, id: \\.rawValue) { suit in\n                   CardView(card: Card(suit: suit))\n               }\n           }\n               .padding()\n               .previewLayout(.sizeThatFits)\n       )\n   }\n}\n```\n\n![multiple cards shown in the same preview](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/preview_color_c1a36666a1.png)\n\n### Orientation\n\n[`_PreviewOrientation`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Preview%20Content/Custom%20Previews/_PreviewOrientation.swift) returns a `Group` containing the original `View` in portrait and landscape modes:\n\n```swift\nstruct _PreviewOrientation<Value: View>: View {\n   private let viewToPreview: Value\n\n   init(_ viewToPreview: Value) {\n       self.viewToPreview = viewToPreview\n   }\n\n   var body: some View {\n       Group {\n           viewToPreview\n           viewToPreview.previewInterfaceOrientation(.landscapeRight)\n       }\n   }\n}\n```\n\nTo use this view builder in a preview, simply pass in the `View` you're previewing:\n\n```swift\nstruct ContentView_Previews: PreviewProvider {\n   static var previews: some View {\n       _PreviewOrientation(\n           ContentView()\n       )\n   }\n}\n```\n\n![Preview showing the same view in portrait and lanscape modes](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/preview_Orientation_400bbf69dc.png)\n\n### No device\n\n[`_PreviewNoDevice`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Preview%20Content/Custom%20Previews/_PreviewNoDevice.swift) returns a view built from adding the `previewLayout` modifier and adding `padding to the input view:\n\n```swift\nstruct _PreviewNoDevice<Value: View>: View {\n   private let viewToPreview: Value\n\n   init(_ viewToPreview: Value) {\n       self.viewToPreview = viewToPreview\n   }\n\n   var body: some View {\n       Group {\n           viewToPreview\n               .previewLayout(.sizeThatFits)\n               .padding()\n       }\n   }\n}\n```\n\nTo use this view builder in a preview, simply pass in the `View` you're previewing:\n\n```swift\nstruct CardView_Previews: PreviewProvider {\n   static var previews: some View {\n       _PreviewNoDevice(\n           CardView(card: Card())\n       )\n   }\n}\n```\n\n![9 Clubs previewed in a small window](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/preview_no_device_1_c61ae16f88.png)\n\n### Multiple devices\n\n[`_PreviewDevices`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Preview%20Content/Custom%20Previews/_PreviewDevices.swift) returns a `Group` containing a copy of the `View` for each device type. You can modify `devices` in the code to include the devices you want to see previews for:\n\n```swift\nstruct _PreviewDevices<Value: View>: View {\n   let devices = [\n       \"iPhone 13 Pro Max\",\n       \"iPhone 13 mini\",\n       \"iPad (9th generation)\"\n   ]\n\n   private let viewToPreview: Value\n\n   init(_ viewToPreview: Value) {\n       self.viewToPreview = viewToPreview\n   }\n\n   var body: some View {\n       Group {\n           ForEach(devices, id: \\.self) { device in\n               viewToPreview\n                   .previewDevice(PreviewDevice(rawValue: device))\n                   .previewDisplayName(device)\n           }\n       }\n   }\n}\n```\n\nI'd be cautious about adding too many devices as it will make any previews using this view builder slow down and consume resources.\n\nTo use this view builder in a preview, simply pass in the `View` you're previewing:\n\n```swift\nstruct ContentView_Previews: PreviewProvider {\n   static var previews: some View {\n       _PreviewDevices(\n           ContentView()\n       )\n   }\n}\n```\n\n![The same view previewed on 3 different device types](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/pasted_image_0_c78c26b145.png)\n\n### Combining multiple view builders\n\nEach view builder receives a view and returns a new view. That means that you can compose the functions by passing the results of one view builder to another. In the extreme case, you can use up to three on the same view preview:\n\n```swift\nstruct ContentView_Previews: PreviewProvider {\n   static var previews: some View {\n       _PreviewOrientation(\n           _PreviewColorScheme(\n               _PreviewDevices(ContentView())\n           )\n       )\n   }\n}\n```\n\nThis produces 12 views to cover all permutations of orientation, appearance, and device.\n\nFor each view, you should consider which modifiers add value. For the `CardView`, it makes sense to use `_PreviewNoDevice` and `_PreviewColorSchem`e, but previewing on different devices and orientations wouldn't add any value.\n\n## Previewing stateful views (Realm)\n\nOften, a SwiftUI view will fetch state from a database such as Realm or Core Data. For that to work, there needs to be data in that database.\n\nPreviews are effectively running on embedded iOS simulators. That helps explain how they are both slower and more powerful than you might expect from a \"preview\" feature. That also means that each preview also contains a Realm database (assuming that you're using the [Realm-Cocoa SDK](https://github.com/realm/realm-cocoa)). The preview can store data in that database, and the view can access that data.\n\nIn the BlackJack training app, the action to take for each player/dealer hand combination is stored in Realm. For example, [`DefaultDecisionView`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Views/Decision%20Matrix/DefaultDecisionView.swift) uses `@ObservedResults` to access data from Realm:\n\n```swift\nstruct DefaultDecisionView: View {\n   @ObservedResults(Decisions.self,\n                    filter: NSPredicate(format: \"isSoft == NO AND isSplit == NO\")) var decisions\n```\n\nTo ensure that there's data for the previewed view to find, the preview checks whether the Realm database already contains data (`Decisions.areDecisionsPopulated`). If not, then it adds the required data (`Decisions.bootstrapDecisions()`):\n\n```swift\nstruct DefaultDecisionView_Previews: PreviewProvider {\n   static var previews: some View {\n       if !Decisions.areDecisionsPopulated {\n           Decisions.bootstrapDecisions()\n       }\n       return _PreviewOrientation(\n           _PreviewColorScheme(\n               Group {\n                   NavigationView {\n                       DefaultDecisionView(myHandValue: 6, dealerCardValue: .nine)\n                   }\n                   NavigationView {\n                       DefaultDecisionView(myHandValue: 6, dealerCardValue: .nine, editable: true)\n                   }\n               }\n                   .navigationViewStyle(StackNavigationViewStyle())\n           )\n       )\n   }\n}\n```\n\n`DefaultDecisionView` is embedded in [`DecisionMatrixView`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Views/Decision%20Matrix/DecisionMatrixView.swift) and so the preview for `DecisionMatrixView` must also conditionally populate the Realm data. In turn, `DecisionMatrixView` is embedded in [`PracticeView`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Views/PracticeView.swift), and `PracticeView` in [`ContentView`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Views/ContentView.swift)—and so, they too need to bootstrap the Realm data so that it's available further down the view hierarchy.\n\nThis is the implementation of the bootstrap functions:\n\n```swift\nextension Decisions {\n   static var areDecisionsPopulated: Bool {\n       do {\n           let realm = try Realm()\n           let decisionObjects = realm.objects(Decisions.self)\n           return decisionObjects.count >= 3\n       } catch {\n           print(\"Error, couldn't read decision objects from Realm: \\(error.localizedDescription)\")\n           return false\n       }\n   }\n\n   static func bootstrapDecisions() {\n       do {\n           let realm = try Realm()\n           let defaultDecisions = Decisions()\n           let softDecisions = Decisions()\n           let splitDecisions = Decisions()\n\n           defaultDecisions.bootstrap(defaults: defaultDefaultDecisions, handType: .normal)\n           softDecisions.bootstrap(defaults: defaultSoftDecisions, handType: .soft)\n           splitDecisions.bootstrap(defaults: defaultSplitDecisions, handType: .split)\n           try realm.write {\n               realm.delete(realm.objects(Decision.self))\n               realm.delete(realm.objects(Decisions.self))\n               realm.delete(realm.objects(Decision.self))\n               realm.delete(realm.objects(Decisions.self))\n               realm.add(defaultDecisions)\n               realm.add(softDecisions)\n               realm.add(splitDecisions)\n           }\n       } catch {\n           print(\"Error, couldn't read decision objects from Realm: \\(error.localizedDescription)\")\n       }\n   }\n}\n```\n\n### Partitioned, synced realms\n\nThe BlackJack training app uses a standalone Realm database. But what happens if the app is using [Realm Sync](https://docs.mongodb.com/realm/sync/)?\n\nOne option could be to have the SwiftUI preview sync data with your backend Realm service. I think that's a bit too complex, and it breaks my paradigm of treating previews like unit tests for views.\n\nI've found that the simplest solution is to make the view aware of whether it's been created by a preview or by a running app. I'll explain how that works.\n\n[`AuthorView`](https://github.com/realm/RChat/blob/main/RChat-iOS/RChat/Views/Chat%20Messages/AuthorView.swift) from the [RChat](https://github.com/realm/RChat/) app fetches data from Realm:\n\n```swift\nstruct AuthorView: View {\n   @ObservedResults(Chatster.self) var chatsters\n   ...\n}\n```\n\nIts preview code bootstraps the embedded realm:\n\n```swift\nstruct AuthorView_Previews: PreviewProvider {\n   static var previews: some View {\n       Realm.bootstrap()\n\n\n       return AppearancePreviews(AuthorView(userName: \"rod@contoso.com\"))\n           .previewLayout(.sizeThatFits)\n           .padding()\n   }\n}\n```\n\nThe app adds bootstrap as an extension to Realm:\n\n```swift\nextension Realm: Samplable {\n   static func bootstrap() {\n       do {\n           let realm = try Realm()\n           try realm.write {\n               realm.deleteAll()\n               realm.add(Chatster.samples)\n               realm.add(User(User.sample))\n               realm.add(ChatMessage.samples)\n           }\n       } catch {\n           print(\"Failed to bootstrap the default realm\")\n       }\n   }\n}\n```\n\nA complication is that `AuthorView` is embedded in [`ChatBubbleView`](https://github.com/realm/RChat/tree/main/RChat-iOS/RChat/Views/Chat%20Messages). For the app to work, `ChatBubbleView` must pass the synced realm configuration to `AuthorView`:\n\n```swift\nAuthorView(userName: authorName)\n   .environment(\\.realmConfiguration,\n                   app.currentUser!.configuration(\n                   partitionValue: \"all-users=all-the-users\"))\n```\n\n**But**, when previewing `ChatBubbleView`, we want `AuthorView` to use the preview's local, embedded realm (not to be dependent on a Realm back-end app). That means that `ChatBubbleView` must check whether or not it's running as part of a preview:\n\n```swift\nstruct ChatBubbleView: View {\n   ...\n   var isPreview = false\n   ...\n   var body: some View {\n       ...\n       if isPreview {\n           AuthorView(userName: authorName)\n       } else {\n           AuthorView(userName: authorName)\n               .environment(\\.realmConfiguration,\n                   app.currentUser!.configuration(\n                       partitionValue: \"all-users=all-the-users\"))\n       }\n       ...\n   }\n}\n```\n\nThe preview is then responsible for bootstrapping the local realm and flagging to `ChatBubbleView` that it's a preview:\n\n```swift\nstruct ChatBubbleView_Previews: PreviewProvider {\n   static var previews: some View {\n       Realm.bootstrap()\n       return ChatBubbleView(\n           chatMessage: .sample,\n           authorName: \"jane\",\n           isPreview: true)\n   }\n}\n```\n\n## Troubleshooting your previews\n\nAs mentioned at the beginning of this article, the error messages for failed previews are actually useful in Xcode 13.\n\nThat's the good news. \n\nThe bad news is that you still can't use breakpoints or print to the console.\n\nOne mitigation is that the `previews` static var in your preview is a `View`. That means that you can replace the `body` of your `ContentView` with your `previews` code. You can then run the app in a simulator and add breakpoints or print to the console. It feels odd to use this approach, but I haven't found a better option yet.\n\n## Conclusion\n\nI've had a mixed relationship with SwiftUI previews.\n\nWhen they work, they're a great tool, making it quicker to write your views. Previews allow you to unit test your views. Previews help you avoid issues when your app is running in dark or landscape mode or on different devices.\n\nBut, they require effort to build. Prior to Xcode 13, it would be tough to justify that effort because of reliability issues.\n\nI believe that Xcode 13 is the tipping point where the efficiency and quality gains far outweigh the effort of writing preview code. That's why I've written this article now.\n\nIn this article, you've seen a number of tips to make previews as useful as possible. I've provided four view builders that you can copy directly into your SwiftUI projects, letting you build the best previews with the minimum of code. Finally, you've seen how you can write previews for views that work with data held in a database such as Realm or Core Data.\n\nPlease provide feedback and ask any questions in the [Realm Community Forum](https://www.mongodb.com/community/forums/c/realm-sdks/58).","description":"Get the most out of iOS Canvas previews to improve your productivity and app quality","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt309010717a3021f5/644c46dc887737866911e969/SwiftUI_Features_ATF.png?branch=prod","description":null}}]},"slug":"/swiftui-previews","title":"*Making SwiftUI Previews Work For You","original_publish_date":"2021-10-29T12:54:10.569Z","strapi_updated_at":"2023-03-06T17:55:29.798Z","expiry_date":"2022-10-29T10:36:05.395Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Mobile","calculated_slug":"/technologies/mobile"}},{"node":{"title":"*iOS","calculated_slug":"/technologies/ios"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to make SwiftUI previews to write better iOS apps faster","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt3507a2ae0fd4d297/644c46de75b18599c346a8ff/SwiftUI_Features_OG.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:13.129Z","publish_details":{"time":"2023-04-28T22:52:57.472Z"}}},{"calculated_slug":"/products/atlas/migrate-azure-cosmosdb-mongodb-atlas-apache-kafka","content":"## Overview\nWhen you are the best of breed, you have many imitators. MongoDB is no different in the database world.  If you are reading this blog, you are most likely an Azure customer that ended up using CosmosDB.  \n\nYou needed a database that could handle unstructured data in Azure and eventually realized CosmosDB wasn’t the best fit.  Perhaps you found that it is too expensive for your workload or not performing well or simply have no confidence in the platform.  You also might have tried using the MongoDB API and found that the queries you wanted to use simply don’t work in CosmosDB because it fails [67% of the compatibility tests](https://www.mongodb.com/cloud/atlas/compare).  \n\nWhatever the path you took to CosmosDB, know that you can easily migrate your data to MongoDB Atlas while still leveraging the full power of Azure.  With MongoDB Atlas in Azure, there are no more failed queries, slow performance, and surprise bills from [not optimizing your RDUs](https://docs.microsoft.com/en-us/azure/cosmos-db/cosmos-db-reserved-capacity).  MongoDB Atlas in Azure also gives you access to the latest releases of MongoDB and the flexibility to leverage any of the three cloud providers if your business needs change.\n\nNote:  When you originally created your CosmosDB, you were presented with these API options:\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/which_api_best_suits_your_workload_aa9a89b977.png)\n\nIf you created your CosmosDB using Azure Cosmos DB API for MongoDB, you can use [mongo tools](https://docs.mongodb.com/database-tools/) such as mongodump, mongorestore, mongoimport, and mongoexport to move your data.  The [Azure CosmosDB Connector](https://www.confluent.io/hub/microsoftcorporation/kafka-connect-cosmos) for Kafka Connect does not work with CosmosDB databases that were created for the Azure Cosmos DB API for MongoDB.\n\nIn this blog post, we will cover how to leverage Apache Kafka to move data from Azure CosmosDB Core (Native API) to MongoDB Atlas.  While there are many ways to move data, using Kafka will allow you to not only perform a one-time migration but to stream data from CosmosDB to MongoDB. This gives you the opportunity to test your application and compare the experience so that you can make the final application change to MongoDB Atlas when you are ready. The complete example code is available in this [GitHub repository](https://github.com/RWaltersMA/CosmosDB2MongoDB).\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/apache_kafka_connect_335ed70aa3.png)\n## Getting started\nYou’ll need access to an Apache Kafka cluster.  There are many options available to you, including Confluent Cloud, or you can deploy your own Apache Kafka via Docker as shown in this blog.  Microsoft Azure also includes an event messaging service called Azure Event Hubs. This service provides a Kafka endpoint that can be used as an alternative to running your own Kafka cluster. Azure Event Hubs exposes the same Kafka Connect API, enabling the use of the MongoDB connector and Azure CosmosDB DB Connector with the Event Hubs service.\n\nIf you do not have an existing Kafka deployment, perform these steps.  You will need docker installed on your local machine:\n```\ngit clone ​​https://github.com/RWaltersMA/CosmosDB2MongoDB.git\n```\nNext, build the docker containers.\n```\ndocker-compose up -d --build\n```\n\nThe docker compose script (docker-compose.yml) will stand up all the components you need, including Apache Kafka and Kafka Connect. Install the CosmosDB and MongoDB connectors.\n## Configuring Kafka Connect\nModify the **cosmosdb-source.json** file and replace the placeholder values with your own.\n```\n{\n  \"name\": \"cosmosdb-source\",\n  \"config\": {\n    \"connector.class\": \"com.azure.cosmos.kafka.connect.source.CosmosDBSourceConnector\",\n    \"tasks.max\": \"1\",\n    \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n    \"connect.cosmos.task.poll.interval\": \"100\",\n    \"connect.cosmos.connection.endpoint\": \n\"https://**<cosmosinstance-name>**.documents.azure.com:443/\",\n    \"connect.cosmos.master.key\": **\"<cosmosdbprimarykey>\",**\n    \"connect.cosmos.databasename\": **\"<database name>\",**\n    \"connect.cosmos.containers.topicmap\": **\"<containers>#<topicname>”,**\n    \"connect.cosmos.offset.useLatest\": false,\n    \"value.converter.schemas.enable\": \"false\",\n    \"key.converter.schemas.enable\": \"false\"\n  }\n}\n\n```\nModify the **mongo-sink.json** file and replace the placeholder values with your own.\n```\n{\"name\": \"mongo-sink\",\n    \"config\": {\n \"connector.class\":\"com.mongodb.kafka.connect.MongoSinkConnector\",\n      \"tasks.max\":\"1\",\n      \"topics\":\"<topicname>\",\n      \"connection.uri\":\"<MongoDB Atlas Connection String>\",\n      \"database\":\"<Desired Database Name>\",\n      \"collection\":\"<Desired Collection Name>\",\n      \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\",\n      \"value.converter\":\"org.apache.kafka.connect.json.JsonConverter\",\n      \"value.converter.schemas.enable\": \"false\",\n      \"key.converter.schemas.enable\": \"false\"\n      \n }}\n\n```\nNote: Before we configure Kafka Connect, make sure that your network settings on both CosmosDB and MongoDB Atlas will allow communication between these two services.  In CosmosDB, select the Firewall and Virtual Networks.  While the easiest configuration is to select “All networks,” you can provide a more secure connection by specifying the IP range from the Firewall setting in the Selected networks option.  MongoDB Atlas Network access also needs to be configured to allow remote connections.  By default, MongoDB Atlas does not allow any external connections. See [Configure IP Access List](https://docs.atlas.mongodb.com/security/ip-access-list/) for more information.\n\nTo configure our two connectors, make a REST API call to the Kafka Connect service:\n\n```\ncurl -X POST -H \"Content-Type: application/json\" -d @cosmosdb-source.json  http://localhost:8083/connectors\n\n\ncurl -X POST -H \"Content-Type: application/json\" -d @mongodb-sink.json  http://localhost:8083/connectors\n\n```\nThat’s it!\n\nProvided the network and database access was configured properly, data from your CosmosDB should begin to flow into MongoDB Atlas.  If you don’t see anything, here are some troubleshooting tips:\n\n* Try connecting to your MongoDB Atlas cluster using the mongosh tool from the server running the docker container.\n* View the docker logs for the Kafka Connect service.\n* Verify that you can connect to the CosmosDB instance using the Azure CLI from the server running the docker container.\n\n\n**Summary**\nIn this post, we explored how to move data from CosmosDB to MongoDB using Apache Kafka. If you’d like to explore this method and other ways to migrate data, check out the 2021 MongoDB partner of the year award winner, [Peerslands'](https://www.peerislands.io/), five-part blog post on [CosmosDB migration](https://www.peerislands.io/azure-cosmos-db-for-mongo-series-part-1-setting-up-of-cosmosdb/).","description":"Learn how to migrate your data in Azure CosmosDB to MongoDB Atlas using Apache Kafka.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt932a20238827e81a/644c46e22bc49413f185ee03/featured.png?branch=prod","description":null}}]},"slug":"/migrate-azure-cosmosdb-mongodb-atlas-apache-kafka","title":"*Migrate from Azure CosmosDB to MongoDB Atlas Using Apache Kafka","original_publish_date":"2021-11-09T14:00:00.248Z","strapi_updated_at":"2022-05-09T19:29:44.151Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Kafka","calculated_slug":"/technologies/kafka"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:12.756Z","publish_details":{"time":"2023-04-28T22:52:57.505Z"}}},{"calculated_slug":"/products/realm/realm-keypath-filtering","content":"## Introduction\n\n[Realm Swift v10.12.0](https://github.com/realm/realm-cocoa/releases) introduced the ability to filter change notifications for desired key paths. This level of granularity has been something we've had our eye on, so it’s really satisfying to release this kind of control and performance benefit. Here’s a quick rundown on what’s changed and why it matters.\n\n## Notifications Before\n\nBy default, notifications return changes for all insertions, modifications, and deletions. Suppose that I have a schema that looks like the one below.\n\n![Graph showing that all sub-components of the object are selected](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/keypath1_4619609829.png)\n\nIf I observe a `Results<Company>` object and the name of one of the companies in the results changes, the notification block would fire and my UI would update: \n\n```swift\nlet results = realm.objects(Company.self)\nlet notificationToken = results.observe() { changes in\n    // update UI\n}\n```\n\nThat’s quite straightforward for non-collection properties. But what about other types, like lists?\n\nNaturally, the block I passed into .`observe` will execute each time an `Order` is added or removed. But the block also executes each time a property on the `Order` list is edited. The same goes for _those_ properties’ collections too (and so on!). Even though I’m observing “just” a collection of `Company` objects, I’ll receive change notifications for properties on a half-dozen other collections.\n\nThis isn’t necessarily an issue for most cases. Small object graphs, or “siloed” objects, that don’t feature many relationships might not experience unneeded notifications at all. But for complex webs of objects, where several layers of children objects exist, an app developer may benefit from a **major performance enhancement and added control from KeyPath filtering**.\n\n## KeyPath Filtering\n\nNow `.observe` comes with an optional `keyPaths` parameter:\n\n```swift\npublic func observe<T: RLMObjectBase>(keyPaths: [String]? = nil,\n                                      on queue: DispatchQueue? = nil,\n                                      _ block: @escaping (ObjectChange<T>) -> Void) -> NotificationToken\n```\n\nThe `.observe `function will only notify on the field or fields specified in the `keyPaths` parameter. Other fields are ignored unless explicitly passed into the parameter.\n\nThis allows the app developer to tailor which relationship paths are observed. This reduces computing cost and grants finer control over when the notification fires.\n\nOur modified code might look like this:\n\n```swift\nlet results = realm.objects(Company.self)\nlet notificationToken = results.observe(keyPaths: [\"orders.status\"]) { changes in\n\t// update UI\n}\n```\n\n`.observe `can alternatively take a `PartialKeyPath`:\n\n```swift\nlet results = realm.objects(Company.self)\nlet notificationToken = results.observe(keyPaths: [\\Company.orders.status]) { changes in\n\t// update UI\n}\n```\n\nIf we applied the above snippets to our previous example, we’d only receive notifications for this portion of the schema:\n\n![Graph showing that just a single path through the Objects components is selected](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/keypath2_a799e8e8ca.png)\n\nThe notification process is no longer traversing an entire tree of relationships each time a modification is made. Within a complex tree of related objects, the change-notification checker will now traverse only the relevant paths. This saves huge amounts of work. \n\nIn a large database, this can be a serious performance boost! The end-user can spend less time with a spinner and more time using your application.\n\n## Conclusion\n\n- `.observe` has a new optional `keyPaths` parameter. \n- The app developer has more granular control over when notifications are fired.\n- This can greatly improve notification performance for large databases and complex object graphs.\n\nPlease provide feedback and ask any questions in the [Realm Community Forum](https://www.mongodb.com/community/forums/c/realm-sdks/58).\n","description":"How to customize your notifications when your iOS app is observing Realm","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt04671d607e7e92fb/644c466a1023738922966c6e/realm-logo.jpg?branch=prod","description":null}}]},"slug":"/realm-keypath-filtering","title":"*Filter Realm Notifications in Your iOS App with KeyPaths","original_publish_date":"2021-11-03T12:28:04.337Z","strapi_updated_at":"2022-05-17T22:43:00.899Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*iOS","calculated_slug":"/technologies/ios"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:12.326Z","publish_details":{"time":"2023-04-28T22:52:58.548Z"}}},{"calculated_slug":"/products/atlas/auto-pausing-inactive-clusters","content":"# Auto Pausing Inactive Clusters\n## Introduction\n\nA couple of years ago I wrote an [article](https://www.mongodb.com/developer/products/atlas/atlas-cluster-automation-using-scheduled-triggers/) on how to pause and/or scale clusters using scheduled triggers. This article represents a twist on that concept, adding a wrinkle that will pause clusters across an entire organization based on inactivity. Specifically, I’m looking at the [Database Access History](https://docs.atlas.mongodb.com/access-tracking/) to determine activity.\n\nIt is important to note this logging limitation: \n\n_If a cluster experiences an activity spike and generates an extremely large quantity of log messages, Atlas may stop collecting and storing new logs for a period of time._\n\nTherefore, this script could get a false positive that a cluster is inactive when indeed quite the opposite is happening. Given, however, that the intent of this script is for managing lower, non-production environments, I don’t see the false positives as a big concern.\n\n## Architecture\n\nThe implementation uses a [Scheduled Trigger](https://www.mongodb.com/docs/atlas/app-services/triggers/scheduled-triggers/). The trigger calls a series of [App Services Functions](https://www.mongodb.com/docs/atlas/app-services/functions/), which use the [Atlas Administration APIs](https://docs.atlas.mongodb.com/reference/api-resources/) to  iterate over the organization’s projects and their associated clusters, testing the cluster inactivity (as explained in the introduction) and finally pausing the cluster if it is indeed inactive.\n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/flow.png\"\n        alt=\"Architecture\"\n    />\n</figure>\n\n## API Keys\nIn order to call the Atlas Administrative APIs, you'll first need an API Key with the [Organization Owner](https://docs.atlas.mongodb.com/reference/user-roles/#mongodb-authrole-Organization-Owner) role. API Keys are created in the Access Manager, which you'll find in the Organization menu on the left:\n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/access-manager.png \"\n        alt=\"Access Manager\"\n    />\n</figure>\n\nor the menu bar at the top:\n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/access-manager2.png \"\n        alt=\"Access Manager\"\n    />\n</figure>\n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/access-manager3.png\"\n        alt=\"Organization Access Manager\"\n    />\n</figure>\n\nClick **Create API Key**. Give the key a description and be sure to set the permissions to **Organization Owner**:\n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/create-api-key.png\"\n        alt=\"Create API Key\"\n    />\n</figure>\n\n\nWhen you click **Next**, you'll be presented with your Public and Private keys. **Save your private key as Atlas will never show it to you again**. \n\nAs an extra layer of security, you also have the option to set an IP Access List for these keys. I'm skipping this step, so my key will work from anywhere.\n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/create-api-key2.png\"\n        alt=\"Create API Key\"\n    />\n</figure>\n\n## Deployment\n\n### Create a Project for Automation\nSince this solution works across your entire Atlas organization, I like to host it in its own dedicated Atlas Project. \n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/create-project.png\"\n        alt=\"Create Project\"\n    />\n</figure>\n\n### Create a App Services Application\n[Atlas App Services](https://www.mongodb.com/atlas/app-services) provide a powerful application development backend as a service. To begin using it, just click the App Services tab.\n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/app-services.png\"\n        alt=\"App Services\"\n    />\n</figure>\n\n You'll see that App Services offers a bunch of templates to get you started. For this use case, just select the first option to   **Build your own App**:\n \n <figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/app-services-welcome.png\"\n        alt=\"Welcome to Atlas App Services\"\n    />\n</figure>\n\nYou'll then be presented with options to link a data source, name your application and choose a deployment model. The current iteration of this utility doesn't use a data source, so you can ignore that step (App Services will create a free cluster for you). You can also leave the [deployment model](https://www.mongodb.com/docs/atlas/app-services/manage-apps/deploy/deployment-models-and-regions/) at its default (Global), unless you want to limit the application to a specific region. \n\nI've named the application **Atlas Cluster Automation**: \n\n \n <figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/app-services-welcome2.png\"\n        alt=\"Welcome to Atlas App Services\"\n    />\n</figure>\n\nAt this point in our journey, you have two options:\n\n1. Simply import the App Services application and adjust any of the functions to fit your needs.\n2. Build the application from scratch (skip to the next section). \n\n## Import Option\n\n### Step 1: Store the API Secret Key.\nThe extract has a dependency on the API Secret Key, thus the import will fail if it is not configured beforehand.\n\nUse the `Values` menu on the left to Create a Secret named `AtlasPrivateKeySecret` containing the private key you created earlier (the secret is not in quotes): \n\n <figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/value-secret.png\"\n        alt=\"Secret Value\"\n    />\n</figure>\n\n\n### Step 1: Install the Atlas App Services CLI (realm-cli)\n\n[Realm CLI](https://www.mongodb.com/docs/atlas/app-services/cli) is available on npm. To install version 2 of the Realm CLI on your system, ensure that you have [Node.js](https://nodejs.org/en/download/) installed and then run the following command in your shell:\n\n```npm install -g mongodb-realm-cli```\n\n### Step 2: Extract the Application Archive\nDownload and extract the [AtlasClusterAutomation.zip](https://github.com/wbleonard/pause-inactive-clusters/blob/main/export/AtlasClusterAutomation.zip?raw=true).\n\n### Step 3: Log into Atlas\nTo configure your app with realm-cli, you must log in to Atlas using your API keys:\n\n```zsh\n✗ realm-cli login --api-key=\"<Public API Key>\" --private-api-key=\"<Private API Key>\"\nSuccessfully logged in\n```\n\n### Step 4: Get the App Services Application ID\nSelect the `App Settings` menu and copy your Application ID:\n\n![Application ID](https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/application-id.png \"Application ID\")\n\n### Step 5: Import the Application\nRun the following `realm-cli push` command from the directory where you extracted the export:\n\n```zsh\nrealm-cli push --remote=\"<Your App ID>\"\n\n...\nA summary of changes\n...\n\n? Please confirm the changes shown above Yes\nCreating draft\nPushing changes\nDeploying draft\nDeployment complete\nSuccessfully pushed app up:\n```\nAfter the import, replace the `AtlasPublicKey' with your API public key value.\n\n <figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/value-public-key.png\"\n        alt=\"Public Key Value\"\n    />\n</figure>\n\n\n### Review the Imported Application\nThe imported application includes 5 [Atlas Functions](https://www.mongodb.com/docs/atlas/app-services/functions/):\n\n <figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/functions.png\"\n        alt=\"Functions\"\n    />\n</figure>\n\n\nAnd the [Scheduled Trigger](https://www.mongodb.com/docs/atlas/app-services/triggers/scheduled-triggers/) which calls the **pauseInactiveClusters** function:\n\n <figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/triggers.png\"\n        alt=\"Triggers\"\n    />\n</figure>\n\nThe trigger is schedule to fire every 30 minutes. Note, the **pauseClusters** function that the trigger calls currently only logs cluster activity. This is so you can monitor and verify that the fuction behaves as you desire. When ready, uncomment the line that calls the **pauseCluster** function:\n\n```Javascript\n   if (!is_active) {\n              console.log(`Pausing ${project.name}:${cluster.name} because it has been inactive for more then ${minutesInactive} minutes`);  \n              //await context.functions.execute(\"pauseCluster\", project.id, cluster.name, pause);\n```\n\nIn addition, the **pauseClusters** function can be configured to exclude projects (such as those dedicated to production workloads):\n\n```javascrsipt\n  /*\n   * These project names are just an example. \n   * The same concept could be used to exclude clusters or even \n   * configure different inactivity intervals by project or cluster.\n   * These configuration options could also be stored and read from \n   * and Atlas database.\n   */\n  excludeProjects = ['PROD1', 'PROD2'];   \n```\n\nNow that you have reviewed the draft, as a final step go ahead and deploy the App Services application. \n\n![Review Draft & Deploy](https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/review-draft.png \"Review Draft & Deploy\")\n## Build it Yourself Option\nTo understand what's included in the application, here are the steps to build it yourself from scratch. \n\n### Step 1: Store the API Keys\n\nThe functions we need to create will call the [Atlas Administration API](https://www.mongodb.com/docs/atlas/api/atlas-admin-api/), so we need to store our API Public and Private Keys, which we will do using [Values & Secrets](https://www.mongodb.com/docs/atlas/app-services/values-and-secrets/). The sample code I provide references these values as `AtlasPublicKey` and `AtlasPrivateKey`, so use those same names unless you want to change the code where they’re referenced.\n\nYou'll find `Values` under the Build menu:\n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/values-menu.png\"\n        alt=\"Values\"\n    />\n</figure>\n\nFirst, create a Value, `AtlasPublicKey`, for your public key (note, the key is in quotes): \n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/value-public-key.png\"\n        alt=\"Public Key Value\"\n    />\n</figure>\n\n\nCreate a Secret, `AtlasPrivateKeySecret`, containing your private key (the secret is not in quotes): \n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/value-secret.png\"\n        alt=\"Secret Value\"\n    />\n</figure>\n\nThe Secret cannot be accessed directly, so create a second Value, `AtlasPrivateKey`, that links to the secret:  \n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/value-private-key.png\"\n        alt=\"Private Key Value\"\n    />\n</figure>\n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/review-draft.png\"\n        alt=\"Review Draft & Deploy\"\n    />\n</figure>\n\n\n### Step 2: Create the Functions\n\nThe four functions that need to be created are pretty self-explanatory, so I’m not going to provide a bunch of additional explanations here. \n#### getProjects\n\nThis standalone function can be test run from the App Services console to see the list of all the projects in your organization. \n\n```Javascript\n/*\n * Returns an array of the projects in the organization\n * See https://docs.atlas.mongodb.com/reference/api/project-get-all/\n *\n * Returns an array of objects, e.g.\n *\n * {\n * \"clusterCount\": {\n *      \"$numberInt\": \"1\"\n *    },\n *    \"created\": \"2021-05-11T18:24:48Z\",\n *    \"id\": \"609acbef1b76b53fcd37c8e1\",\n *    \"links\": [\n *      {\n *        \"href\": \"https://cloud.mongodb.com/api/atlas/v1.0/groups/609acbef1b76b53fcd37c8e1\",\n *        \"rel\": \"self\"\n *      }\n *    ],\n *    \"name\": \"mg-training-sample\",\n *    \"orgId\": \"5b4e2d803b34b965050f1835\"\n *  }\n  *\n */\nexports = async function() {\n  \n  // Get stored credentials...\n  const username = await context.values.get(\"AtlasPublicKey\");\n  const password = await context.values.get(\"AtlasPrivateKey\");\n  \n  const arg = { \n    scheme: 'https', \n    host: 'cloud.mongodb.com', \n    path: 'api/atlas/v1.0/groups', \n    username: username, \n    password: password,\n    headers: {'Content-Type': ['application/json'], 'Accept-Encoding': ['bzip, deflate']}, \n    digestAuth:true,\n  };\n  \n  // The response body is a BSON.Binary object. Parse it and return.\n  response = await context.http.get(arg);\n\n  return EJSON.parse(response.body.text()).results; \n};\n\n```\n#### getProjectClusters\n\nAfter `getProjects` is called, the trigger iterates over the results, passing the `projectId` to this `getProjectClusters` function. \n\n_To test this function, you need to supply a `projectId`. By default, the Console supplies ‘Hello world!’, so I test for that input and provide some default values for easy testing._\n\n```Javascript\n/*\n * Returns an array of the clusters for the supplied project ID.\n * See https://docs.atlas.mongodb.com/reference/api/clusters-get-all/\n *\n * Returns an array of objects. See the API documentation for details.\n * \n */\nexports = async function(project_id) {\n  \n  if (project_id == \"Hello world!\") { // Easy testing from the console\n    project_id = \"5e8f8268d896f55ac04969a1\"\n  }\n  \n  // Get stored credentials...\n  const username = await context.values.get(\"AtlasPublicKey\");\n  const password = await context.values.get(\"AtlasPrivateKey\");\n  \n  const arg = { \n    scheme: 'https', \n    host: 'cloud.mongodb.com', \n    path: `api/atlas/v1.0/groups/${project_id}/clusters`, \n    username: username, \n    password: password,\n    headers: {'Content-Type': ['application/json'], 'Accept-Encoding': ['bzip, deflate']}, \n    digestAuth:true,\n  };\n  \n  // The response body is a BSON.Binary object. Parse it and return.\n  response = await context.http.get(arg);\n\n  return EJSON.parse(response.body.text()).results; \n};\n\n```\n\n#### clusterIsActive\n\nThis function contains the logic that determines if the cluster can be paused. \n\nMost of the work in this function is manipulating the timestamp in the database access log so it can be compared to the current time and lookback window. \n\nIn addition to returning true (active) or false (inactive), the function logs it’s findings, for example: \\\n \\\n`Checking if cluster 'SA-SHARED-DEMO' has been active in the last 60 minutes`\n\n\n```ZSH\n   Wed Nov 03 2021 19:52:31 GMT+0000 (UTC) - job is being run\n   Wed Nov 03 2021 18:52:31 GMT+0000 (UTC) - cluster inactivity before this time will be reported inactive\n   Wed Nov 03 2021 19:48:45 GMT+0000 (UTC) - last logged database access\nCluster is Active: Username 'brian' was active in cluster 'SA-SHARED-DEMO' 4 minutes ago.\n```\n\n\nLike `getClusterProjects`, there’s a block you can use to provide some test project ID and cluster names for easy testing from the App Services console.\n\n```Javascript\n/*\n * Used the database access history to determine if the cluster is in active use.\n * See https://docs.atlas.mongodb.com/reference/api/access-tracking-get-database-history-clustername/\n * \n * Returns true (active) or false (inactive)\n * \n */\nexports = async function(project_id, clusterName, minutes) {\n  \n  if (project_id == 'Hello world!') { // We're testing from the console\n    project_id = \"5e8f8268d896f55ac04969a1\";\n    clusterName = \"SA-SHARED-DEMO\";\n    minutes = 60;\n  } /*else {\n    console.log (`project_id: ${project_id}, clusterName: ${clusterName}, minutes: ${minutes}`)\n  }*/\n  \n  // Get stored credentials...\n  const username = await context.values.get(\"AtlasPublicKey\");\n  const password = await context.values.get(\"AtlasPrivateKey\");\n  \n  const arg = { \n    scheme: 'https', \n    host: 'cloud.mongodb.com', \n    path: `api/atlas/v1.0/groups/${project_id}/dbAccessHistory/clusters/${clusterName}`, \n    //query: {'authResult': \"true\"},\n    username: username, \n    password: password,\n    headers: {'Content-Type': ['application/json'], 'Accept-Encoding': ['bzip, deflate']}, \n    digestAuth:true,\n  };\n  \n  // The response body is a BSON.Binary object. Parse it and return.\n  response = await context.http.get(arg);\n  \n  accessLogs = EJSON.parse(response.body.text()).accessLogs; \n\n  now = Date.now();\n  const MS_PER_MINUTE = 60000;\n  var durationInMinutes = (minutes < 30, 30, minutes);   // The log granularity is 30 minutes.\n  var idleStartTime = now - (durationInMinutes * MS_PER_MINUTE);\n  \n  nowString = new Date(now).toString();\n  idleStartTimeString = new Date(idleStartTime).toString();\n  console.log(`Checking if cluster '${clusterName}' has been active in the last ${durationInMinutes} minutes`)\n  console.log(`   ${nowString} - job is being run`);\n  console.log(`   ${idleStartTimeString} - cluster inactivity before this time will be reported inactive`);\n  \n  clusterIsActive = false;\n  \n  accessLogs.every(log => {\n    if (log.username != 'mms-automation' && log.username != 'mms-monitoring-agent') {\n      \n      // Convert string log date to milliseconds \n      logTime = Date.parse(log.timestamp);\n\n      logTimeString = new Date(logTime);\n      console.log(`   ${logTimeString} - last logged database access`);\n      \n      var elapsedTimeMins = Math.round((now - logTime)/MS_PER_MINUTE, 0);\n      \n      if (logTime > idleStartTime ) {\n        console.log(`Cluster is Active: Username '${log.username}' was active in cluster '${clusterName}' ${elapsedTimeMins} minutes ago.`);\n        clusterIsActive = true;\n        return false;\n      } else {\n        // The first log entry is older than our inactive window\n        console.log(`Cluster is Inactive: Username '${log.username}' was active in cluster '${clusterName}' ${elapsedTimeMins} minutes ago.`);\n        clusterIsActive = false;\n        return false;\n      }\n    }\n    return true;\n\n  });\n\n  return clusterIsActive;\n\n};\n\n\n```\n\n#### pauseCluster\n\nFinally, if the cluster is inactive, we pass the project Id and cluster name to `pauseCluster`. This function can also resume a cluster, although that feature is not utilized for this use case.\n\n```Javascript\n/*\n * Pauses the named cluster \n * See https://docs.atlas.mongodb.com/reference/api/clusters-modify-one/\n *\n */\nexports = async function(projectID, clusterName, pause) {\n  \n  // Get stored credentials...\n  const username = await context.values.get(\"AtlasPublicKey\");\n  const password = await context.values.get(\"AtlasPrivateKey\");\n  \n  const body = {paused: pause};\n  \n  const arg = { \n    scheme: 'https', \n    host: 'cloud.mongodb.com', \n    path: `api/atlas/v1.0/groups/${projectID}/clusters/${clusterName}`, \n    username: username, \n    password: password,\n    headers: {'Content-Type': ['application/json'], 'Accept-Encoding': ['bzip, deflate']}, \n    digestAuth:true,\n    body: JSON.stringify(body)\n  };\n  \n  // The response body is a BSON.Binary object. Parse it and return.\n  response = await context.http.patch(arg);\n\n  return EJSON.parse(response.body.text()); \n};\n```\n\n### pauseInactiveClusters\n\nThis function will be called by a trigger. As it's not possible to pass a parameter to a scheduled trigger, it uses a hard-coded lookback window of 60 minutes that you can change to meet your needs. You could even store the value in an Atlas database and build a UI to manage its setting :-).\n\nThe function will evaluate all projects and clusters in the organization where it’s hosted. Understanding that there are likely projects or clusters that you never want paused, the function also includes an excludeProjects array, where you can specify a list of project names to exclude from evaluation.\n\nFinally, you’ll notice the call to `pauseCluster` is commented out. I suggest you run this function for a couple of days and review the Trigger logs to verify it behaves as you’d expect.\n\n```Javascript\n/*\n * Iterates over the organizations projects and clusters, \n * pausing clusters inactive for the configured minutes.\n */\nexports = async function() {\n  \n  minutesInactive = 60;\n  \n  /*\n   * These project names are just an example. \n   * The same concept could be used to exclude clusters or even \n   * configure different inactivity intervals by project or cluster.\n   * These configuration options could also be stored and read from \n   * and Atlas database.\n   */\n  excludeProjects = ['PROD1', 'PROD2'];   \n  \n  const projects = await context.functions.execute(\"getProjects\");\n  \n  projects.forEach(async project => {\n    \n    if (excludeProjects.includes(project.name)) {\n      console.log(`Project '${project.name}' has been excluded from pause.`)\n    } else {\n      \n      console.log(`Checking project '${project.name}'s clusters for inactivity...`);\n\n      const clusters = await context.functions.execute(\"getProjectClusters\", project.id);\n      \n      clusters.forEach(async cluster => {\n        \n        if (cluster.providerSettings.providerName != \"TENANT\") {   // It's a dedicated cluster than can be paused\n        \n          if (cluster.paused == false) {\n        \n            is_active =  await context.functions.execute(\"clusterIsActive\", project.id, cluster.name, minutesInactive);\n            \n            if (!is_active) {\n              console.log(`Pausing ${project.name}:${cluster.name} because it has been inactive for more then ${minutesInactive} minutes`);  \n              //await context.functions.execute(\"pauseCluster\", project.id, cluster.name, true);\n            } else {\n              console.log(`Skipping pause for ${project.name}:${cluster.name} because it has active database users in the last ${minutesInactive} minutes.`);\n            }\n          }\n        }\n      });\n     }\n    });\n\n  return true;\n};\n```\n\n### Step 3: Create the Scheduled Trigger\n\nYes, we’re still using a [scheduled trigger](https://www.mongodb.com/docs/atlas/app-services/triggers/scheduled-triggers/), but this time the trigger will run periodically to check for cluster inactivity. Now, your developers working late into the night will no longer have the cluster paused underneath them. \n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/trigger.png\"\n        alt=\"Scheduled Trigger\"\n    />\n</figure>\n\n\n### Step 4: Deploy\n\nAs a final step you need to deploy the App Services application. \n\n<figure align=\"center\">\n    <img\n        style=\"border-radius: 10px\"\n     src=\"https://raw.githubusercontent.com/wbleonard/pause-inactive-clusters/main/images/review-draft.png\"\n        alt=\"Review Draft & Deploy\"\n    />\n</figure>\n\n\n## Summary\n\nThe genesis for this article was a customer, when presented my previous article on scheduling cluster pauses, asked if the same could be achieved based on inactivity. It’s my belief that with the Atlas APIs, anything could be achieved. The only question was what constitutes inactivity? Given the heartbeat and replication that naturally occurs, there’s always some “activity” on the cluster. Ultimately, I settled on database access as the guide. Over time, that metric may be combined with some additional metrics or changed to something else altogether, but the bones of the process are here.\n","description":"One of Atlas' many great features is that it provides you the ability to pause clusters that are not currently needed, which primarily includes non-prod environments. This article shows you how to automatically pause clusters that go unused for a any period of time that you desire.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltc265ccf28c47e0bf/644c46e41efe6d34289518af/og-api.png?branch=prod","description":null}}]},"slug":"/auto-pausing-inactive-clusters","title":"*Auto Pausing Inactive Clusters","original_publish_date":"2022-01-19T14:30:11.286Z","strapi_updated_at":"2022-11-03T15:47:32.432Z","expiry_date":"2022-11-03T21:10:13.320Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:11.932Z","publish_details":{"time":"2023-04-28T22:52:58.579Z"}}},{"calculated_slug":"/products/realm/advanced-modeling-realm-dotnet","content":"Realm's intuitive data model approach means that in most cases, you don't even think of Realm models as entities. You just declare your POCOs, have them inherit from `RealmObject`, and you're done. Now you have persistable models, with `INotifyPropertyChanged` capabilities all wired up, that are also \"live\"—i.e., every time you access a property, you get the latest state and not some snapshot from who knows how long ago. This is great and most of our users absolutely love the simplicity. Still, there are some use cases where being aware that you're working with a database can really bring your data models to the next level. In this blog post, we'll evaluate three techniques you can apply to make your models fit your needs even better.\n\n> **Build better mobile apps with Atlas Device Sync**: Atlas Device Sync is a fully-managed mobile backend-as-a-service. Leverage out-of-the-box infrastructure, data synchronization capabilities, built-in network handling, and much more to quickly launch enterprise-grade mobile apps. [Get started now by build: Deploy Sample for Free](https://www.mongodb.com/realm/register?tck=devhub_advanceddatamodelling)!\n\n## Constructor Validation\n\nOne of the core requirements of Realm is that all models need to have a parameterless constructor. This is needed because Realm needs to be able to instantiate an object without figuring out what arguments to pass to the constructor. What not many people know is that you can make this parameterless constructor private to communicate expectations  to your callers. This means that if you have a `Person` class where you absolutely expect that a `Name` is provided upon object creation, you can have a public constructor with a `name` argument and a private parameterless one for use by Realm:\n\n```csharp\nclass Person : RealmObject\n{\n    public string Name { get; set; }\n\n    public Person(string name)\n    {\n        ValidateName(name);\n\n        Name = name;\n    }\n\n    // This is used by Realm, even though it's private\n    private Person()\n    {\n    }\n}\n```\n\nAnd I know what some of you may be thinking: \"Oh no! 😱 Does that mean Realm uses the suuuuper slow reflection to create object instances?\" Fortunately, the answer is no. Instead, at compile time, Realm injects a nested helper class in each model that has a `CreateInstance` method. Since the helper class is nested in the model classes, it has access to private members and is thus able to invoke the private constructor.\n\n## Property Access Modifiers\n\nSimilar to the point above, another relatively unknown feature of Realm is that persisted properties don't need to be public. You can either have the entire property be private or just one of the accessors. This synergizes nicely with the private constructor technique that we mentioned above. If you expose a constructor that explicitly validates the person's name, it would be fairly annoying to do all that work and have some code accidentally set the property to `null` the very next line. So it would make sense to make the setter of the name property above private:\n\n```csharp\nclass Person : RealmObject\n{\n    public string Name { get; private set; }\n\n    public Person(string name)\n    {\n        // ...\n    }\n}\n```\n\nThat way, you're communicating clearly to the class consumers that they need to provide the name at object creation time and that it can't be changed later. A very common use case here is to make the `Id` setter private and generate a random `Id` at object creation time:\n\n```csharp\nclass Transaction : RealmObject\n{\n    public Guid Id { get; private set; } = Guid.NewGuid();\n}\n```\n\nSometimes, it makes sense to make the entire property private—typically, when you want to expose a different public property that wraps it. If we go back to our `Person` and `Name` example, perhaps we want to allow changing the name, but we want to still validate the new name before we persist it. Then, we create a private autoimplemented property that Realm will use for persistence, and a public one that does the validation:\n\n```csharp\nclass Person : RealmObject\n{\n    [MapTo(\"Name\")]\n    private string _Name { get; set; }\n\n    public string Name\n    {\n        get => _Name;\n        set\n        {\n            ValidateName(value);\n            _Name = value;\n        }\n    }\n}\n```\n\nThis is quite neat as it makes the public API of your model safe, while preserving its persistability. Of note is the `MapTo` attribute applied to `_Name`. It is not strictly necessary. I just added it to avoid having ugly column names in the database. You can use it or not use it.  It's totally up to you. One thing to note when utilizing this technique is that Realm is completely unaware of the relationship between `Name` and `_Name`. This has two implications. 1) Notifications will be emitted for `_Name` only, and 2) You can't use LINQ queries to filter `Person` objects by name. Let's see how we can mitigate both:\n\nFor notifications, we can override `OnPropertyChanged` and raise a notification for `Name` whenever `_Name` changes:\n\n```csharp\nclass Person : RealmObject\n{\n    protected override void OnPropertyChanged(string propertyName)\n    {\n        base.OnPropertyChanged(propertyName);\n\n        if (propertyName == nameof(_Name))\n        {\n            RaisePropertyChanged(nameof(Name));\n        }\n    }\n}\n```\n\nThe code is fairly straightforward. `OnPropertyChanged` will be invoked whenever any property on the object changes and we just re-raise it for the related `Name` property. Note that, as an optimization, `OnPropertyChanged` will only be invoked if there are subscribers to the `PropertyChanged` event. So if you're testing this out and don't see the code get executed, make sure you added a subscriber first.\n\nThe situation with queries is slightly harder to work around. The main issue is that because the property is private, you can't use it in a LINQ query—e.g., `realm.All<Person>().Where(p => p._Name == \"Peter\")` will result in a compile-time error. On the other hand, because Realm doesn't know that `Name` is tied to `_Name`, you can't use `p.Name == \"Peter\"` either. You can still use the string-based queries, though. Just remember to use the name that Realm knows about—i.e., the string argument of `MapTo` if you remapped the property name or the internal property (`_Name`) if you didn't:\n\n```csharp\n// _Name is mapped to 'Name' which is what we use here\nvar peters = realm.All<Person>().Filter(\"Name == 'Peter'\");\n```\n\n## Using Unpersistable Data Types\n\nRealm has a wide variety of supported data types—most primitive types in the Base Class Library (BCL), as  well as advanced collections, such as sets and dictionaries. But sometimes, you'll come across a data type that Realm can't store yet, the most obvious example being enums. In such cases, you can build on top of the previous technique to expose enum properties in your models and have them be persisted as one of the supported data types:\n\n```csharp\nenum TransactionState\n{\n    Pending,\n    Settled,\n    Error\n}\n\nclass Transaction : RealmObject\n{\n    private string _State { get; set; }\n\n    public TransactionState State\n    {\n        get => Enum.Parse<TransactionState>(_State);\n        set => _State = value.ToString();\n    }\n}\n```\n\nUsing this technique, you can persist many other types, as long as they can be converted losslessly to a persistable primitive type. In this case, we chose `string`, but we could have just as easily used integer. The string representation takes a bit more memory but is also more explicit and less error prone—e.g., if you rearrange the enum members, the data will still be consistent.\n\nAll that is pretty cool, but we can take it up a notch. By building on top of this idea, we can also devise a strategy for representing complex data types, such as `Vector3` in a Unity game or a `GeoCoordinate` in a location-aware app. To do so, we'll take advantage of embedded objects—a Realm concept that represents a complex data structure that is owned entirely by its parent. Embedded objects are a great fit for this use case because we want to have a strict 1:1 relationship and we want to make sure that deleting the parent also cleans up the embedded objects it owns. Let's see this in action:\n\n```csharp\nclass Vector3Model : EmbeddedObject\n{\n    // Casing of the properties here is unusual for C#,\n    // but consistent with the Unity casing.\n    private float x { get; set; }\n    private float y { get; set; }\n    private float z { get; set; }\n\n    public Vector3Model(Vector3 vector)\n    {\n        x = vector.x;\n        y = vector.y;\n        z = vector.z;\n    }\n\n    private Vector3Model()\n    {\n    }\n\n    public Vector3 ToVector3() => new Vector3(x, y, z);\n}\n\nclass Powerup : RealmObject\n{\n    [MapTo(\"Position\")]\n    private Vector3Model _Position { get; set; }\n\n    public Vector3 Position\n    {\n        get => _Position?.ToVector3() ?? Vector3.zero;\n        set => _Position = new Vector3Model(value);\n    }\n\n    protected override void OnPropertyChanged(string propertyName)\n    {\n        base.OnPropertyChanged(propertyName);\n\n        if (propertyName == nameof(_Position))\n        {\n            RaisePropertyChanged(nameof(Position));\n        }\n    }\n}\n```\n\nIn this example, we've defined a `Vector3Model` that roughly mirrors Unity's `Vector3`. It has three float properties representing the three components of the vector. We've also utilized what we learned in the previous sections. It has a private constructor to force consumers to always construct it with a `Vector3` argument. We've also marked its properties as private as we don't want consumers directly interacting with them. We want users to always call `ToVector3` to obtain the Unity type. And for our `Powerup` model, we're doing exactly that in the publicly exposed `Position` property. Note that similarly to our `Person` example, we're making sure to raise a notification for `Position` whenever `_Position` changes.\n\nAnd similarly to the exaple in the previous section, this approach makes querying via LINQ impossible and we have to fall back to the string query syntax if we want to find all powerups in a particular area:\n\n```csharp\nIQueryable<Powerup> PowerupsAroundLocation(Vector3 location, float radius)\n{\n    // Note that this query returns a cube around the location, not a sphere.\n    var powerups = realm.All<Powerup>().Filter(\n        \"Position.x > $0 AND Position.x < $1 AND Position.y > $2 AND Position.y < $3 AND Position.z > $4 AND Position.z < $5\",\n        location.x - radius, location.x + radius,\n        location.y - radius, location.y + radius,\n        location.z - radius, location.z  + radius);\n}\n```\n\n## Conclusion\n\nThe list of techniques above is by no means meant to be exhaustive. Neither is it meant to imply that this is the only, or even \"the right,\" way to use Realm. For most apps, simple POCOs with a list of properties is perfectly sufficient. But if you need to add extra validations or persist complex data types that you're using a lot, but Realm doesn't support natively, we hope that these examples will give you ideas for how to do that. And if you do come up with an ingenious way to use Realm, we definitely want to hear about it. Who knows? Perhaps we can feature it in our \"Advanced^2 Data Modeling\" article!","description":"Learn how to structure your Realm models to add validation, protect certain properties, and even persist complex objects coming from third-party packages.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt1929eac8bb9f9073/644c463847e9ccb5b3cd0921/realm-logo.jpeg?branch=prod","description":null}}]},"slug":"/advanced-modeling-realm-dotnet","title":"*Advanced Data Modeling with Realm .NET","original_publish_date":"2022-02-01T16:00:00.211Z","strapi_updated_at":"2022-10-19T12:01:06.508Z","expiry_date":"2022-11-05T13:21:05.390Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*CSharp","calculated_slug":"/languages/csharp"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:11.557Z","publish_details":{"time":"2023-04-28T22:52:58.610Z"}}},{"calculated_slug":"/products/atlas/cloudflare-worker-rest-api","content":"## Introduction\n\n[Cloudflare Workers](https://workers.cloudflare.com/) provides a serverless execution environment that allows you to create entirely new applications or augment existing ones without configuring or maintaining infrastructure.\n\n[MongoDB Atlas](https://www.mongodb.com/cloud/atlas) allows you to create, manage, and monitor MongoDB clusters in the cloud provider of your choice (AWS, GCP, or Azure) while the [Web SDK](https://www.mongodb.com/docs/realm/web/) can provide a layer of authentication and define access rules to the collections.\n\nIn this blog post, we will combine all these technologies together and create a REST API with a Cloudflare worker using a MongoDB Atlas cluster to store the data.\n\n## TL;DR!\n\nThe worker is in this [GitHub repository](https://github.com/mongodb-developer/cloudflare-worker-rest-api-atlas). The [README](https://github.com/mongodb-developer/cloudflare-worker-rest-api-atlas/blob/main/README.md) will get you up and running in no time, if you know what you are doing. Otherwise, I suggest you follow this step-by-step blog post. ;-)\n\n```shell\n$ git clone git@github.com:mongodb-developer/cloudflare-worker-rest-api-atlas.git\n```\n\n## Prerequisistes\n\n- NO credit card! You can run this entire tutorial for free!\n- [Git](https://git-scm.com/) and [cURL](https://en.wikipedia.org/wiki/CURL).\n- [MongoDB Atlas account](https://www.mongodb.com/cloud/atlas/).\n- [MongoDB Atlas Cluster (a free M0 cluster is fine)](https://docs.atlas.mongodb.com/tutorial/deploy-free-tier-cluster/).\n- [Cloudflare](https://www.cloudflare.com/) account (free plan is fine) with a `*.workers.dev` subdomain for the workers. Follow [steps 1 to 3 from this documentation](https://developers.cloudflare.com/workers/get-started/guide) to get everything you need.\n\nWe will create the Atlas App Services application (formerly known as a MongoDB Realm application) together in the next section. This will provide you the AppID and API key that we need.\n\nTo deploy our Cloudflare worker, we will need:\n- The [application ID](https://www.mongodb.com/docs/atlas/app-services/manage-apps/create/create-with-ui/) (top left corner in your app—see next section).\n- The Cloudflare account login/password.\n- The Cloudflare account ID (in Workers tab > Overview).\n\nTo test (or interact with) the REST API, we need:\n- The authentication API key (more about that below, but it's in Authentication tab > API Keys).\n- The Cloudflare `*.workers.dev` subdomain (in Workers tab > Overview).\n\nIt was created during this step of your set-up:\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/create_subdomain_445263d59f.png)\n\n## Create and Configure the Atlas Application\n\nTo begin with, head to your MongoDB Atlas main page where you can see your cluster and access the 'App Services' tab at the top.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/App_Services_UI_7c1682e8a4.png)\n\n[Create an empty application](https://www.mongodb.com/docs/atlas/app-services/manage-apps/create/create-with-ui/) (no template) as close as possible to your MongoDB Atlas cluster to avoid latency between your cluster and app. My app is \"local\" in Ireland (eu-west-1) in my case.\n\nNow that our app is created, we need to set up two things: authentication via API keys and collection rules. Before that, note that you can retrieve your app ID in the top left corner of your new application.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Cloudflare_App_ID_4bbe1c06b2.png)\n\n### Authentication Via API Keys\n\nHead to Authentication > API Keys.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Cloudflare_Auth_4c4dbb77dd.png)\n\nActivate the provider and save the draft.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Enable_Auth_40a8469d77.png)\n\nWe need to create an API key, but we can only do so if the provider is already deployed. Click on review and deploy.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Review_Draft_00f943f822.png)\n\nNow you can create an API key and **save it somewhere**! It will only be displayed **once**. If you lose it, discard this one and create a new one.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/API_Key_4005d64dc9.png)\n\nWe only have a single user in our application as we only created a single API key. Note that this tutorial would work with any other authentication method if you update the authentication code accordingly in the worker.\n\n### Collection Rules\n\nBy default, your application cannot access any collection from your MongoDB Atlas cluster. To define how users can interact with the data, you must [define roles and permissions](https://docs.mongodb.com/realm/mongodb/define-roles-and-permissions/).\n\nIn our case, we want to create a basic REST API where each user can read and write their own data in a single collection `todos` in the `cloudflare` database.\n\nHead to the Rules tab and let's create this new `cloudflare.todos` collection.\n\nFirst, click \"create a collection\".\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Create_Collection_b35881aff7.png)\n\nNext, name your database  `cloudflare` and collection  `todos`. Click create!\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Name_Collection_9435b5eb39.png)\n\nEach document in this collection will belong to a unique user defined by the `owner` field. This field will contain the user ID that you can see in the `App Users` tab.\n\nTo limit users to only reading and writing their own data, click on your new `todos` collection in the Rules UI. Start with the read and write all default role.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Correct_Rule_b5633a3c11.png)\n\nNow, edit the default role's name to owner and add `{\"owner\": \"user.id\"}` in the \"Apply When\" section.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Right_Rule2_7dcbd16a9d.png)\n\nYou can now click one more time on `Review Draft and Deploy`. Our application is now ready to use.\n\n## Set Up and Deploy the Cloudflare Worker\n\nThe Cloudflare worker is available in [GitHub repository](https://github.com/mongodb-developer/cloudflare-worker-rest-api-realm-atlas). Let's clone the repository.\n\n```shell\n$ git clone git@github.com:mongodb-developer/cloudflare-worker-rest-api-atlas.git\n$ cd cloudflare-worker-rest-api-realm-atlas\n$ npm install\n```\n\nNow that we have the worker template, we just need to change the configuration to deploy it on your Cloudflare account.\n\nEdit the file `wrangler.toml`:\n- Replace `CLOUDFLARE_ACCOUNT_ID` with your real Cloudflare account ID.\n- Replace `MONGODB_REALM_APPID` with your real MongoDB Atlas App Services app ID.\n\nYou can now deploy your worker to your Cloudflare account using [Wrangler](https://developers.cloudflare.com/workers/cli-wrangler/install-update):\n\n```shell\n$ npm i @cloudflare/wrangler -g\n$ wrangler login\n$ wrangler publish\n```\n\nHead to your Cloudflare account. You should now see your new worker in the Workers tab > Overview.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/cloudflare_worker_deployed_e2f606f4c5_37e33747b4.png)\n\n## Check Out the REST API Code\n\nBefore we test the API, please take a moment to read the [code of the REST API](https://github.com/mongodb-developer/cloudflare-worker-rest-api-realm-atlas/blob/main/src/index.ts) we just deployed, which is in the `src/index.ts` file:\n\n```typescript\nimport * as Realm from 'realm-web';\nimport * as utils from './utils';\n\n// The Worker's environment bindings. See `wrangler.toml` file.\ninterface Bindings {\n    // MongoDB Atlas App Services Application ID\n    REALM_APPID: string;\n}\n\n// Define type alias; available via `realm-web`\ntype Document = globalThis.Realm.Services.MongoDB.Document;\n\n// Declare the interface for a \"todos\" document\ninterface Todo extends Document {\n    owner: string;\n    done: boolean;\n    todo: string;\n}\n\nlet App: Realm.App;\nconst ObjectId = Realm.BSON.ObjectID;\n\n// Define the Worker logic\nconst worker: ExportedHandler<Bindings> = {\n    async fetch(req, env) {\n        const url = new URL(req.url);\n        App = App || new Realm.App(env.REALM_APPID);\n\n        const method = req.method;\n        const path = url.pathname.replace(/[/]$/, '');\n        const todoID = url.searchParams.get('id') || '';\n\n        if (path !== '/api/todos') {\n            return utils.toError(`Unknown \"${path}\" URL; try \"/api/todos\" instead.`, 404);\n        }\n\n        const token = req.headers.get('authorization');\n        if (!token) return utils.toError('Missing \"authorization\" header; try to add the header \"authorization: REALM_API_KEY\".', 401);\n\n        try {\n            const credentials = Realm.Credentials.apiKey(token);\n            // Attempt to authenticate\n            var user = await App.logIn(credentials);\n            var client = user.mongoClient('mongodb-atlas');\n        } catch (err) {\n            return utils.toError('Error with authentication.', 500);\n        }\n\n        // Grab a reference to the \"cloudflare.todos\" collection\n        const collection = client.db('cloudflare').collection<Todo>('todos');\n\n        try {\n            if (method === 'GET') {\n                if (todoID) {\n                    // GET /api/todos?id=XXX\n                    return utils.reply(\n                        await collection.findOne({\n                            _id: new ObjectId(todoID)\n                        })\n                    );\n                }\n\n                // GET /api/todos\n                return utils.reply(\n                    await collection.find()\n                );\n            }\n\n            // POST /api/todos\n            if (method === 'POST') {\n                const {todo} = await req.json();\n                return utils.reply(\n                    await collection.insertOne({\n                        owner: user.id,\n                        done: false,\n                        todo: todo,\n                    })\n                );\n            }\n\n            // PATCH /api/todos?id=XXX&done=true\n            if (method === 'PATCH') {\n                return utils.reply(\n                    await collection.updateOne({\n                        _id: new ObjectId(todoID)\n                    }, {\n                        $set: {\n                            done: url.searchParams.get('done') === 'true'\n                        }\n                    })\n                );\n            }\n\n            // DELETE /api/todos?id=XXX\n            if (method === 'DELETE') {\n                return utils.reply(\n                    await collection.deleteOne({\n                        _id: new ObjectId(todoID)\n                    })\n                );\n            }\n\n            // unknown method\n            return utils.toError('Method not allowed.', 405);\n        } catch (err) {\n            const msg = (err as Error).message || 'Error with query.';\n            return utils.toError(msg, 500);\n        }\n    }\n}\n\n// Export for discoverability\nexport default worker;\n```\n\n## Test the REST API\n\nNow that you are a bit more familiar with this REST API, let's test it!\n\nNote that we decided to pass the values as parameters and the authorization API key as a header like this:\n\n```\nauthorization: API_KEY_GOES_HERE\n```\n\nYou can use [Postman](https://www.postman.com/) or anything you want to test your REST API, but to make it easy, I made some bash script in the `api_tests` folder.\n\nIn order to make them work, we need to edit the file `api_tests/variables.sh` and provide them with:\n\n- The Cloudflare worker URL: Replace `YOUR_SUBDOMAIN`, so the final worker URL matches yours.\n- The MongoDB Atlas App Service API key: Replace `YOUR_REALM_AUTH_API_KEY` with your auth API key.\n\nFinally, we can execute all the scripts like this, for example:\n\n```shell\n$ cd api_tests\n\n$ ./post.sh \"Write a good README.md for Github\"\n{\n  \"insertedId\": \"618615d879c8ad6d1129977d\"\n}\n\n$ ./post.sh \"Commit and push\"\n{\n  \"insertedId\": \"618615e479c8ad6d11299e12\"\n}\n\n$ ./findAll.sh \n[\n  {\n    \"_id\": \"618615d879c8ad6d1129977d\",\n    \"owner\": \"6186154c79c8ad6d11294f60\",\n    \"done\": false,\n    \"todo\": \"Write a good README.md for Github\"\n  },\n  {\n    \"_id\": \"618615e479c8ad6d11299e12\",\n    \"owner\": \"6186154c79c8ad6d11294f60\",\n    \"done\": false,\n    \"todo\": \"Commit and push\"\n  }\n]\n\n$ ./findOne.sh 618615d879c8ad6d1129977d\n{\n  \"_id\": \"618615d879c8ad6d1129977d\",\n  \"owner\": \"6186154c79c8ad6d11294f60\",\n  \"done\": false,\n  \"todo\": \"Write a good README.md for Github\"\n}\n\n$ ./patch.sh 618615d879c8ad6d1129977d true\n{\n  \"matchedCount\": 1,\n  \"modifiedCount\": 1\n}\n\n$ ./findAll.sh \n[\n  {\n    \"_id\": \"618615d879c8ad6d1129977d\",\n    \"owner\": \"6186154c79c8ad6d11294f60\",\n    \"done\": true,\n    \"todo\": \"Write a good README.md for Github\"\n  },\n  {\n    \"_id\": \"618615e479c8ad6d11299e12\",\n    \"owner\": \"6186154c79c8ad6d11294f60\",\n    \"done\": false,\n    \"todo\": \"Commit and push\"\n  }\n]\n\n$ ./deleteOne.sh 618615d879c8ad6d1129977d\n{\n  \"deletedCount\": 1\n}\n\n$ ./findAll.sh \n[\n  {\n    \"_id\": \"618615e479c8ad6d11299e12\",\n    \"owner\": \"6186154c79c8ad6d11294f60\",\n    \"done\": false,\n    \"todo\": \"Commit and push\"\n  }\n]\n```\n\nAs you can see, the REST API works like a charm!\n\n## Wrap Up\n\nCloudflare offers a Workers [KV](https://developers.cloudflare.com/workers/runtime-apis/kv) product that _can_ make for a quick combination with Workers, but it's still a simple key-value datastore and most applications will outgrow it. By contrast, MongoDB is a powerful, full-featured database that unlocks the ability to store, query, and index your data without compromising the security or scalability of your application.\n\nAs demonstrated in this blog post, it is possible to take full advantage of both technologies. As a result, we built a powerful and secure serverless REST API that will scale very well.\n\n> Another option for connecting to Cloudflare is the [MongoDB Atlas Data API](https://www.mongodb.com/docs/atlas/api/data-api/). The Atlas Data API provides a lightweight way to connect to MongoDB Atlas that can be thought of as similar to a REST API. To learn more, view [this tutorial](https://www.mongodb.com/developer/products/atlas/atlas-data-api-introduction/) from my fellow developer advocate Mark Smith! \n\nIf you have questions, please head to our [developer community website](https://www.mongodb.com/community/forums/) where the MongoDB engineers and the MongoDB community will help you build your next big idea with MongoDB. If your question is related to Cloudflare, I encourage you to join their [active Discord community](https://workers.community).\n","description":"Learn how to create a serverless REST API using Cloudflare workers and MongoDB Atlas.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf62a6641d599bfa6/644c46e53c3aa664cc9de87a/cloudflare-logo-black-sth.png?branch=prod","description":null}}]},"slug":"/cloudflare-worker-rest-api","title":"*Create a REST API with Cloudflare Workers and MongoDB Atlas","original_publish_date":"2021-11-15T14:23:49.255Z","strapi_updated_at":"2023-01-26T16:56:03.124Z","expiry_date":"2022-11-06T06:48:51.654Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*TypeScript","calculated_slug":"/languages/typescript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*Serverless","calculated_slug":"/technologies/serverless"}},{"node":{"title":"*Cloudflare","calculated_slug":"/technologies/cloudflare"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf62a6641d599bfa6/644c46e53c3aa664cc9de87a/cloudflare-logo-black-sth.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:11.154Z","publish_details":{"time":"2023-04-28T22:52:58.641Z"}}},{"calculated_slug":"/products/atlas/atlas-data-api-introduction","content":"# Introduction to the MongoDB Atlas Data API\nThere are a lot of options for connecting to [MongoDB Atlas](https://www.mongodb.com/atlas/database) as an application developer. One of the newest options is the [MongoDB Atlas Data API](https://www.mongodb.com/docs/atlas/api/data-api/). The Atlas Data API provides a lightweight way to connect to MongoDB Atlas that can be thought of as similar to a REST API. This tutorial will show you how to enable the Data API and perform basic CRUD operations using curl. It’s the first in a series showing different uses for the Data API and how you can use it to build data-centric applications and services faster.\n\n[Access the full API reference.](https://www.mongodb.com/docs/atlas/api/data-api/)\n\nThis post assumes you already have an Atlas cluster. You can either use an existing one or you can sign up for a cloud account and create your first database cluster by [following the instructions](https://www.mongodb.com/developer/quickstart/free-atlas-cluster/).\n\n## Enabling the Atlas Data API\n\nEnabling the Data API is very easy once you have a cluster in Atlas.\n\nFirst, Click \"Data API\" in the bar on the left of your Atlas deployment.\n\n![A screenshot, showing the Data API navigation item on the left of the screen](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_data_api_nav_2f01841b63.png)\n\nThen select which data source or sources you want the Data API to have access to. For this example, I am selecting just the default Cluster0.\n\n![A screenshot, showing a drop-down list with all my clusters. A single cluster is selected, called \"Cluster0\"](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_enable_api_f7b08d9e9c.png)\n\nThen, select the large \"Enable the Data API\" button.\n\n![A big green button, reading \"Enable the Data API\"](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_big_enable_button_cae89356fe.png)\n\nYou will then have a screen confirming what clusters you have enabled for the Data API.\n\n![A screenshot showing a screen that lists all clusters where the Data API has been enabled.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_clusters_28881c27b0.png)\n\nIn the \"Data API Access\" column, select \"Read and Write\" for now, and then click on the button at the top left that says \"Create API Key.\" Choose a name. It's not important what name you choose, as long as it's useful to you.\n\n![A screenshot, demonstrating where the API key can be found.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_create_api_key_6b3e0ac85f.png)\n\nFinally, click \"Generate API Key\" and take a note of the key displayed in a secure place as you will not be able to see it again in Atlas. You can click the \"Copy\" button to copy it to your clipboard. I pasted mine into a .envrc file in my project.\n\nIf you want to test out a simple command, you can select one of your database collections in the dropdowns and copy-paste some code into your terminal to see some results. While writing this post, I did it just to check that I got some results back. When you're done, click \"Close\" to go back to the Data API screen. If you need to manage the keys you've created, you can click the \"API Keys\" tab on this screen.\n\n![A screenshot highlighting the \"API Keys\" tab](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_api_keys_tab_a3d48b472b.png)\n\nYou are now ready to call the Data API!\n\n## Be careful with your API key!\n\nThe API key you've just created should never be shared with anyone, or sent to the browser. Anyone who gets hold of the key can use it to make changes to the data in your database! In fact, the Data API blocks browser access, because there's currently no secure way to make Data API requests securely without sharing an API key.\n\n## Calling the Data API\nAll the Data API endpoints use HTTPS POST. Though it might seem logical to use GET when reading data, GET requests are intended to be cached and many platforms will do so automatically. To ensure you never have stale query results, all of the API endpoints use POST. Time to get started!\n\n### Adding data to Atlas\n\nTo add documents to MongoDB, you will use the InsertOne or InsertMany action endpoints.\n\n### InsertOne\n\nWhen you insert a document with the API, you must provide the \"dataSource\" (which is your cluster name), \"database,\" \"collection,\" and \"document\" as part of a JSON payload document.\nFor authentication, you will need to pass the API key as a header. The API always uses HTTPS, so this is safe and secure from network snooping.\n\nTo call with curl, use the following command:\n\n```shell\ncurl --location --request POST 'https://data.mongodb-api.com/app/data-YOUR_ID/endpoint/data/v1/action/insertOne' \\\n  --header 'Content-Type: application/json' \\\n  --header 'Access-Control-Request-Headers: *' \\\n  --header \"api-key: YOUR_API_KEY\" \\\n  --data-raw '{\n      \"dataSource\":\"Cluster0\",\n      \"database\":\"household\",\n      \"collection\":\"pets\",\n      \"document\" : {  \"name\": \"Harvest\",\n                      \"breed\": \"Labrador\",\n                      \"age\": 5 }\n  }'\n```\n\nFor example, my call looks like this:\n\n```shell\ncurl --location --request POST 'https://data.mongodb-api.com/app/data-abcde/endpoint/data/v1/action/insertOne' \\\n  --header 'Content-Type: application/json' \\\n  --header 'Access-Control-Request-Headers: *' \\\n  --header \"api-key: abcdMgLSoqpQdCfLO3QAiif61iI0v6JrvOYIBHeIBWS1zccqKLuDzyAAg\" \\\n  --data-raw '{\n      \"dataSource\":\"Cluster0\",\n      \"database\":\"household\",\n      \"collection\":\"pets\",\n      \"document\" : {  \"name\": \"Harvest\",\n                      \"breed\": \"Labrador\",\n                      \"age\": 5 }\n  }'\n```\n\nNote that the URL I'm using is my Data API URL endpoint, with `/action/insertOne` appended. When I ran this command with my values for `YOUR_ID` and `YOUR_API_KEY`, curl printed the following:\n\n```json\n{\"insertedId\":\"62c6da4f0836cbd6ebf68589\"}\n```\n\nThis means you've added a new document to a collection called “pets” in a database called “household.” Due to MongoDB’s flexible dynamic model, neither the database nor collection needed to be defined in advance.\n\nThis API call returned a JSON document with the _id of the new document. As I didn't explicitly supply any value for _id ( the primary key in MongoDB), one was created for me and it was of type ObjectId. The API returns standard JSON by default, so this is displayed as a string. \n\n### FindOne\n\nTo look up the document I just added by _id, I'll need to provide the _id that was just printed by curl. In the document that was printed, the value looks like a string, but it isn't. It's an ObjectId, which is the type of value that's created by MongoDB when no value is provided for the _id.\n\nWhen querying for the ObjectId value, you need to wrap this string as an [EJSON ObjectId](https://www.mongodb.com/docs/manual/reference/mongodb-extended-json/#mongodb-bsontype-ObjectId) type, like this: `{ \"$oid\" : <String Value> }`. If you don't provide this wrapper, MongoDB will mistakenly believe you are looking for a string value, not the ObjectId that's actually there.\n\nThe findOne query looks much like the insertOne query, except that the action name in the URL is now findOne, and this call takes a \"filter\" field instead of a \"document\" field.\n\n```shell\ncurl --location --request POST 'https://data.mongodb-api.com/app/data-abcde/endpoint/data/v1/action/findOne' \\\n  --header 'Content-Type: application/json' \\\n  --header 'Access-Control-Request-Headers: *' \\\n  --header \"api-key: abcdMgLSoqpQdCfLO3QAiif61iI0v6JrvOYIBHeIBWS1zccqKLuDzyAAg\" \\\n  --data-raw '{\n      \"dataSource\":\"Cluster0\",\n      \"database\":\"household\",\n      \"collection\":\"pets\",\n      \"filter\" : {  \"_id\": { \"$oid\": \"62c6da4f0836cbd6ebf68589\" } }\n  }'\n```\n\nThis printed out the following JSON for me:\n\n```json\n{\"document\":{\n        \"_id\":\"62c6da4f0836cbd6ebf68589\",\n        \"name\":\"Harvest\",\n        \"breed\":\"Labrador\",\n        \"age\":5}}\n```\n\n### Getting Extended JSON from the API\nNote that in the output above, the _id is again being converted to \"plain\" JSON, and so the \"_id\" value is being converted to a string. Sometimes, it's useful to keep the type information, so you can specify that you would like Extended JSON (EJSON) output, for any Data API call, by supplying an \"Accept\" header, with the value of \"application/ejson\":\n\n```shell\ncurl --location --request POST 'https://data.mongodb-api.com/app/data-abcde/endpoint/data/v1/action/findOne' \\\n  --header 'Content-Type: application/json' \\\n  --header 'Access-Control-Request-Headers: *' \\\n --header 'Accept: application/ejson' \\\n  --header \"api-key: abcdMgLSoqpQdCfLO3QAiif61iI0v6JrvOYIBHeIBWS1zccqKLuDzyAAg\" \\\n  --data-raw '{\n      \"dataSource\":\"Cluster0\",\n      \"database\":\"household\",\n      \"collection\":\"pets\",\n      \"filter\" : {  \"_id\": { \"$oid\": \"62c6da4f0836cbd6ebf68589\" } }\n  }'\n```\n\nWhen I ran this, the \"_id\" value was provided with the \"$oid\" wrapper, to declare that it's an ObjectId value:\n\n```json\n{\"document\":{\n        \"_id\":{\"$oid\":\"62c6da4f0836cbd6ebf68589\"},\n        \"name\":\"Harvest\",\n        \"breed\":\"Labrador\",\n        \"age\":{\"$numberInt\":\"5\"}}}\n```\n\n### InsertMany\nIf you're inserting several documents into a collection, it’s much more efficient to make a single HTTPS call with the insertMany action. This endpoint works in a very similar way to the insertOne action, but it takes a \"documents\" field instead of a single \"document\" field, containing an array of documents:\n\n```shell\ncurl --location --request POST 'https://data.mongodb-api.com/app/data-abcde/endpoint/data/v1/action/insertMany' \\\n  --header 'Content-Type: application/json' \\\n  --header 'Access-Control-Request-Headers: *' \\\n  --header \"api-key: abcdMgLSoqpQdCfLO3QAiif61iI0v6JrvOYIBHeIBWS1zccqKLuDzyAAg\" \\\n  --data-raw '{\n      \"dataSource\":\"Cluster0\",\n      \"database\":\"household\",\n      \"collection\":\"pets\",\n      \"documents\" : [{\n            \"name\": \"Brea\",\n            \"breed\": \"Labrador\",\n            \"age\": 9,\n            \"colour\": \"black\"\n        },\n        {\n            \"name\": \"Bramble\",\n            \"breed\": \"Labrador\",\n            \"age\": 1,\n            \"colour\": \"black\"\n        }]\n  }'\n```\n\nWhen I ran this, the output looked like this:\n\n```json\n{\"insertedIds\":[\"62c6e8a15a3411a70813c21e\",\"62c6e8a15a3411a70813c21f\"]}\n```\n\nThis endpoint returns JSON with an array of the values for _id for the documents that were added.\n\n### Querying data\nQuerying for more than one document is done with the find endpoint, which returns an array of results. The following query looks up all the labradors that are two years or older, sorted by age:\n\n```shell\ncurl --location --request POST 'https://data.mongodb-api.com/app/data-abcde/endpoint/data/v1/action/find' \\\n  --header 'Content-Type: application/json' \\\n  --header 'Access-Control-Request-Headers: *' \\\n  --header \"api-key: abcdMgLSoqpQdCfLO3QAiif61iI0v6JrvOYIBHeIBWS1zccqKLuDzyAAg\" \\\n  --data-raw '{\n      \"dataSource\":\"Cluster0\",\n      \"database\":\"household\",\n      \"collection\":\"pets\",\n      \"filter\": { \"breed\": \"Labrador\",\n                  \"age\": { \"$gt\" : 2} },\n      \"sort\": { \"age\": 1 } }'\n```\n\nWhen I ran this, I received documents for the two oldest dogs, Harvest and Brea:\n\n```json\n{\"documents\":[\n        {\"_id\":\"62c6da4f0836cbd6ebf68589\",\"name\":\"Harvest\",\"breed\":\"Labrador\",\"age\":5},\n        {\"_id\":\"62c6e8a15a3411a70813c21e\",\"name\":\"Brea\",\"breed\":\"Labrador\",\"age\":9,\"colour\":\"black\"}]}\n```\n\nThis object contains a field ”documents,” that is an array of everything that matched. If I wanted to fetch a subset of the results in pages, I could use the skip and limit parameter to set which result to start at and how many to return.\n\n```shell\ncurl --location --request POST https://data.mongodb-api.com/app/data-abcde/endpoint/data/v1/action/updateOne \\\n    --header 'Content-Type: application/json' \\\n    --header 'Access-Control-Request-Headers: *' \\\n    --header \"api-key: abcdMgLSoqpQdCfLO3QAiif61iI0v6JrvOYIBHeIBWS1zccqKLuDzyAAg\" \\\n    --data-raw '{\n        \"dataSource\": \"Cluster0\",\n        \"database\": \"household\", \n        \"collection\": \"pets\",\n        \"filter\" : { \"name\" : \"Harvest\"},\n        \"update\" : { \"$set\" : { \"colour\": \"yellow\" }}\n    }'\n```\n\nBecause this both matched one document and changed its content, my output looked like this:\n\n```json\n{\"matchedCount\":1,\"modifiedCount\":1}\n```\n\nI only wanted to update a single document (because I only expected to find one document for Harvest). To change all matching documents, I would call updateMany with the same parameters.\n\n### Run an aggregation pipeline to compute something\n\nYou can also run [aggregation pipelines](https://docs.mongodb.com/manual/core/aggregation-pipeline?_ga=2.192845546.1556629630.1656933725-1320244468.1594731877). As a simple example of how to call the aggregate endpoint, let's determine the count and average age for each color of labrador.\n\nAggregation pipelines are the more powerful part of the MongoDB Query API. As well as looking up documents, a pipeline allows you to calculate aggregate values across multiple documents. The following example extracts all labrador documents from the \"pets\" collection, groups them by their \"colour\" field, and then calculates the number of dogs ($sum of 1 for each dog document) and the average age of dog (using $avg) for each colour.\n\n```shell\ncurl --location --request POST https://data.mongodb-api.com/app/data-abcde/endpoint/data/v1/action/aggregate \\\n    --header 'Content-Type: application/json' \\\n    --header 'Access-Control-Request-Headers: *' \\\n    --header \"api-key: abcdMgLSoqpQdCfLO3QAiif61iI0v6JrvOYIBHeIBWS1zccqKLuDzyAAg\" \\\n    --data-raw '{\n        \"dataSource\": \"Cluster0\",\n        \"database\": \"household\", \n        \"collection\": \"pets\",\n        \"pipeline\" : [ { \"$match\": {\"breed\": \"Labrador\"}}, \n                       { \"$group\": { \"_id\" : \"$colour\",\n                          \"count\" : { \"$sum\" : 1},\n                          \"average_age\": {\"$avg\": \"$age\" }}}]}'\n    }'\n```\n\nWhen I ran the above query, the result looked like this:\n\n```json\n{\"documents\":[{\"_id\":\"yellow\",\"count\":1,\"average_age\":5},{\"_id\":\"black\",\"count\":2,\"average_age\":5}]}\n```\n\nIt's worth noting that there are [some limitations](https://www.mongodb.com/docs/atlas/app-services/mongodb/crud-and-aggregation-apis/#std-label-mongodb-crud-and-aggregation-apis-aggregation) when running aggregation pipelines through the Data API.\n\n## Advanced features\n\nWhen it comes to authentication and authorization, or just securing access to the Data API in general, you have a few options. These features use a neat feature of the Data API, which is that your Data API is a MongoDB Atlas Application Services app behind the scenes!\n\nYou can access the application by clicking on \"Advanced Settings\" on your Data API console page:\n\n![Advanced Settings can be found to the right of the URL Endpoint field](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_advanced_settings_125c0cf192.png)\n\nThe rest of this section will use the features of this Atlas Application Services app, rather than the high level Data API pages.\n\n### Restrict access by IP address\n\nRestricting access to your API endpoint from only the servers that should have access is a relatively straightforward but effective way of locking down your API. You can change the list of IP addresses by clicking on \"App Settings\" in the left-hand navigation bar, and then clicking on the \"IP Access List\" tab on the settings pane.\n\n![The IP Access List tab is found by clicking on the App Settings navigation button on the left of the screen.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_ip_access_85a592c039.png)\n\nBy default, all IP addresses are allowed to access your API endpoint (that's what 0.0.0.0 means). If you want to lock down access to your API, you should delete this entry and add entries for servers that should be able to access your data. There's a convenient button to add your current IP address for when you're writing code against your API endpoint.\n\n### Authentication using JWTs and JWK\n\nIn all the examples in this post, I've shown you how to use an API key to access your data. But by using the Atlas Application Services app, you can lock down access to your data using JSON Web Tokens (or JWTs) and email/password credentials. JWT has the benefit that you can use an external authentication service or identity providers, like Auth0 or Okta, to authenticate users of your application. The auth service can provide a JWT that your application can use to make authenticated queries using the Data API, and provides a JWK (JSON Web Keys) URL that can be used by the Data API to ensure any incoming requests have been authenticated by the authentication service.\n\nMy colleague Jesse (you may know him as [codeSTACKr](https://www.youtube.com/c/codeSTACKr)) has written a [great tutorial](https://github.com/mongodb-developer/social-app-demo/tree/7-lesson) for getting this up and running with the Data API and [Auth0](https://auth0.com/), and the same process applies for accepting JWTs with the Data API. By first clicking on \"Advanced Settings\" to access the configuration of the app that provides your Data API endpoints behind the scenes and going into “Authentication,” you can enable the provider with the appropriate signing key and algorithm.\n\nInstead of setting up a trigger to create a new user document when a new JWT is encountered, however, set \"Create User Upon Authentication\" in the User Settings panel on the Data API configuration to \"on.\" \n\n![Click on the toggle button under \"Create User Upon Authentication\"](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_create_user_on_auth_33d18d7e39.png)\n\n### Giving role-based access to the Data API\n\nFor each cluster, you can set high-level access permissions like Read-Only Access, Read & Write Access, or No Access. However, you can also take this one step further by setting custom role-based access-control with the App Service Rules. \n\n![Selecting \"Custom Access\" from the \"Data API Access\" drop-down menu](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_custom_access_ab92e11864.png)\n\nSelecting Custom Access will allow you to set up additional roles on who can access what data, either at the cluster, collection, document, or field level. \n\nFor example, you can restrict certain API key holders to only be able to insert documents but not delete them. These user.id fields are associated with each API key created:\n\n![A screenshot of the different API keys, showing that each one has a user id associated with it.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_key_list_93fecf77a2.png)\n\n![Select different roles when setting custom access](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_roles_fa77c0c0a5.png)\n\n### Add additional business logic with custom API endpoints\n\nThe Data API provides the basic CRUD and aggregation endpoints I've described above. For accessing and manipulating the data in your MongoDB database, because the Data API is provided by an Atlas App Services application, you get all the goodness that goes with that, including the ability to add more API endpoints yourself that can use all the power available to MongoDB Atlas Functions.\n\nFor example, I could write a serverless function that would look up a user's tweets using the Twitter API, combine those with a document looked up in MongoDB, and return the result:\n\n```javascript\nexports = function({ query, headers, body}, response) {\n  const collection = context.services.get(\"mongodb-atlas\").db(\"user_database\").collection(\"twitter_users\");\n\n  const username = query.user;\n\n  const userDoc = collection.findOne({ \"username\": username });\n\n  // This function is for illustration only!\n  const tweets = twitter_api.get_tweets(userDoc.twitter_id);\n\n  return {\n    user: userDoc,\n    tweets: tweets\n  }\n};\n```\n\nBy configuring this as an HTTPS endpoint, I can set things like the \n\n1. API route.\n2. HTTPS method.\n3. Custom authentication or authorization logic.\n\nIn this example, I’ve made this function available via a straightforward HTTPS GET request.\n\n![The \"Add Endpoint\" screen](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/dapi_add_endpoint_711fdc0583.png)\n\nIn this way, you can build an API to handle all of your application's data service requirements, all in one place. The endpoint above could be accessed with the following curl command:\n\n```shell\ncurl --location --request GET 'https://data.mongodb-api.com/app/data-abcde/endpoint/data/v1/action/aggregate?user=mongodb' \\\n    --header 'Content-Type: application/json' \\\n    --header 'Access-Control-Request-Headers: *' \\\n    --header \"api-key: abcdMgLSoqpQdCfLO3QAiif61iI0v6JrvOYIBHeIBWS1zccqKLuDzyAAg\"\n```\n\nAnd the results would look something like this:\n\n```json\n{\"user\": { \"username\": \"mongodb\", \"twitter_id\": \"MongoDB\" },\n \"tweets\": { \"count\": 10, \"tweet_data\": [...]}}\n```\n\n## Conclusion\nThe Data API is a powerful new MongoDB Atlas feature, giving you the ability to query your database from any environment that supports HTTPS. It also supports powerful social authentication possibilities using the standard JWT and JWK technologies. And finally, you can extend your API using all the features like Rules, Authentication, and HTTPS Endpoints.","description":"This article introduces the Atlas Data API and describes how to enable it and then call it from cURL.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt76de59e734bdf723/644c46e6c6aa71a5837e7d2b/Building_hero_purple.png?branch=prod","description":null}}]},"slug":"/atlas-data-api-introduction","title":"*An Introduction to the MongoDB Atlas Data API","original_publish_date":"2021-11-18T14:11:46.718Z","strapi_updated_at":"2022-09-15T17:33:35.015Z","expiry_date":"2022-11-15T11:28:51.986Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt24f712f91a651134/644c46e7bad93abf6e396f92/Branded_1920x1080_5.jpg?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:10.772Z","publish_details":{"time":"2023-04-28T22:52:58.672Z"}}},{"calculated_slug":"/products/atlas/building-service-based-atlas-management","content":"\n\n<div class=\"introduction\">\n\n## Developer Productivity\n\n[MongoDB Atlas](https://www.mongodb.com/cloud/atlas) is changing the database industry standards when it comes to database provisioning, maintenance, and scaling, as it just works. However, even superheroes like Atlas know that with Great Power Comes Great Responsibility.\n\nFor this reason, Atlas provides Enterprise-grade security features for your clusters and a set of [user management roles](https://docs.atlas.mongodb.com/reference/user-roles/) that can be assigned to log in users or [programmatic API keys](https://docs.atlas.mongodb.com/configure-api-access/).\n\nHowever, since the management roles were built for a wide use case of\nour customers there are some customers who need more fine-grained\npermissions for specific teams or user types. Although, at the moment\nthe management roles are predefined, with the help of a simple Realm\nservice and the programmatic API we can allow user access for very\nspecific management/provisioning features without exposing them to a\nwider sudo all ability.\n\nTo better understand this scenario I want to focus on the specific use\ncase of database user creation for the application teams. In this\nscenario perhaps each developer per team may need its own user and\nspecific database permissions. With the current Atlas user roles you\nwill need to grant the team a `Cluster Manager Role`, which allows them\nto change cluster properties as well as pause and resume a cluster. In\nsome cases this power is unnecessary for your users.\n\n> If you haven't yet set up your free cluster on [MongoDB Atlas](http://bit.ly/mongodbatlas), now is a great time to do so. You have all the instructions in this [blog post](/quickstart/free-atlas-cluster/).\n\n</div>\n\n<div class=\"content\">\n\n## Proposed Solution\n\nYour developers will submit their requests to a pre-built service which\nwill authenticate them and request an input for the user description.\nFurthermore, the service will validate the input and post it to the\nAtlas Admin API without exposing any additional information or API keys.\n\nThe user will receive a confirmation that the user was created and ready\nto use.\n\n## Work Flow\n\nTo make the service more accessible for users I am using a form-based\nservice called [Typeform](https://www.typeform.com/), you can choose many other available form builders (e.g [Google Forms](https://www.google.com/forms/about/)). This form will gather the information and password/secret for the service authentication from the user and pass it to the Realm webhook which will perform the action.\n\n![The user fills the form and security\ninformation](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/article/article-building-service-based-atlas-management/DrawingFlow.png \"The user fills the form and security information.\")  \n\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/article/article-building-service-based-atlas-management/FormUsers.png)\n\nThe input is an [Atlas Admin API user object](https://docs.atlas.mongodb.com/reference/api/database-users-create-a-user/#request) that we want to create, looking something like the following object:\n\n``` javascript\n{\n  \"databaseName\": <DB>,\n  \"password\": <PWD>,\n  \"roles\": [...],\n  \"username\":  <USERNAME>\n}\n```\n\nFor more information please refer to our Atlas Role Based Authentication\n[documentation](https://docs.atlas.mongodb.com/security-add-mongodb-users/).\n\n## Webhook Back End\n\nThis section will require you to use an existing Realm Application or\n[build a new one](https://docs.mongodb.com/realm/procedures/create-realm-app/).\n\nMongoDB Realm is a serverless platform and mobile database. In our case\nwe will use the following features:\n\n-   [Realm webhooks](https://docs.mongodb.com/realm/services/configure/service-webhooks/)\n-   [Realm context HTTP Module](https://docs.mongodb.com/realm/functions/context/index.html#context-http)\n-   [Realm Values/Secrets](https://docs.mongodb.com/realm/values-and-secrets/)\n\nYou will also need to [configure an Atlas Admin API key](https://docs.atlas.mongodb.com/configure-api-access/) for the relevant Project and obtain it's Project Id. This can be done from your Atlas project url (e.g., `https://cloud.mongodb.com/v2/<PROJECT_ID>#clusters`).\n\nThe main part of the Realm application is to hold the Atlas Admin API keys and information as [private secure secrets](https://docs.mongodb.com/realm/values-and-secrets/define-a-secret/).\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/article/article-building-service-based-atlas-management/Values-secrets.png)\n\nThis is the webhook configuration that will call our Realm Function each\ntime the form is sent:\n\n![The Webhook definition.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/article/article-building-service-based-atlas-management/ServiceWebbhok.png \"The Webhook should be defined as above.\")\n\nThe function below receives the request. Fetch the needed API\ninformation and sends the Atlas Admin API command. The result of which is\nreturned to the Form.\n\n``` javascript\n// This function is the webhook's request handler.\nexports = async function(payload, response) {\n     // Get payload\n     const body = JSON.parse(payload.body.text());\n\n     // Get secrets for the Atlas Admin API\n     const username = context.values.get(\"AtlasPublicKey\");\n     const password = context.values.get(\"AtlasPrivateKey\");\n     const projectID = context.values.get(\"AtlasGroupId\");\n\n     //Extract the Atlas user object description\n     const userObject = JSON.parse(body.form_response.answers[0].text);\n\n     // Database users post command\n     const postargs = {\n     scheme: 'https',\n     host: 'cloud.mongodb.com',\n     path: 'api/atlas/v1.0/groups/' + projectID + '/databaseUsers',\n     username: username,\n     password: password,\n     headers: {'Content-Type': ['application/json'], 'Accept': ['application/json']},\n     digestAuth:true,\n     body: JSON.stringify(userObject)};\n\n     var res = await context.http.post(postargs);\n     console.log(JSON.stringify(res));\n\n     // Check status of the user creation and report back to the user.\n     if (res.statusCode == 201)\n     {\n       response.setStatusCode(200)\n       response.setBody(`Successfully created ${userObject.username}.`);\n     } else {\n       // Respond with a malformed request error\n       response.setStatusCode(400)\n       response.setBody(`Could not create user ${userObject.username}.`);\n     }\n};\n```\n\nOnce the webhook is set and ready we can use it as a webhook url input\nin the Typeform configuration.\n\nThe Realm webhook url can now be placed in the Typform webhook section.\nNow the submitted data on the form will be forwarded via Webhook\nintegration to our webhook:\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/article/article-building-service-based-atlas-management/Webhook.png)\n\nTo strengthen the security around our Realm app we can strict the\nallowed domain for the webhook request origin. Go to Realm application\n\"Manage\" - \"Settings\" \\> \"Allowed Request Origins\":\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/article/article-building-service-based-atlas-management/AllowOrigins.png)\n\nWe can test the form now by providing an [Atlas Admin API user\nobject](https://docs.atlas.mongodb.com/reference/api/database-users-create-a-user/#request).\n\n![Submitting the object will result in a created user in the respectful\nAtlas\nProject](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/article/article-building-service-based-atlas-management/RunningExample.png \"Submitting the object will result in a created user in the respectful Atlas Project\")\n\nIf you go to the Atlas UI under the Database Access tab you will see the\ncreated user.\n\n![Atlas Database Access users\nUI](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/article/article-building-service-based-atlas-management/UserExample.png)\n\n</div>\n\n<div class=\"summary\">\n\n## Summary\n\nNow our developers will be able to create users quickly without being\nexposed to any unnecessary privileges or human errors.\n\nThe webhook code can be converted to a function that can be called from\nother webhooks or triggers allowing us to build sophisticated controlled\nand secure provisioning methods. For example, we can configure a\nscheduled trigger that pulls any newly created clusters and continuously\nprovision any new required users for our applications or edit any\nexisting users to add the needed new set of permissions.\n\n[MongoDB Atlas](https://www.mongodb.com/cloud/atlas) and [Realm](https://www.mongodb.com/realm) platforms can work in great synergy allowing us to bring our devops and development cycles to the\nnext level.\n\n</div>\n","description":"Learn how to build Service-Based Atlas Cluster Management webhooks/functionality with Atlas Admin API and MongoDB Realm.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt013eab56363fc340/644c46e8ec6405567eea3919/dev-tools.png?branch=prod","description":null}}]},"slug":"/building-service-based-atlas-management","title":"*Building Service-Based Atlas Cluster Management","original_publish_date":"2021-11-25T10:13:13.067Z","strapi_updated_at":"2022-09-23T00:30:52.707Z","expiry_date":"2022-11-18T18:26:19.135Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to build Service-Based Atlas Cluster Management webhooks/functionality with Atlas Admin API and MongoDB Realm.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltfeb6810359025655/644c46eab3d108a35f7e568d/og-mdb-developer.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@pavel.duchovny"},"system":{"updated_at":"2023-04-28T22:52:10.377Z","publish_details":{"time":"2023-04-28T22:52:58.700Z"}}},{"calculated_slug":"/languages/javascript/mongoose-versus-nodejs-driver","content":"In this article, we’ll explore the [Mongoose](https://mongoosejs.com/) library for MongoDB. Mongoose is a Object Data Modeling (ODM) library for MongoDB distributed as an npm package. We'll compare and contrast Mongoose to using the native [MongoDB Node.js driver](https://docs.mongodb.com/drivers/node/current/) together with [MongoDB Schema Validation](https://docs.mongodb.com/manual/core/schema-validation/).\n\nWe’ll see how the MongoDB Schema Validation helps us enforce a database schema, while still allowing for great flexibility when needed. Finally, we’ll see if the additional features that Mongoose provides are worth the overhead of introducing a third-party library into our applications.\n\n## What is Mongoose?\n\nMongoose is a Node.js-based Object Data Modeling (ODM) library for MongoDB. It is akin to an Object Relational Mapper (ORM) such as [SQLAlchemy](https://www.sqlalchemy.org/) for traditional SQL databases. The problem that Mongoose aims to solve is allowing developers to enforce a specific schema at the application layer. In addition to enforcing a schema, Mongoose also offers a variety of hooks, model validation, and other features aimed at making it easier to work with MongoDB.\n\n## What is MongoDB Schema Validation?\n\n[MongoDB Schema Validation](https://docs.mongodb.com/manual/core/schema-validation/) makes it possible to easily enforce a schema against your MongoDB database, while maintaining a high degree of flexibility, giving you the best of both worlds. In the past, the only way to enforce a schema against a MongoDB collection was to do it at the application level using an ODM like Mongoose, but that posed significant challenges for developers.\n\n## Getting Started\n\nIf you want to follow along with this tutorial and play around with schema validations but don't have a MongoDB instance set up, you can set up a [free MongoDB Atlas cluster here](https://mongodb.com/atlas).\n\n## Object Data Modeling in MongoDB\n\nA huge benefit of using a NoSQL database like MongoDB is that you are not constrained to a rigid data model. You can add or remove fields, nest data multiple layers deep, and have a truly flexible data model that meets your needs today and can adapt to your ever-changing needs tomorrow. But being too flexible can also be a challenge. If there is no consensus on what the data model should look like, and every document in a collection contains vastly different fields, you're going to have a bad time.\n\n### Mongoose Schema and Model\n\nOn one end of the spectrum, we have ODM's like Mongoose, which from the get-go force us into a semi-rigid schema. With Mongoose, you would define a `Schema` object in your application code that maps to a collection in your MongoDB database. The `Schema` object defines the structure of the documents in your collection. Then, you need to create a `Model` object out of the schema. The model is used to interact with the collection.\n\nFor example, let's say we're building a blog and want to represent a blog post. We would first define a schema and then create an accompanying Mongoose model:\n\n``` javascript\nconst blog = new Schema({\n   title: String,\n   slug: String,\n   published: Boolean,\n   content: String,\n   tags: [String],\n   comments: [{\n       user: String,\n       content: String,\n       votes: Number\n   }]\n});\n \nconst Blog = mongoose.model('Blog', blog);\n```\n\n### Executing Operations on MongoDB with Mongoose\n\nOnce we have a Mongoose model defined, we could run queries for fetching,updating, and deleting data against a MongoDB collection that alignswith the Mongoose model. With the above model, we could do things like:\n\n``` javascript\n// Create a new blog post\nconst article = new Blog({\n   title: 'Awesome Post!',\n   slug: 'awesome-post',\n   published: true,\n   content: 'This is the best post ever',\n   tags: ['featured', 'announcement'],\n});\n \n// Insert the article in our MongoDB database\narticle.save();\n \n// Find a single blog post\nBlog.findOne({}, (err, post) => {\n   console.log(post);\n});\n```\n\n### Mongoose vs MongoDB Node.js Driver: A Comparison\n\nThe benefit of using Mongoose is that we have a schema to work against in our application code and an explicit relationship between our MongoDB documents and the Mongoose models within our application. The downside is that we can only create blog posts and they have to follow the above defined schema. If we change our Mongoose schema, we are changing the relationship completely, and if you're going through rapid development, this can greatly slow you down.\n\nThe other downside is that this relationship between the schema and model only exists within the confines of our Node.js application. Our MongoDB database is not aware of the relationship, it just inserts or retrieves data it is asked for without any sort of validation. In the event that we used a different programming language to interact with our database, all the constraints and models we defined in Mongoose would be worthless.\n\nOn the other hand, if we decided to use just the [MongoDB Node.js](https://mongodb.github.io/node-mongodb-native/3.5/) driver, we could\nrun queries against any collection in our database, or create new ones on the fly. The MongoDB Node.js driver does not have concepts of object data modeling or mapping.\n\nWe simply write queries against the database and collection we wish to work with to accomplish the business goals. If we wanted to insert a new blog post in our collection, we could simply execute a command like so:\n\n``` javascript\ndb.collection('posts').insertOne({\n   title: 'Better Post!',\n   slug: 'a-better-post',\n   published: true,\n   author: 'Ado Kukic',\n   content: 'This is an even better post',\n   tags: ['featured'],\n});\n```\n\nThis `insertOne()` operation would run just fine using the Node.js Driver. If we tried to save this data using our Mongoose `Blog` model, it would fail, because we don't have an `author` property defined in our Blog Mongoose model.\n\nJust because the Node.js driver doesn't have the concept of a model, does not mean we couldn't create models to represent our MongoDB data at the application level. We could just as easily create a generic model or use a library such as [objectmodel](https://objectmodel.js.org/). We could create a `Blog` model like so:\n\n``` javascript\nfunction Blog(post) {\n   this.title = post.title;\n   this.slug = post.slug;\n   ...\n}\n```\n\nWe could then use this model in conjunction with our MongoDB Node.js driver, giving us both the flexibility of using the model, but not being constrained by it.\n\n``` javascript\ndb.collection('posts').findOne({}).then((err, post) => {\n   let article = new Blog(post);\n});\n```\n\nIn this scenario, our MongoDB database is still blissfully unaware of our Blog model at the application level, but our developers can work with it, add specific methods and helpers to the model, and would know that this model is only meant to be used within the confines of our Node.js application. Next, let's explore schema validation.\n\n## Adding Schema Validation\n\nWe can choose between two different ways of adding schema validation to our MongoDB collections. The first is to use application-level validators, which are defined in the Mongoose schemas. The second is to use MongoDB schema validation, which is defined in the MongoDB collection itself. The huge difference is that native MongoDB schema validation is applied at the database level. Let's see why that matters by exploring both methods.\n\n### Schema Validation with Mongoose\n\nWhen it comes to schema validation, Mongoose enforces it at the application layer as we've seen in the previous section. It does this in two ways.\n\nFirst, by defining our model, we are explicitly telling our Node.js application what fields and data types we'll allow to be inserted into a specific collection. For example, our Mongoose Blog schema defines a `title` property of type `String`. If we were to try and insert a blog post with a `title` property that was an array, it would fail. Anything outside of the defined fields, will also not be inserted in the database.\n\nSecond, we further validate that the data in the defined fields matches our defined set of criteria. For example, we can expand on our Blog model by adding specific validators such as requiring certain fields, ensuring a minimum or maximum length for a specific field, or coming up with our custom logic even. Let's see how this looks with Mongoose. In our code we would simply expand on the property and add our validators:\n\n``` javascript\nconst blog = new Schema({\n   title: {\n       type: String,\n       required: true,\n   },\n   slug: {\n       type: String,\n       required: true,\n   },\n   published: Boolean,\n   content: {\n       type: String,\n       required: true,\n       minlength: 250\n   },\n   ...\n});\n \nconst Blog = mongoose.model('Blog', blog);\n```\n\nMongoose takes care of model definition and schema validation in one fell swoop. The downside though is still the same. These rules only apply at the application layer and MongoDB itself is none the wiser.\n\nThe MongoDB Node.js driver itself does not have mechanisms for inserting or managing validations, and it shouldn't. We can define schema validation rules for our MongoDB database using the [MongoDB Shell](https://docs.mongodb.com/mongodb-shell/#mongodb-binary-bin.mongosh) or [Compass](https://www.mongodb.com/products/compass).\n\nWe can create a schema validation when creating our collection or after the fact on an existing collection. Since we've been working with this blog idea as our example, we'll add our schema validations to it. I will use Compass and [MongoDB Atlas](https://mongodb.com/atlas). For a great resource on how to programmatically add schema validations, check out this [series](https://www.mongodb.com/blog/post/json-schema-validation--locking-down-your-model-the-smart-way).\n\n> If you want to follow along with this tutorial and play around with\n> schema validations but don't have a MongoDB instance set up, you can\n> set up a [free MongoDB Atlas cluster here](https://mongodb.com/atlas).\n\nCreate a collection called `posts` and let's insert our two documents that we've been working with. The documents are:\n\n``` javascript\n[{\"title\":\"Better Post!\",\"slug\":\"a-better-post\",\"published\":true,\"author\":\"Ado Kukic\",\"content\":\"This is an even better post\",\"tags\":[\"featured\"]}, {\"_id\":{\"$oid\":\"5e714da7f3a665d9804e6506\"},\"title\":\"Awesome Post\",\"slug\":\"awesome-post\",\"published\":true,\"content\":\"This is an awesome post\",\"tags\":[\"featured\",\"announcement\"]}]\n```\n\nNow, within the Compass UI, I will navigate to the **Validation** tab. As expected, there are currently no validation rules in place, meaning our database will accept any document as long as it is valid BSON. Hit the **Add a Rule** button and you'll see a user interface for creating your own validation rules.\n\n![Valid Document Schema](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/schema-validation/blog-valid-schema.png \"Valid Document Schema\")\n\nBy default, there are no rules, so any document will be marked as passing. Let's add a rule to require the `author` property. It will look like this:\n\n``` javascript\n{\n  $jsonSchema: {\n    bsonType: \"object\",\n    required: [ \"author\" ]\n  }\n}\n```\n\nNow we'll see that our initial post, that does not have an `author` field has failed validation, while the post that does have the `author` field is good to go.\n\n![Invalid Document Schema](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/schema-validation/blog-invalid-schema.png \"Invalid Document Schema\")\n\nWe can go further and add validations to individual fields as well. Say for SEO purposes we wanted all the titles of the blog posts to be a minimum of 20 characters and have a maximum length of 80 characters. We can represent that like this:\n\n``` javascript\n{\n  $jsonSchema: {\n    bsonType: \"object\",\n    required: [ \"tags\" ],\n    properties: {\n      title: {\n        type: \"string\",\n        minLength: 20,\n        maxLength: 80\n      }\n    }\n  }\n}\n```\n\nNow if we try to insert a document into our `posts` collection either via the Node.js Driver or via Compass, we will get an error.\n\n![Validation Error](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/schema-validation/blog-validation-error.png)\n\nThere are many more rules and validations you can add. Check out the full list [here](https://docs.mongodb.com/manual/reference/operator/query/jsonSchema/#json-schema). For a more advanced guided approach, check out the articles on [schema](https://www.mongodb.com/blog/post/json-schema-validation--checking-your-arrays) validation with arrays and [dependencies](https://www.mongodb.com/blog/post/json-schema-validation--dependencies-you-can-depend-on).\n\n### Expanding on Schema Validation\n\nWith Mongoose, our data model and schema are the basis for our interactions with MongoDB. MongoDB itself is not aware of any of these constraints, Mongoose takes the role of judge, jury, and executioner on what queries can be executed and what happens with them.\n\nBut with MongoDB native schema validation, we have additional flexibility. When we implement a schema, validation on existing documents does not happen automatically. Validation is only done on updates and inserts. If we wanted to leave existing documents alone though, we could change the `validationLevel` to only validate new documents inserted in the database.\n\nAdditionally, with schema validations done at the MongoDB database level, we can choose to still insert documents that fail validation. The `validationAction` option allows us to determine what happens if a query fails validation. By default, it is set to `error`, but we can change it to `warn` if we want the insert to still occur. Now instead of an insert or update erroring out, it would simply warn the user that the operation failed validation.\n\nAnd finally, if we needed to, we can bypass document validation altogether by passing the `bypassDocumentValidation` option with our query. To show you how this works, let's say we wanted to insert just a `title` in our `posts` collection and we didn't want any other data. If we tried to just do this...\n\n``` javascript\ndb.collection('posts').insertOne({ title: 'Awesome' });\n```\n\n... we would get an error saying that document validation failed. But if we wanted to skip document validation for this insert, we would simply do this:\n\n``` javascript\ndb.collection('posts').insertOne(\n   { title: 'Awesome' },\n   { bypassDocumentValidation: true }\n);\n```\n\nThis would not be possible with Mongoose. MongoDB schema validation is more in line with the entire philosophy of MongoDB where the focus is on a flexible design schema that is quickly and easily adaptable to your use cases.\n\n## Populate and Lookup\n\nThe final area where I would like to compare Mongoose and the Node.js MongoDB driver is its support for pseudo-joins. Both Mongoose and the native Node.js driver support the ability to combine documents from multiple collections in the same database, similar to a join in traditional relational databases.\n\nThe Mongoose approach is called **Populate**. It allows developers to create data models that can reference each other and then, with a simple API, request data from multiple collections. For our example, let's expand on the blog post and add a new collection for users.\n\n``` javascript\nconst user = new Schema({\n   name: String,\n   email: String\n});\n \nconst blog = new Schema({\n   title: String,\n   slug: String,\n   published: Boolean,\n   content: String,\n   tags: [String],\n   comments: [{\n       user: { Schema.Types.ObjectId, ref: 'User' },\n       content: String,\n       votes: Number\n   }]\n});\n \nconst User = mongoose.model('User', user);\nconst Blog = mongoose.model('Blog', blog);\n```\n\nWhat we did above was we created a new model and schema to represent users leaving comments on blog posts. When a user leaves a comment, instead of storing information on them, we would just store that user’s `_id`. So, an update operation to add a new comment to our post may look something like this:\n\n``` javascript\nBlog.updateOne({\n   comments: [{ user: \"12345\", content: \"Great Post!!!\" }]\n});\n```\n\nThis is assuming that we have a user in our `User` collection with the `_id` of `12345`. Now, if we wanted to **populate** our `user` property when we do a query—and instead of just returning the `_id` return the entire document—we could do:\n\n``` javascript\nBlog.\n   findOne({}).\n   populate('comments.user').\n   exec(function (err, post) {\n       console.log(post.comments[0].user.name) // Name of user for 1st comment\n   });\n```\n\nPopulate coupled with Mongoose data modeling can be very powerful, especially if you're coming from a relational database background. The drawback though is the amount of magic going on under the hood to make this happen. Mongoose would make two separate queries to accomplish this task and if you're joining multiple collections, operations can quickly slow down.\n\nThe other issue is that the populate concept only exists at the application layer. So while this does work, relying on it for your database management can come back to bite you in the future.\n\nMongoDB as of version 3.2 introduced a new operation called `$lookup` that allows to developers to essentially do a left outer join on collections within a single MongoDB database. If we wanted to populate the user information using the Node.js driver, we could create an aggregation pipeline to do it. Our starting point using the `$lookup` operator could look like this:\n\n``` javascript\ndb.collection('posts').aggregate([\n  {\n    '$lookup': {\n      'from': 'users', \n      'localField': 'comments.user', \n      'foreignField': '_id', \n      'as': 'users'\n    }\n  }, {}\n], (err, post) => {\n    console.log(post.users); //This would contain an array of users\n});\n```\n\nWe could further create an additional step in our aggregation pipeline to replace the user information in the `comments` field with the users data, but that's a bit out of the scope of this article. If you wish to learn more about how aggregation pipelines work with MongoDB, check out the [aggregation docs](https://docs.mongodb.com/manual/aggregation/).\n\n## Final Thoughts: Do I Really Need Mongoose?\n\nBoth Mongoose and the MongoDB Node.js driver support similar functionality. While Mongoose does make MongoDB development familiar to someone who may be completely new, it does perform a lot of magic under the hood that could have unintended consequences in the future.\n\nI personally believe that you don't need an ODM to be successful with MongoDB. I am also not a huge fan of ORMs in the relational database world. While they make initial dive into a technology feel familiar, they abstract away a lot of the power of a database.\n\nDevelopers have a lot of choices to make when it comes to building applications. In this article, we looked at the differences between using an ODM versus the native driver and showed that the difference between the two is not that big. Using an ODM like Mongoose can make development feel familiar but forces you into a rigid design, which is an anti-pattern when considering building with MongoDB.\n\nThe MongoDB Node.js driver works natively with your MongoDB database to give you the best and most flexible development experience. It allows the database to do what it's best at while allowing your application to focus on what it's best at, and that's probably not managing data models.","description":"Learn why using an Object Data Modeling library may not be the best choice when building MongoDB apps with Node.js.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt63f3d0a396ee8471/644c46ed2c3b2b582beba83f/NodeJS_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/mongoose-versus-nodejs-driver","title":"*MongoDB & Mongoose: Compatibility and Comparison","original_publish_date":"2021-11-25T17:25:33.587Z","strapi_updated_at":"2023-04-13T08:13:40.503Z","expiry_date":"2022-11-18T18:29:31.878Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*Nodejs","calculated_slug":"/technologies/nodejs"}}]}},"seo":{"canonical_url":"","meta_description":"Learn why using an Object Data Modeling library may not be the best choice when building MongoDB apps with Node.js.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt5d55a3b73299d926/644c46ee28676e98e5b6ccc4/NodeJS_Tile_360x360.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@kukicado"},"system":{"updated_at":"2023-04-28T22:52:09.996Z","publish_details":{"time":"2023-04-28T22:52:58.727Z"}}},{"calculated_slug":"/products/realm/create-data-api-10-min-realm","content":"\n\n<div class=\"introduction\">\n\n## Objectives\n\n-   Deploy a [Free Tier Cluster](https://docs.atlas.mongodb.com/tutorial/deploy-free-tier-cluster)\n-   Load [Sample Data into your MongoDB Atlas Cluster](https://docs.atlas.mongodb.com/sample-data)\n-   Create a [MongoDB Realm](https://www.mongodb.com/realm) application\n-   Create a [3rd Party Service](https://docs.mongodb.com/realm/services), an API with an HTTP service listener\n-   Test the API using [Postman](https://postman.com)\n\n## Prerequisites\n\n-   MongoDB Atlas Account with a Cluster Running\n-   Postman Installed - See <https://www.postman.com/downloads/>\n\n## Getting Started\n\nCreating an Application Programming Interface (API) that exposes data and responds to HTTP requests is very straightforward. With MongoDB Realm, you can create a data enabled endpoint in about 10 minutes or less. In this article, I'll explain the steps to follow to quickly create an API that exposes data from a sample database in MongoDB Atlas. We'll deploy the sample dataset, create a Realm App with an HTTP listener, and then we'll test it using [Postman](https://postman.com).\n\n> I know that some folks prefer to watch and learn, so I've created this video overview. Be sure to pause the video at the various points where you need to install the required components and complete some of the required steps.\n>\n> :youtube[]{vid=bM3fcw4M-yk}\n\n## Step 1: Deploy a Free Tier Cluster\n\nIf you haven't done so already, visit [this link](https://docs.atlas.mongodb.com/tutorial/deploy-free-tier-cluster) and follow along to deploy a free tier cluster. This cluster will be where we store and manage the data associated with our data API.\n\n## Step 2: Load Sample Datasets into Your Atlas Cluster\n\nMongoDB Atlas offers several sample datasets that you can easily deploy once you launch a cluster. Load the sample datasets by clicking on the three dots button to see additional options, and then select \"Load Sample Dataset.\" This process will take approximately five minutes and will add a number of really helpful databases and collections to your cluster. Be aware that these will consume approximately 350mb of storage. If you intend to use your free tier cluster for an application, you may want to remove some of the datasets that you no longer need. You can always re-deploy these should you need them.\n\n![Load Sample Dataset](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/10-min-data-api/load_sample_data.png)\n\nNavigate to the **Collections** tab to see them all. All of the datasets will be created as separate databases prefixed with `sample_` and then the name of the dataset. The one we care about for our API is called `sample_analytics`. Open this database up and you'll see one collection called `customers`. Click on it to see the data we will be working with.\n\n![Sample Analytics Dataset](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/10-min-data-api/sample_analytics_collection_view.png)\n\nThis collection will have 500 documents, with each containing sample Analytics Customer documents. Don't worry about all the fields or the structure of these documents just now—we'll just be using this as a simple data source.\n\n## Step 3: Create a New App\n\nTo begin creating a new Application Service, navigation from Atlas to App Services.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Pasted_Image_7_28_22_12_13_PM_66b736019c.png)\n\nAt the heart of the entire process are Application Services. There are several from which to choose and to create a data enabled endpoint, you'll choose the HTTP Service with HTTPS Endpoints. HTTPS Endpoints, like they sound, are simply hooks into the web interface of the back end. Coming up, I'll show you the code (a function) that gets executed when the hook receives data from your web client.\n\nTo access and create 3rd Party Services, click the link in the left-hand navigation labeled \"3rd Party Services.\"\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/10-min-data-api/click_3rd_party_services.png\" class=\"align-center\" alt=\"3rd Party Services\" />\n\nNext, let's add a service. Find, and click the button labeled \"Add a Service.\"\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/10-min-data-api/click_add_a_3rd_party_service.png\" class=\"align-center\" alt=\"3rd Party Services\" />\n\nNext, we'll specify that we're creating an HTTP service and we'll provide a name for the service. The name is not incredibly significant. I'm using `api` in this example.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/10-min-data-api/add-a-service.png\" class=\"align-center\" alt=\"Adding a realm service\" />\n\nWhen you create an HTTP Service, you're enabling access to this service from [Realm's serverless functions](https://docs.mongodb.com/realm/functions) in the form of an object called `context.services`. More on that later when we create a serverless function attached to this service. Name and add the service and you'll then get to create an [Incoming HTTPS Endpoint](https://docs.mongodb.com/realm/services/webhook-requests-and-responses). This is the process that will be contacted when your clients request data of your API.\n\nCall the HTTPS Endpoint whatever you like, and set the parameters as you see below:\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/10-min-data-api/webhook_settings.png\" class=\"align-center\" alt=\"Webhook Settings\" />   \n\n ##### HTTPS Endpoint Properties \n| Property                     | Description                                                                                                                                                                                     |\n|------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Name                         | Choose a name for your HTTPS Endpoint... any value will do.                                                                                                                                            |\n| Authentication               | This is how your HTTPS Endpoint will authenticate users of your API. For this simple exercise, let's choose `System`.                                                                                  |\n| Log Function Arguments       | Enabling this allows you to get additional log content with the arguments sent from your web clients. Turn this on.                                                                             |\n| HTTPS Endpoint URL                  | This is the URL created by Realm. Take note of this - we'll be using this URL to test our API.                                                                                                  |\n| HTTP Method                  | Our API can listen for the various HTTP methods (GET, POST, PATCH, etc.). Set this to POST for our example.                                                                                     |\n| Respond with Result          | Our API can respond to web client requests with a dataset result. You'll want this on for our example.                                                                                          |\n| AUTHORIZATION - Can evaluate | This is a JSON expression that must evaluate to TRUE before the function may run. If this field is blank, it will evaluate to TRUE. This expression is evaluated before service-specific rules. |\n| Request Validation           | Realm can validate incoming requests to protect against DDOS attacks and users that you don't want accessing your API. Set this to `Require Secret` for our example.                            |\n| Secret                       | This is the secret passphrase we'll create and use from our web client. We'll send this using a `PARAM` in the POST request. More on this below.                                                |\n\nAs mentioned above, our example API will respond to `POST` requests. Next up, you'll get to create the logic in a function that will be executed whenever your API is contacted with a POST request.\n\n### Defining the Function\n\nLet's define the function that will be executed when the HTTPS Endpoint receives a POST request.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/10-min-data-api/default_function.png\" class=\"align-center\" alt=\"Defining the function\" />\n\n> As you modify the function, and save settings, you will notice a blue bar appear at the top of the console.\n>\n> <img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/10-min-data-api/review_deploy.png\" class=\"align-center\" alt=\"Review and Deploy\" />\n>\n> This appears to let you know you have modified your Realm Application but have not yet deployed those changes. It's good practice to batch your changes. However, make sure you remember to review and deploy prior to testing.\n\nRealm gives you the ability to specify what logic gets executed as a result of receiving a request on the HTTPS Endpoint URL. What you see above is the default function that's created for you when you create the service. It's meant to be an example and show you some of the things you can do in a Realm Backend function. Pay close attention to the `payload` variable. This is what's sent to you by the calling process. In our case, that's going to be from a form, or from an external JavaScript script. We'll come back to this function shortly and modify it accordingly.\n\nUsing our sample database `sample_analytics` and our `customers`, let's write a basic function to return 10 customer documents.\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/10-min-data-api/data_API_04-pgqg0la2ka.gif\" class=\"align-center\" alt=\"Creating a function to return data from a collection in MongoDB Atlas\" />\n\nAnd here's the source:\n\n``` JavaScript\nexports = function(payload) {\n   const mongodb = context.services.get(\"mongodb-atlas\");\n   const mycollection = mongodb.db(\"sample_analytics\").collection(\"customers\");\n   return mycollection.find({}).limit(10).toArray();\n};\n```\n\nThis is JavaScript - ECMAScript 6, to be specific, also known as ES6 and ECMAScript 2015, was the second major revision to JavaScript.\n\nLet's call out an important element of this script: `context`.\n\nRealm functions can interact with connected services, user information, predefined values, and other functions through modules attached to the global `context` variable.\n\nThe `context` variable contains the following modules:\n\n| Property            | Description                                                                  |\n|---------------------|------------------------------------------------------------------------------|\n| `context.services`  | Access service clients for the services you've configured.                   |\n| `context.values`    | Access values that you've defined.                                           |\n| `context.user`      | Access information about the user that initiated the request.                |\n| `context.request`   | Access information about the HTTP request that triggered this function call. |\n| `context.functions` | Execute other functions in your Realm app.                                   |\n| `context.http`      | Access the HTTP service for get, post, put, patch, delete, and head actions. |\nOnce you've set your configuration for the Realm HTTPS Endpoint, copy the HTTPS Endpoint URL, and take note of the Secret you created. You'll need these to begin sending data and testing.\n\nSpeaking of testing... [Postman](https://www.postman.com/) is a great tool that enables you to test an API like the one we've just created. Postman acts like a web client - either a web application or a browser.\n\n> If you don't have Postman installed, visit this link (it's free!): <https://www.postman.com/downloads/>\n\nLet's test our API with Postman:\n\n<img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/10-min-data-api/testing_with_postman.gif\" class=\"align-center\" alt=\"Here&#39;s how to test your API with postman\" />\n\n1.  Launch Postman and click the plus (+ New) to add a new request. You may also use the Launch screen - whichever you're more comfortable with.\n2.  Give your request a name and description, and choose/create a collection to save it in.\n3.  Paste the HTTPS Endpoint URL you created above into the URL bar in Postman labeled `Enter request URL`.\n4.  Change the `METHOD` from `GET` to `POST` - this will match the `HTTP Method` we configured in our HTTPS Endpoint above.\n5.  We need to append our `secret` parameter to our request so that our HTTPS Endpoint validates and authorizes the request. Remember, we set the secret parameter above. There are two ways you can send the secret parameter. The first is by appending it to the HTTPS Endpoint URL by adding `?secret=YOURSECRET`. The other is by creating a `Parameter` in Postman. Either way will work.\n\nOnce you've added the secret, you can click `SEND` to send the request to your newly created HTTPS Endpoint.\n\nIf all goes well, Postman will send a POST request to your API and Realm will execute the Function you created, returning 10 records from the `Sample_Analytics` database, and the `Customers` collection...\n\n``` javascript\n[\n{\n   \"_id\": {\n         \"$oid\": \"5ca4bbcea2dd94ee58162a68\"\n   },\n   \"username\": \"fmiller\",\n   \"name\": \"Elizabeth Ray\",\n   \"address\": \"9286 Bethany Glens\\nVasqueztown, CO 22939\",\n   \"birthdate\": {\n         \"$date\": {\n            \"$numberLong\": \"226117231000\"\n         }\n   },\n   \"email\": \"arroyocolton@gmail.com\",\n   \"active\": true,\n   \"accounts\": [\n         {\n            \"$numberInt\": \"371138\"\n         },\n         ...\n   ],\n   \"tier_and_details\": {\n         \"0df078f33aa74a2e9696e0520c1a828a\": {\n            \"tier\": \"Bronze\",\n            \"id\": \"0df078f33aa74a2e9696e0520c1a828a\",\n            \"active\": true,\n            \"benefits\": [\n               \"sports tickets\"\n            ]\n         },\n         \"699456451cc24f028d2aa99d7534c219\": {\n            \"tier\": \"Bronze\",\n            \"benefits\": [\n               \"24 hour dedicated line\",\n               \"concierge services\"\n            ],\n            \"active\": true,\n            \"id\": \"699456451cc24f028d2aa99d7534c219\"\n         }\n   }\n},\n// remaining documents clipped for brevity\n...\n]\n```\n\n## Taking This Further\n\nIn just a few minutes, we've managed to create an API that exposes (READs) data stored in a MongoDB Database. This is just the beginning, however. From here, you can now expand on the API and create additional methods that handle all aspects of data management, including inserts, updates, and deletes.\n\nTo do this, you'll create additional HTTPS Endpoints, or modify this HTTPS Endpoint to take arguments that will control the flow and behavior of your API.\n\nConsider the following example, showing how you might evaluate parameters sent by the client to manage data.\n\n``` JavaScript\nexports = async function(payload) {\n\n   const mongodb = context.services.get(\"mongodb-atlas\");\n   const db = mongodb.db(\"sample_analytics\");\n   const customers = db.collection(\"customers\");\n\n   const cmd=payload.query.command;\n   const doc=payload.query.doc;\n\n   switch(cmd) {\n   case \"create\":\n      const result= await customers.insertOne(doc);\n      if(result) {\n            return { text: `Created customer` };   \n      }\n      return { text: `Error stashing` };\n   case \"read\":\n      const findresult = await customers.find({'username': doc.username}).toArray();\n      return { findresult };\n   case \"delete\":\n      const delresult = await customers.deleteOne( { username: { $eq: payload.query.username }});\n      return { text: `Deleted ${delresult.deletedCount} stashed items` };\n   default:\n      return { text: \"Unrecognized command.\" };\n   }\n}\n```\n\n</div>\n\n<div class=\"summary\">\n\n## Conclusion\n\nMongoDB Realm enables developers to quickly create fully functional application components without having to implement a lot of boilerplate code typically required for APIs. Note that the above example, while basic, should provide you with a good starting point. for you. Please join me in the [Community Forums](https://community.mongodb.com) if you have questions.\n\nYou may also be interested in learning more from an episode of the MongoDB Podcast where we covered [Mobile Application Development with Realm](https://mongodb.libsyn.com/ep-7-mobile-app-development-with-mongodb-realm).\n\n#### Other Resources\nData API Documentation - docs:https://docs.atlas.mongodb.com/api/data-api/\n</div>\n","description":"Learn how to create a data API with Atlas Data API in 10 minutes or less","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt66b1165715749812/644c46f0aa54d01718da3475/dev-api.png?branch=prod","description":null}}]},"slug":"/create-data-api-10-min-realm","title":"*Create a Custom Data Enabled API in MongoDB Atlas in 10 Minutes or Less","original_publish_date":"2021-11-30T11:54:37.082Z","strapi_updated_at":"2022-09-23T00:27:28.408Z","expiry_date":"2022-11-18T18:39:34.465Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to create a data api with MongoDB Atlas in 10 minutes or less","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltc265ccf28c47e0bf/644c46e41efe6d34289518af/og-api.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@mlynn"},"system":{"updated_at":"2023-04-28T22:52:09.584Z","publish_details":{"time":"2023-04-28T22:52:58.761Z"}}},{"calculated_slug":"/products/atlas/nodejs-python-ruby-atlas-api","content":"\n\n<div class=\"introduction\">\n\nThe real power of a cloud-hosted, fully managed service like [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) is that you can create whole new database deployment architectures automatically, using the services API. Getting to the [MongoDB Atlas Administration API](https://docs.atlas.mongodb.com/api/) is relatively simple and, once unlocked, it opens up a massive opportunity to integrate and automate the management of database deployments from creation to deletion. The API itself is an extensive REST API. There's role-based access control and you can have [user or app-specific credentials](https://docs.atlas.mongodb.com/configure-api-access/) to access it.\n\nThere is one tiny thing that can trip people up though. The credentials have to be passed over using the [digest authentication mechanism](https://en.wikipedia.org/wiki/Digest_access_authentication), not the more common basic authentication or using an issued token. Digest authentication, at its simplest, waits to get an HTTP `401 Unauthorized` response from the web endpoint. That response comes with data and the client then sends an encrypted form of the username and password as a digest and the server works with that.\n\nAnd that's why we’re here today: to show you how to do that with the least fuss in Python, Node, and Ruby. In each example, we'll try and access the base URL of the Atlas Administration API which returns a JSON document about the underlying applications name, build and other facts.\nYou can find all code samples in the dedicated [Github repository](https://github.com/mongodb-developer/atlas-admin-api-digest-auth).\n\n## Setup\n\nTo use the Atlas Administration API, you need… a MongoDB Atlas cluster! If you don’t have one already, follow the [Get Started with Atlas](https://www.mongodb.com/docs/atlas/getting-started/) guide to create your first cluster.\n\nThe next requirement is the organization API key. You can set it up in two steps:\n\n[Create an API key](https://www.mongodb.com/docs/atlas/configure-api-access/#std-label-create-org-api-key) in your Atlas organization. Make sure the key has the Organization Owner permission.\nAdd your IP address to the [API Access List for the API key](https://www.mongodb.com/docs/atlas/configure-api-access/#add-an-api-access-list-entry).\n\nThen, open a new terminal and export the following environment variables, where `ATLAS_USER` is your public key and `ATLAS_USER_KEY` is your private key.\n\n```\nexport ATLAS_USER=<public_key>\nexport ATLAS_USER_KEY=<private_key>\n```\n\nYou’re all set up! Let’s see how we can use the Admin API with Python, Node, and Ruby.\n\n</div>\n\n<div class=\"content\">\n\n## Python\n\nWe start with the simplest and most self-contained example: Python.\n\nIn the Python version, we lean on the `requests` library for most of the heavy lifting. We can install it with `pip`:\n\n``` bash\npython -m pip install requests\n```\n\nThe implementation of the digest authentication itself is the following:\n\n``` python\nimport os\nimport requests\nfrom requests.auth import HTTPDigestAuth\nimport pprint\n\nbase_url = \"https://cloud.mongodb.com/api/atlas/v1.0/\"\nauth = HTTPDigestAuth(\n   os.environ[\"ATLAS_USER\"],\n   os.environ[\"ATLAS_USER_KEY\"]\n)\n\nresponse = requests.get(base_url, auth = auth)\npprint.pprint(response.json())\n```\n\nAs well as importing `requests`, we also bring in `HTTPDigestAuth` from requests' `auth` module to handle digest authentication. The `os` import is just there so we can get the environment variables `ATLAS_USER` and `ATLAS_USER_KEY` as credentials, and the `pprint` import is just to format our results.\n\nThe critical part is the addition of `auth = HTTPDigestAuth(...)` to the `requests.get()` call. This installs the code needed to respond to the server when it asks for the digest.\n\nIf we now run this program...\n\n![Screenshot of the terminal emulator after the execution of the request script for Python. The printed message shows that the request was successful.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/atlaspython_tin8y89s18_02b8a26fbe.png)\n\n…we have our API response.\n\n\n## Node.js\n\nFor Node.js, we’ll take advantage of the `urllib` package which supports digest authentication.\n\n``` bash\nnpm install urllib\n```\n\nThe code for the Node.js HTTP request is the following:\n\n``` javascript\nconst urllib = require('urllib');\n\nconst baseUrl = 'https://cloud.mongodb.com/api/atlas/v1.0/';\nconst { ATLAS_USER, ATLAS_USER_KEY } = process.env;\nconst options = {\n  digestAuth: `${ATLAS_USER}:${ATLAS_USER_KEY}`,\n};\n\nurllib.request(baseUrl, options, (error, data, response) => {\n  if (error || response.statusCode !== 200) {\n    console.error(`Error: ${error}`);\n    console.error(`Status code: ${response.statusCode}`);\n  } else {\n    console.log(JSON.parse(data));\n  }\n});\n```\n\nTaking it from the top… we first require and import the `urllib` package. Then, we extract the `ATLAS_USER` and `ATLAS_USER_KEY` variables from the process environment and use them to construct the authentication key. Finally, we send the request and handle the response in the passed callback. \n\nAnd we’re ready to run:\n\n![Screenshot of the terminal emulator after the execution of the request script for Node.js. The printed message shows that the request was successful.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/atlasnode_8rd944aaw7_048b949048.png)\n\nOn to our final language...\n\n## Ruby\n\nHTTParty is a widely used Gem which is used by the Ruby and Rails community to perform HTTP operations. It also, luckily, supports digest authentication. So, to get the party started:\n\n``` bash\ngem install httparty\n```\n\nThere are two ways to use HTTParty. One is creating an object which abstracts the calls away while the other is just directly calling methods on HTTParty itself. For brevity, we'll do the latter. Here's the code:\n\n``` ruby\nrequire 'httparty'\nrequire 'json'\n\nbase_url = 'https://cloud.mongodb.com/api/atlas/v1.0/'\noptions = {\n  :digest_auth => {\n    :username=>ENV['ATLAS_USER'],\n    :password=>ENV['ATLAS_USER_KEY']\n  }\n}\n\nresult = HTTParty.get(base_url, options)\n\npp JSON.parse(result.body())\n```\n\nWe require the HTTParty and JSON gems first. We then create a dictionary with our username and key, mapped for HTTParty's authentication, and set a variable to hold the base URL. We're ready to do our GET request now, and in the `options` (the second parameter of the GET request), we pass `:digest_auth=>auth` to switch on the digest support. We wrap up by JSON parsing the resulting body and pretty printing that. Put it all together and run it and we get:\n\n![Screenshot of the terminal emulator after the execution of the request script for Ruby. The printed message shows that the request was successful.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/atlasruby_e82kmt50uy_d1e1a11ad7.png)\n\n</div>\n\n<div class=\"summary\">\n\n## Next Stop - The API\n\nIn this article, we learned how to call the [MongoDB Atlas Administration API](https://docs.atlas.mongodb.com/api/) using digest authentication. We took advantage of the vast library ecosystems of Python, Node.js, and Ruby, and used the following open-source community libraries:\n\nRequests for Python\nurllib for JavaScript\nhttparty for Ruby\n\nIf your project requires it, you can implement digest authentication yourself by following the [official specification](https://datatracker.ietf.org/doc/html/rfc7616). You can draw inspiration from the implementations in the aforementioned libraries.\n\nAdditionally, you can find all code samples from the article in Github.\n\nWith the authentication taken care of, just remember to be fastidious with your API key security and make sure you revoke unused keys. You can now move on to explore the API itself. Start in the [documentation](https://docs.atlas.mongodb.com/api/) and see what you can automate today.\n\n\n</div>\n","description":"Learn how to use digest authentication for the MongoDB Atlas Administration API from Python, Node.js, and Ruby.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt977970c45015356e/644c46f347e2e27737c6d033/nodejs.png?branch=prod","description":null}}]},"slug":"/nodejs-python-ruby-atlas-api","title":"*Calling the MongoDB Atlas Administration API: How to Do it from Node, Python, and Ruby","original_publish_date":"2021-11-30T10:44:19.580Z","strapi_updated_at":"2023-04-13T08:13:40.503Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Ruby","calculated_slug":"/languages/ruby"}},{"node":{"title":"*Python","calculated_slug":"/languages/python"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*Nodejs","calculated_slug":"/technologies/nodejs"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to use digest authentication for the MongoDB Atlas Administration API from Python, Node.js, and Ruby.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltc01ed7d34f47ea0a/644c46f5add18e87d2aa9280/og-build.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@ StanimiraVlaeva"},"system":{"updated_at":"2023-04-28T22:52:09.208Z","publish_details":{"time":"2023-04-28T22:52:58.793Z"}}},{"calculated_slug":"/products/atlas/pause-resume-atlas-clusters","content":"\n\n<div class=\"introduction\">\n\nOne of the most important things to think about in the cloud is what is burning dollars while you sleep. In the case of MongoDB Atlas, that is your live clusters. The minute you start a cluster (with the exception of our free tier), we start accumulating cost.\n\nIf you're using a dedicated cluster—not one of the cheaper, shared cluster types, such as M0, M2 or M5—then it's easy enough to pause a cluster using the Atlas UI, but logging in over 2FA can be a drag. Wouldn't it be great if we could just jump on a local command line to look at our live clusters?\n\nThis you can do with a command line tool like `curl`, some programming savvy, and knowledge of the [MongoDB Atlas Admin API](https://docs.atlas.mongodb.com/api/). But who has time for that? Not me, for sure.\n\n</div>\n\n<div class=\"content\">\n\nThat is why I wrote a simple script to automate those steps. It's now a Python package up on PyPi called [mongodbatlas](https://pypi.org/project/mongodbatlas/).\n\nYou will need Python 3.6 or better installed to run the script. (This is your chance to escape the clutches of 2.x.)\n\nJust run:\n\n``` bash\n$ pip install mongodbatlas\n  Collecting mongodbatlas\n   Using cached mongodbatlas-0.2.6.tar.gz (17 kB)\n  ...\n  ...\n  Building wheels for collected packages: mongodbatlas\n   Building wheel for mongodbatlas (setup.py) ... done\n   Created wheel for mongodbatlas: filename=mongodbatlas-0.2.6-py3-none-any.whl size=23583       sha256=d178ab386a8104f4f5100a6ccbe61670f9a1dd3501edb5dcfb585fb759cb749c\n   Stored in directory: /Users/jdrumgoole/Library/Caches/pip/wheels/d1/84/74/3da8d3462b713bfa67edd02234c968cb4b1367d8bc0af16325\n  Successfully built mongodbatlas\n  Installing collected packages: certifi, chardet, idna, urllib3, requests, six, python-dateutil, mongodbatlas\n  Successfully installed certifi-2020.11.8 chardet-3.0.4 idna-2.10 mongodbatlas-0.2.6 python-dateutil-2.8.1 requests-2.25.0 six-1.15.0 urllib3-1.26.1\n```\n\nNow you will have a script installed called `atlascli`. To test the install worked, run `atlascli -h`.\n\n``` bash\n$ atlascli -h\n  usage: atlascli [-h] [--publickey PUBLICKEY] [--privatekey PRIVATEKEY]\n                 [-p PAUSE_CLUSTER] [-r RESUME_CLUSTER] [-l] [-lp] [-lc]\n                 [-pid PROJECT_ID_LIST] [-d]\n\n  A command line program to list organizations,projects and clusters on a\n  MongoDB Atlas organization.You need to enable programmatic keys for this\n  program to work. See https://docs.atlas.mongodb.com/reference/api/apiKeys/\n\n  optional arguments:\n   -h, --help            show this help message and exit\n   --publickey PUBLICKEY\n                         MongoDB Atlas public API key.Can be read from the\n                         environment variable ATLAS_PUBLIC_KEY\n   --privatekey PRIVATEKEY\n                         MongoDB Atlas private API key.Can be read from the\n                         environment variable ATLAS_PRIVATE_KEY\n   -p PAUSE_CLUSTER, --pause PAUSE_CLUSTER\n                         pause named cluster in project specified by project_id\n                         Note that clusters that have been resumed cannot be\n                         paused for the next 60 minutes\n   -r RESUME_CLUSTER, --resume RESUME_CLUSTER\n                         resume named cluster in project specified by\n                         project_id\n   -l, --list            List everything in the organization\n   -lp, --listproj       List all projects\n   -lc, --listcluster    List all clusters\n   -pid PROJECT_ID_LIST, --project_id PROJECT_ID_LIST\n                         specify the project ID for cluster that is to be\n                         paused\n   -d, --debug           Turn on logging at debug level\n\n  Version: 0.2.6\n```\n\nTo make this script work, you will need to do a little one-time setup on your cluster. You will need a [programmatic key](https://docs.atlas.mongodb.com/configure-api-access/#atlas-prog-api-key) for your cluster. You will also need to [enable the IP address](https://docs.atlas.mongodb.com/configure-api-access/#edit-the-api-whitelist) that the client is making requests from.\n\nThere are two ways to create an API key:\n\n-   If you have a single project, it's probably easiest to create a [single project API key](#single-project-api-key)\n-   If you have multiple projects, you should probably create an [organization API key](#organization-api-key) and add it to each of your projects.\n\n## <a name=\"single-project-api-key\">Single Project API Key\n\nGoing to your \"Project Settings\" page by clicking on the \"three dot\" button next your project name at the top-left of the screen and selecting \"Project Settings\". Then click on \"Access Manager\" on the left side of the screen and click on \"Create API Key\". Take a note of the public *and* private parts of the key, and ensure that the key has the \"Project Cluster Manager\" permission. More detailed steps can be found in the [documentation](https://docs.atlas.mongodb.com/configure-api-access/#create-one-api-key-for-one-project).\n\n![A screenshot of the project's Access Manager page.](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/pause-resume-atlas-clusters/annotated-project-key.png)\n\n## <a name=\"organization-api-key\">Organization API Key\n\nClick on the cog icon next to your organization name at the top-left of the screen. Click on \"Access Manager\" on the left-side of the screen and click on \"Create API Key\". Take a note of the public *and* private parts of the key. Don't worry about selecting any specific organization permissions.\n\n![A screenshot showing the buttons described above this image](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/how-to/pause-resume-atlas-clusters/annotated-org-key.png)\n\nNow you'll need to invite the API key to each of the projects containing clusters you wish to control. Click on \"Projects' on the left-side of the screen. For each of the projects, click on the \"three dots\" icon on the same row in the project table and select \"Visit Project Settings\" Click on \"Access Manager\", and click on \"Invite to Project\" on the top-right. Paste your public key into the search box and select it in the menu that appears. Ensure that the key has the \"Project Cluster Manager\" permission that it will need to pause and resume clusters in that project.\n\nMore detailed steps can be found in the [documentation](https://docs.atlas.mongodb.com/configure-api-access/#create-one-api-key-in-one-organization).\n\n## Configuring `atlascli`\n\nThe programmatic key has two parts: a public key and a private key. Both of these are used by the `atlascli` program to query the projects and clusters associated with the organization.\n\nYou can pass the keys in on the command line, but this is not recommended because they will be stored in the command line history. It's better to store them in environment variables, and the `atlascli` program will look for these two:\n\n-   `ATLAS_PUBLIC_KEY`: stores the public key part of the programmatic key\n-   `ATLAS_PRIVATE_KEY`: stores the private part of the programmatic key\n\nOnce you have created these environment variables, you can run `atlascli -l` to list the organization and its associated projects and clusters. I've blocked out part of the actual IDs with `xxxx` characters for security purposes:\n\n``` bash\n$ atlascli -l\n  {'id': 'xxxxxxxxxxxxxxxx464d175c',\n  'isDeleted': False,\n  'links': [{'href': 'https://cloud.mongodb.com/api/atlas/v1.0/orgs/599eeced9f78f769464d175c',\n             'rel': 'self'}],\n  'name': 'Open Data at MongoDB'}\n  Organization ID:xxxxxxxxxxxxf769464d175c Name:'Open Data at MongoDB'\n      project ID:xxxxxxxxxxxxd6522bc457f1 Name:'DevHub'\n        Cluster ID:'xxxxxxxxxxxx769c2577a54' name:'DRA-Data'                state=running\n      project ID:xxxxxxxxx2a0421d9bab Name:'MUGAlyser Project'\n        Cluster ID:'xxxxxxxxxxxb21250823bfba' name:'MUGAlyser'              state=paused\n      project ID:xxxxxxxxxxxxxxxx736dfdcddf Name:'MongoDBLive'\n      project ID:xxxxxxxxxxxxxxxa9a5a04e7 Name:'Open Data Covid-19'\n        Cluster ID:'xxxxxxxxxxxxxx17cec56acf' name:'pre-prod'               state=running\n        Cluster ID:'xxxxxxxxxxxxxx5fbfe04313' name:'dev'                    state=running\n        Cluster ID:'xxxxxxxxxxxxxx779f979879' name:'covid-19'               state=running\n      project ID xxxxxxxxxxxxxxxxa132a8010 Name:'Open Data Project'\n        Cluster ID:'xxxxxxxxxxxxxx5ce1ef94dd' name:'MOT'                    state=paused\n        Cluster ID:'xxxxxxxxxxxxxx22bf6c226f' name:'GDELT'                  state=paused\n        Cluster ID:'xxxxxxxxxxxxxx5647797ac5' name:'UKPropertyPrices'       state=paused\n        Cluster ID:'xxxxxxxxxxxxxx0f270da18a' name:'New-York-Taxi'          state=paused\n        Cluster ID:'xxxxxxxxxxxxxx11eab32cf8' name:'demodata'               state=running\n        Cluster ID:'xxxxxxxxxxxxxxxdcaef39c8' name:'stackoverflow'          state=paused\n      project ID:xxxxxxxxxxc9503a77fcce0c Name:'Realm'\n```\n\nTo pause a cluster, you will need to specify the `project ID` and the `cluster name`. Here is an example:\n\n``` bash\n$ atlascli --project_id xxxxxxxxxxxxxxxxa132a8010 --pause demodata\n  Pausing 'demodata'\n  Paused cluster 'demodata'\n```\n\nTo resume the same cluster, do the converse:\n\n``` bash\n$ atlascli --project_id xxxxxxxxxxxxxxxxa132a8010 --resume demodata\n  Resuming cluster 'demodata'\n  Resumed cluster 'demodata'\n```\n\nNote that once a cluster has been resumed, it cannot be paused again for a while.\n\nThis delay allows the Atlas service to apply any pending changes or patches to the cluster that may have accumulated while it was paused.\n\n</div>\n\n<div class=\"summary\">\n\nNow go save yourself some money. This script can easily be run from a `crontab` entry or the Windows Task Scheduler.\n\nWant to see the code? It's in this [repo](https://github.com/jdrumgoole/MongoDB-Atlas-API) on GitHub.\n\nFor a much more full-featured Atlas Admin API in Python, please check out my colleague [Matthew Monteleone's](https://www.linkedin.com/in/monteleone/) PyPI package [AtlasAPI](https://pypi.org/project/atlasapi/).\n\n> If you have questions, please head to our [developer community website](https://community.mongodb.com/) where the MongoDB engineers and the MongoDB community will help you build your next big idea with MongoDB.\n\n</div>\n","description":"Learn how to easily pause and resume MongoDB Atlas clusters.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltebb77b7149ddad65/644c46f8bc3ab51d11612ad2/green2.png?branch=prod","description":null}}]},"slug":"/pause-resume-atlas-clusters","title":"*How to Easily Pause and Resume MongoDB Atlas Clusters","original_publish_date":"2021-11-30T11:07:53.516Z","strapi_updated_at":"2022-09-23T14:50:17.414Z","expiry_date":"2022-11-18T18:45:43.809Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to easily pause and resume MongoDB Atlas clusters.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt10d67f440a9f4d46/644c46fa1d9ecc4432e86344/og-green-pattern.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@jdrumgoole"},"system":{"updated_at":"2023-04-28T22:52:08.809Z","publish_details":{"time":"2023-04-28T22:52:58.825Z"}}},{"calculated_slug":"/products/atlas/adl-sql-integration","content":">As of June 2022, the functionality previously known as Atlas Data Lake is now named Atlas Data Federation. Atlas Data Federation’s functionality is unchanged and you can learn more about it [here](http://mongodb.com/atlas/data-federation). Atlas Data Lake will remain in the Atlas Platform, with newly introduced functionality that you can learn about [here](http://mongodb.com/atlas/data-lake).\n\nModern platforms have a wide variety of data sources. As businesses grow, they have to constantly evolve their data management and have sophisticated, scalable, and convenient tools to analyse data from all sources to produce business insights.\n\nMongoDB has developed a rich and powerful [query language](https://docs.mongodb.com/manual/tutorial/query-documents/), including a very robust [aggregation framework](https://docs.mongodb.com/manual/aggregation/). \n\nThese were mainly done to optimize the way developers work with data and provide great tools to manipulate and query MongoDB documents.\n\nHaving said that, many developers, analysts, and tools still prefer the legacy SQL language to interact with the data sources. SQL has a strong foundation around joining data as this was a core concept of the legacy relational databases normalization model. \n\nThis makes SQL have a convenient syntax when it comes to describing joins. \n\nProviding MongoDB users the ability to leverage SQL to analyse multi-source documents while having a flexible schema and data store is a compelling solution for businesses.\n\n## Data Sources and the Challenge\n\nConsider a requirement to create a single view to analyze data from operative different systems. For example:\n\n- Customer data is managed in the user administration systems (REST API).\n- Financial data is managed in a financial cluster (Atlas cluster).\n- End-to-end transactions are stored in files on cold storage gathered from various external providers (S3 store).\n\nHow can we combine and best join this data? \n\n[MongoDB Atlas Data Lake](https://www.mongodb.com/atlas/data-lake) connects multiple data sources using the different [source types](https://docs.mongodb.com/datalake/deployment/deploy-adl/). Once the data sources are mapped, we can create collections consuming this data. Those collections can have [SQL schema](https://docs.mongodb.com/datalake/reference/cli/sql/sqlgenerateschema/) generated, allowing us to perform sophisticated joins and do JDBC queries from various BI tools.\n\nIn this article, we will showcase the extreme power hidden in the Data Lake [SQL interface](https://docs.mongodb.com/datalake/admin/query-with-sql/).\n\n## Setting Up My Data Lake\nIn the following view, I have created three main data sources: \n- S3 Transaction Store (S3 sample data).\n- Accounts from my Atlas clusters (Sample data sample_analytics.accounts).\n- Customer data from a secure https source.\n\n![Data Lake Configuration](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_10_17_at_15_22_06_3bc2c2c9e7.png)\n\nI mapped the stores into three collections under `FinTech` database:\n\n- `Transactions`\n- `Accounts`\n- `CustomerDL`\n\nNow, I can see them through a data lake connection as MongoDB collections.\n\nLet's grab our data lake connection string from the Atlas UI.\n\n![Data Lake Connection String](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_10_06_at_11_40_43_5ced192e43.png)\n\nThis connection string can be used with our BI tools or client applications to run SQL queries.\n\n## Connecting and Using $sql\n\nOnce we connect to the data lake via a mongosh shell, we can generate a SQL schema for our collections. This is required for the JDBC or $sql operators to recognise collections as SQL “tables.”\n\n#### Generate SQL schema for each collection:\n```js\nuse admin;\ndb.runCommand({sqlGenerateSchema: 1, sampleNamespaces: [\"FinTech.customersDL\"], sampleSize: 1000, setSchemas: true})\n{\n  ok: 1,\n  schemas: [ { databaseName: 'FinTech', namespaces: [Array] } ]\n}\ndb.runCommand({sqlGenerateSchema: 1, sampleNamespaces: [\"FinTech.accounts\"], sampleSize: 1000, setSchemas: true})\n{\n  ok: 1,\n  schemas: [ { databaseName: 'FinTech', namespaces: [Array] } ]\n}\ndb.runCommand({sqlGenerateSchema: 1, sampleNamespaces: [\"FinTech.transactions\"], sampleSize: 1000, setSchemas: true})\n{\n  ok: 1,\n  schemas: [ { databaseName: 'FinTech', namespaces: [Array] } ]\n}\n```\n#### Running SQL queries and joins using $sql stage:\n```js\nuse FinTech;\ndb.aggregate([{\n  $sql: {\n    statement: \"SELECT a.* , t.transaction_count FROM accounts a, transactions t where a.account_id = t.account_id SORT BY t.transaction_count DESC limit 2\",\n    format: \"jdbc\",\n    formatVersion: 2,\n    dialect: \"mysql\",\n  }\n}])\n```\n\nThe above query will prompt account information and the transaction counts of each account.\n\n## Connecting Via JDBC\n\nLet’s connect a powerful BI tool like Tableau with the JDBC driver.\n\n[Download JDBC Driver](https://search.maven.org/search?q=a:mongodb-jdbc).\n\n![JDBC driver](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/download_jdbc_for_tableau_7d0e5cb7d6.png)\n\nSetting `connection.properties` file.\n```\nuser=root\npassword=*******\nauthSource=admin\ndatabase=FinTech\nssl=true\ncompressors=zlib\n```\n\n#### Connect to Tableau\n\nClick the  “Other Databases (JDBC)” connector and load the connection.properties file pointing to our data lake URI.\n\n![Tableau Connector Selection](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/tableau_select_other_db_1c75d7023c.png)\n\nOnce the data is read successfully, the collections will appear on the right side.\n\n#### Setting and Joining Data\n\n![Setting Tables](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_10_17_at_11_19_53_8913b663c6.png)\n\nWe can drag and drop collections from different sources and link them together.\n\n![Joining Tables with Drag & Drop](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_10_17_at_15_14_36_0e2fdcae97.png)\n\nIn my case, I connected `Transactions` => `Accounts` based on the `Account Id` field, and accounts and users based on the `Account Id` to `Accounts` field.\n\n![Unified View of Accounts, Transactions and Users data](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Screen_Shot_2021_10_17_at_15_06_50_9e906438d5.png)\n\nIn this view, we will see a unified table for all accounts with usernames and their transactions start quarter. \n\n## Summary\n\nMongoDB has all the tools to read, transform, and analyse your documents for almost any use-case.  \n\nWhether your data is in an Atlas operational cluster, in a service, or on cold storage like S3, Atlas Data Lake will provide you with the ability to join the data in real time. With the option to use powerful join SQL syntax and SQL-based BI tools like Tableau, you can get value out of the data in no time.\n\nTry Atlas Data Lake with your BI tools and SQL today.\n\n","description":"Learn how new SQL-based syntax can power your data lake insights in minutes. Integrate this capability with powerful BI tools like Tableau to get immediate value out of your data. ","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltc820fd46672c7f44/644c46fd1023734c4a966c72/WebReady-Datalake_files_hero_green.JPG?branch=prod","description":null}}]},"slug":"/adl-sql-integration","title":"*Atlas Data Lake SQL Integration to Form Powerful Data Interactions","original_publish_date":"2021-11-23T18:25:13.433Z","strapi_updated_at":"2022-06-07T14:35:51.578Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltc3078e8907437fc9/644c46febad93a13fa396f96/Datalake_files_hero_green.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:08.424Z","publish_details":{"time":"2023-04-28T22:52:59.927Z"}}},{"calculated_slug":"/products/atlas/build-your-own-function-retry-mechanism-with-realm","content":"## What is Realm?\n  \nWondering what it's all about? Realm is an object-oriented data model database that will persist data on disk, doesn’t need an ORM, and lets you write less code with full offline capabilities… but Realm is also a fully-managed back-end service that helps you deliver best-in-class apps across Android, iOS, and Web.  \n  \nTo leverage the full BaaS capabilities, Functions allow you to define and execute server-side logic for your application. You can call functions from your client applications as well as from other functions and in JSON expressions throughout Realm.  \n  \nFunctions are written in modern JavaScript (ES6+) and execute in a serverless manner. When you call a function, you can dynamically access components of the current application as well as information about the request to execute the function and the logged-in user that sent the request.  \n  \nBy default, Realm Functions have no Node.js modules available for import. If you would like to make use of any such modules, you can upload external dependencies to make them available to import into your Realm Functions.  \n  \n## Motivation \n  \nThis tutorial is born to show how we can create a retry mechanism for our functions. We have to keep in mind that triggers have their own internal automatic retry mechanism that ensures they are executed. However, functions lack such a mechanism. Realm functions are executed as HTTP requests, so it is our responsibility to create a mechanism to retry if they fail.  \n  \nNext, we will show how we can achieve this mechanism in a simple way that could be applied to any project.  \n  \n## Flow Diagram\n  \nThe main basis of this mechanism will be based on states. In this way, we will be able to contemplate **four different states**. Thus, we will have:  \n  \n*   **0: Not tried**: Initial state. When creating a new event that will need to be processed, it will be assigned the initial status **0**.  \n*   **1: Success**: Successful status. When an event is successfully executed through our function, it will be assigned this status so that it will not be necessary to retry again.  \n*   **2: Failed**: Failed status. When, after executing an event, it results in an error, it will be necessary to retry and therefore it will be assigned a status **2 or failed**.  \n*   **3: Error**: It is important to note that we cannot always retry. We must have a limit of retries. When this limit is exhausted, the status will change to **error or 3**.  \n  \nThe algorithm that will define the passage between states will be the following:   \n  \n<img alt=\"Flow diagram that shows the algorithm behind the state control for events\" src=\"https://miro.medium.com/max/1378/0*mitAXIBRT6nuYEYh\" width=\"689\" height=\"790\" srcSet=\"https://miro.medium.com/max/552/0*mitAXIBRT6nuYEYh 276w, https://miro.medium.com/max/1104/0*mitAXIBRT6nuYEYh 552w, https://miro.medium.com/max/1280/0*mitAXIBRT6nuYEYh 640w, https://miro.medium.com/max/1378/0*mitAXIBRT6nuYEYh 689w\" sizes=\"689px\"/>  \n  \nFlow diagram  \n  \n## System Architecture\n  \nThe system is based on two collections and a trigger. The trigger will be defined as a **database trigger** that will react each time there is an insert or update in a specific collection. The collection will keep track of the events that need to be processed. Each time this trigger is activated, the event is processed in a function linked to it. The function, when processing the event, may or may not fail, and we need to capture the failure to retry.  \n  \nWhen the function fails, the event state is updated in the event collection, and as the trigger reacts on inserts and updates, it will call the function again to reprocess the same.  \n  \nA maximum number of retries will be defined so that, once exhausted, the event will not be reprocessed and will be marked as an error in the **error** collection.  \n  \n## Sequence Diagram\n  \nThe following diagram shows the three use cases contemplated for this scenario.  \n  \n## Use Case 1:\n  \nA new document is inserted in the collection of events to be processed. Its initial state is **0 (new)** and the number of retries is **0**. The trigger is activated and executes the function for this event. The function is executed successfully and the event status is updated to **1 (success).**  \n  \n## Use Case 2:\n  \nA new document is inserted into the collection of events to be processed. Its initial state is **0 (new)** and the number of retries is **0.** The trigger is activated and executes the function for this event. The function fails and the event status is updated to **2 (failed)** and the number of retries is increased to **1**.  \n  \n## Use Case 3:\n  \nA document is updated in the collection of events to be processed. Its initial status is **2 (failed)** and the number of retries is less than the maximum allowed. The trigger is activated and executes the function for this event. The function fails, the status remains at **2 (failed),** and the counter increases. If the counter for retries is greater than the maximum allowed, the event is sent to the **error** collection and deleted from the event collection.  \n  \n## Use Case 4:\n  \nA document is updated in the event collection to be processed. Its initial status is **2 (failed)** and the number of retries is less than the maximum allowed. The trigger is activated and executes the function for this event. The function is executed successfully, and the status changes to **1 (success).**  \n  \n<img alt=\"Use cases sequence diagram\" src=\"https://miro.medium.com/max/1400/0*9juaBsYmfdWBuw27\" width=\"700\" height=\"708\" srcSet=\"https://miro.medium.com/max/552/0*9juaBsYmfdWBuw27 276w, https://miro.medium.com/max/1104/0*9juaBsYmfdWBuw27 552w, https://miro.medium.com/max/1280/0*9juaBsYmfdWBuw27 640w, https://miro.medium.com/max/1400/0*9juaBsYmfdWBuw27 700w\" sizes=\"700px\"/>  \n  \nSequence Diagram  \n  \n## Project Example Repository\n  \nWe can find a simple project that illustrates the above [here](https://github.com/josmanperez/realmFunctionRetry).  \n  \nThis project uses a trigger, **newEventsGenerator**, to generate a new document every two minutes through a cron job in the **Events** collection. This will simulate the creation of events to be processed.  \n  \nThe trigger **eventsProcessor** will be in charge of processing the events inserted or updated in the **Events** collection. To simulate a failure, a function is used that generates a random number and returns whether it is divisible or not by two. In this way, both states can be simulated.  \n  \n```  \nfunction getFailOrSuccess() { \n    // Random number between 1 and 10 \n    const number = Math.floor(Math.random() * 10) + 1; \n    return ((number % 2) === 0);\n}  \n```  \n  \n## Conclusion\n  \nThis tutorial illustrates in a simple way how we can create our own retry mechanism to increase the reliability of our application. Realm allows us to create our application completely serverless, and thanks to the Realm functions, we can define and execute the server-side logic for our application in the cloud.  \n  \nWe can use the functions to handle low-latency, short-lived connection logic, and other server-side interactions. Functions are especially useful when we want to work with multiple services, behave dynamically based on the current user, or abstract the implementation details of our client applications.  \n  \nThis retry mechanism we have just created will allow us to handle interaction with other services in a more robust way, letting us know that the action will be reattempted in case of failure.","description":"This tutorial is born to show how we can create a retry mechanism for our functions. Realm Functions allow you to define and execute server-side logic for your application. You can call functions from your client applications as well as from other functions and in JSON expressions throughout Realm. ","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltd05b12537cdaa0d0/644c47006d8c26dd4f944a5f/1_BCLtaH2nEmjcL3vT668Ccg.png?branch=prod","description":null}}]},"slug":"/build-your-own-function-retry-mechanism-with-realm","title":"*Build Your Own Function Retry Mechanism with Realm","original_publish_date":"2021-11-23T23:37:02.775Z","strapi_updated_at":"2022-07-19T16:19:21.975Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"https://medium.com/realm/build-your-own-function-retry-mechanism-with-realm-b3de60a0bc","meta_description":"Wondering what is all about? Realm is an object-oriented data model database that will persist data on disk, doesn’t need an ORM, and lets you write less code with full offline capabilities… but…","og_description":"Learn to create your own retry mechanism for your Realm functions.","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt597bf3174fbc591c/644c4702c605e47bd93512ea/https_miro.medium.com_max_781_0_9juaBsYmfdWBuw27?branch=prod"}}]},"og_type":"article","og_url":"https://medium.com/realm/build-your-own-function-retry-mechanism-with-realm-b3de60a0bc","twitter_creator":"@josman_perez"},"system":{"updated_at":"2023-04-28T22:52:08.053Z","publish_details":{"time":"2023-04-28T22:52:59.952Z"}}},{"calculated_slug":"/products/realm/realm-kotlin-0-6-0","content":"![](https://miro.medium.com/max/1400/1*MS0GvkkuVA8XszTdEnOhaw.jpeg)  \n  \nRealm Kotlin 0.6.0  \n==================  \n  \nWe just released v0.6.0 of [Realm Kotlin](https://github.com/realm/realm-kotlin/). It contains support for Kotlin/JVM, indexed fields as well as a number of bug fixes.  \n\n> **Build better mobile apps with Atlas Device Sync**: Atlas Device Sync is a fully-managed mobile backend-as-a-service. Leverage out-of-the-box infrastructure, data synchronization capabilities, built-in network handling, and much more to quickly launch enterprise-grade mobile apps. [Get started now by build: Deploy Sample for Free](https://www.mongodb.com/realm/register?tck=devhub_realmkotlin060)!\n  \nKotlin/JVM support  \n==================  \n  \nThe new Realm Kotlin SDK was designed from its inception to support Multiplatform. So far, we’ve been focusing on [KMM](https://kotlinlang.org/docs/kmm-overview.html) targets i.e Android and iOS but there was a push from the community to add JVM support, this is now possible using 0.6.0 by enabling the following DSL into your project:  \n  \n```  \nkotlin { jvm() // other targets …}  \n\n```  \n  \nNow your app can target:  \n  \nAndroid, iOS, macOS and JVM (Linux _since Centos 7_, macOS _x86\\_64_ and Windows _8.1 64_).  \n  \nWhat to build with Kotlin/JVM?  \n==============================  \n  \n*   You can build desktop applications using Compose Desktop (see examples: [MultiplatformDemo](https://github.com/realm/realm-kotlin-samples/tree/main/MultiplatformDemo) and [FantasyPremierLeague](https://github.com/joreilly/FantasyPremierLeague)).  \n*   You can build a classic Java console application (see [JVMConsole](https://github.com/realm/realm-kotlin-samples/tree/main/JVMConsole)).  \n*   You can run your Android tests on JVM (note there’s a current issue on IntelliJ where the execution of Android tests from the common source-set is not possible, see/upvote :) [https://youtrack.jetbrains.com/issue/KTIJ-15152](https://youtrack.jetbrains.com/issue/KTIJ-15152), alternatively you can still run them as a Gradle task).  \n  \nWhere is it installed?  \n======================  \n  \nThe native library dependency is extracted from the [cinterop-jar](https://repo1.maven.org/maven2/io/realm/kotlin/cinterop-jvm/0.6.0/) and installed into a default location on your machine:  \n  \n*   _Linux_:  \n  \n```  \n$HOME/.cache/io.realm.kotlin/  \n\n```  \n  \n*   _macOS:_  \n  \n```  \n$HOME/Library/Caches/io.realm.kotlin/  \n\n```  \n  \n*   _Windows:_  \n  \n```  \n%localappdata%\\io-realm-kotlin\\  \n\n```  \n  \nSupport Indexed fields  \n======================  \n  \nTo index a field, use the _@Index_ annotation. Like primary keys, this makes writes slightly slower, but makes reads faster. It’s best to only add indexes when you’re optimizing the read performance for specific situations.  \n  \nAbstracted public API into interfaces  \n=====================================  \n  \nIf you tried out the previous version, you will notice that we did an internal refactoring of the project in order to make public APIs consumable via interfaces instead of classes (ex: [Realm](https://docs.mongodb.com/realm-sdks/kotlin/0.6.0/-realm%20-kotlin%20-multiplatform%20-s-d-k/io.realm/-realm/index.html) and [RealmConfiguration](https://docs.mongodb.com/realm-sdks/kotlin/0.6.0/-realm%20-kotlin%20-multiplatform%20-s-d-k/io.realm/-realm-configuration/index.html)), this should increase decoupling and make mocking and testability easier for developers.  \n  \n🎉 Thanks for reading. Now go forth and build amazing apps with Realm! As always, we’re around on [GitHub](https://github.com/realm/realm-kotlin), [Twitter](https://twitter.com/realm) and [#realm](https://kotlinlang.slack.com/archives/C6J79CSDP) channel on the official [Kotlin Slack](https://kotlinlang.slack.com/).  \n  \nSee the full [changelog](https://github.com/realm/realm-kotlin/blob/master/CHANGELOG.md#060-2021-10-15) for all the details.","description":"We just released v0.6.0 of Realm Kotlin. It contains support for Kotlin/JVM, indexed fields as well as a number of bug fixes.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt32693af5bd7ba6be/644c47053ac54310e6a3f641/dev-tools.png?branch=prod","description":null}}]},"slug":"/realm-kotlin-0-6-0","title":"*Realm Kotlin 0.6.0.","original_publish_date":"2021-11-23T23:56:52.948Z","strapi_updated_at":"2022-10-19T12:25:36.741Z","expiry_date":"2022-11-23T23:48:16.368Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*News & Announcements","calculated_slug":"/news"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Kotlin","calculated_slug":"/languages/kotlin"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"https://medium.com/realm/realm-kotlin-0-6-0-baa26dcbbb9","meta_description":"We just released v0.6.0 of Realm Kotlin. It contains support for Kotlin/JVM, indexed fields as well as a number of bug fixes. The new Realm Kotlin SDK was designed from its inception to support…","og_description":"We just released v0.6.0 of Realm Kotlin. It contains support for Kotlin/JVM, indexed fields as well as a number of bug fixes.","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blta8ffa841f6305eac/644c4706d676972ab70af23b/https_miro.medium.com_max_1000_1_MS0GvkkuVA8XszTdEnOhaw.jpeg?branch=prod"}}]},"og_type":"article","og_url":"https://medium.com/realm/realm-kotlin-0-6-0-baa26dcbbb9","twitter_creator":"@nabil_hachicha"},"system":{"updated_at":"2023-04-28T22:52:07.628Z","publish_details":{"time":"2023-04-28T22:52:59.982Z"}}},{"calculated_slug":"/products/mongodb/designing-developing-analyzing-new-mongodb-shell","content":"There are many methods available for interacting with MongoDB and depending on what you're trying to accomplish, one way to work with MongoDB might be better than another. For example, if you're a power user of Visual Studio Code, then the [MongoDB Extension for Visual Studio Code](https://code.visualstudio.com/docs/azure/mongodb) might make sense. If you're constantly working with infrastructure and deployments, maybe the [MongoDB CLI](https://docs.mongodb.com/mongocli/stable/) makes the most sense. If you're working with data but prefer a command line experience, the MongoDB Shell is something you'll be interested in.\n\nThe [MongoDB Shell](https://www.mongodb.com/products/shell) gives you a rich experience to work with your data through syntax highlighting, intelligent autocomplete, clear error messages, and the ability to extend and customize it however you'd like.\n\nIn this article, we're going to look a little deeper at the things we can do with the MongoDB Shell.\n\n## Syntax Highlighting and Intelligent Autocomplete\n\nIf you're like me, looking at a wall of code or text that is a single color can be mind-numbing to you. It makes it difficult to spot things and creates overall strain, which could damage productivity. Most development IDEs don't have this problem because they have proper syntax highlighting, but it's common for command line tools to not have this luxury. However, this is no longer true when it comes to the MongoDB Shell because it is a command line tool that has syntax highlighting.\n\n![MongoDB Shell Syntax Highlighting](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/mongosh_syntax_highlight_3c9ebac7b9.png \"MongoDB Shell Syntax Highlighting\")\n\nWhen you write commands and view results, you'll see colors that match your command line setup as well as pretty-print formatting that is readable and easy to process.\n\nFormatting and colors are only part of the battle that typical command line advocates encounter. The other common pain-point, that the MongoDB Shell fixes, is around autocomplete. Most IDEs have autocomplete functionality to save you from having to memorize every little thing, but it is less common in command line tools.\n\nAs you're using the MongoDB Shell, simply pressing the \"Tab\" key on your keyboard will bring up valuable suggestions related to what you're trying to accomplish.\n\n![Autocomplete with the MongoDB Shell](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/mongosh_autocomplete_7a1ac066ae.gif \"Autocomplete with the MongoDB Shell\")\n\nSyntax highlighting, formatting, and autocomplete are just a few small things that can go a long way towards making the developer experience significantly more pleasant.\n\n## Error Messages that Actually Make Sense\n\nHow many times have you used a CLI, gotten some errors you didn't understand, and then either wasted half your day finding a missing comma or rage quit? It's happened to me too many times because of poor error reporting in whatever tool I was using.\n\nWith the MongoDB Shell, you'll get significantly better error reporting than a typical command line tool.\n\n![MongoDB Shell Error Reporting](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/mongosh_error_reporting_4455bb3eca.png \"MongoDB Shell Error Reporting\")\n\nIn the above example, I've forgotten a comma, something I do regularly along with colons and semi-colons, and it told me, along with providing a general area on where the comma should go. That's a lot better than something like \"Generic Runtime Error 0x234223.\"\n\n## Extending the MongoDB Shell with Plugins Known as Snippets\n\nIf you use the MongoDB Shell enough, you'll probably reach a point in time where you wish it did something specific to your needs on a repetitive basis. Should this happen, you can always extend the tool with snippets, which are similar to plugins.\n\nTo get an idea of some of the official MongoDB Shell snippets, execute the following from the MongoDB Shell:\n\n```bash\nsnippet search\n```\n\nThe above command searches the snippets found in a repository on [GitHub](https://github.com/mongodb-labs/mongosh-snippets).\n\nYou can always define your own repository of snippets, but if you wanted to use one of the available but optional snippets, you could run something like this:\n\n```bash\nsnippet install analyze-schema\n```\n\nThe above snippet allows you to analyze any collection that you specify. So in the example of my \"recipes\" collection, I could do the following:\n\n```bash\nuse food;\nschema(db.recipes);\n```\n\nThe results of the schema analysis, at least for my collection, is the following:\n\n```\n┌─────────┬───────────────┬───────────┬────────────┐\n│ (index) │       0       │     1     │     2      │\n├─────────┼───────────────┼───────────┼────────────┤\n│    0    │ '_id        ' │ '100.0 %' │ 'ObjectID' │\n│    1    │ 'ingredients' │ '100.0 %' │  'Array'   │\n│    2    │ 'name       ' │ '100.0 %' │  'String'  │\n└─────────┴───────────────┴───────────┴────────────┘\n```\n\nSnippets aren't the only way to extend functionality within the MongoDB Shell. You can also use Node.js in all its glory directly within the MongoDB Shell using custom scripts.\n\n## Using Node.js Scripts within the MongoDB Shell\n\nSo let's say you've got a data need that you can't easily accomplish with the [MongoDB Query API](https://www.mongodb.com/mongodb-query-api) or an aggregation pipeline. If you can accomplish what you need using Node.js, you can accomplish what you need in the MongoDB Shell.\n\nLet's take this example.\n\nSay you need to consume some data from a remote service and store it in MongoDB. Typically, you'd probably write an application, download the data, maybe store it in a file, and load it into MongoDB or load it with one of the programming drivers. You can skip a few steps and make your life a little easier.\n\nTry this.\n\nWhen you are connected to the MongoDB Shell, execute the following commands:\n\n```bash\nuse pokemon\n.editor\n```\n\nThe first will switch to a database—in this case, \"pokemon\"—and the second will open the editor. From the editor, paste in the following code:\n\n```javascript\nasync function getData(url) {\n    const fetch = require(\"node-fetch\");\n    const results = await fetch(url)\n        .then(response => response.json());\n    db.creatures.insertOne(results);\n}\n```\n\nThe above function will make use of the [node-fetch](https://www.npmjs.com/package/node-fetch) package from NPM. Then, using the package, we can make a request to a provided URL and store the results in a \"creatures\" collection.\n\nYou can execute this function simply by doing something like the following:\n\n```bash\ngetData(\"https://pokeapi.co/api/v2/pokemon/pikachu\");\n```\n\nIf it ran successfully, your collection should have new data in it.\n\nIn regards to the NPM packages, you can either install them globally or to your current working directory. The MongoDB Shell will pick them up when you need them.\n\nIf you'd like to use your own preferred editor rather than the one that the MongoDB Shell provides you, execute the following command prior to attempting to open an editor:\n\n```bash\nconfig.set(\"editor\", \"vi\");\n```\n\nThe above command will make VI the default editor to use from within the MongoDB Shell. More information on using an external editor can be found in the [documentation](https://docs.mongodb.com/mongodb-shell/reference/editor-mode/#using-an-external-editor).\n\n## Conclusion\n\nYou can do some neat things with the MongoDB Shell, and while it isn't for everyone, if you're a power user of the command line, it will certainly improve your productivity with MongoDB.\n\nIf you have questions, stop by the [MongoDB Community Forums](https://community.mongodb.com)!","description":"Learn about the benefits of using the new MongoDB Shell for interacting with databases, collections, and the data inside.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltdec947d2c3db85e2/644c4708a89f6204a5ec5820/docs_bb8ada8243.png?branch=prod","description":null}}]},"slug":"/designing-developing-analyzing-new-mongodb-shell","title":"*Designing, Developing, and Analyzing with the New MongoDB Shell","original_publish_date":"2021-12-01T16:15:03.494Z","strapi_updated_at":"2023-02-03T16:11:51.130Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Article","calculated_slug":"/articles"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Shell","calculated_slug":"/products/mongodb/shell"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Bash","calculated_slug":"/languages/bash"}},{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":"@nraboy"},"system":{"updated_at":"2023-04-28T22:52:07.247Z","publish_details":{"time":"2023-04-28T22:53:00.008Z"}}},{"calculated_slug":"/code-examples/python/test-har","content":"## First heading\n\nSome content goes hereghegbg;ngf\n\n\n\n<br />\ndsndvbdsf\n\n``` python\ndef f(x):\n    print(x)\n```\n\n\n::::tabs\n:::tab[]{tabid=\"Javascript\"}\n```\nconst j = 123\nconst i = 456\n```\n:::\n:::tab[]{tabid=\"Python\"}\n```\nj = 789\ni = 000\n```\n:::\n::::\n\n\n![tiger image dir](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/puppy_avif_31ad68d544.AVIF)\n\n<img alt=\"tiger image\" src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/puppy_avif_31ad68d544.AVIF\" />\n\n\n<figure align=\"center\">\n<img alt=\"Data Access Triangle with shared data, duplicated data and isolated data.\" src=\"https://www.mongodb.com/developer/images/article/six-principles-resilient-evolvability/triangle.png\" />\n</figure>\n\n\n## Second Heading\n\nSome additional content\n\n* Action item\n* Action item 2\n\n## Third Heading\n\nMore content, go to [DevHub home](https://developer.mongodb.com/)\n\n:charts[]{url=https://charts.mongodb.com/charts-storage-costs-sbekh id=740dea93-d2da-44c3-8104-14ccef947662}\n\n:charts[]{url=https://charts.mongodb.com/charts-open-data-covid-19-zddgb id=60da4f45-f168-4593-8e0e-d37ce88ffa15 theme=dark autorefresh=3600}\n\n:charts[]{url=https://charts.mongodb.com/charts-open-data-covid-19-zddgb id=60da4f45-f168-43e7-8a6d-d37ce88ffa30 theme=light autorefresh=3600}\n\n:charts[]{url=https://charts.mongodb.com/charts-open-data-covid-19-zddgb id=60da4f45-f168-42b4-8b88-d37ce88ffa3a theme=light autorefresh=3600}\n\n:charts[]{url=https://charts.mongodb.com/charts-open-data-covid-19-zddgb id=60da4f45-f168-44c9-87f5-d37ce88ffa34 theme=light autorefresh=3600}\n\n:charts[]{url=https://charts.mongodb.com/charts-open-data-covid-19-zddgb id=60da4f45-f168-41a8-8106-d37ce88ffa2c theme=dark autorefresh=3600}\n\n:charts[]{url=https://charts.mongodb.com/charts-open-data-covid-19-zddgb id=60da4f45-f168-4cdc-8686-d37ce88ff9fc theme=dark autorefresh=3600}\n\n:charts[]{url=https://charts.mongodb.com/charts-open-data-covid-19-zddgb id=60da4f45-f168-47fd-88bd-d37ce88ffa0d theme=light autorefresh=3600 width=760 height=1000}\n\n### Fourth heading\n\n<div style=\"width: 400px;\">:youtube[]{vid=iz37fDe1XoM} </div>\n\n| Heading 1 | Heading 2 |\n| :-------: | :--------- |\n|\n```cs\nabc=123\n```\n | <div style=\"width: 400px;\">:youtube[]{vid=iz37fDe1XoM} </div>|\n| <img src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/prof_27cd26e2be.JPG\" width=\"100px\"> | **Bold text** |\n\n> \n>:youtube[]{vid=iz37fDe1XoM}\n>\n\n:youtube[]{vid=iz37fDe1XoM}\n>\n>\n>:youtube[]{vid=8CZs-0it9r4 start=720 end=840}\n>\n>Prefer to learn by video? I've got you covered.\n>\n>\n\n:youtube[]{vid=8CZs-0it9r4 start=720}\n\n","description":"","imageConnection":{"edges":[]},"slug":"/test-har","title":"*testhar","original_publish_date":null,"strapi_updated_at":"2023-04-20T16:23:54.186Z","expiry_date":"2022-12-02T17:30:17.706Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Python","calculated_slug":"/languages/python"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Code Example","calculated_slug":"/code-examples"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Time series","calculated_slug":"/products/mongodb/time-series"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Python","calculated_slug":"/languages/python"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:06.884Z","publish_details":{"time":"2023-05-06T17:06:51.177Z"}}},{"calculated_slug":"/products/atlas/implement-full-text-search-graphql-api-atlas","content":"GraphQL can be an extremely powerful and efficient way to create APIs, and MongoDB Atlas makes it easy by allowing you to connect your collections to GraphQL schemas without writing a single line of code. I wrote about some of the basics behind configuring MongoDB Atlas for GraphQL in [an announcement](https://www.mongodb.com/developer/how-to/graphql-support-atlas-stitch/) tutorial a while back.\n\nAs you find yourself needing to do more advanced things with GraphQL, you're going to need to familiarize yourself with [custom resolvers](https://www.mongodb.com/docs/atlas/app-services/graphql/custom-resolvers/). If you can't map collection fields to a schema from within Atlas and you need to write custom logic, this is where the custom resolvers come into play. Take the example of needing to use an aggregation pipeline within MongoDB. The complex logic that you add to your aggregation pipeline isn't something you can map. The good news is that you don't need to abandon MongoDB Atlas for these scenarios, but you can leverage GraphQL custom resolvers within Atlas App Services instead.\n\nIn this tutorial, we're going to see how to create a custom resolver that implements [Atlas Search](https://www.mongodb.com/cloud/atlas/lp/search) for our GraphQL API using Atlas Functions, enabling you to add fast, relevant full-text search to your applications.\n\nTo be clear, MongoDB will instantly create a GraphQL API for basic CRUD operations without having to create a function or write any code. However, you can easily extend the functionality to do things like full-text search, which is what we're going to accomplish here.\n\n## The requirements\n\nTo be successful with this tutorial, you'll need a couple of things:\n\n- A [MongoDB Cloud](https://cloud.mongodb.com) account, free tier or higher.\n- A basic understanding of GraphQL, not specific to MongoDB Atlas.\n\nWe'll be using a MongoDB Atlas cluster for our data, an [Atlas function](https://www.mongodb.com/docs/atlas/app-services/functions/) for our custom resolver, and some basic Atlas App Services configurations for our API. All of this can be accessed from your MongoDB Cloud account.\n\nThe expectation is that your Atlas cluster already has network access rules, users, data, etc., all configured so we can jump right into the GraphQL side of things. We're also expecting that you have an App Services application created, even if it isn't configured. If you need help with configurations, check out [this tutorial](https://docs.atlas.mongodb.com/getting-started/) on the subject.\n\n## Configuring MongoDB Atlas to use GraphQL\n\nAt this point, you should at least have a blank Atlas application. To get up and running with GraphQL, we need to accomplish the following:\n\n- Configure authentication methods.\n- Define a JSON Schema.\n- Establish access rules for authenticated users.\n\nIt might sound like a lot of work, but our configuration is mostly point and click.\n\nWithin your App Services application, navigate to the “Authentication” tab. From this dashboard, you're going to want to enable \"API Keys\" and create a new API key.\n\nThe name of your API key is not too important for this example, but it could be if you're in production and need to keep track of your keys.\n\nAfter making note of your API key, click the “Schema” tab from within the dashboard.\n\nIf you don't already have a schema defined for the collection that you plan to use, click “Configure Collection” and then “Generate Schema” from within the “Schema” sub-tab. Generating a schema will analyze your collection and create a suitable schema for what it finds in the sample data. You can also define your own schema if you don't want it automatically generated.\n\nIt's not too important for this example, but my schema looks something like this:\n\n```json\n{\n    \"title\": \"recipe\",\n    \"properties\": {\n        \"_id\": {\n            \"bsonType\": \"objectId\"\n        },\n        \"ingredients\": {\n            \"bsonType\": \"array\",\n             \"items\": {\n                \"bsonType\": \"string\"\n            }\n        },\n        \"name\": {\n            \"bsonType\": \"string\"\n        }\n    }\n}\n```\n\nThe documents in my collection have three fields — a recipe name, a string-based array of ingredients, and a document id.\n\nThe final step to configure is the rules on who can access the data from your API.\n\nWithin the App Services dashboard, click the “Rules” tab.\n\nFind the collection you would like to apply a rule to and then make the default rule read-only by selecting the checkboxes. In a production scenario that is outside of the scope of this example, you'll probably want better-defined rules.\n\nAt this point, we can work on our custom resolver.\n\n## Creating a GraphQL Custom Resolver Function within Atlas\n\nThe GraphQL API for our collection should work as of now. You can test it with GraphiQL from the “GraphQL” tab or using your GraphQL client of choice.\n\nFrom the “GraphQL\" tab of the App Services dashboard, click the “Custom Resolvers” sub-tab. Click the “Add a Custom Resolver” button to be brought to a configuration screen.\n\nFrom this screen, we're going to configure the following:\n\n- GraphQL Field Name\n- Parent Type\n- Function\n- Input Type\n- Payload Type\n\nThe GraphQL field name is the field that you'll be using to query with. Since we are going to do an Atlas Search query, it might make sense to call it a `search` field.\n\nThe parent type is how we plan to access the field. Do we plan to access it as just another field within our collection, do we plan to use it as part of a mutation for creating or updating documents, or do we plan to use it for querying? Because we want to provide a search query, it makes sense to make the parent type a `Query` as the choice.\n\nThe function is where all the magic is going to happen. Choose to create a new function and give it a name that works for you. Before we start writing our custom resolver logic, let's complete the rest of the configuration.\n\nThe input type is the type of data that we plan to send to our function from a GraphQL query. Since we plan to provide a text search query, we plan to use string data. For this, choose `Scalar Type` and `String` when prompted. Search queries aren't limited to strings, so you could also use numerical or temporal if needed.\n\nFinally, we have the payload type, which is the expected response from the custom resolver. We're searching for documents so it makes sense to return however many of that document type come back. This means we can choose `Existing Type (List)` because we might receive more than one, and `[Recipe]` for the type. In my circumstance, `recipe` is the name of my collection and Atlas is referring to it as `Recipe` within the schema. Your existing type might differ.\n\nWe need to add some logic to the custom resolver function now.\n\nAdd the following JavaScript code:\n\n```javascript\nexports = async (query) => {\n    const cluster = context.services.get(\"mongodb-atlas\");\n    const recipes = cluster.db(\"food\").collection(\"recipes\");\n    const result = await recipes.aggregate([\n        {\n            \"$search\": {\n                \"text\": {\n                    \"query\": query,\n                    \"path\": \"name\",\n                    \"fuzzy\": {\n                        \"maxEdits\": 2,\n                        \"prefixLength\": 2\n                    }\n                }\n            }\n        }\n    ]).toArray();\n    return result;\n};\n```\n\nLet's break down what's happening for this particular resolver function.\n\n```javascript\nconst cluster = context.services.get(\"mongodb-atlas\");\nconst recipes = cluster.db(\"food\").collection(\"recipes\");\n```\n\nIn the above code, we are getting a handle to our `recipes` collection from within the `food` database. This is the database and collection I chose to use for this example so yours may differ depending on what you did for the previous steps of this tutorial.\n\nNext, we run a single-stage aggregation pipeline:\n\n```javascript\nconst result = await recipes.aggregate([\n    {\n        \"$search\": {\n            \"text\": {\n                \"query\": query,\n                \"path\": \"name\",\n                \"fuzzy\": {\n                    \"maxEdits\": 2,\n                    \"prefixLength\": 2\n                }\n            }\n        }\n    }\n]).toArray();\n```\n\nIn the above code, we are doing a text search using the client-provided `query` string. Remember, we defined an input type in the previous step. We're searching the `name` field for our documents and we're using a fuzzy search.\n\nThe results are transformed into an array and returned to the GraphQL client.\n\n## Create an Atlas Search Index for the Custom Resolver Function\nBefore we can make use of the function, we need to create an Atlas Search index because we are using the `$search` stage in our pipeline.\n\nIn the main Atlas dashboard, click \"Browse Collections\" for the deployment we plan to use. Next, click the \"Search\" tab where we'll be able to create and manage indexes specific to Atlas Search.\n\nThere are a few ways we can go about this index based on the code we have in our aggregation pipeline.\n\n1. We can create a very dynamic default index at the cost of performance.\n2. We can create specific index with static mappings for the fields we plan to use.\n\nWe're going to take a look at both, starting with the default index. You can use the JSON editor to add the following:\n\n```json\n{\n    \"mappings\": {\n        \"dynamic\": true\n    }\n}\n```\n\nThe above index will map every field, current, or future. If we created just this index and gave it a name of \"default,\" the function and GraphQL would work fine. However, performance would not be optimized so your performance results may vary.\n\nInstead, we could do something like the following:\n\n```json\n{\n    \"mappings\": {\n        \"dynamic\": false,\n        \"fields\": {\n            \"name\": {\n                \"type\": \"string\"\n            }\n        }\n    }\n}\n```\nWe know that we are searching on the `name` path in our aggregation pipeline and nothing else. The above index reflects that we are doing a static mapping on that one field. This is a better choice.\n\nIf you name the index \"default,\" you won't need to update your function code. However, if you name it something else, you can update your function code to the following:\n\n```javascript\nconst result = await recipes.aggregate([\n    {\n        \"$search\": {\n            \"index\": \"your_index_name\",\n            \"text\": {\n                \"query\": query,\n                \"path\": \"name\",\n                \"fuzzy\": {\n                    \"maxEdits\": 2,\n                    \"prefixLength\": 2\n                }\n            }\n        }\n    }\n]).toArray();\n```\nNotice that we've defined an index to use in the above code. If you want to learn more about Atlas Search indexes, check out a [previous tutorial](https://www.mongodb.com/developer/products/atlas/introduction-indexes-mongodb-atlas-search/) on the topic.\n\nBecause we have an Atlas Search index created, the GraphQL custom resolver should be usable.\n\n## Testing the GraphQL API and Atlas Search Query\n\nSo how can we confirm it’s working? We can test it in GraphiQL, Postman, or anything else that can make HTTP requests.\n\nFrom the “GraphQL” tab of the App Services dashboard, visit the “Explore” sub-tab if it isn't already selected.\n\nInclude the following in the GraphiQL editor:\n\n```\nquery {\n    recipes {\n        _id\n        ingredients\n        name\n    }\n    search(input:\"chip\") {\n        name\n    }\n}\n```\n\nThe first `recipes` query will return all documents in the collection while the second `search` query will return whatever is found in our function.\n\nWhile we didn't use the API key when using the GraphiQL editor that was included in the App Services dashboard, you'd need to use it in your own applications. With Atlas's authentication options, you can utilize Atlas Search via GraphQL in your client-side and backend applications.\n\n## Conclusion\n\nYou can add a lot of power to your GraphQL APIs with custom resolvers and when done with MongoDB Atlas Functions, you don't even need to deploy your own infrastructure. You can take full advantage of serverless and the entire MongoDB Atlas ecosystem.\n\nDon't forget to stop by the [MongoDB Community Forums](https://community.mongodb.com) to see what everyone else is doing with GraphQL!\n","description":"Learn how to add full-text search to your GraphQL API using custom resolvers and MongoDB Atlas Search.","imageConnection":{"edges":[]},"slug":"/implement-full-text-search-graphql-api-atlas","title":"*Implement Full-Text Search over a GraphQL API in Atlas","original_publish_date":"2021-12-08T16:47:21.581Z","strapi_updated_at":"2023-02-03T16:11:51.130Z","expiry_date":"2022-12-07T22:15:15.305Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Tutorial","calculated_slug":"/tutorials"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Search","calculated_slug":"/products/atlas/search"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*GraphQL","calculated_slug":"/technologies/graphql"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to add full-text search to your GraphQL API using custom resolvers and MongoDB Atlas Search.","og_description":"Learn how to add full-text search to your GraphQL API using custom resolvers and MongoDB Atlas Search.","og_imageConnection":{"edges":[]},"og_type":"","og_url":"","twitter_creator":"@nraboy"},"system":{"updated_at":"2023-04-28T22:52:06.516Z","publish_details":{"time":"2023-04-28T22:53:00.047Z"}}},{"calculated_slug":"/products/atlas/atlas-data-api-excel-power-query","content":"## Data Science and the Ubiquity of Excel\n\n\n> This tutorial discusses the preview version of the Atlas Data API which is now generally available with more features and functionality. Learn more about the GA version [here](https://www.mongodb.com/developer/products/atlas/atlas-data-api-introduction/).\n\nWhen you ask what tools you should learn to be a data scientist, you will hear names like *Spark, Jupyter notebooks, R, Pandas*, and *Numpy* mentioned. Many enterprise data wranglers, on the other hand, have been using, and continue to use, industry heavyweights like SAS, SPSS, and Matlab as they have for the last few decades.\n\nThe truth is, though, that the majority of back-office data science is still performed using the ubiquitous *Microsoft Excel*.\n\nExcel has been the go-to choice for importing, manipulating, analysing, and visualising data for 34 years and has more capabilities and features than most of us would ever believe. It would therefore be wrong to have a series on accessing data in MongoDB with the data API without including how to get data into Excel.\n\nThis is also unique in this series or articles in not requiring any imperative coding at all. We will use the Power Query functionality in Excel to both fetch raw data, and to push summarization tasks down to MongoDB and retrieve the results.\n\nThe [MongoDB Atlas Data API](https://docs.atlas.mongodb.com/api/data-api/) is an HTTPS-based API that allows us to read and write data in Atlas, where a MongoDB driver library is either not available or not desirable. In this article, we will see how a business analyst or other back-office user, who often may not be a professional Developer, can access data from, and record data, in Atlas. The Atlas Data API can easily be used by users, unable to create or configure back-end services, who simply want to work with data in tools they know like Google Sheets or Excel.\n\n## Prerequisites\n\nTo access the data API using Power Query in Excel, we will need a version of Excel that supports it. Power Query is only available on the Windows desktop version, not on a Mac or via the browser-based Office 365 version of Excel.\n\nWe will also need an Atlas cluster for which we have enabled the data API, and our **endpoint URL** and **API key**. You can learn how to get these in this [article](https://www.mongodb.com/developer/quickstart/atlas_data_api_introduction) or this [video](https://www.youtube.com/watch?v=46I0wZiTFi4) if you do not have them already.\n\nA common use-case of Atlas with Microsoft Excel sheets might be to retrieve some subset of business data to analyse or to produce an export for a third party. To demonstrate this, we first need to have some business data available in MongoDB Atlas, this can be added by selecting the three dots next to our cluster name and choosing \"Load Sample Dataset\" or following instructions [here](https://docs.mongodb.com/charts/saas/tutorial/order-data/prerequisites-setup/).\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot0_9ab3f45d71.png)\n\n## Using Excel Power Query with HTTPS POST Requests\n\nIf we open up a new blank Excel workbook and then go to the **Data** ribbon, we can see on the left-hand side an option to get data **From Web**. Unfortunately, Microsoft has chosen in the wizard that this launches, to restrict data retrieval to API's that use *GET* rather than *POST* as the HTTP verb to request data.\n\n> An HTTP GET request is passed all of its data as part of the URL, the values after the website and path encodes additional parts to the request, normally in a simple key-value format. A POST request sends the data as a second part of the request and is not subject to the same length and security limitations a GET has.\n\nHTTP *GET* is used for many simple read-only APIs, but the richness and complexity of queries and aggregations possible using the Atlas Data API. do not lend themselves to passing data in a GET rather than the body of a *POST*, so we are required to use a *POST* request instead.\n\nFortunately, Excel and Power Query do support *POST* requests when creating a query from scratch using what Microsoft calls a **Blank Query**.\n\nTo call a web service with a *POST* from Excel, start with a new **Blank Workbook**.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot1_3f58488b0d.png)\nClick on **Data** on the menu bar to show the Data Ribbon. Then click **Get Data** on the far left and choose **From Other Sources->Blank Query**. It's right at the bottom of the ribbon bar dropdown.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot2_8aa8883433.png)\n\nWe are then presented with the *Query Editor*.\n\nWe now need to use the *Advanced Editor* to define our 'JSON' payload, and send it via an HTTP *POST* request. Click **Advanced Editor** on the left to show the existing *Blank* Query.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Shot4_2d7ab676c5.png)\n\nThis has two blocks. The *let* part is a set of transformations to fetch and manipulate data and the *in* part defines what the final data set should be called.\n\nThis is using [*Power Query M*](https://docs.microsoft.com/en-us/powerquery-m/power-query-m-type-system) syntax. To help understand the next steps, let's summarise the syntax for that.\n## Power Query M syntax in a nutshell\nPower Query M can have constant strings and numbers. Constant strings are denoted by double quotes like \"MongoDB.\" Numbers are just the unquoted number alone, i.e., 5.23. Constants cannot be on the left side of an assignment.\n\nSomething not surrounded by quotes is a variable—e.g., *People* or *Source* and can be used either side of an assignment. To allow variable names to contain any character, including spaces, without ambiguity variables can also be declared as a hash symbol followed by double quotes so ` #\"Number of runs\"` is a variable name, not a constant.\n\n*Power Query M* defines arrays/lists of values as a comma separated list enclosed in braces (a.k.a. curly brackets) so `#\"State Names\" = { \"on\", \"off\", \"broken\" }` defines a variable called *State Names* as a list of three string values.\n\n*Power Query M* defines *Records* (Dynamic Key->Value mappings) using a comma separated set of `variable=value` statements inside square brackets, for example `Person = [Name=\"John\",Dogs=3]`. These data types can be nested—for example, P`erson = [Name=\"John\",Dogs={ [name=\"Brea\",age=10],[name=\"Harvest\",age=5],[name=\"Bramble\",age=1] }]`.\n\nIf you are used to pretty much any other programming language, you may find the contrarian syntax of *Power Query M* either amusing or difficult.\n\n## Defining a JSON Object to POST to the Atlas Data API with Power Query M\n\nWe can set the value of the variable Source to an explicit JSON object by passing a Power Query M Record to the function Json.FromValue like this.\n\n```\nlet\npostData = Json.FromValue([filter=[property_type=\"House\"],dataSource=\"Cluster0\", database=\"sample_airbnb\",collection=\"listingsAndReviews\"]),\nSource = postData\nin\nSource\n```\n\nThis is the request we are going to send to the Data API. This request will search the collection *listingsAndReviews* in a Cluster called *Cluster0* for documents where the field *property\\_type* equals \"*House*\".\n\nWe paste the code above into the advanced Editor, and verify that there is a green checkmark at the bottom with the words \"No syntax errors have been detected,\" and then we can click **Done**. We see a screen like this.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/Shot5_55925199f6.png)\n\nThe small CSV icon in the grey area represents our single JSON Document. Double click it and Power Query will apply a basic transformation to a table with JSON fields as values as shown below.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot6_884041d5ce.png)\n\n## Posting payload JSON to the Find Endpoint in Atlas from Excel\n\nTo get our results from Atlas, we need to post this payload to our Atlas *API find endpoint* and parse the response. Click **Advanced Editor** again and change the contents to those in the box below changing the value \"**data-amzuu**\" in the endpoint to match your endpoint and the value of **YOUR-API-KEY** to match your personal API key. You will also need to change **Cluster0** if your database cluster has a different name.\n\nYou will notice that two additional steps were added to the Query to convert it to the CSV we saw above. Overwrite these so the box just contains the lines below and click Done.\n\n```\nlet\npostData = Json.FromValue([filter=[property_type=\"House\"],dataSource=\"Cluster0\", database=\"sample_airbnb\",collection=\"listingsAndReviews\"]),\nresponse = Web.Contents( \"https://data.mongodb-api.com/app/data-amzuu/endpoint/data/beta/action/find\",\n[ Headers = [#\"Content-Type\" = \"application/json\",\n#\"api-key\"=\"YOUR-API-KEY\"] ,\nContent=postData]),\nSource = Json.Document(response)\nin\nSource\n```\n\nYou will now see this screen, which is telling us it has retrieved a list of JSON documents.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot7_bc05db3be9.png)\n\nBefore we go further and look at how to parse this result into our worksheet, let us first review the connection we have just set up.\n\nThe first line, as before, is defining *postData* as a JSON string containing the payload for the Atlas API.\n\nThe next line, seen below, makes an HTTPS call to Atlas by calling the Web.Contents function and puts the return value in the variable *response*.\n\n```\nresponse = Web.Contents(\n\"https://data.mongodb-api.com/app/data-amzuu/endpoint/data/beta/action/find\",\n[ Headers = [#\"Content-Type\" = \"application/json\",\n#\"api-key\"=\"YOUR-API-KEY\"] ,\nContent=postData]),\n```\n\nThe first parameter to *Web.Contents* is our endpoint URL as a string.\n\nThe second parameter is a *record* specifying options for the request. We are specifying two options: *Headers* and *Content*.\n\n*Headers* is a *record* used to specify the HTTP Headers to the request. In our case, we specify *Content-Type* and also explicitly include our credentials using a header named *api-key.*\n\n> Ideally, we would use the functionality built into Excel to handle web authentication and not need to include the API key in the query, but Microsoft has disabled this for POST requests out of security concerns with Windows federated authentication ([DataSource.Error: Web.Contents with the Content option is only supported when connecting anonymously](https://social.technet.microsoft.com/Forums/windows/en-US/cc1ba704-f1c2-4ba3-8faa-c4c5385ce1ac/json-query-datasourceerror-webcontents-with-the-content-option-is-only-supported-when?forum=powerquery)). We unfortunately need to, therefore, supply it explicitly as a header.\n\nWe also specify `Content=postData` , this is what makes this become a POST request rather than a GET request and pass our JSON payload to the HTTP API.\n\nThe next line `Source = Json.Document(response)` parses the JSON that gets sent back in the response, creating a Power Query *record* from the JSON data and assigning it to a variable named *Source.*\n\n## Converting documents from MongoDB Atlas into Excel Rows\n\nSo, getting back to parsing our returned data, we are now looking at something like this.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot7_bc05db3be9.png)\nThe parsed JSON has returned a single record with one value, documents, which is a list.In JSON it would look like this `{documents : [ { … }, { … } , { … } ] }`\n\nHow do we parse it? The first step is to press the **Into Table** button in the Ribbon bar which converts the record into a *table*.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot8_56fc39f43f.png)\nNow we have a table with one value 'Documents' of type list. We need to break that down.\n\nRight click the second column (**value**) and select **Drill Down** from the menu. As we do each of these stages, we see it being added to the list of transformations in the *Applied Steps* list on the right-hand side.\n\nWe now have a list of JSON documents but we want to convert that into rows.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot9_bbce12e75a.png)\nFirst, we want to right-click on the word **list** in row 1 and select **Drill Down** from the menu again.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot10_df92d35838.png)\n\nNow we have a set of records, convert them to a table by clicking the **To Table** button and setting the delimiter to **None** in the dialog that appears. We now see a table but with a single column called *Column1*.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot11_d6349e4a53.png)\nFinally, If you select the Small icon at the right-hand end of the column header you can choose which columns you want. Select all the columns then click **OK**.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot12_752ad672c0.png)\n\nFinally, click **Close and Load** from the ribbon bar to write the results back to the sheet and save the Query.\n\n## Parameterising Power Queries using JSON Parameters\n\nWe hardcoded this to fetch us properties of type \"House\"' but what if we want to perform different queries? We can use the Excel Power Query Parameters to do this.\n\nSelect the **Data** Tab on the worksheet. Then, on the left, **Get Data->Launch Power Query Editor**.\n\nFrom the ribbon of the editor, click **Manage Parameters** to open the parameter editor. Parameters are variables you can edit via the GUI or populate from functions. Click **New** (it's not clear that it is clickable) and rename the new parameter to **Mongo Query**. Wet the *type* to **Text** and the *current value* to **{ beds: 2 }**, then click **OK**.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot14_261da2add3.png)\nNow select **Query1** again on the left side of the window and click **Advanced Editor** in the ribbon bar. Change the source to match the code below. *Note that we are only changing the postData line.*\n\n```\nlet\npostData = Json.FromValue([filter=Json.Document(#\"Mongo Query\"),dataSource=\"Cluster0\", database=\"sample_airbnb\",collection=\"listingsAndReviews\"]),\nresponse = Web.Contents(\"https://data.mongodb-api.com/app/data-amzuu/endpoint/data/beta/action/find\",\n[ Headers = [#\"Content-Type\" = \"application/json\",\n#\"api-key\"= \"YOUR-API-KEY\"] , Content=postData]),\nSource = Json.Document(response),\ndocuments = Source[documents],\n#\"Converted to Table\" = Table.FromList(documents, Splitter.SplitByNothing(), null, null, ExtraValues.Error),\n#\"Expanded Column1\" = Table.ExpandRecordColumn(#\"Converted to Table\", \"Column1\", {\"_id\", \"listing_url\", \"name\", \"summary\", \"space\", \"description\", \"neighborhood_overview\", \"notes\", \"transit\", \"access\", \"interaction\", \"house_rules\", \"property_type\", \"room_type\", \"bed_type\", \"minimum_nights\", \"maximum_nights\", \"cancellation_policy\", \"last_scraped\", \"calendar_last_scraped\", \"first_review\", \"last_review\", \"accommodates\", \"bedrooms\", \"beds\", \"number_of_reviews\", \"bathrooms\", \"amenities\", \"price\", \"security_deposit\", \"cleaning_fee\", \"extra_people\", \"guests_included\", \"images\", \"host\", \"address\", \"availability\", \"review_scores\", \"reviews\"}, {\"Column1._id\", \"Column1.listing_url\", \"Column1.name\", \"Column1.summary\", \"Column1.space\", \"Column1.description\", \"Column1.neighborhood_overview\", \"Column1.notes\", \"Column1.transit\", \"Column1.access\", \"Column1.interaction\", \"Column1.house_rules\", \"Column1.property_type\", \"Column1.room_type\", \"Column1.bed_type\", \"Column1.minimum_nights\", \"Column1.maximum_nights\", \"Column1.cancellation_policy\", \"Column1.last_scraped\", \"Column1.calendar_last_scraped\", \"Column1.first_review\", \"Column1.last_review\", \"Column1.accommodates\", \"Column1.bedrooms\", \"Column1.beds\", \"Column1.number_of_reviews\", \"Column1.bathrooms\", \"Column1.amenities\", \"Column1.price\", \"Column1.security_deposit\", \"Column1.cleaning_fee\", \"Column1.extra_people\", \"Column1.guests_included\", \"Column1.images\", \"Column1.host\", \"Column1.address\", \"Column1.availability\", \"Column1.review_scores\", \"Column1.reviews\"})\nin\n#\"Expanded Column1\"\n```\n\nWhat we have done is make *postData* take the value in the *Mongo Query* parameter, and parse it as JSON. This lets us create arbitrary filters by specifying MongoDB queries in the Mongo Query Parameter. The changed line is shown below.\n\n```\npostData = Json.FromValue([filter=Json.Document(#\"Mongo Query\"), dataSource=\"Cluster0\",database=\"sample_airbnb\",collection=\"listingsAndReviews\"]),\n```\n\n## Running MongoDB Aggregation Pipelines from Excel\n\nWe can apply this same technique to run arbitrary MongoDB Aggregation Pipelines. Right click on Query1 in the list on the left and select Duplicate. Then right-click on Query1(2) and rename it to Aggregate. Select it and then click Advanced Editor on the ribbon. Change the word find in the URL to aggregate and the word filter in the payload to pipeline.\n\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot15_e46c51d1f1.png)\n\nYou will get an error at first like this.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot16_e67695172f.png)\nThis is because the parameter Mongo Query is not a valid Aggregation Pipeline. Click **Manage Parameters** on the ribbon and change the value to **[{$sortByCount : \"$beds\" }**]. Then Click the X next to *Expanded Column 1* on the right of the screen  as the expansion is now incorrect.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot17_c4335a6619.png)\n\nAgain, click on the icon next to **Column1** and Select **All Columns** to see how many properties there are for a given number of beds - processing the query with an aggregation pipeline on the server.\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot18_1ffc51b66e.png)\n![](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/shot19_403834713a.png)\n\n## Putting it all together\n\nUsing Power Query with parameters, we can specify the cluster, collection, database, and parameters such as the query, fields returned, sort order ,and limit. We can also choose, by changing the endpoint, to perform a simple query or run an aggregation pipeline.\n\nTo simplify this, there is an Excel workbook available [here](https://github.com/mongodb-developer/DataAPIExcel/raw/main/AtlasFromPowerQuery.xlsx) which has all of these things parameterised so you can simply set the parameters required and run the Power Query to query your Atlas cluster. You can use this as a starting point in exploring how to further use the Excel and Power Query to access data in MongoDB Atlas.","description":"This Article shows you how to run Queries and Aggregations again MongoDB Atlas using the Power Query function in Microsoft Excel.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt6e54dcfba9905160/644c4709bad93a39b7396f9a/twitter-mongoimport.png?branch=prod","description":null}}]},"slug":"/atlas-data-api-excel-power-query","title":"*Using the Atlas Data API from Excel with Power Query","original_publish_date":"2021-12-09T14:03:48.779Z","strapi_updated_at":"2022-08-26T19:55:34.284Z","expiry_date":"2022-12-09T10:35:43.944Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Data API","calculated_slug":"/products/atlas/data-api"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*Excel","calculated_slug":"/technologies/excel"}}]}},"seo":{"canonical_url":"","meta_description":"","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt20b0ba52f5f83245/644c470a3e6620fd619ef09d/twitter-mongoimport.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:06.132Z","publish_details":{"time":"2023-04-28T22:53:00.139Z"}}},{"calculated_slug":"/products/realm/realm-swift-query-api","content":"## Introduction\n\nI'm not a fan of writing code using pseudo-English text strings. It's a major context switch when you've been writing \"native\" code. Compilers don't detect errors in the strings, whether syntax errors or mismatched types, leaving you to learn of your mistakes when your app crashes.\n\nI spent more than seven years working at MySQL and Oracle, and still wasn't comfortable writing anything but the simplest of SQL queries. I left to join MongoDB because I knew that the object/document model was the way that developers should work with their data. I also knew that idiomatic queries for each programming language were the way to go.\n\nThat's why I was really excited when MongoDB acquired Realm—a leading mobile **object** database. You work with Realm objects in your native language (in this case, Swift) to manipulate your data.\n\nHowever, there was one area that felt odd in Realm's Swift SDK. You had to use `NSPredicate` when searching for Realm objects that match your criteria. `NSPredicate`s are strings with variable substitution. 🤦‍♂️\n\n`NSPredicate`s are used when searching for data in Apple's Core Data database, and so it was a reasonable design decision. It meant that iOS developers could reuse the skills they'd developed while working with Core Data.\n\nBut, I hate writing code as strings.\n\nThe good news is that the Realm SDK for Swift has added the option to use type-safe queries through the Realm Swift Query API. 🥳.\n\nYou now have the option whether to filter using `NSPredicate`s:\n\n```swift\nlet predicate = NSPredicate(format: \"isSoft == %@\", NSNumber(value: wantSoft)\nlet decisions = unfilteredDecisions.filter(predicate)\n```\n\nor with the new Realm Swift Query API:\n\n```swift\nlet decisions = unfilteredDecisions.where { $0.isSoft == wantSoft }\n```\n\nIn this article, I'm going to show you some examples of how to use the Realm Swift Query API. I'll also show you an example where wrangling with `NSPredicate` strings has frustrated me.\n\n> **Build better mobile apps with Atlas Device Sync**: Atlas Device Sync is a fully-managed mobile backend-as-a-service. Leverage out-of-the-box infrastructure, data synchronization capabilities, built-in network handling, and much more to quickly launch enterprise-grade mobile apps. [Get started now by build: Deploy Sample for Free](https://www.mongodb.com/realm/register?tck=devhub_goodbyenspredicate)!\n\n## Prerequisites\n\n- [Realm-Cocoa 10.19.0](https://github.com/realm/realm-cocoa/releases/tag/v10.19.0)+\n\n## Using The Realm Swift Query API\n\nI have a number of existing Realm iOS apps using `NSPredicate`s. When I learnt of the new query API, the first thing I wanted to do was try to replace some of \"legacy\" queries. I'll start by describing that experience, and then show what other type-safe queries are possible.\n\n### Replacing an NSPredicate\n\nI'll start with the example I gave in the introduction (and how the `NSPredicate` version had previously frustrated me).\n\nI have an [app to train you on what decisions to make in Black Jack](https://github.com/mongodb-developer/BlackJackTrainer) (based on the cards you've been dealt and the card that the dealer is showing). There are three different decision matrices based on the cards you've been dealt:\n\n- Whether you have the option to split your hand (you've been dealt two cards with the same value)\n- Your hand is \"soft\" (you've been dealt an ace, which can take the value of either one or eleven)\n- Any other hand\n\nAll of the decision-data for the app is held in [`Decisions`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Model/Decisions.swift) objects:\n\n```swift\nclass Decisions: Object, ObjectKeyIdentifiable {\n   @Persisted var decisions = List<DecisionList>()\n   @Persisted var isSoft = false\n   @Persisted var isSplit = false\n   ...\n}\n```\n\n[`SoftDecisionView`](https://github.com/mongodb-developer/BlackJackTrainer/blob/main/Black%20Jack%20Trainer/Black%20Jack%20Trainer/Views/Decision%20Matrix/SoftDecisionView.swift) needs to find the `Decisions` object where `isSoft` is set to `true`. That requires a simple `NSPredicate`:\n\n```swift\nstruct SoftDecisionView: View {\n   @ObservedResults(Decisions.self, filter: NSPredicate(format: \"isSoft == YES\")) var decisions\n   ...\n}\n```\n\nBut, what if I'd mistyped the attribute name? There's no Xcode auto-complete to help when writing code within a string, and this code builds with no errors or warnings:\n\n```swift\nstruct SoftDecisionView: View {\n   @ObservedResults(Decisions.self, filter: NSPredicate(format: \"issoft == YES\")) var decisions\n   ...\n}\n```\n\nWhen I run the code, it works initially. But, when I'm dealt a soft hand, I get this runtime crash:\n\n```\nTerminating app due to uncaught exception 'Invalid property name', reason: 'Property 'issoft' not found in object of type 'Decisions''\n```\n\nRather than having a dedicated view for each of the three types of hand, I want to experiment with having a single view to handle all three.\n\nSwiftUI doesn't allow me to use variables (or even named constants) as part of the filter criteria for `@ObservedResults`. This is because the `struct` hasn't been initialized until after the `@ObservedResults` is defined. To live within SwitfUIs constraints, the filtering is moved into the view's body:\n\n```swift\nstruct SoftDecisionView: View {\n   @ObservedResults(Decisions.self) var unfilteredDecisions\n   let isSoft = true\n\n   var body: some View {\n       let predicate = NSPredicate(format: \"isSoft == %@\", isSoft)\n       let decisions = unfilteredDecisions.filter(predicate)\n   ...\n}\n```\n\nAgain, this builds, but the app crashes as soon as I'm dealt a soft hand. This time, the error is much more cryptic:\n\n```\nThread 1: EXC_BAD_ACCESS (code=1, address=0x1)\n```\n\nIt turns out that, you need to convert the boolean value to an `NSNumber` before substituting it into the `NSPredicate` string:\n\n```swift\nstruct SoftDecisionView: View {\n   @ObservedResults(Decisions.self) var unfilteredDecisions\n\n\n   let isSoft = true\n\n\n   var body: some View {\n       let predicate = NSPredicate(format: \"isSoft == %@\", NSNumber(value: isSoft))\n       let decisions = unfilteredDecisions.filter(predicate)\n   ...\n}\n```\n\nWho knew? OK, StackOverflow did, but it took me quite a while to find the solution.\n\nHopefully, this gives you a feeling for why I don't like writing strings in place of code.\n\nThis is the same code using the new (type-safe) Realm Swift Query API:\n\n```swift\nstruct SoftDecisionView: View {\n   @ObservedResults(Decisions.self) var unfilteredDecisions\n   let isSoft = true\n\n   var body: some View {\n       let decisions = unfilteredDecisions.where { $0.isSoft == isSoft }\n   ...\n}\n```\n\nThe code's simpler, and (even better) Xcode won't let me use the wrong field name or type—giving me this error before I even try running the code:\n\n![Xcode showing the error \"Binary operator '==' cannot be applied to operands of type 'Query<Boo>' and 'Int'](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/xcode_type_warning_5e1c3b18e6.png \"Xcode showing the error Binary operator '==' cannot be applied to operands of type 'Query<Bool>' and 'Int'\")\n\n### Experimenting With Other Sample Queries\n\nIn my [RCurrency app](https://github.com/realm/RCurrency), I was able to replace this `NSPredicate`-based code:\n\n```swift\nstruct CurrencyRowContainerView: View {\n   @ObservedResults(Rate.self) var rates\n   let baseSymbol: String\n   let symbol: String\n\n   var rate: Rate? {\n       NSPredicate(format: \"query.from = %@ AND query.to = %@\", baseSymbol, symbol)).first\n   }\n   ...\n}\n```\n\nWith this:\n\n```swift\nstruct CurrencyRowContainerView: View {\n   @ObservedResults(Rate.self) var rates\n   let baseSymbol: String\n   let symbol: String\n\n   var rate: Rate? {\n       rates.where { $0.query.from == baseSymbol && $0.query.to == symbol }.first\n   }\n   ...\n}\n```\n\nAgain, I find this more Swift-like, and bugs will get caught as I type/build rather than when the app crashes.\n\nI'll use this simple `Task` `Object` to show a few more example queries:\n\n```swift\nclass Task: Object, ObjectKeyIdentifiable {\n   @Persisted var name = \"\"\n   @Persisted var isComplete = false\n   @Persisted var assignee: String?\n   @Persisted var priority = 0\n   @Persisted var progressMinutes = 0\n}\n```\n\nAll in-progress tasks assigned to name:\n\n```swift\nlet myStartedTasks = realm.objects(Task.self).where {\n   ($0.progressMinutes > 0) && ($0.assignee == name)\n}\n```\n\nAll tasks where the `priority` is higher than `minPriority`:\n\n```swift\nlet highPriorityTasks = realm.objects(Task.self).where {\n   $0.priority >= minPriority\n}\n```\n\nAll tasks that have a `priority` that's an integer between `-1` and `minPriority`:\n\n```swift\nlet lowPriorityTasks = realm.objects(Task.self).where {\n   $0.priority.contains(-1...minPriority)\n}\n```\n\nAll tasks where the `assignee` name string includes `namePart`:\n\n```swift\nlet tasksForName = realm.objects(Task.self).where {\n   $0.assignee.contains(namePart)\n}\n```\n\n### Filtering on Sub-Objects\n\nYou may need to filter your Realm objects on values within their sub-objects. Those sub-object may be `EmbeddedObject`s or part of a `List`.\n\nI'll use the `Project` class to illustrate filtering on the attributes of sub-documents:\n\n```swift\nclass Project: Object, ObjectKeyIdentifiable {\n   @Persisted var name = \"\"\n   @Persisted var tasks: List<Task>\n}\n```\n\nAll projects that include a task that's in-progress, and is assigned to a given user:\n\n```swift\nlet myActiveProjects = realm.objects(Project.self).where {\n   ($0.tasks.progressMinutes >= 1) && ($0.tasks.assignee == name)\n}\n```\n\n### Including the Query When Creating the Original Results (SwiftUI)\n\nAt the time of writing, this feature wasn't released, but it can be tested using [this PR](https://github.com/realm/realm-cocoa/pull/7518).\n\nYou can include the where modifier directly in your `@ObservedResults` call. That avoids the need to refine your results inside your view's body:\n\n```swift\n@ObservedResults(Decisions.self, where: { $0.isSoft == true }) var decisions\n```\n\nUnfortunately, SwiftUI rules still mean that you can't use variables or named constants in your `where` block for `@ObservedResults`.\n\n## Conclusion\n\nRealm type-safe queries provide a simple, idiomatic way to filter results in Swift. If you have a bug in your query, it should be caught by Xcode rather than at run-time.\n\nYou can find more information in the [docs](https://docs.mongodb.com/realm/sdk/ios/examples/filter-data/#std-label-ios-realm-swift-query-api). If you want to see hundreds of examples, and how they map to equivalent `NSPredicate` queries, then take a look at the [test cases](https://github.com/realm/realm-cocoa/blob/v10.19.0/RealmSwift/Tests/QueryTests.swift).\n\nFor those that prefer working with `NSPredicate`s, you can continue to do so. In fact the Realm Swift Query API runs on top of the `NSPredicate` functionality, so they're not going anywhere soon.\n\nPlease provide feedback and ask any questions in the [Realm Community Forum](https://www.mongodb.com/community/forums/c/realm-sdks/58).","description":"New type-safe queries in Realm's Swift SDK","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt7526a5a12ce962f0/644c470c2d3da47aa782ed13/TSQ_-_ATF_720x720.png?branch=prod","description":null}}]},"slug":"/realm-swift-query-api","title":"*Goodbye NSPredicate, hello Realm Swift Query API","original_publish_date":"2021-12-13T14:00:00.222Z","strapi_updated_at":"2022-10-19T12:24:19.869Z","expiry_date":"2022-12-10T12:51:08.793Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*News & Announcements","calculated_slug":"/news"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Realm","calculated_slug":"/products/realm"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Swift","calculated_slug":"/languages/swift"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"New Realm Swift Query API supersedes NSPredicate with type-safe queries","og_description":"New Realm Swift Query API supersedes NSPredicate with type-safe queries","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt20597265732493f2/644c470d6988f4b210d76501/TSQ_-_OG_1200x630.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":""},"system":{"updated_at":"2023-04-28T22:52:05.732Z","publish_details":{"time":"2023-04-28T22:53:00.174Z"}}},{"calculated_slug":"/products/mongodb/bson-data-types-date","content":"<div>\n    <img style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-bson.png\"\n        alt=\"BSON Quickstart badge\"\n    />\n\nDates and times in programming can be a challenge. Which Time Zone is the event happening in? What date format is being used? Is it `MM/DD/YYYY` or `DD/MM/YYYY`? Settling on a standard is important for data storage and then again when displaying the date and time. The recommended way to store dates in MongoDB is to use the BSON Date data type.\n</div>\n\nThe [BSON Specification](http://bsonspec.org/spec.html) refers to the `Date` type as the *UTC datetime* and is a 64-bit integer. It represents the number of milliseconds since the [Unix epoch](https://en.wikipedia.org/wiki/Unix_time), which was 00:00:00 UTC on 1 January 1970. This provides a lot of flexibilty in past and future dates. With a 64-bit integer in use, we are able to represent dates *roughly* 290 million years before and after the epoch. As a signed 64-bit integer we are able to represent dates *prior* to 1 Jan 1970 with a negative number and positive numbers represent dates *after* 1 Jan 1970.\n\n## Why & Where to Use\n\nYou'll want to use the `Date` data type whenever you need to store date and/or time values in MongoDB. You may have seen a `timestamp` data type as well and thought \"Oh, that's what I need.\" However, the `timestamp` data type should be left for **internal** usage in MongoDB. The `Date` type is the data type we'll want to use for application development.\n\n## How to Use\n\nThere are some benefits to using the `Date` data type in that it comes with some handy features and methods. Need to assign a `Date` type to a variable? We have you covered there:\n\n``` javascript\nvar newDate = new Date();\n```\n\nWhat did that create exactly?\n\n``` none\n> newDate;\nISODate(\"2020-05-11T20:14:14.796Z\")\n```\n\nVery nice, we have a date and time wrapped as an ISODate. If we need that printed in a `string` format, we can use the `toString()` method.\n\n``` none\n> newDate.toString();\nMon May 11 2020 13:14:14 GMT-0700 (Pacific Daylight Time)\n```\n\n## Wrap Up\n\n>Get started exploring BSON types, like Date, with [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) today!\n\nThe `date` field is the recommended data type to use when you want to store date and time information in MongoDB. It provides the flexibility to store date and time values in a consistent format that can easily be stored and retrieved by your application. Give the BSON `Date` data type a try for your applications.","description":"Working with dates and times can be a challenge. The Date BSON data type is an unsigned 64-bit integer with a UTC (Universal Time Coordinates) time zone.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt38de6a12bfd5908c/644c470ff8f8f5710768e3a0/bson.png?branch=prod","description":null}}]},"slug":"/bson-data-types-date","title":"*Quick Start: BSON Data Types - Date","original_publish_date":"2022-01-31T15:42:42.440Z","strapi_updated_at":"2022-09-23T00:31:53.901Z","expiry_date":"2022-12-14T02:01:55.024Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Working with dates and times can be a challenge. The Date BSON data type is an unsigned 64-bit integer with a UTC (Universal Time Coordinates) time zone.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt6227a07c9e70831f/644c47113e662029ad9ef0a1/og-blue-pattern.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@kenwalger"},"system":{"updated_at":"2023-04-28T22:52:05.366Z","publish_details":{"time":"2023-04-28T22:53:00.205Z"}}},{"calculated_slug":"/products/mongodb/bson-data-types-decimal128","content":"<div>\n    <img style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n        src=\"https://devhub-cms.corp.mongodb.com/public/new-articles/qs-badges/qs-badge-bson.png\"\n        alt=\"BSON Quickstart badge\"\n    />\n\nThink back to when you were first introduced to the concept of decimals in numerical calculations. Doing math problems along the lines of 3.231 / 1.28 caused problems when starting out because 1.28 doesn't go into 3.231 evenly. This causes a long string of numbers to be created to provide a more precise answer. In programming languages, we must choose which number format is correct depending on the amount of precision we need. When one needs high precision when working with [BSON data types](http://bsonspec.org/), the `decimal128` is the one to use.\n</div>\n\nAs the name suggests, decimal128 provides 128 bits of decimal representation for storing really big (or really small) numbers when rounding decimals exactly is important. Decimal128 supports 34 decimal digits of precision, or [significand](https://en.wikipedia.org/wiki/Significand) along with an exponent range of -6143 to +6144. The significand is not normalized in the decimal128 standard allowing for multiple possible representations: 10 x 10^-1 = 1 x 10^0 = .1 x 10^1 = .01 x 10^2 and so on. Having the ability to store maximum and minimum values in the order of 10^6144 and 10^-6143, respectively, allows for a lot of precision.\n\n![Decimal128 Precision Details](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/decimal128-precision-details.png)\n\n## Why & Where to Use\n\nSometimes when doing mathematical calculations in a programmatic way, results are unexpected. For example in Node.js:\n\n``` bash\n> 0.1\n0.1\n> 0.2\n0.2\n> 0.1 * 0.2\n0.020000000000000004\n> 0.1 + 0.1\n0.010000000000000002\n```\n\nThis issue is not unique to Node.js, in Java:\n\n``` java\nclass Main {\n public static void main(String[] args) {\n   System.out.println(\"0.1 * 0.2:\");\n   System.out.println(0.1 * 0.2);\n }\n}\n```\n\nProduces an output of:\n\n``` bash\n0.1 * 0.2:\n0.020000000000000004\n```\n\nThe same computations in Python, Ruby, Rust, and others produce the same results. What's going on here? Are these languages just bad at math? Not really, binary floating-point numbers just aren't great at representing base 10 values. For example, the `0.1` used in the above examples is represented in binary as `0.0001100110011001101`.\n\nFor many situations, this isn't a huge issue. However, in monetary applications precision is very important. Who remembers the [half-cent issue from Superman III](https://www.youtube.com/watch?v=iLw9OBV7HYA)?  When precision and accuracy are important for computations, decimal128 should be the data type of choice.\n\n## How to Use\n\nIn MongoDB, storing data in decimal128 format is relatively straight forward with the [NumberDecimal()](https://docs.mongodb.com/manual/core/shell-types/#shell-type-decimal) constructor:\n\n``` bash\nNumberDecimal(\"9823.1297\")\n```\n\nPassing in the decimal value as a string, the value gets stored in the database as:\n\n``` bash\nNumberDecimal(\"9823.1297\")\n```\n\nIf values are passed in as `double` values:\n\n``` bash\nNumberDecimal(1234.99999999999)\n```\n\nLoss of precision can occur in the database:\n\n``` bash\nNumberDecimal(\"1234.50000000000\")\n```\n\nAnother consideration, beyond simply the usage in MongoDB, is the usage and support your programming has for decimal128. Many languages don't natively support this feature and will require a plugin or additional package to get the functionality. Some examples...\n\nPython: The [`decimal.Decimal`](https://docs.python.org/3/library/decimal.html) module can be used for floating-point arithmetic.\n\nJava: The [Java BigDecimal](https://docs.oracle.com/javase/1.5.0/docs/api/java/math/BigDecimal.html) class provides support for decimal128 numbers.\n\nNode.js: There are several packages that provide support, such as [js-big-decimal](https://www.npmjs.com/package/js-big-decimal) or [node.js bigdecimal](https://www.npmjs.com/package/bigdecimal) available on [npm](https://www.npmjs.com).\n\n## Wrap Up\n\n>Get started exploring BSON types, like decimal128, with [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) today!\n\nThe `decimal128` field came about in August 2009 as part of the [IEEE 754-2008](https://en.wikipedia.org/wiki/IEEE_754-2008_revision) revision of floating points. MongoDB 3.4 is when support for decimal128 first appeared and to use the `decimal` data type with MongoDB, you'll want to make sure you [use a driver](https://docs.mongodb.com/ecosystem/drivers/) version that supports this great feature. Decimal128 is great for huge (or very tiny) numbers and for when precision in those numbers is important.","description":"Working with decimal numbers can be a challenge. The Decimal128 BSON data type allows for high precision options when working with numbers.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt01c5224b591c1e8b/644c47152992e662e22143ee/bson.png?branch=prod","description":null}}]},"slug":"/bson-data-types-decimal128","title":"*Quick Start: BSON Data Types - Decimal128","original_publish_date":"2022-01-31T15:48:11.019Z","strapi_updated_at":"2022-09-23T14:27:59.926Z","expiry_date":"2022-12-14T02:02:02.975Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Working with decimal numbers can be a challenge. The Decimal128 BSON data type allows for high precision options when working with numbers.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt6f6e06a94f070dfe/644c47178dd7fea8ff73733b/og-mdb-developer.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@kenwalger"},"system":{"updated_at":"2023-04-28T22:52:04.983Z","publish_details":{"time":"2023-04-28T22:53:00.233Z"}}},{"calculated_slug":"/products/mongodb/bson-data-types-objectid","content":"<div>\n    <img style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-bson.png\"\n        alt=\"BSON Quickstart badge\"\n    />\n\nIn the database world, it is frequently important to have unique identifiers associated with a record. In a legacy, tabular database, these unique identifiers are often used as primary keys. In a modern database, such as MongoDB, we need a unique identifier in an `_id` field as a primary key as well. MongoDB provides an automatic unique identifier for the `_id` field in the form of an `ObjectId` data type.\n\n</div>\n\nFor those that are familiar with MongoDB [Documents](https://alger.me/mongodb-documents) you've likely come across the `ObjectId` data type in the `_id` field. For those unfamiliar with MongoDB Documents, the [ObjectId](https://alger.me/mongodb-objectid) datatype is automatically generated as a unique document identifier if no other identifier is provided. But what is an `ObjectId` field? What makes them unique? This post will unveil some of the magic behind the BSON ObjectId data type.  First, though, what is BSON?\n\n## Binary JSON (BSON)\n\nMany programming languages have JavaScript Object Notation (JSON) support or similar data structures. MongoDB uses JSON documents to store records. However, behind the scenes, MongoDB represents these documents in a binary-encoded format called [BSON](http://bsonspec.org/). BSON provides additional data types and ordered fields to allow for efficient support across a variety of languages. One of these additional data types is ObjectId.\n\n## Makeup of an ObjectId\n\nLet's start with an examination of what goes into an ObjectId. If we take a look at the construction of the ObjectId value, in its current implementation, it is a 12-byte hexadecimal value. This 12-byte configuration is smaller than a typical [universally unique identifier](https://alger.me/uuid-defined) (UUID), which is, typically, 128-bits. Beginning in MongoDB 3.4, an ObjectId consists of the following values:\n\n-   4-byte value representing the seconds since the [Unix epoch](https://en.wikipedia.org/wiki/Unix_time),\n-   5-byte random value, and\n-   3-byte counter, starting with a random value.\n\n![BSON ObjectId diagram](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/objectid.png)\n\nWith this makeup, ObjectIds are *likely* to be globally unique and unique per collection. Therefore, they make a good candidate for the unique requirement of the `_id` field. While the `_id` in a collection can be an auto-assigned `ObjectId`, it can be user-defined as well, as long as it is unique within a collection. Remember that if you aren't using a MongoDB generated `ObjectId` for the `_id` field, the application creating the document will have to ensure the value is unique.\n\n## History of ObjectId\n\nThe makeup of the ObjectId has changed over time. Through version 3.2, it consisted of the following values:\n\n-   4-byte value representing the seconds since the Unix epoch,\n-   3-byte machine identifier,\n-   2-byte process id, and\n-   3-byte counter, starting with a random value.\n\nThe change from including a machine-specific identifier and process id to a random value increased the likelihood that the `ObjectId` would be globally unique. These machine-specific 5-bytes of information became less likely to be random with the prevalence of Virtual Machines (VMs) that had the same MAC addresses and processes that started in the same order. While it still isn't guaranteed, removing machine-specific information from the `ObjectId` increases the chances that the same machine won't generate the same `ObjectId`.\n\n## ObjectId Odds of Uniqueness\n\nThe randomness of the last eight bytes in the current implementation makes the likelihood of the same ObjectId being created pretty small.  How small depends on the number of inserts per second that your application does. Let's do some quick math and look at the odds.\n\nIf we do one insert per second, the first four bytes of the ObjectId would change so we can't have a duplicate ObjectId. What are the odds though when multiple documents are inserted in the same second that *two* ObjectIds are the same? Since there are *eight* bits in a byte, and *eight* random bytes in our Object Id (5 random + 3 random starting values), the denominator in our odds ratio would be 2^(8\\*8), or 1.84467441x10'^19. For those that have forgotten scientific notation, that's 18,446,744,100,000,000,000. Yes, that's correct, 18 quintillion and change. As a bit of perspective, the odds of being struck by lightning in the U.S. in a given year are 1 in 700,000, according to [National Geographic](https://www.nationalgeographic.com/news/2005/6/flash-facts-about-lightning/).  The odds of winning the [Powerball Lottery](https://www.powerball.com/games/home) jackpot are 1 in 292,201,338. The numerator in our odds equation is the number of documents per second. Even in a write-heavy system with 250 million writes/second, the odds are, while not zero, pretty good against duplicate ObjectIds being generated.\n\n## Wrap Up\n\n>Get started exploring BSON types, like ObjectId, with [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) today!\n\n\nObjectId is one data type that is part of the [BSON Specification](http://bsonspec.org/) that MongoDB uses for data storage. It is a binary representation of JSON and includes [other data types](https://www.mongodb.com/blog/post/the-top-12-bson-data-types-you-wont-find-in-json) beyond those defined in JSON. It is a powerful data type that is incredibly useful as a unique identifier in MongoDB Documents.","description":"MongoDB provides an automatic unique identifier for the _id field in the form of an ObjectId data type.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt773b020ef1998c4c/644c471af8f8f51ca168e3a4/bson.png?branch=prod","description":null}}]},"slug":"/bson-data-types-objectid","title":"*Quick Start: BSON Data Types - ObjectId","original_publish_date":"2022-01-31T15:53:08.493Z","strapi_updated_at":"2022-09-23T13:42:08.258Z","expiry_date":"2022-12-14T02:02:10.927Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"MongoDB provides an automatic unique identifier for the _id field in the form of an ObjectId data type.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt81faf23d177efec6/644c471cec64057cb6ea3929/og-mdb-developer.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@kenwalger"},"system":{"updated_at":"2023-04-28T22:52:04.591Z","publish_details":{"time":"2023-04-28T22:53:00.260Z"}}},{"calculated_slug":"/products/mongodb/cheat-sheet","content":"First steps in the MongoDB World? This cheat sheet is filled with some handy tips, commands, and quick references to get you connected and CRUD'ing in no time!\n\n-   Get a [free MongoDB cluster](/quickstart/free-atlas-cluster) in [MongoDB Atlas](https://www.mongodb.com/cloud/atlas).\n-   Follow a course in [MongoDB University](https://university.mongodb.com/).\n\n## Table of Contents\n\n-   [Connect MongoDB Shell](#connect-mongodb-shell)\n-   [Helpers](#helpers)\n-   [CRUD](#crud)\n-   [Databases and Collections](#databases-and-collections)\n-   [Indexes](#indexes)\n-   [Handy commands](#handy-commands)\n-   [Change Streams](#change-streams)\n-   [Replica Set](#replica-set)\n-   [Sharded Cluster](#sharded-cluster)\n-   [Wrap-up](#wrap-up)\n\n## Connect MongoDB Shell\n\n``` bash\nmongo # connects to mongodb://127.0.0.1:27017 by default\nmongo --host <host> --port <port> -u <user> -p <pwd> # omit the password if you want a prompt\nmongo \"mongodb://192.168.1.1:27017\"\nmongo \"mongodb+srv://cluster-name.abcde.mongodb.net/<dbname>\" --username <username> # MongoDB Atlas\n```\n\n-   [More documentation about the MongoDB Shell](https://docs.mongodb.com/manual/mongo/).\n-   [To connect with the new mongosh](https://docs.mongodb.com/mongodb-shell/connect#std-label-mdb-shell-connect), just replace `mongo` by `mongosh`.\n\n🔝 [Table of Contents](#table-of-contents) 🔝\n\n## Helpers\n\n### Show Databases\n\n``` javascript\nshow dbs\ndb // prints the current database\n```\n\n### Switch Database\n\n``` javascript\nuse <database_name>\n```\n\n### Show Collections\n\n``` javascript\nshow collections\n```\n\n### Run JavaScript File\n\n``` javascript\nload(\"myScript.js\")\n```\n\n🔝 [Table of Contents](#table-of-contents) 🔝\n\n## CRUD\n\n### Create\n\n``` javascript\ndb.coll.insertOne({name: \"Max\"})\ndb.coll.insert([{name: \"Max\"}, {name:\"Alex\"}]) // ordered bulk insert\ndb.coll.insert([{name: \"Max\"}, {name:\"Alex\"}], {ordered: false}) // unordered bulk insert\ndb.coll.insert({date: ISODate()})\ndb.coll.insert({name: \"Max\"}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}})\n```\n\n### Read\n\n``` javascript\ndb.coll.findOne() // returns a single document\ndb.coll.find()    // returns a cursor - show 20 results - \"it\" to display more\ndb.coll.find().pretty()\ndb.coll.find({name: \"Max\", age: 32}) // implicit logical \"AND\".\ndb.coll.find({date: ISODate(\"2020-09-25T13:57:17.180Z\")})\ndb.coll.find({name: \"Max\", age: 32}).explain(\"executionStats\") // or \"queryPlanner\" or \"allPlansExecution\"\ndb.coll.distinct(\"name\")\n\n// Count\ndb.coll.count({age: 32})          // estimation based on collection metadata\ndb.coll.estimatedDocumentCount()  // estimation based on collection metadata\ndb.coll.countDocuments({age: 32}) // alias for an aggregation pipeline - accurate count\n\n// Comparison\ndb.coll.find({\"year\": {$gt: 1970}})\ndb.coll.find({\"year\": {$gte: 1970}})\ndb.coll.find({\"year\": {$lt: 1970}})\ndb.coll.find({\"year\": {$lte: 1970}})\ndb.coll.find({\"year\": {$ne: 1970}})\ndb.coll.find({\"year\": {$in: [1958, 1959]}})\ndb.coll.find({\"year\": {$nin: [1958, 1959]}})\n\n// Logical\ndb.coll.find({name:{$not: {$eq: \"Max\"}}})\ndb.coll.find({$or: [{\"year\" : 1958}, {\"year\" : 1959}]})\ndb.coll.find({$nor: [{price: 1.99}, {sale: true}]})\ndb.coll.find({\n  $and: [\n    {$or: [{qty: {$lt :10}}, {qty :{$gt: 50}}]},\n    {$or: [{sale: true}, {price: {$lt: 5 }}]}\n  ]\n})\n\n// Element\ndb.coll.find({name: {$exists: true}})\ndb.coll.find({\"zipCode\": {$type: 2 }})\ndb.coll.find({\"zipCode\": {$type: \"string\"}})\n\n// Aggregation Pipeline\ndb.coll.aggregate([\n  {$match: {status: \"A\"}},\n  {$group: {_id: \"$cust_id\", total: {$sum: \"$amount\"}}},\n  {$sort: {total: -1}}\n])\n\n// Text search with a \"text\" index\ndb.coll.find({$text: {$search: \"cake\"}}, {score: {$meta: \"textScore\"}}).sort({score: {$meta: \"textScore\"}})\n\n// Regex\ndb.coll.find({name: /^Max/})   // regex: starts by letter \"M\"\ndb.coll.find({name: /^Max$/i}) // regex case insensitive\n\n// Array\ndb.coll.find({tags: {$all: [\"Realm\", \"Charts\"]}})\ndb.coll.find({field: {$size: 2}}) // impossible to index - prefer storing the size of the array & update it\ndb.coll.find({results: {$elemMatch: {product: \"xyz\", score: {$gte: 8}}}})\n\n// Projections\ndb.coll.find({\"x\": 1}, {\"actors\": 1})               // actors + _id\ndb.coll.find({\"x\": 1}, {\"actors\": 1, \"_id\": 0})     // actors\ndb.coll.find({\"x\": 1}, {\"actors\": 0, \"summary\": 0}) // all but \"actors\" and \"summary\"\n\n// Sort, skip, limit\ndb.coll.find({}).sort({\"year\": 1, \"rating\": -1}).skip(10).limit(3)\n\n// Read Concern\ndb.coll.find().readConcern(\"majority\")\n```\n\n-   [db.collection.find()](https://docs.mongodb.com/manual/reference/method/db.collection.find/)\n-   [Query and Projection Operators](https://docs.mongodb.com/manual/reference/operator/query/)\n-   [BSON types](https://docs.mongodb.com/manual/reference/operator/query/type/#available-types)\n-   [Read Concern](https://docs.mongodb.com/manual/reference/read-concern/)\n\n### Update\n\n``` javascript\ndb.coll.update({\"_id\": 1}, {\"year\": 2016}) // WARNING! Replaces the entire document\ndb.coll.update({\"_id\": 1}, {$set: {\"year\": 2016, name: \"Max\"}})\ndb.coll.update({\"_id\": 1}, {$unset: {\"year\": 1}})\ndb.coll.update({\"_id\": 1}, {$rename: {\"year\": \"date\"} })\ndb.coll.update({\"_id\": 1}, {$inc: {\"year\": 5}})\ndb.coll.update({\"_id\": 1}, {$mul: {price: NumberDecimal(\"1.25\"), qty: 2}})\ndb.coll.update({\"_id\": 1}, {$min: {\"imdb\": 5}})\ndb.coll.update({\"_id\": 1}, {$max: {\"imdb\": 8}})\ndb.coll.update({\"_id\": 1}, {$currentDate: {\"lastModified\": true}})\ndb.coll.update({\"_id\": 1}, {$currentDate: {\"lastModified\": {$type: \"timestamp\"}}})\n\n// Array\ndb.coll.update({\"_id\": 1}, {$push :{\"array\": 1}})\ndb.coll.update({\"_id\": 1}, {$pull :{\"array\": 1}})\ndb.coll.update({\"_id\": 1}, {$addToSet :{\"array\": 2}})\ndb.coll.update({\"_id\": 1}, {$pop: {\"array\": 1}})  // last element\ndb.coll.update({\"_id\": 1}, {$pop: {\"array\": -1}}) // first element\ndb.coll.update({\"_id\": 1}, {$pullAll: {\"array\" :[3, 4, 5]}})\ndb.coll.update({\"_id\": 1}, {$push: {scores: {$each: [90, 92, 85]}}})\ndb.coll.updateOne({\"_id\": 1, \"grades\": 80}, {$set: {\"grades.$\": 82}})\ndb.coll.updateMany({}, {$inc: {\"grades.$[]\": 10}})\ndb.coll.update({}, {$set: {\"grades.$[element]\": 100}}, {multi: true, arrayFilters: [{\"element\": {$gte: 100}}]})\n\n// Update many\ndb.coll.update({\"year\": 1999}, {$set: {\"decade\": \"90's\"}}, {\"multi\":true})\ndb.coll.updateMany({\"year\": 1999}, {$set: {\"decade\": \"90's\"}})\n\n// FindOneAndUpdate\ndb.coll.findOneAndUpdate({\"name\": \"Max\"}, {$inc: {\"points\": 5}}, {returnNewDocument: true})\n\n// Upsert\ndb.coll.update({\"_id\": 1}, {$set: {item: \"apple\"}, $setOnInsert: {defaultQty: 100}}, {upsert: true})\n\n// Replace\ndb.coll.replaceOne({\"name\": \"Max\"}, {\"firstname\": \"Maxime\", \"surname\": \"Beugnet\"})\n\n// Save\ndb.coll.save({\"item\": \"book\", \"qty\": 40})\n\n// Write concern\ndb.coll.update({}, {$set: {\"x\": 1}}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}})\n```\n\n### Delete\n\n``` javascript\ndb.coll.remove({name: \"Max\"})\ndb.coll.remove({name: \"Max\"}, {justOne: true})\ndb.coll.remove({}) // WARNING! Deletes all the docs but not the collection itself and its index definitions\ndb.coll.remove({name: \"Max\"}, {\"writeConcern\": {\"w\": \"majority\", \"wtimeout\": 5000}})\ndb.coll.findOneAndDelete({\"name\": \"Max\"})\n```\n\n🔝 [Table of Contents](#table-of-contents) 🔝\n\n## Databases and Collections\n\n### Drop\n\n``` javascript\ndb.coll.drop()    // removes the collection and its index definitions\ndb.dropDatabase() // double check that you are *NOT* on the PROD cluster... :-)\n```\n\n### Create Collection\n\n``` javascript\n// Create collection with a $jsonschema\ndb.createCollection(\"contacts\", {\n   validator: {$jsonSchema: {\n      bsonType: \"object\",\n      required: [\"phone\"],\n      properties: {\n         phone: {\n            bsonType: \"string\",\n            description: \"must be a string and is required\"\n         },\n         email: {\n            bsonType: \"string\",\n            pattern: \"@mongodb\\.com$\",\n            description: \"must be a string and match the regular expression pattern\"\n         },\n         status: {\n            enum: [ \"Unknown\", \"Incomplete\" ],\n            description: \"can only be one of the enum values\"\n         }\n      }\n   }}\n})\n```\n\n### Other Collection Functions\n\n``` javascript\ndb.coll.stats()\ndb.coll.storageSize()\ndb.coll.totalIndexSize()\ndb.coll.totalSize()\ndb.coll.validate({full: true})\ndb.coll.renameCollection(\"new_coll\", true) // 2nd parameter to drop the target collection if exists\n```\n\n🔝 [Table of Contents](#table-of-contents) 🔝\n\n## Indexes\n\n### List Indexes\n\n``` javascript\ndb.coll.getIndexes()\ndb.coll.getIndexKeys()\n```\n\n### Create Indexes\n\n``` javascript\n// Index Types\ndb.coll.createIndex({\"name\": 1})                // single field index\ndb.coll.createIndex({\"name\": 1, \"date\": 1})     // compound index\ndb.coll.createIndex({foo: \"text\", bar: \"text\"}) // text index\ndb.coll.createIndex({\"$**\": \"text\"})            // wildcard text index\ndb.coll.createIndex({\"userMetadata.$**\": 1})    // wildcard index\ndb.coll.createIndex({\"loc\": \"2d\"})              // 2d index\ndb.coll.createIndex({\"loc\": \"2dsphere\"})        // 2dsphere index\ndb.coll.createIndex({\"_id\": \"hashed\"})          // hashed index\n\n// Index Options\ndb.coll.createIndex({\"lastModifiedDate\": 1}, {expireAfterSeconds: 3600})      // TTL index\ndb.coll.createIndex({\"name\": 1}, {unique: true})\ndb.coll.createIndex({\"name\": 1}, {partialFilterExpression: {age: {$gt: 18}}}) // partial index\ndb.coll.createIndex({\"name\": 1}, {collation: {locale: 'en', strength: 1}})    // case insensitive index with strength = 1 or 2\ndb.coll.createIndex({\"name\": 1 }, {sparse: true})\n```\n\n### Drop Indexes\n\n``` javascript\ndb.coll.dropIndex(\"name_1\")\n```\n\n### Hide/Unhide Indexes\n\n``` javascript\ndb.coll.hideIndex(\"name_1\")\ndb.coll.unhideIndex(\"name_1\")\n```\n\n-   [Indexes documentation](https://docs.mongodb.com/manual/indexes/)\n\n🔝 [Table of Contents](#table-of-contents) 🔝\n\n## Handy commands\n\n``` javascript\nuse admin\ndb.createUser({\"user\": \"root\", \"pwd\": passwordPrompt(), \"roles\": [\"root\"]})\ndb.dropUser(\"root\")\ndb.auth( \"user\", passwordPrompt() )\n\nuse test\ndb.getSiblingDB(\"dbname\")\ndb.currentOp()\ndb.killOp(123) // opid\n\ndb.fsyncLock()\ndb.fsyncUnlock()\n\ndb.getCollectionNames()\ndb.getCollectionInfos()\ndb.printCollectionStats()\ndb.stats()\n\ndb.getReplicationInfo()\ndb.printReplicationInfo()\ndb.isMaster()\ndb.hostInfo()\ndb.printShardingStatus()\ndb.shutdownServer()\ndb.serverStatus()\n\ndb.setSlaveOk()\ndb.getSlaveOk()\n\ndb.getProfilingLevel()\ndb.getProfilingStatus()\ndb.setProfilingLevel(1, 200) // 0 == OFF, 1 == ON with slowms, 2 == ON\n\ndb.enableFreeMonitoring()\ndb.disableFreeMonitoring()\ndb.getFreeMonitoringStatus()\n\ndb.createView(\"viewName\", \"sourceColl\", [{$project:{department: 1}}])\n```\n\n🔝 [Table of Contents](#table-of-contents) 🔝\n\n## Change Streams\n\n``` javascript\nwatchCursor = db.coll.watch( [ { $match : {\"operationType\" : \"insert\" } } ] )\n\nwhile (!watchCursor.isExhausted()){\n   if (watchCursor.hasNext()){\n      print(tojson(watchCursor.next()));\n   }\n}\n```\n\n🔝 [Table of Contents](#table-of-contents) 🔝\n\n## Replica Set\n\n``` javascript\nrs.status()\nrs.initiate({\"_id\": \"replicaTest\",\n  members: [\n    { _id: 0, host: \"127.0.0.1:27017\" },\n    { _id: 1, host: \"127.0.0.1:27018\" },\n    { _id: 2, host: \"127.0.0.1:27019\", arbiterOnly:true }]\n})\nrs.add(\"mongodbd1.example.net:27017\")\nrs.addArb(\"mongodbd2.example.net:27017\")\nrs.remove(\"mongodbd1.example.net:27017\")\nrs.conf()\nrs.isMaster()\nrs.printReplicationInfo()\nrs.printSlaveReplicationInfo()\nrs.reconfig(<valid_conf>)\nrs.slaveOk()\nrs.stepDown(20, 5) // (stepDownSecs, secondaryCatchUpPeriodSecs)\n```\n\n🔝 [Table of Contents](#table-of-contents) 🔝\n\n## Sharded Cluster\n\n``` javascript\nsh.status()\nsh.addShard(\"rs1/mongodbd1.example.net:27017\")\nsh.shardCollection(\"mydb.coll\", {zipcode: 1})\n\nsh.moveChunk(\"mydb.coll\", { zipcode: \"53187\" }, \"shard0019\")\nsh.splitAt(\"mydb.coll\", {x: 70})\nsh.splitFind(\"mydb.coll\", {x: 70})\nsh.disableAutoSplit()\nsh.enableAutoSplit()\n\nsh.startBalancer()\nsh.stopBalancer()\nsh.disableBalancing(\"mydb.coll\")\nsh.enableBalancing(\"mydb.coll\")\nsh.getBalancerState()\nsh.setBalancerState(true/false)\nsh.isBalancerRunning()\n\nsh.addTagRange(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }, \"NY\")\nsh.removeTagRange(\"mydb.coll\", {state: \"NY\", zip: MinKey }, { state: \"NY\", zip: MaxKey }, \"NY\")\nsh.addShardTag(\"shard0000\", \"NYC\")\nsh.removeShardTag(\"shard0000\", \"NYC\")\n\nsh.addShardToZone(\"shard0000\", \"JFK\")\nsh.removeShardFromZone(\"shard0000\", \"NYC\")\nsh.removeRangeFromZone(\"mydb.coll\", {a: 1, b: 1}, {a: 10, b: 10})\n```\n\n🔝 [Table of Contents](#table-of-contents) 🔝\n\n## Wrap-up\n\nI hope you liked my little but - hopefully - helpful cheat sheet. Of course, this list isn't exhaustive at all. There are a lot more commands but I'm sure you will find them in the [MongoDB documentation](https://docs.mongodb.com/).\n\nIf you feel like I forgot a critical command in this list, please [send me a tweet](https://twitter.com/MBeugnet) and I will make sure to fix it.\n\nCheck out our [free courses on MongoDB University](https://university.mongodb.com/) if you are not too sure what some of the above commands are doing.\n\n>\n>\n>If you have questions, please head to our [developer community website](https://community.mongodb.com/) where the MongoDB engineers and the MongoDB community will help you build your next big idea with MongoDB.\n>\n>\n\n🔝 [Table of Contents](#table-of-contents) 🔝\n","description":"MongoDB Cheat Sheet by MongoDB for our awesome MongoDB Community <3.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltaf235a582fb14452/644c471e8cebd4cd409c6e1a/productivity.png?branch=prod","description":null}}]},"slug":"/cheat-sheet","title":"*MongoDB Cheat Sheet","original_publish_date":"2022-01-31T17:21:55.497Z","strapi_updated_at":"2023-01-26T16:56:03.124Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"MongoDB Cheat Sheet by MongoDB for our awesome MongoDB Community <3.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt4443b18731b9b019/644c4721c3e5e7a5d8d1c02e/og-mdb-developer.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@MBeugnet"},"system":{"updated_at":"2023-04-28T22:52:04.209Z","publish_details":{"time":"2023-04-28T22:53:01.290Z"}}},{"calculated_slug":"/languages/csharp/csharp-crud-tutorial","content":"<div>\n    <img style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-csharp.png\"\n        alt=\"C# badge\"\n    />\n\nIn this Quick Start post, I'll show how to set up connections between C# and MongoDB. Then I'll walk through the database Create, Read, Update, and Delete (CRUD) operations. As you already know, C# is a general-purpose language and MongoDB is a general-purpose data platform.  Together, C# and MongoDB are a powerful combination.\n\n</div>\n\n## Series Tools & Versions\n\nThe tools and versions I'm using for this series are:\n\n-   MongoDB Atlas with an M0 free cluster,\n-   MongoDB Sample Dataset loaded, specifically the `sample_training` and `grades` dataset,\n-   Windows 10,\n-   Visual Studio Community 2019,\n-   [NuGet](http://bit.ly/nuget-home) packages,\n-   MongoDB [C# Driver](http://bit.ly/mongodb-csharp-driver-github): version 2.9.1,\n-   MongoDB BSON Library: version 2.9.1.\n\n>C# is a popular language when using the .NET framework. If you're going to be developing in .NET and using MongoDB as your data layer, the C# driver makes it easy to do so.\n\n## Setup\n\nTo follow along, I'll be using Visual Studio 2019 on Windows 10 and connecting to a MongoDB Atlas cluster. If you're using a different OS, IDE, or text editor, the walkthrough might be slightly different, but the code itself should be fairly similar. Let's jump in and take a look at how nicely C# and MongoDB work together.\n\n>Get started with an M0 cluster on [MongoDB Atlas](http://bit.ly/mongodb-meetatlas) today. It's free forever and you'll be able to work alongside this blog series.\n\nFor this demonstration, I've chosen a Console App (.NET Core), and I've named it `MongoDBConnectionDemo`. Next, we need to install the MongoDB Driver for C#/.NET for a Solution. We can do that quite easily with [NuGet](http://bit.ly/nuget-home). Inside Visual Studio for Windows, by going to *Tools* -> *NuGet Package Manager* -> Manage NuGet Packages for Solution... We can browse for *MongoDB.Driver*. Then click on our Project and select the driver version we want. In this case, the [latest stable version](http://bit.ly/mongodb-csharp-driver) is 2.9.1. Then click on *Install*. Accept any license agreements that pop up and head into `Program.cs` to get started.\n\n### Putting the Driver to Work\n\nTo use the `MongoDB.Driver` we need to add a directive.\n\n``` csp\nusing MongoDB.Driver;\n```\n\nInside the `Main()` method we'll establish a connection to [MongoDB Atlas](http://bit.ly/mongodb-atlas) with a connection string and to test the connection we'll print out a list of the databases on the server.  The Atlas cluster to which we'll be connecting has the MongoDB Atlas [Sample Dataset](http://bit.ly/atlas-sample-data-blog) installed, so we'll be able to see a nice database list.\n\nThe first step is to pass in the MongoDB Atlas connection string into a MongoClient object, then we can get the list of databases and print them out.\n\n``` csp\nMongoClient dbClient = new MongoClient(<<YOUR ATLAS CONNECTION STRING>>);\n\nvar dbList = dbClient.ListDatabases().ToList();\n\nConsole.WriteLine(\"The list of databases on this server is: \");\nforeach (var db in dbList)\n{\n    Console.WriteLine(db);\n}\n```\n\nWhen we run the program, we get the following out showing the list of databases:\n\n``` bash\nThe list of databases on this server is:\n{ \"name\" : \"sample_airbnb\", \"sizeOnDisk\" : 57466880.0, \"empty\" : false }\n{ \"name\" : \"sample_geospatial\", \"sizeOnDisk\" : 1384448.0, \"empty\" : false }\n{ \"name\" : \"sample_mflix\", \"sizeOnDisk\" : 45084672.0, \"empty\" : false }\n{ \"name\" : \"sample_supplies\", \"sizeOnDisk\" : 1347584.0, \"empty\" : false }\n{ \"name\" : \"sample_training\", \"sizeOnDisk\" : 73191424.0, \"empty\" : false }\n{ \"name\" : \"sample_weatherdata\", \"sizeOnDisk\" : 4427776.0, \"empty\" : false }\n{ \"name\" : \"admin\", \"sizeOnDisk\" : 245760.0, \"empty\" : false }\n{ \"name\" : \"local\", \"sizeOnDisk\" : 1919799296.0, \"empty\" : false }\n```\n\nThe whole program thus far comes in at just over 20 lines of code:\n\n``` csp\nusing System;\nusing MongoDB.Driver;\n\nnamespace test\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            MongoClient dbClient = new MongoClient(<<YOUR ATLAS CONNECTION STRING>>);\n\n            var dbList = dbClient.ListDatabases().ToList();\n\n            Console.WriteLine(\"The list of databases on this server is: \");\n            foreach (var db in dbList)\n            {\n                Console.WriteLine(db);\n            }\n        }\n    }\n}\n```\n\nWith a connection in place, let's move on and start doing CRUD operations inside the MongoDB Atlas database. The first step there is to *Create* some data.\n\n## Create\n\n### Data\n\nMongoDB stores data in JSON Documents. Actually, they are stored as Binary JSON (BSON) objects on disk, but that's another blog post. In our sample dataset, there is a `sample_training` with a `grades` collection.  Here's what a sample document in that collection looks like:\n\n``` json\n{\n  \"_id\":{\"$oid\":\"56d5f7eb604eb380b0d8d8ce\"},\n  \"student_id\":{\"$numberDouble\":\"0\"},\n  \"scores\":[\n    {\"type\":\"exam\",\"score\":{\"$numberDouble\":\"78.40446309504266\"}},\n    {\"type\":\"quiz\",\"score\":{\"$numberDouble\":\"73.36224783231339\"}},\n    {\"type\":\"homework\",\"score\":{\"$numberDouble\":\"46.980982486720535\"}},\n    {\"type\":\"homework\",\"score\":{\"$numberDouble\":\"76.67556138656222\"}}\n  ],\n  \"class_id\":{\"$numberDouble\":\"339\"}\n}\n```\n\n### Connecting to a Specific Collection\n\nThere are 10,000 students in this collection, 0-9,999. Let's add one more by using C#. To do this, we'll need to use another package from NuGet, `MongoDB.Bson`. I'll start a new Solution in Visual Studio and call it `MongoDBCRUDExample`. I'll install the `MongoDB.Bson` and `MongoDB.Driver` packages and use the connection string provided from MongoDB Atlas. Next, I'll access our specific database and collection, `sample_training` and `grades`, respectively.\n\n``` csp\nusing System;\nusing MongoDB.Bson;\nusing MongoDB.Driver;\n\nnamespace MongoDBCRUDExample\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            MongoClient dbClient = new MongoClient(<<YOUR ATLAS CONNECTION STRING>>);\n\n    var database = dbClient.GetDatabase(\"sample_training\");\n            var collection = database.GetCollection<BsonDocument>(\"grades\");\n\n        }\n    }\n}\n```\n\n#### Creating a BSON Document\n\nThe `collection` variable is now our key reference point to our data.  Since we are using a `BsonDocument` when assigning our `collection` variable, I've indicated that I'm not going to be using a pre-defined schema. This utilizes the power and flexibility of MongoDB's document model. I could define a plain-old-C#-object (POCO) to more strictly define a schema. I'll take a look at that option in a future post. For now, I'll create a new `BsonDocument` to insert into the database.\n\n``` csp\nvar document = new BsonDocument\n            {\n                { \"student_id\", 10000 },\n                { \"scores\", new BsonArray\n                    {\n                    new BsonDocument{ {\"type\", \"exam\"}, {\"score\", 88.12334193287023 } },\n                    new BsonDocument{ {\"type\", \"quiz\"}, {\"score\", 74.92381029342834 } },\n                    new BsonDocument{ {\"type\", \"homework\"}, {\"score\", 89.97929384290324 } },\n                    new BsonDocument{ {\"type\", \"homework\"}, {\"score\", 82.12931030513218 } }\n                    }\n                },\n                { \"class_id\", 480}\n            };\n```\n\n### Create Operation\n\nThen to *Create* the document in the `sample_training.grades` collection, we can do an insert operation.\n\n``` csp\ncollection.InsertOne(document);\n```\n\nIf you need to do that insert asynchronously, the MongoDB C# driver is fully async compatible. The same operation could be done with:\n\n``` csp\nawait collection.InsertOneAsync(document);\n```\n\nIf you have a need to insert multiple documents at the same time, MongoDB has you covered there as well with the `InsertMany` or `InsertManyAsync` methods.\n\nWe've seen how to structure a BSON Document in C# and then *Create* it inside a MongoDB database. The MongoDB C# Driver makes it easy to do with the `InsertOne()`, `InsertOneAsync()`, `InsertMany()`, or `InsertManyAsync()` methods. Now that we have *Created* data, we'll want to *Read* it.\n\n## Read\n\nTo *Read* documents in MongoDB, we use the [Find()](https://mongodb.github.io/mongo-csharp-driver/2.6/apidocs/html/Overload_MongoDB_Driver_IMongoCollectionExtensions_Find.htm) method. This method allows us to chain a variety of methods to it, some of which I'll explore in this post. To get the first document in the collection, we can use the `FirstOrDefault` or `FirstOrDefaultAsync` method, and print the result to the console.\n\n``` csp\nvar firstDocument = collection.Find(new BsonDocument()).FirstOrDefault();\nConsole.WriteLine(firstDocument.ToString());\n```\n\nreturns...\n\n``` json\n{ \"_id\" : ObjectId(\"56d5f7eb604eb380b0d8d8ce\"),\n\"student_id\" : 0.0,\n\"scores\" : [\n{ \"type\" : \"exam\", \"score\" : 78.404463095042658 },\n{ \"type\" : \"quiz\", \"score\" : 73.362247832313386 },\n{ \"type\" : \"homework\", \"score\" : 46.980982486720535 },\n{ \"type\" : \"homework\", \"score\" : 76.675561386562222 }\n],\n\"class_id\" : 339.0 }\n```\n\nYou may wonder why we aren't using `Single` as that returns one document too. Well, that has to also ensure the returned document is the only document like that in the collection and that means scanning the whole collection.\n\n### Reading with a Filter\n\nLet's find the [document we created](#created-data) and print it out to the console. The first step is to create a filter to query for our specific document.\n\n``` csp\nvar filter = Builders<BsonDocument>.Filter.Eq(\"student_id\", 10000);\n```\n\nHere we're setting a filter to look for a document where the `student_id` is equal to `10000`. We can pass the filter into the `Find()` method to get the first document that matches the query.\n\n``` csp\nvar studentDocument = collection.Find(filter).FirstOrDefault();\nConsole.WriteLine(studentDocument.ToString());\n```\n\nreturns...\n\n``` json\n{ \"_id\" : ObjectId(\"5d88f88cec6103751b8a0d7f\"),\n\"student_id\" : 10000,\n\"scores\" : [\n{ \"type\" : \"exam\", \"score\" : 88.123341932870233 },\n{ \"type\" : \"quiz\", \"score\" : 74.923810293428346 },\n{ \"type\" : \"homework\", \"score\" : 89.979293842903246 },\n{ \"type\" : \"homework\", \"score\" : 82.129310305132179 }\n],\n\"class_id\" : 480 }\n```\n\nIf a document isn't found that matches the query, the `Find()` method returns null. Finding the first document in a collection, or with a query is a frequent task. However, what about situations when all documents need to be returned, either in a collection or from a query?\n\n### Reading All Documents\n\nFor situations in which the expected result set is small, the `ToList()` or `ToListAsync()` methods can be used to retrieve all documents from a query or in a collection.\n\n``` csp\nvar documents = collection.Find(new BsonDocument()).ToList();\n```\n\nFilters can be passed in here as well, for example, to get documents with exam scores equal or above 95. The filter here looks slightly more complicated, but thanks to the MongoDB driver syntax, it is relatively easy to follow. We're filtering on documents in which inside the `scores` array there is an `exam` subdocument with a `score` value greater than or equal to 95.\n\n``` csp\nvar highExamScoreFilter = Builders<BsonDocument>.Filter.ElemMatch<BsonValue>(\n\"scores\", new BsonDocument { { \"type\", \"exam\" },\n{ \"score\", new BsonDocument { { \"$gte\", 95 } } }\n});\nvar highExamScores = collection.Find(highExamScoreFilter).ToList();\n```\n\nFor situations where it's necessary to iterate over the documents that are returned there are a couple of ways to accomplish that as well. In a synchronous situation, a C# `foreach` statement can be used with the `ToEnumerable` adapter method. In this situation, instead of using the `ToList()` method, we'll use the `ToCursor()` method.\n\n``` csp\nvar cursor = collection.Find(highExamScoreFilter).ToCursor();\nforeach (var document in cursor.ToEnumerable())\n{\n     Console.WriteLine(document);\n}\n```\n\nThis can be accomplished in an asynchronous fashion with the `ForEachAsync` method as well:\n\n``` csp\nawait collection.Find(highExamScoreFilter).ForEachAsync(document => Console.WriteLine(document));\n```\n\n### Sorting\n\nWith many documents coming back in the result set, it is often helpful to sort the results. We can use the [Sort()](https://mongodb.github.io/mongo-csharp-driver/2.6/apidocs/html/M_MongoDB_Driver_IFindFluent_2_Sort.htm) method to accomplish this to see which student had the highest exam score.\n\n``` csp\nvar sort = Builders<BsonDocument>.Sort.Descending(\"student_id\");\n\nvar highestScores = collection.Find(highExamScoreFilter).Sort(sort);\n```\n\nAnd we can append the `First()` method to that to just get the top student.\n\n``` csp\nvar highestScore = collection.Find(highExamScoreFilter).Sort(sort).First();\n\nConsole.WriteLine(highestScore);\n```\n\nBased on the [Atlas Sample Data Set](https://docs.atlas.mongodb.com/sample-data/), the document with a `student_id` of 9997 should be returned with an exam score of 95.441609472871946.\n\nYou can see the full code for both the *Create* and *Read* operations I've shown in the [gist here](https://gist.github.com/kenwalger/37299af2b43cfe548e4d3a3154a31e6d).\n\nThe C# Driver for MongoDB provides many ways to *Read* data from the database and supports both synchronous and asynchronous methods for querying the data. By passing a filter into the `Find()` method, we are able to query for specific records. The syntax to build filters and query the database is straightforward and easy to read, making this step of CRUD operations in C# and MongoDB simple to use.\n\nWith the data created and being able to be read, let's take a look at how we can perform *Update* operations.\n\n## Update\n\nSo far in this C# Quick Start for MongoDB CRUD operations, we have explored how to *Create* and *Read* data into a MongoDB database using C#. We saw how to add filters to our query and how to sort the data. This section is about the *Update* operation and how C# and MongoDB work together to accomplish this important task.\n\nRecall that we've been working with this `BsonDocument` version of a student record:\n\n``` csp\nvar document = new BsonDocument\n            {\n                { \"student_id\", 10000 },\n                { \"scores\", new BsonArray\n                    {\n                    new BsonDocument{ {\"type\", \"exam\"}, {\"score\", 88.12334193287023 } },\n                    new BsonDocument{ {\"type\", \"quiz\"}, {\"score\", 74.92381029342834 } },\n                    new BsonDocument{ {\"type\", \"homework\"}, {\"score\", 89.97929384290324 } },\n                    new BsonDocument{ {\"type\", \"homework\"}, {\"score\", 82.12931030513218 } }\n                    }\n                },\n                { \"class_id\", 480}\n            };\n```\n\nAfter getting part way through the grading term, our sample student's instructor notices that he's been attending the wrong class section. Due to this error the school administration has to change, or *update*, the `class_id` associated with his record. He'll be moving into section 483.\n\n### Updating Data\n\nTo update a document we need two bits to pass into an `Update` command.  We need a filter to determine *which* documents will be updated. Second, we need what we're wanting to update.\n\n### Update Filter\n\nFor our example, we want to filter based on the document with `student_id` equaling 10000.\n\n``` csp\nvar filter = Builders<BsonDocument>.Filter.Eq(\"student_id\", 10000)\n```\n\n### Data to be Changed\n\nNext, we want to make the change to the `class_id`. We can do that with `Set()` on the `Update()` method.\n\n``` csp\nvar update = Builders<BsonDocument>.Update.Set(\"class_id\", 483);\n```\n\nThen we use the `UpdateOne()` method to make the changes. Note here that MongoDB will update at most one document using the `UpdateOne()` method.  If no documents match the filter, no documents will be updated.\n\n``` csp\ncollection.UpdateOne(filter, update);\n```\n\n### Array Changes\n\nNot all changes are as simple as changing a single field. Let's use a different filter, one that selects a document with a particular score type for quizes:\n\n``` csp\nvar arrayFilter = Builders<BsonDocument>.Filter.Eq(\"student_id\", 10000) & Builders<BsonDocument>\n                  .Filter.Eq(\"scores.type\", \"quiz\");\n```\n\nNow if we want to make the change to the quiz score we can do that with `Set()` too, but to identify which particular element should be changed is a little different. We can use the [positional $ operator](https://docs.mongodb.com/manual/reference/operator/update/positional/) to access the quiz `score` in the array. The $ operator on its own says \"change the array element that we matched within the query\" - the filter matches with `scores.type` equal to `quiz` and that's the element will get updated with the set.\n\n``` csp\nvar arrayUpdate = Builders<BsonDocument>.Update.Set(\"scores.$.score\", 84.92381029342834);\n```\n\nAnd again we use the `UpdateOne()` method to make the changes.\n\n``` csp\ncollection.UpdateOne(arrayFilter , arrayUpdate);\n```\n\n### Additional Update Methods\n\nIf you've been reading along in this blog series I've mentioned that the C# driver supports both sync and async interactions with MongoDB.  Performing data *Updates* is no different. There is also an `UpdateOneAsync()` method available. Additionally, for those cases in which multiple documents need to be updated at once, there are `UpdateMany()` or `UpdateManyAsync()` options. The `UpdateMany()` and `UpdateManyAsync()` methods match the documents in the `Filter` and will update *all* documents that match the filter requirements.\n\n`Update` is an important operator in the CRUD world. Not being able to update things as they change would make programming incredibly difficult. Fortunately, C# and MongoDB continue to work well together to make the operations possible and easy to use. Whether it's updating a student's grade or updating a user's address, *Update* is here to handle the changes. The code for the *Create*, *Read*, and *Update* operations can be found in [this gist](https://gist.github.com/kenwalger/f5cf317aa85aad2aa0f9d627d7a8095c).\n\nWe're winding down this MongoDB C# Quick Start CRUD operation series with only one operation left to explore, *Delete*.\n\n>Remember, you can get started with an M0 cluster on [MongoDB Atlas](http://bit.ly/mongodb-atlas) today. It's free forever and you'll be able to work alongside this blog series.\n\n## Delete\n\nTo continue along with the student story, let's take a look at how what would happen if the student dropped the course and had to have their grades deleted. Once again, the MongoDB driver for C# makes it a breeze. And, it provides both sync and async options for the operations.\n\n### Deleting Data\n\nThe first step in the deletion process is to create a filter for the document(s) that need to be deleted. In the example for this series, I've been using a document with a `student_id` value of `10000` to work with. Since I'll only be deleting that single record, I'll use the `DeleteOne()` method (for async situations the `DeleteOneAsync()` method is available). However, when a filter matches more than a single document and all of them need to be deleted, the `DeleteMany()` or `DeleteManyAsync` method can be used.\n\nHere's the record I want to delete.\n\n``` json\n{\n    { \"student_id\", 10000 },\n    { \"scores\", new BsonArray\n        {\n        new BsonDocument{ {\"type\", \"exam\"}, {\"score\", 88.12334193287023 } },\n        new BsonDocument{ {\"type\", \"quiz\"}, {\"score\", 84.92381029342834 } },\n        new BsonDocument{ {\"type\", \"homework\"}, {\"score\", 89.97929384290324 } },\n        new BsonDocument{ {\"type\", \"homework\"}, {\"score\", 82.12931030513218 } }\n        }\n    },\n    { \"class_id\", 483}\n};\n```\n\nI'll define the filter to match the `student_id` equal to `10000` document:\n\n``` csp\nvar deleteFilter = Builders<BsonDocument>.Filter.Eq(\"student_id\", 10000);\n```\n\nAssuming that we have a `collection` variable assigned to for the `grades` collection, we next pass the filter into the `DeleteOne()` method.\n\n``` csp\ncollection.DeleteOne(deleteFilter);\n```\n\nIf that command is run on the `grades` collection, the document with `student_id` equal to `10000` would be gone. Note here that `DeleteOne()` will delete the first document in the collection that matches the filter. In our example dataset, since there is only a single student with a `student_id` equal to `10000`, we get the desired results.\n\nFor the sake of argument, let's imagine that the rules for the educational institution are incredibly strict. If you get below a score of 60 on the first exam, you are automatically dropped from the course.  We could use a `for` loop with `DeleteOne()` to loop through the entire collection, find a single document that matches an exam score of less than 60, delete it, and repeat. Recall that `DeleteOne()` only deletes the first document it finds that matches the filter. While this could work, it isn't very efficient as multiple calls to the database are made. How do we handle situations that require deleting multiple records then? We can use `DeleteMany()`.\n\n### Multiple Deletes\n\nLet's define a new filter to match the exam score being less than 60:\n\n``` csp\nvar deleteLowExamFilter = Builders<BsonDocument>.Filter.ElemMatch<BsonValue>(\"scores\",\n     new BsonDocument { { \"type\", \"exam\" }, {\"score\", new BsonDocument { { \"$lt\", 60 }}}\n});\n```\n\nWith the filter defined, we pass it into the `DeleteMany()` method:\n\n``` csp\ncollection.DeleteMany(deleteLowExamFilter);\n```\n\nWith that command being run, all of the student record documents with low exam scores would be deleted from the collection.\n\nCheck out the [gist for all of the CRUD commands](https://gist.github.com/kenwalger/4a3da771b8471c43d190327556ebc3ab) wrapped into a single file.\n\n## Wrap Up\n\nThis C# Quick Start series has covered the various CRUD Operations (Create, Read, Update, and Delete) operations in MongoDB using basic BSON Documents. We've seen how to use filters to match specific documents that we want to read, update, or delete. This series has, thus far, been a gentle introduction to C Sharp and MongoDB.\n\nBSON Documents are not, however, the only way to be able to use MongoDB with C Sharp. In our applications, we often have classes defining objects. We can map our classes to BSON Documents to work with data as we would in code. I'll take a look at mapping in a future post.","description":"Learn how to perform CRUD operations using C Sharp for MongoDB databases.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blte14cab904de65863/644c4724783e05d8f9ae3e14/csharp.png?branch=prod","description":null}}]},"slug":"/csharp-crud-tutorial","title":"*MongoDB & C Sharp: CRUD Operations Tutorial","original_publish_date":"2022-02-01T15:24:52.433Z","strapi_updated_at":"2022-09-23T13:28:22.387Z","expiry_date":"2022-12-14T02:02:27.085Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*CSharp","calculated_slug":"/languages/csharp"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*CSharp","calculated_slug":"/languages/csharp"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to perform CRUD operations using C Sharp for MongoDB databases.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt1a81302aaf45a63b/644c4726e554e924c3f7a683/og-mdb-developer.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@kenwalger"},"system":{"updated_at":"2023-04-28T22:52:03.843Z","publish_details":{"time":"2023-04-28T22:53:01.323Z"}}},{"calculated_slug":"/products/atlas/free-atlas-cluster","content":"**You probably already know that MongoDB Atlas is MongoDB as a service in the public cloud of your choice but did you know we also offer a free forever cluster? In this Quick Start, we'll show you why you should get one and how to create one.**\n\nMongoDB Atlas's Free Tier clusters - which are also known as M0 Sandboxes - are limited to only 512MB of storage but it's more than enough for a pet project or to learn about MongoDB with our [free MongoDB University](http://bit.ly/mongodbuniversity) courses.\n\nThe only restriction on them is that they are available in a few regions for each of our three cloud providers: currently there are six on AWS, five on Azure, and four on Google Cloud Platform.\n\nIn this tutorial video, I will show you [how to create an account](http://bit.ly/mdb-atlas). Then I'll show you how to create your first 3 node cluster and populate it with sample data.\n\n:youtube[]{vid=rPqRyYJmx2g}\n\nNow that you understand the basics of [MongoDB Atlas](https://docs.atlas.mongodb.com/), you may want to explore some of our advanced features that are not available in the Free Tier clusters:\n\n-   [Peering your MongoDB Clusters](https://docs.atlas.mongodb.com/security-vpc-peering/) with your AWS, GCP or Azure machines is only available for dedicated instances (M10 at least),\n-   [LDAP Authentication and Authorization](https://docs.atlas.mongodb.com/security-ldaps/),\n-   [AWS PrivateLink](https://docs.atlas.mongodb.com/billing/additional-services/#private-endpoints).\n\nOur new [Lucene-based Full-Text Search engine](https://docs.atlas.mongodb.com/atlas-search/) is now available for free tier clusters directly.\n","description":"Want to know the quickest way to start with MongoDB? It begins with getting yourself a free MongoDB Atlas Cluster so you can leverage your learning","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt0d0e1c64111a2dd7/644c4729d527e33a6417fdf1/atlas-cluster.png?branch=prod","description":null}}]},"slug":"/free-atlas-cluster","title":"*Getting Your Free MongoDB Atlas Cluster","original_publish_date":"2022-02-01T15:35:02.597Z","strapi_updated_at":"2023-01-26T16:56:03.124Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*Atlas","calculated_slug":"/products/atlas"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Azure","calculated_slug":"/technologies/azure"}},{"node":{"title":"*GCP","calculated_slug":"/technologies/gcp"}}]}},"seo":{"canonical_url":"","meta_description":"Want to know the quickest way to start with MongoDB? It begins with getting yourself a free MongoDB Atlas Cluster so you can leverage your learning","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt4457df82d5e9c4a0/644c472b75b185109546a909/og-mdb-developer.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@MBeugnet"},"system":{"updated_at":"2023-04-28T22:52:03.438Z","publish_details":{"time":"2023-04-28T22:53:01.358Z"}}},{"calculated_slug":"/languages/go/golang-change-streams","content":"<div>\n    <img style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-go.png\"\n        alt=\"QuickStart Golang Logo\"\n    />\n\nIf you've been keeping up with my getting started with Go and MongoDB tutorial series, you'll remember that we've accomplished quite a bit so far. We've had a look at everything from CRUD interaction with the database to data modeling, and more. To play catch up with everything we've done, you can have a look at the following tutorials in the series:\n</div>\n\n-   [How to Get Connected to Your MongoDB Cluster with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--starting-and-setup)\n-   [Creating MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-create-documents)\n-   [Retrieving and Querying MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-read-documents)\n-   [Updating MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-update-documents)\n-   [Deleting MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-delete-documents)\n-   [Modeling MongoDB Documents with Native Go Data Structures](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--modeling-documents-with-go-data-structures)\n-   [Performing Complex MongoDB Data Aggregation Queries with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--data-aggregation-pipeline)\n\nIn this tutorial we're going to explore [change streams](https://docs.mongodb.com/manual/changeStreams/) in MongoDB and how they might be useful, all with the Go programming language (Golang).\n\nBefore we take a look at the code, let's take a step back and understand what change streams are and why there's often a need for them.\n\nImagine this scenario, one of many possible:\n\nYou have an application that engages with internet of things (IoT) clients. Let's say that this is a geofencing application and the IoT clients are something that can trigger the geofence as they come in and out of range. Rather than having your application constantly run queries to see if the clients are in range, wouldn't it make more sense to watch in real-time and react when it happens?\n\nWith MongoDB change streams, you can create a pipeline to watch for changes on a collection level, database level, or deployment level, and write logic within your application to do something as data comes in based on your pipeline.\n\n## Creating a Real-Time MongoDB Change Stream with Golang\n\nWhile there are many possible use-cases for change streams, we're going to continue with the example that we've been using throughout the scope of this getting started series. We're going to continue working with podcast show and podcast episode data.\n\nLet's assume we have the following code to start:\n\n``` go\npackage main\n\nimport (\n   \"context\"\n   \"fmt\"\n   \"os\"\n   \"sync\"\n\n   \"go.mongodb.org/mongo-driver/bson\"\n   \"go.mongodb.org/mongo-driver/mongo\"\n   \"go.mongodb.org/mongo-driver/mongo/options\"\n)\n\nfunc main() {\n   client, err := mongo.Connect(context.TODO(), options.Client().ApplyURI(os.Getenv(\"ATLAS_URI\")))\n   if err != nil {\n      panic(err)\n   }\n   defer client.Disconnect(context.TODO())\n\n   database := client.Database(\"quickstart\")\n   episodesCollection := database.Collection(\"episodes\")\n}\n```\n\nThe above code is a very basic connection to a MongoDB cluster, something that we explored in the [How to Get Connected to Your MongoDB Cluster with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--starting-and-setup), tutorial.\n\nTo watch for changes, we can do something like the following:\n\n``` go\nepisodesStream, err := episodesCollection.Watch(context.TODO(), mongo.Pipeline{})\nif err != nil {\n   panic(err)\n}\n```\n\nThe above code will watch for any and all changes to documents within the `episodes` collection. The result is a cursor that we can iterate over indefinitely for data as it comes in.\n\nWe can iterate over the curser and make sense of our data using the following code:\n\n``` go\nepisodesStream, err := episodesCollection.Watch(context.TODO(), mongo.Pipeline{})\nif err != nil {\n   panic(err)\n}\n\ndefer episodesStream.Close(context.TODO())\n\nfor episodesStream.Next(context.TODO()) {\n   var data bson.M\n   if err := episodesStream.Decode(&data); err != nil {\n      panic(err)\n   }\n   fmt.Printf(\"%v\\n\", data)\n}\n```\n\nIf data were to come in, it might look something like the following:\n\n``` none\nmap[_id:map[_data:825E4EFCB9000000012B022C0100296E5A1004D960EAE47DBE4DC8AC61034AE145240146645F696400645E3B38511C9D4400004117E80004] clusterTime:{1582234809 1} documentKey:map[_id:ObjectID(\"5e3b38511c9d\n4400004117e8\")] fullDocument:map[_id:ObjectID(\"5e3b38511c9d4400004117e8\") description:The second episode duration:30 podcast:ObjectID(\"5e3b37e51c9d4400004117e6\") title:Episode #3] ns:map[coll:episodes \ndb:quickstart] operationType:replace]\n```\n\nIn the above example, I've done a `Replace` on a particular document in the collection. In addition to information about the data, I also receive the full document that includes the change. The results will vary depending on the `operationType` that takes place.\n\nWhile the code that we used would work fine, it is currently a blocking operation. If we wanted to watch for changes and continue to do other things, we'd want to use a [goroutine](https://tour.golang.org/concurrency/1) for iterating over our change stream cursor.\n\nWe could make some changes like this:\n\n``` go\npackage main\n\nimport (\n   \"context\"\n   \"fmt\"\n   \"os\"\n   \"sync\"\n\n   \"go.mongodb.org/mongo-driver/bson\"\n   \"go.mongodb.org/mongo-driver/mongo\"\n   \"go.mongodb.org/mongo-driver/mongo/options\"\n)\n\nfunc iterateChangeStream(routineCtx context.Context, waitGroup sync.WaitGroup, stream *mongo.ChangeStream) {\n   defer stream.Close(routineCtx)\n   defer waitGroup.Done()\n   for stream.Next(routineCtx) {\n      var data bson.M\n      if err := stream.Decode(&data); err != nil {\n            panic(err)\n      }\n      fmt.Printf(\"%v\\n\", data)\n   }\n}\n\nfunc main() {\n   client, err := mongo.Connect(context.TODO(), options.Client().ApplyURI(os.Getenv(\"ATLAS_URI\")))\n   if err != nil {\n      panic(err)\n   }\n   defer client.Disconnect(context.TODO())\n\n   database := client.Database(\"quickstart\")\n   episodesCollection := database.Collection(\"episodes\")\n\n   var waitGroup sync.WaitGroup\n\n   episodesStream, err := episodesCollection.Watch(context.TODO(), mongo.Pipeline{})\n   if err != nil {\n      panic(err)\n   }\n   waitGroup.Add(1)\n   routineCtx, cancelFn := context.WithCancel(context.Background())\n   go iterateChangeStream(routineCtx, waitGroup, episodesStream)\n\n   waitGroup.Wait()\n}\n```\n\nA few things are happening in the above code. We've moved the stream iteration into a separate function to be used in a goroutine. However, running the application would result in it terminating quite quickly because the `main` function will terminate not too longer after creating the goroutine. To resolve this, we are making use of a `WaitGroup`. In our example, the `main` function will wait until the `WaitGroup` is empty and the `WaitGroup` only becomes empty when the goroutine terminates.\n\nMaking use of the `WaitGroup` isn't an absolute requirement as there are other ways to keep the application running while watching for changes.  However, given the simplicity of this example, it made sense in order to see any changes in the stream.\n\nTo keep the `iterateChangeStream` function from running indefinitely, we are creating and passing a context that can be canceled. While we don't demonstrate canceling the function, at least we know it can be done.\n\n## Complicating the Change Stream with the Aggregation Pipeline\n\nIn the previous example, the aggregation pipeline that we used was as basic as you can get. In other words, we were looking for any and all changes that were happening to our particular collection. While this might be good in a lot of scenarios, you'll probably get more out of using a better defined aggregation pipeline.\n\nTake the following for example:\n\n``` go\nmatchPipeline := bson.D{\n   {\n      \"$match\", bson.D{\n            {\"operationType\", \"insert\"},\n            {\"fullDocument.duration\", bson.D{\n               {\"$gt\", 30},\n            }},\n      },\n   },\n}\n\nepisodesStream, err := episodesCollection.Watch(context.TODO(), mongo.Pipeline{matchPipeline})\n```\n\nIn the above example, we're still watching for changes to the `episodes` collection. However, this time we're only watching for new documents that have a `duration` field greater than 30. Any other insert or other change stream operation won't be detected.\n\nThe results of the above code, when a match is found, might look like the following:\n\n``` none\nmap[_id:map[_data:825E4F03CF000000012B022C0100296E5A1004D960EAE47DBE4DC8AC61034AE145240146645F696400645E4F03A01C9D44000063CCBD0004] clusterTime:{1582236623 1} documentKey:map[_id:ObjectID(\"5e4f03a01c9d\n44000063ccbd\")] fullDocument:map[_id:ObjectID(\"5e4f03a01c9d44000063ccbd\") description:a quick start into mongodb duration:35 podcast:1234 title:getting started with mongodb] ns:map[coll:episodes db:qui\nckstart] operationType:insert]\n```\n\nWith change streams, you'll have access to a subset of the MongoDB aggregation pipeline and its operators. You can learn more about what's available in the [official documentation](http://docs.mongodb.com/manual/changeStreams/#modify-change-stream-output).\n\n## Conclusion\n\nYou just saw how to use MongoDB change streams in a Golang application using the MongoDB Go driver. As previously pointed out, change streams make it very easy to react to database, collection, and deployment changes without having to constantly query the cluster. This allows you to efficiently plan out aggregation pipelines to respond to as they happen in real-time.\n\nIf you're looking to catch up on the other tutorials in the MongoDB with Go quick start series, you can find them below:\n\n-   [How to Get Connected to Your MongoDB Cluster with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--starting-and-setup)\n-   [Creating MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-create-documents)\n-   [Retrieving and Querying MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-read-documents)\n-   [Updating MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-update-documents)\n-   [Deleting MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-delete-documents)\n-   [Modeling MongoDB Documents with Native Go Data Structures](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--modeling-documents-with-go-data-structures)\n-   [Performing Complex MongoDB Data Aggregation Queries with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--data-aggregation-pipeline)\n\nTo bring the series to a close, the next tutorial will focus on transactions with the MongoDB Go driver.","description":"Learn how to use change streams to react to changes to MongoDB documents, databases, and clusters in real-time using the Go programming language.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt6618f9fb27908673/644c472dcc5633a3cf147e1e/Golang_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/golang-change-streams","title":"*Reacting to Database Changes with MongoDB Change Streams and Go","original_publish_date":"2022-02-01T16:39:46.451Z","strapi_updated_at":"2023-02-03T16:11:51.130Z","expiry_date":"2022-12-14T02:02:44.443Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Go","calculated_slug":"/languages/go"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Change Streams","calculated_slug":"/products/mongodb/change-streams"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Go","calculated_slug":"/languages/go"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to use change streams to react to changes to MongoDB documents, databases, and clusters in real-time using the Go programming language.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blte4f18d0d84990b07/644c472eba3aa52270525cc4/Golang_Tile_360x360.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@nraboy"},"system":{"updated_at":"2023-04-28T22:52:03.058Z","publish_details":{"time":"2023-04-28T22:53:01.391Z"}}},{"calculated_slug":"/languages/go/golang-multi-document-acid-transactions","content":"\n\n<div class=\"introduction\">\n\nThe past few months have been an adventure when it comes to getting started with MongoDB using the Go programming language (Golang). We've explored everything from create, retrieve, update, and delete (CRUD) operations, to data modeling, and to change streams. To bring this series to a solid finish, we're going to take a look at a popular requirement that a lot of organizations need, and that requirement is transactions.\n\nSo why would you want transactions?\n\nThere are some situations where you might need atomicity of reads and writes to multiple documents within a single collection or multiple collections. This isn't always a necessity, but in some cases, it might be.\n\nTake the following for example.\n\nLet's say you want to create documents in one collection that depend on documents in another collection existing. Or let's say you have schema validation rules in place on your collection. In the scenario that you're trying to create documents and the related document doesn't exist or your schema validation rules fail, you don't want the operation to proceed. Instead, you'd probably want to roll back to before it happened.\n\nThere are other reasons that you might use transactions, but you can use your imagination for those.\n\nIn this tutorial, we're going to look at what it takes to use transactions with Golang and MongoDB. Our example will rely more on schema validation rules passing, but it isn't a limitation.\n\n## Understanding the Data Model and Applying Schema Validation\n\nSince we've continued the same theme throughout the series, I think it'd be a good idea to have a refresher on the data model that we'll be using for this example.\n\nIn the past few tutorials, we've explored working with potential podcast data in various collections. For example, our Go data model looks something like this:\n\n``` go\ntype Episode struct {\n    ID          primitive.ObjectID `bson:\"_id,omitempty\"`\n    Podcast     primitive.ObjectID `bson:\"podcast,omitempty\"`\n    Title       string             `bson:\"title,omitempty\"`\n    Description string             `bson:\"description,omitempty\"`\n    Duration    int32              `bson:\"duration,omitempty\"`\n}\n```\n\nThe fields in the data structure are mapped to MongoDB document fields through the BSON annotations. You can learn more about using these annotations in the [previous tutorial](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--modeling-documents-with-go-data-structures) I wrote on the subject.\n\nWhile we had other collections, we're going to focus strictly on the `episodes` collection for this example.\n\nRather than coming up with complicated code for this example to demonstrate operations that fail or should be rolled back, we're going to go with schema validation to force fail some operations. Let's assume that no episode should be less than two minutes in duration, otherwise it is not valid. Rather than implementing this, we can use features baked into MongoDB.\n\nTake the following schema validation logic:\n\n``` json\n{\n    \"$jsonSchema\": {\n        \"additionalProperties\": true,\n        \"properties\": {\n            \"duration\": {\n                \"bsonType\": \"int\",\n                \"minimum\": 2\n            }\n        }\n    }\n}\n```\n\nThe above logic would be applied using the MongoDB CLI or with Compass, but we're essentially saying that our schema for the `episodes` collection can contain any fields in a document, but the `duration` field must be an integer and it must be at least two. Could our schema validation be more complex? Absolutely, but we're all about simplicity in this example. If you want to learn more about schema validation, check out [this awesome tutorial](https://www.mongodb.com/blog/post/json-schema-validation--locking-down-your-model-the-smart-way) on the subject.\n\nNow that we know the schema and what will cause a failure, we can start implementing some transaction code that will commit or roll back changes.\n\n## Starting and Committing Transactions\n\nBefore we dive into starting a session for our operations and committing transactions, let's establish a base point in our project. Let's assume that your project has the following boilerplate MongoDB with Go code:\n\n``` go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"os\"\n\n    \"go.mongodb.org/mongo-driver/bson/primitive\"\n    \"go.mongodb.org/mongo-driver/mongo\"\n    \"go.mongodb.org/mongo-driver/mongo/options\"\n)\n\n// Episode represents the schema for the \"Episodes\" collection\ntype Episode struct {\n    ID          primitive.ObjectID `bson:\"_id,omitempty\"`\n    Podcast     primitive.ObjectID `bson:\"podcast,omitempty\"`\n    Title       string             `bson:\"title,omitempty\"`\n    Description string             `bson:\"description,omitempty\"`\n    Duration    int32              `bson:\"duration,omitempty\"`\n}\n\nfunc main() {\n    client, err := mongo.Connect(context.TODO(), options.Client().ApplyURI(os.Getenv(\"ATLAS_URI\")))\n    if err != nil {\n        panic(err)\n    }\n    defer client.Disconnect(context.TODO())\n\n    database := client.Database(\"quickstart\")\n    episodesCollection := database.Collection(\"episodes\")\n\n    database.RunCommand(context.TODO(), bson.D{{\"create\", \"episodes\"}})\n}\n```\n\nThe collection must exist prior to working with transactions. When using the `RunCommand`, if the collection already exists, an error will be returned. For this example, the error is not important to us since we just want the collection to exist, even if that means creating it.\n\nNow let's assume that you've correctly included the MongoDB Go driver as seen in a previous tutorial titled, [How to Get Connected to Your MongoDB Cluster with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--starting-and-setup).\n\nThe goal here will be to try to insert a document that complies with our schema validation as well as a document that doesn't so that we have a commit that doesn't happen.\n\n``` go\n// ...\n\nfunc main() {\n    // ...\n\n    wc := writeconcern.New(writeconcern.WMajority())\n    rc := readconcern.Snapshot()\n    txnOpts := options.Transaction().SetWriteConcern(wc).SetReadConcern(rc)\n\n    session, err := client.StartSession()\n    if err != nil {\n        panic(err)\n    }\n    defer session.EndSession(context.Background())\n\n    err = mongo.WithSession(context.Background(), session, func(sessionContext mongo.SessionContext) error {\n        if err = session.StartTransaction(txnOpts); err != nil {\n            return err\n        }\n        result, err := episodesCollection.InsertOne(\n            sessionContext,\n            Episode{\n                Title:    \"A Transaction Episode for the Ages\",\n                Duration: 15,\n            },\n        )\n        if err != nil {\n            return err\n        }\n        fmt.Println(result.InsertedID)\n        result, err = episodesCollection.InsertOne(\n            sessionContext,\n            Episode{\n                Title:    \"Transactions for All\",\n                Duration: 1,\n            },\n        )\n        if err != nil {\n            return err\n        }\n        if err = session.CommitTransaction(sessionContext); err != nil {\n            return err\n        }\n        fmt.Println(result.InsertedID)\n        return nil\n    })\n    if err != nil {\n        if abortErr := session.AbortTransaction(context.Background()); abortErr != nil {\n            panic(abortErr)\n        }\n        panic(err)\n    }\n}\n```\n\nIn the above code, we start by defining the read and write concerns that will give us the desired level of isolation in our transaction. To learn more about the available read and write concerns, check out the [documentation](https://docs.mongodb.com/manual/core/transactions/#read-concern-write-concern-read-preference).\n\nAfter defining the transaction options, we start a session which will encapsulate everything we want to do with atomicity. After, we start a transaction that we'll use to commit everything in the session.\n\nA `Session` represents a MongoDB logical session and can be used to enable casual consistency for a group of operations or to execute operations in an ACID transaction. More information on how they work in Go can be found in the [documentation](https://godoc.org/go.mongodb.org/mongo-driver/mongo#Session).\n\nInside the session, we are doing two `InsertOne` operations. The first would succeed because it doesn't violate any of our schema validation rules. It will even print out an object id when it's done. However, the second operation will fail because it is less than two minutes. The `CommitTransaction` won't ever succeed because of the error that the second operation created. When the `WithSession` function returns the error that we created, the transaction is aborted using the `AbortTransaction` function. For this reason, neither of the `InsertOne` operations will show up in the database.\n\n## Using a Convenient Transactions API\n\nStarting and committing transactions from within a logical session isn't the only way to work with ACID transactions using Golang and MongoDB. Instead, we can use what might be thought of as a more convenient transactions API.\n\nTake the following adjustments to our code:\n\n``` go\n// ...\n\nfunc main() {\n    // ...\n\n    wc := writeconcern.New(writeconcern.WMajority())\n    rc := readconcern.Snapshot()\n    txnOpts := options.Transaction().SetWriteConcern(wc).SetReadConcern(rc)\n\n    session, err := client.StartSession()\n    if err != nil {\n        panic(err)\n    }\n    defer session.EndSession(context.Background())\n\n    callback := func(sessionContext mongo.SessionContext) (interface{}, error) {\n        result, err := episodesCollection.InsertOne(\n            sessionContext,\n            Episode{\n                Title:    \"A Transaction Episode for the Ages\",\n                Duration: 15,\n            },\n        )\n        if err != nil {\n            return nil, err\n        }\n        result, err = episodesCollection.InsertOne(\n            sessionContext,\n            Episode{\n                Title:    \"Transactions for All\",\n                Duration: 2,\n            },\n        )\n        if err != nil {\n            return nil, err\n        }\n        return result, err\n    }\n\n    _, err = session.WithTransaction(context.Background(), callback, txnOpts)\n    if err != nil {\n        panic(err)\n    }\n}\n```\n\nInstead of using `WithSession`, we are now using `WithTransaction`, which handles starting a transaction, executing some application code, and then committing or aborting the transaction based on the success of that application code. Not only that, but retries can happen for specific errors if certain operations fail.\n\n## Conclusion\n\nYou just saw how to use transactions with the MongoDB Go driver. While in this example we used schema validation to determine if a commit operation succeeds or fails, you could easily apply your own application logic within the scope of the session.\n\nIf you want to catch up on other tutorials in the getting started with Golang series, you can find some below:\n\n-   [How to Get Connected to Your MongoDB Cluster with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--starting-and-setup)\n-   [Creating MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-create-documents)\n-   [Retrieving and Querying MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-read-documents)\n-   [Updating MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-update-documents)\n-   [Deleting MongoDB Documents with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--how-to-delete-documents)\n-   [Modeling MongoDB Documents with Native Go Data Structures](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--modeling-documents-with-go-data-structures)\n-   [Performing Complex MongoDB Data Aggregation Queries with Go](https://www.mongodb.com/blog/post/quick-start-golang--mongodb--data-aggregation-pipeline)\n-   [Reacting to Database Changes with MongoDB Change Streams and Go](https://developer.mongodb.com/quickstart/golang-change-streams)\n\nSince transactions brings this tutorial series to a close, make sure you keep a lookout for more tutorials that focus on more niche and interesting topics that apply everything that was taught while getting started.","description":"Learn how to accomplish ACID transactions and logical sessions with MongoDB and the Go programming language (Golang).","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt6618f9fb27908673/644c472dcc5633a3cf147e1e/Golang_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/golang-multi-document-acid-transactions","title":"*Multi-Document ACID Transactions in MongoDB with Go","original_publish_date":"2022-02-01T16:41:27.022Z","strapi_updated_at":"2023-02-03T16:11:51.130Z","expiry_date":null,"authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Go","calculated_slug":"/languages/go"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Go","calculated_slug":"/languages/go"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to accomplish ACID transactions and logical sessions with MongoDB and the Go programming language (Golang).","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blte4f18d0d84990b07/644c472eba3aa52270525cc4/Golang_Tile_360x360.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@nraboy"},"system":{"updated_at":"2023-04-28T22:52:02.688Z","publish_details":{"time":"2023-04-28T22:53:01.420Z"}}},{"calculated_slug":"/products/mongodb/introduction-aggregation-framework","content":"<div>\n    <img style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-aggregation-framework.png\"\n        alt=\"BSON Quickstart badge\"\n    />\n\nOne of the difficulties when storing any data is knowing how it will be accessed in the future. What reports need to be run on it? What information is \"hidden\" in there that will allow for meaningful insights for your business? After spending the time to [design your data schema](https://www.mongodb.com/blog/post/building-with-patterns-a-summary) in an appropriate fashion for your application, one needs to be able to retrieve it. In MongoDB, there are two basic ways that data retrieval can be done: through queries with the [find()](https://docs.mongodb.com/manual/reference/method/db.collection.find/) command, and through analytics using the aggregation framework and the [aggregate()](https://docs.mongodb.com/manual/reference/method/db.collection.aggregate/) command.\n\n</div>\n\n`find()` allows for the querying of data based on a condition. One can filter results, do basic document transformations, sort the documents, limit the document result set, etc. The `aggregate()` command opens the door to a whole new world with the [aggregation framework](https://docs.mongodb.com/manual/aggregation/). In this series of posts, I'll take a look at some of the reasons why using the aggregation framework is so powerful, and how to harness that power.\n\n## Why Aggregate with MongoDB?\n\nA frequently asked question is why do aggregation inside MongoDB at all?  From the MongoDB documentation:\n\n>\n>\n>Aggregation operations process data records and return computed results.  Aggregation operations group values from multiple documents together, and can perform a variety of operations on the grouped data to return a single result.\n>\n>\n\nBy using the built-in aggregation operators available in MongoDB, we are able to do analytics on a cluster of servers we're already using without having to move the data to another platform, like Apache [Spark](https://spark.apache.org/) or [Hadoop](https://hadoop.apache.org/). While those, and similar, platforms are fast, the data transfer from MongoDB to them can be slow and potentially expensive. By using the aggregation framework the work is done inside MongoDB and then the final results can be sent to the application typically resulting in a smaller amount of data being moved around. It also allows for the querying of the **LIVE** version of the data and not an older copy of data from a batch.\n\nAggregation in MongoDB allows for the transforming of data and results in a more powerful fashion than from using the `find()` command. Through the use of multiple stages and expressions, you are able to build a \"pipeline\" of operations on your data to perform analytic operations.  What do I mean by a \"pipeline\"? The aggregation framework is conceptually similar to the `*nix` command line pipe, `|`. In the `*nix` command line pipeline, a pipe transfers the standard output to some other destination. The output of one command is sent to another command for further processing.\n\n![*nix pipeline example](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/agg-framework/nix-pipeline.gif)\n\nIn the aggregation framework, we think of stages instead of commands.  And the stage \"output\" is documents. Documents go into a stage, some work is done, and documents come out. From there they can move onto another stage or provide output.\n\n## Aggregation Stages\n\nAt the time of this writing, there are twenty-eight different aggregation stages available. These different stages provide the ability to do a wide variety of tasks. For example, we can build an aggregation pipeline that *matches* a set of documents based on a set of criteria, *groups* those documents together, *sorts* them, then returns that result set to us.\n\n![Aggreggation Pipeline example](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/agg-framework/agg-framework-pipeline.gif)\n\nOr perhaps our pipeline is more complicated and the document flows through the `$match`, `$unwind`, `$group`, `$sort`, `$limit`, `$project`, and finally a `$skip` stage.\n\nThis can be confusing and some of these concepts are worth repeating.  Therefore, let's break this down a bit further:\n\n-   A pipeline starts with documents\n-   These documents come from a collection, a view, or a specially designed stage\n-   In each stage, documents enter, work is done, and documents exit\n-   The stages themselves are defined using the document syntax\n\nLet's take a look at an example pipeline. Our documents are from the [Sample Data](https://docs.atlas.mongodb.com/sample-data/available-sample-datasets/) that's available in MongoDB Atlas and the `routes` collection in the `sample_training` database. Here's a sample document:\n\n``` json\n{\n\"_id\":{\n    \"$oid\":\"56e9b39b732b6122f877fa31\"\n},\n\"airline\":{\n   \"id\":{\n       \"$numberInt\":\"410\"\n   },\n   \"name\":\"Aerocondor\"\n   ,\"alias\":\"2B\"\n   ,\"iata\":\"ARD\"\n},\n\"src_airport\":\"CEK\",\n\"dst_airport\":\"KZN\",\n\"Codeshare\":\"\",\n\"stops\":{\n    \"$numberInt\":\"0\"\n},\n\"airplane\":\"CR2\"\n}\n```\n\n>\n>\n>If you haven't yet set up your free cluster on [MongoDB Atlas](http://bit.ly/mongodbatlas), now is a great time to do so. You have all the instructions in this [blog post](/quickstart/free-atlas-cluster).\n>\n>\n\nFor this example query, let's find the top three airlines that offer the most direct flights out of the airport in Portland, Oregon, USA (PDX).  To start with, we'll do a `$match` stage so that we can concentrate on doing work only on those documents that meet a base of conditions. In this case, we'll look for documents with a `src_airport`, or source airport, of PDX and that are direct flights, i.e. that have zero stops.\n\n``` javascript\n{\n  $match: {\n    \"src_airport\": \"PDX\",\n    \"stops\": 0\n  }\n}\n```\n\nThat reduces the number of documents in our pipeline down from 66,985 to 113. Next, we'll group by the airline name and count the number of flights:\n\n``` javascript\n{\n    $group: {\n        _id: {\n            \"airline name\": \"$airline.name\"\n        },\n        count: {\n            $sum: 1\n        }\n    }\n}\n```\n\nWith the addition of the `$group` stage, we're down to 16 documents.  Let's sort those with a `$sort` stage and sort in descending order:\n\n``` javascript\n{\n    $sort: {\n        count: -1\n}\n```\n\nThen we can add a `$limit` stage to just have the top three airlines that are servicing Portland, Oregon:\n\n``` javascript\n{\n   $limit: 3\n}\n```\n\nAfter putting the documents in the `sample_training.routes` collection through this aggregation pipeline, our results show us that the top three airlines offering non-stop flights departing from PDX are Alaska, American, and United Airlines with 39, 17, and 13 flights, respectively.\n\nHow does this look in code? It's fairly straightforward with using the `db.aggregate()` function. For example, in Python you would do something like:\n\n``` python\nfrom pymongo import MongoClient\n\n# Requires the PyMongo package.\n# The dnspython package is also required to use a mongodb+src URI string\n# https://api.mongodb.com/python/current\n\nclient = MongoClient('YOUR-ATLAS-CONNECTION-STRING')\nresult = client['sample_training']['routes'].aggregate([\n    {\n        '$match': {\n            'src_airport': 'PDX',\n            'stops': 0\n        }\n    }, {\n        '$group': {\n            '_id': {\n                'airline name': '$airline.name'\n            },\n            'count': {\n                '$sum': 1\n            }\n        }\n    }, {\n        '$sort': {\n            'count': -1\n        }\n    }, {\n        '$limit': 3\n    }\n])\n```\n\nThe aggregation code is pretty similar in other languages as well.\n\n## Wrap Up\n\nThe MongoDB aggregation framework is an extremely powerful set of tools.  The processing is done on the server itself which results in less data being sent over the network. In the example used here, instead of pulling **all** of the documents into an application and processing them in the application, the aggregation framework allows for only the three documents we wanted from our query to be sent back to the application.\n\nThis was just a brief introduction to some of the operators available.  Over the course of this series, I'll take a closer look at some of the most popular aggregation framework operators as well as some interesting, but less used ones. I'll also take a look at performance considerations of using the aggregation framework.\n","description":"Learn about MongoDB's aggregation framework and aggregation operators.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltd2ff4d87e130f9e2/644c47316b06fe00042844cf/aggregation-framework_ATF.png?branch=prod","description":null}}]},"slug":"/introduction-aggregation-framework","title":"*Introduction to the MongoDB Aggregation Framework","original_publish_date":"2022-02-01T16:45:48.647Z","strapi_updated_at":"2022-09-23T14:28:55.658Z","expiry_date":"2022-12-14T02:03:01.376Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Aggregation Framework","calculated_slug":"/products/mongodb/aggregation-framework"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn about MongoDB's aggregation framework and aggregation operators.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt20eceb68d1ecee83/644c473347e2e2c51ec6d037/og-mdb-developer.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@kenwalger"},"system":{"updated_at":"2023-04-28T22:52:02.292Z","publish_details":{"time":"2023-04-28T22:53:01.454Z"}}},{"calculated_slug":"/languages/java/java-aggregation-pipeline","content":"## Updates\n\nThe MongoDB Java quickstart repository is [available on Github](https://github.com/mongodb-developer/java-quick-start).\n\n### March 25th, 2021\n\n-   Update Java Driver to 4.2.2.\n-   Added Client Side Field Level Encryption example.\n\n### October 21th, 2020\n\n-   Update Java Driver to 4.1.1.\n-   The Java Driver logging is now enabled via the popular [SLF4J](http://www.slf4j.org/) API so I added logback in the `pom.xml` and a configuration file `logback.xml`.\n\n## What's the Aggregation Pipeline?\n\n<div>\n    <img style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-java.png\" alt=\"Java badge\" />\n\nThe [aggregation pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/) is a framework for data aggregation modeled on the concept of data processing pipelines, just like the \"pipe\" in the Linux Shell. Documents enter a multi-stage pipeline that transforms the documents into aggregated results.\n</div>\n\nIt's the most powerful way to work with your data in MongoDB. It will allow us to make advanced queries like grouping documents, manipulate arrays, reshape document models, etc.\n\nLet's see how we can harvest this power using Java.\n\n## Getting Set Up\n\nI will use the same repository as usual in this series. If you don't have a copy of it yet, you can clone it or just update it if you already have it:\n\n``` sh\ngit clone https://github.com/mongodb-developer/java-quick-start\n```\n\n>If you didn't set up your free cluster on MongoDB Atlas, now is great time to do so. You have all the instructions in this [blog post](/quickstart/free-atlas-cluster/).\n\n## First Example with Zips\n\nIn the [MongoDB Sample Dataset](https://docs.atlas.mongodb.com/sample-data/available-sample-datasets/) in [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/), let's explore a bit the `zips` collection in the `sample_training` database.\n\n``` javascript\nMongoDB Enterprise Cluster0-shard-0:PRIMARY> db.zips.find({city:\"NEW YORK\"}).limit(2).pretty()\n{\n    \"_id\" : ObjectId(\"5c8eccc1caa187d17ca72f8a\"),\n    \"city\" : \"NEW YORK\",\n    \"zip\" : \"10001\",\n    \"loc\" : {\n        \"y\" : 40.74838,\n        \"x\" : 73.996705\n    },\n    \"pop\" : 18913,\n    \"state\" : \"NY\"\n}\n{\n    \"_id\" : ObjectId(\"5c8eccc1caa187d17ca72f8b\"),\n    \"city\" : \"NEW YORK\",\n    \"zip\" : \"10003\",\n    \"loc\" : {\n        \"y\" : 40.731253,\n        \"x\" : 73.989223\n    },\n    \"pop\" : 51224,\n    \"state\" : \"NY\"\n}\n```\n\nAs you can see, we have one document for each zip code in the USA and for each, we have the associated population.\n\nTo calculate the population of New York, I would have to sum the population of each zip code to get the population of the entire city.\n\nLet's try to find the 3 biggest cities in the state of Texas. Let's design this on paper first.\n\n-   I don't need to work with the entire collection. I need to filter only the cities in Texas.\n-   Once this is done, I can regroup all the zip code from a same city together to get the total population.\n-   Then I can order my cities by descending order or population.\n-   Finally I can keep the first 3 cities of my list.\n\nThe easiest way to build this pipeline in MongoDB is to use the [aggregation pipeline builder](https://docs.mongodb.com/compass/master/aggregation-pipeline-builder/) that is available in [MongoDB Compass](https://www.mongodb.com/products/compass) or in [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/) in the `Collections` tab.\n\nOnce this is done, you can [export your pipeline to Java](https://docs.mongodb.com/compass/master/export-pipeline-to-language/) using the export button.\n\nAfter a little code refactoring, here is what I have:\n\n``` java\n/**\n * find the 3 most densely populated cities in Texas.\n * @param zips sample_training.zips collection from the MongoDB Sample Dataset in MongoDB Atlas.\n */\nprivate static void threeMostPopulatedCitiesInTexas(MongoCollection<Document> zips) {\n    Bson match = match(eq(\"state\", \"TX\"));\n    Bson group = group(\"$city\", sum(\"totalPop\", \"$pop\"));\n    Bson project = project(fields(excludeId(), include(\"totalPop\"), computed(\"city\", \"$_id\")));\n    Bson sort = sort(descending(\"totalPop\"));\n    Bson limit = limit(3);\n\n    List<Document> results = zips.aggregate(Arrays.asList(match, group, project, sort, limit))\n                                 .into(new ArrayList<>());\n    System.out.println(\"==> 3 most densely populated cities in Texas\");\n    results.forEach(printDocuments());\n}\n```\n\nThe MongoDB driver provides a lot of helpers to make the code easy to write and to read.\n\nAs you can see, I solved this problem with:\n\n-   A [$match stage](https://docs.mongodb.com/manual/reference/operator/aggregation/match/) to filter my documents and keep only the zip code in Texas,\n-   A [$group stage](https://docs.mongodb.com/manual/reference/operator/aggregation/group/) to regroup my zip codes in cities,\n-   A [$project stage](https://docs.mongodb.com/manual/reference/operator/aggregation/project/) to rename the field `_id` in `city` for a clean output (not mandatory but I'm classy),\n-   A [$sort stage](https://docs.mongodb.com/manual/reference/operator/aggregation/sort/) to sort by population descending,\n-   A [$limit stage](https://docs.mongodb.com/manual/reference/operator/aggregation/limit/) to keep only the 3 most populated cities.\n\nHere is the output we get:\n\n``` json\n==> 3 most densely populated cities in Texas\n{\n  \"totalPop\": 2095918,\n  \"city\": \"HOUSTON\"\n}\n{\n  \"totalPop\": 940191,\n  \"city\": \"DALLAS\"\n}\n{\n  \"totalPop\": 811792,\n  \"city\": \"SAN ANTONIO\"\n}\n```\n\nIn MongoDB 4.2, there are 30 different aggregation pipeline stages that you can use to manipulate your documents. If you want to know more, I encourage you to follow this course on MongoDB University: [M121: The MongoDB Aggregation Framework](https://university.mongodb.com/courses/M121/about).\n\n## Second Example with Posts\n\nThis time, I'm using the collection `posts` in the same database.\n\n``` json\nMongoDB Enterprise Cluster0-shard-0:PRIMARY> db.posts.findOne()\n{\n    \"_id\" : ObjectId(\"50ab0f8bbcf1bfe2536dc3f9\"),\n    \"body\" : \"Amendment I\\n<p>Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof; or abridging the freedom of speech, or of the press; or the right of the people peaceably to assemble, and to petition the Government for a redress of grievances.\\n<p>\\nAmendment II\\n<p>\\nA well regulated Militia, being necessary to the security of a free State, the right of the people to keep and bear Arms, shall not be infringed.\\n<p>\\nAmendment III\\n<p>\\nNo Soldier shall, in time of peace be quartered in any house, without the consent of the Owner, nor in time of war, but in a manner to be prescribed by law.\\n<p>\\nAmendment IV\\n<p>\\nThe right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures, shall not be violated, and no Warrants shall issue, but upon probable cause, supported by Oath or affirmation, and particularly describing the place to be searched, and the persons or things to be seized.\\n<p>\\nAmendment V\\n<p>\\nNo person shall be held to answer for a capital, or otherwise infamous crime, unless on a presentment or indictment of a Grand Jury, except in cases arising in the land or naval forces, or in the Militia, when in actual service in time of War or public danger; nor shall any person be subject for the same offence to be twice put in jeopardy of life or limb; nor shall be compelled in any criminal case to be a witness against himself, nor be deprived of life, liberty, or property, without due process of law; nor shall private property be taken for public use, without just compensation.\\n<p>\\n\\nAmendment VI\\n<p>\\nIn all criminal prosecutions, the accused shall enjoy the right to a speedy and public trial, by an impartial jury of the State and district wherein the crime shall have been committed, which district shall have been previously ascertained by law, and to be informed of the nature and cause of the accusation; to be confronted with the witnesses against him; to have compulsory process for obtaining witnesses in his favor, and to have the Assistance of Counsel for his defence.\\n<p>\\nAmendment VII\\n<p>\\nIn Suits at common law, where the value in controversy shall exceed twenty dollars, the right of trial by jury shall be preserved, and no fact tried by a jury, shall be otherwise re-examined in any Court of the United States, than according to the rules of the common law.\\n<p>\\nAmendment VIII\\n<p>\\nExcessive bail shall not be required, nor excessive fines imposed, nor cruel and unusual punishments inflicted.\\n<p>\\nAmendment IX\\n<p>\\nThe enumeration in the Constitution, of certain rights, shall not be construed to deny or disparage others retained by the people.\\n<p>\\nAmendment X\\n<p>\\nThe powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people.\\\"\\n<p>\\n\",\n    \"permalink\" : \"aRjNnLZkJkTyspAIoRGe\",\n    \"author\" : \"machine\",\n    \"title\" : \"Bill of Rights\",\n    \"tags\" : [\n        \"watchmaker\",\n        \"santa\",\n        \"xylophone\",\n        \"math\",\n        \"handsaw\",\n        \"dream\",\n        \"undershirt\",\n        \"dolphin\",\n        \"tanker\",\n        \"action\"\n    ],\n    \"comments\" : [\n        {\n            \"body\" : \"Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\",\n            \"email\" : \"HvizfYVx@pKvLaagH.com\",\n            \"author\" : \"Santiago Dollins\"\n        },\n        {\n            \"body\" : \"Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\",\n            \"email\" : \"glbeRCMi@KwnNwhzl.com\",\n            \"author\" : \"Omar Bowdoin\"\n        }\n    ],\n    \"date\" : ISODate(\"2012-11-20T05:05:15.231Z\")\n}\n```\n\nThis collection of 500 posts has been generated artificially but it contains arrays and I want to show you how we can manipulate arrays in a pipeline.\n\nLet's try to find the three most popular tags and for each tag, I also want the list of post titles they are tagging.\n\nHere is my solution in Java.\n\n``` java\n/**\n * find the 3 most popular tags and their post titles\n * @param posts sample_training.posts collection from the MongoDB Sample Dataset in MongoDB Atlas.\n */\nprivate static void threeMostPopularTags(MongoCollection<Document> posts) {\n    Bson unwind = unwind(\"$tags\");\n    Bson group = group(\"$tags\", sum(\"count\", 1L), push(\"titles\", \"$title\"));\n    Bson sort = sort(descending(\"count\"));\n    Bson limit = limit(3);\n    Bson project = project(fields(excludeId(), computed(\"tag\", \"$_id\"), include(\"count\", \"titles\")));\n    List<Document> results = posts.aggregate(Arrays.asList(unwind, group, sort, limit, project)).into(new ArrayList<>());\n    System.out.println(\"==> 3 most popular tags and their posts titles\");\n    results.forEach(printDocuments());\n}\n```\n\nHere I'm using the very useful [$unwind stage](https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/) to break down my array of tags.\n\nIt allows me in the following $group stage to group my tags, count the posts and collect the titles in a new array `titles`.\n\nHere is the final output I get.\n\n``` json\n==> 3 most popular tags and their posts titles\n{\n  \"count\": 8,\n  \"titles\": [\n    \"Gettysburg Address\",\n    \"US Constitution\",\n    \"Bill of Rights\",\n    \"Gettysburg Address\",\n    \"Gettysburg Address\",\n    \"Declaration of Independence\",\n    \"Bill of Rights\",\n    \"Declaration of Independence\"\n  ],\n  \"tag\": \"toad\"\n}\n{\n  \"count\": 8,\n  \"titles\": [\n    \"Bill of Rights\",\n    \"Gettysburg Address\",\n    \"Bill of Rights\",\n    \"Bill of Rights\",\n    \"Declaration of Independence\",\n    \"Declaration of Independence\",\n    \"Bill of Rights\",\n    \"US Constitution\"\n  ],\n  \"tag\": \"forest\"\n}\n{\n  \"count\": 8,\n  \"titles\": [\n    \"Bill of Rights\",\n    \"Declaration of Independence\",\n    \"Declaration of Independence\",\n    \"Gettysburg Address\",\n    \"US Constitution\",\n    \"Bill of Rights\",\n    \"US Constitution\",\n    \"US Constitution\"\n  ],\n  \"tag\": \"hair\"\n}\n```\n\nAs you can see, some titles are repeated. As I said earlier, the collection was generated so the post titles are not uniq. I could solve this \"problem\" by using the [$addToSet operator](https://docs.mongodb.com/manual/reference/operator/aggregation/addToSet/index.html) instead of the [$push](https://docs.mongodb.com/manual/reference/operator/aggregation/push/) one if this was really an issue.\n\n## Final Code\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.client.MongoClient;\nimport com.mongodb.client.MongoClients;\nimport com.mongodb.client.MongoCollection;\nimport com.mongodb.client.MongoDatabase;\nimport org.bson.Document;\nimport org.bson.conversions.Bson;\nimport org.bson.json.JsonWriterSettings;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.function.Consumer;\n\nimport static com.mongodb.client.model.Accumulators.push;\nimport static com.mongodb.client.model.Accumulators.sum;\nimport static com.mongodb.client.model.Aggregates.*;\nimport static com.mongodb.client.model.Filters.eq;\nimport static com.mongodb.client.model.Projections.*;\nimport static com.mongodb.client.model.Sorts.descending;\n\npublic class AggregationFramework {\n\n    public static void main(String[] args) {\n        String connectionString = System.getProperty(\"mongodb.uri\");\n        try (MongoClient mongoClient = MongoClients.create(connectionString)) {\n            MongoDatabase db = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Document> zips = db.getCollection(\"zips\");\n            MongoCollection<Document> posts = db.getCollection(\"posts\");\n            threeMostPopulatedCitiesInTexas(zips);\n            threeMostPopularTags(posts);\n        }\n    }\n\n    /**\n     * find the 3 most densely populated cities in Texas.\n     * @param zips sample_training.zips collection from the MongoDB Sample Dataset in MongoDB Atlas.\n     */\n    private static void threeMostPopulatedCitiesInTexas(MongoCollection<Document> zips) {\n        Bson match = match(eq(\"state\", \"TX\"));\n        Bson group = group(\"$city\", sum(\"totalPop\", \"$pop\"));\n        Bson project = project(fields(excludeId(), include(\"totalPop\"), computed(\"city\", \"$_id\")));\n        Bson sort = sort(descending(\"totalPop\"));\n        Bson limit = limit(3);\n\n        List<Document> results = zips.aggregate(Arrays.asList(match, group, project, sort, limit))\n                                     .into(new ArrayList<>());\n        System.out.println(\"==> 3 most densely populated cities in Texas\");\n        results.forEach(printDocuments());\n    }\n\n    /**\n     * find the 3 most popular tags and their post titles\n     * @param posts sample_training.posts collection from the MongoDB Sample Dataset in MongoDB Atlas.\n     */\n    private static void threeMostPopularTags(MongoCollection<Document> posts) {\n        Bson unwind = unwind(\"$tags\");\n        Bson group = group(\"$tags\", sum(\"count\", 1L), push(\"titles\", \"$title\"));\n        Bson sort = sort(descending(\"count\"));\n        Bson limit = limit(3);\n        Bson project = project(fields(excludeId(), computed(\"tag\", \"$_id\"), include(\"count\", \"titles\")));\n\n        List<Document> results = posts.aggregate(Arrays.asList(unwind, group, sort, limit, project)).into(new ArrayList<>());\n        System.out.println(\"==> 3 most popular tags and their posts titles\");\n        results.forEach(printDocuments());\n    }\n\n    private static Consumer<Document> printDocuments() {\n        return doc -> System.out.println(doc.toJson(JsonWriterSettings.builder().indent(true).build()));\n    }\n}\n```\n\n## Wrapping Up\n\nThe aggregation pipeline is very powerful. We have just scratched the surface with these two examples but trust me if I tell you that it's your best ally if you can master it.\n\n>I encourage you to follow the [M121 course on MongoDB University](https://university.mongodb.com/courses/M121/about) to become an aggregation pipeline jedi.\n>\n>If you want to learn more and deepen your knowledge faster, I recommend you check out the M220J: MongoDB for Java Developers training available for free on [MongoDB University](https://university.mongodb.com/).\n\nIn the next blog post, I will explain to you the [Change Streams](https://docs.mongodb.com/manual/changeStreams/) in Java.","description":"Learn how to use the Aggregation Pipeline using the MongoDB Java Driver.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blte0424980e374823d/644c4736efd5716730fe6f7b/Java_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/java-aggregation-pipeline","title":"*Java - Aggregation Pipeline","original_publish_date":"2022-02-01T18:03:01.322Z","strapi_updated_at":"2023-01-26T16:56:03.124Z","expiry_date":"2022-12-14T02:03:09.513Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Aggregation Framework","calculated_slug":"/products/mongodb/aggregation-framework"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to use the Aggregation Pipeline using the MongoDB Java Driver.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltea9590d8bc695e7a/644c473775b185097146a90d/Java_Tile_360x360.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@MBeugnet"},"system":{"updated_at":"2023-04-28T22:52:01.895Z","publish_details":{"time":"2023-04-28T22:53:01.484Z"}}},{"calculated_slug":"/languages/java/java-change-streams","content":"## Updates\n\nThe MongoDB Java quickstart repository is [available on Github](https://github.com/mongodb-developer/java-quick-start).\n\n### March 25th, 2021\n\n-   Update Java Driver to 4.2.2.\n-   Added Client Side Field Level Encryption example.\n\n### October 21th, 2020\n\n-   Update Java Driver to 4.1.1.\n-   The Java Driver logging is now enabled via the popular [SLF4J](http://www.slf4j.org/) API so I added logback in the `pom.xml` and a configuration file `logback.xml`.\n\n## Introduction\n\n<div>\n    <img\n        style=\"float: right;\n            border-radius: 10px;\n            margin-bottom: 30px;\n            vertical-align: bottom;\n            width: 30%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-java.png\" alt=\"Java badge\" />\n\nChange Streams were introduced in MongoDB 3.6. They allow applications to access real-time data changes without the complexity and risk of tailing the [oplog](https://docs.mongodb.com/manual/reference/glossary/#term-oplog).\n</div>\n\nApplications can use change streams to subscribe to all data changes on a single collection, a database, or an entire deployment, and immediately react to them. Because change streams use the aggregation framework, an application can also filter for specific changes or transform the notifications at will.\n\nIn this blog post, as promised in the [first blog post](/quickstart/java-setup-crud-operations/) of this series, I will show you how to leverage MongoDB Change Streams using Java.\n\n## Getting Set Up\n\nI will use the same repository as usual in this series. If you don't have a copy of it yet, you can clone it or just update it if you already have it:\n\n``` sh\ngit clone https://github.com/mongodb-developer/java-quick-start\n```\n\n>If you haven't yet set up your free cluster on MongoDB Atlas, now is a great time to do so. You have all the instructions in this [blog post](/quickstart/free-atlas-cluster/).\n\n## Change Streams\n\nIn this blog post, I will be working on the file called `ChangeStreams.java`, but Change Streams are **super** easy to work with.\n\nI will show you 5 different examples to showcase some features of the Change Streams. For the sake of simplicity, I will only show you the pieces of code related to the Change Streams directly. You can find the entire code sample at the bottom of this blog post or in the [Github repository](https://github.com/mongodb-developer/java-quick-start).\n\nFor each example, you will need to start 2 Java programs in the correct order if you want to reproduce my examples.\n\n-   The first program is always the one that contains the Change Streams code.\n-   The second one will be one of the Java programs we already used in this Java blog posts series. You can find them in the Github repository. They will generate MongoDB operations that we will observe in the Change Streams output.\n\n### A simple Change Streams without filters\n\nLet's start with the most simple Change Stream we can make:\n\n``` java\nMongoCollection<Grade> grades = db.getCollection(\"grades\", Grade.class);\nChangeStreamIterable<Grade> changeStream = grades.watch();\nchangeStream.forEach((Consumer<ChangeStreamDocument<Grade>>) System.out::println);\n```\n\nAs you can see, all we need is `myCollection.watch()`! That's it.\n\nThis returns a `ChangeStreamIterable` which, as indicated by its name, can be iterated to return our change events. Here, I'm iterating over my Change Stream to print my change event documents in the Java standard output.\n\nI can also simplify this code like this:\n\n``` java\ngrades.watch().forEach(printEvent());\n\nprivate static Consumer<ChangeStreamDocument<Grade>> printEvent() {\n    return System.out::println;\n}\n```\n\nI will reuse this functional interface in my following examples to ease the reading.\n\nTo run this example:\n\n-   Uncomment only the example 1 from the `ChangeStreams.java` file and start it in your IDE or a dedicated console using Maven in the root of your project.\n\n``` bash\nmvn compile exec:java -Dexec.mainClass=\"com.mongodb.quickstart.ChangeStreams\" -Dmongodb.uri=\"mongodb+srv://USERNAME:PASSWORD@cluster0-abcde.mongodb.net/test?w=majority\"\n```\n\n-   Start `MappingPOJO.java` in another console or in your IDE.\n\n``` bash\nmvn compile exec:java -Dexec.mainClass=\"com.mongodb.quickstart.MappingPOJO\" -Dmongodb.uri=\"mongodb+srv://USERNAME:PASSWORD@cluster0-abcde.mongodb.net/test?w=majority\"\n```\n\nIn MappingPOJO, we are doing 4 MongoDB operations:\n\n-   I'm creating a new `Grade` document with the `insertOne()` method,\n-   I'm searching for this `Grade` document using the `find()` method,\n-   I'm replacing entirely this `Grade` using the `findOneAndReplace()` method,\n-   and finally, I'm deleting this `Grade` using the `deleteOne()` method.\n\nThis is confirmed in the standard output from `MappingJava`:\n\n``` javascript\nGrade inserted.\nGrade found:    Grade{id=5e2b4a28c9e9d55e3d7dbacf, student_id=10003.0, class_id=10.0, scores=[Score{type='homework', score=50.0}]}\nGrade replaced: Grade{id=5e2b4a28c9e9d55e3d7dbacf, student_id=10003.0, class_id=10.0, scores=[Score{type='homework', score=50.0}, Score{type='exam', score=42.0}]}\nGrade deleted:  AcknowledgedDeleteResult{deletedCount=1}\n```\n\nLet's check what we have in the standard output from `ChangeStreams.java` (prettified):\n\n``` javascript\nChangeStreamDocument{\n   operationType=OperationType{ value='insert' },\n   resumeToken={ \"_data\":\"825E2F3E40000000012B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E2F3E400C47CF19D59361620004\" },\n   namespace=sample_training.grades,\n   destinationNamespace=null,\n   fullDocument=Grade{\n      id=5e2f3e400c47cf19d5936162,\n      student_id=10003.0,\n      class_id=10.0,\n      scores=[ Score { type='homework', score=50.0 } ]\n   },\n   documentKey={ \"_id\":{ \"$oid\":\"5e2f3e400c47cf19d5936162\" } },\n   clusterTime=Timestamp{\n      value=6786711608069455873,\n      seconds=1580154432,\n      inc=1\n   },\n   updateDescription=null,\n   txnNumber=null,\n   lsid=null\n}\nChangeStreamDocument{ operationType=OperationType{ value= 'replace' },\n   resumeToken={ \"_data\":\"825E2F3E40000000032B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E2F3E400C47CF19D59361620004\" },\n   namespace=sample_training.grades,\n   destinationNamespace=null,\n   fullDocument=Grade{\n      id=5e2f3e400c47cf19d5936162,\n      student_id=10003.0,\n      class_id=10.0,\n      scores=[ Score{ type='homework', score=50.0 }, Score{ type='exam', score=42.0 } ]\n   },\n   documentKey={ \"_id\":{ \"$oid\":\"5e2f3e400c47cf19d5936162\" } },\n   clusterTime=Timestamp{\n      value=6786711608069455875,\n      seconds=1580154432,\n      inc=3\n   },\n   updateDescription=null,\n   txnNumber=null,\n   lsid=null\n}\nChangeStreamDocument{\n   operationType=OperationType{ value='delete' },\n   resumeToken={ \"_data\":\"825E2F3E40000000042B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E2F3E400C47CF19D59361620004\" },\n   namespace=sample_training.grades,\n   destinationNamespace=null,\n   fullDocument=null,\n   documentKey={ \"_id\":{ \"$oid\":\"5e2f3e400c47cf19d5936162\" } },\n   clusterTime=Timestamp{\n      value=6786711608069455876,\n      seconds=1580154432,\n      inc=4\n   },\n   updateDescription=null,\n   txnNumber=null,\n   lsid=null\n}\n```\n\nAs you can see, only 3 operations appear in the Change Stream: - insert, - replace, - delete.\n\nIt was expected because the `find()` operation is just a reading document from MongoDB. It's not changing anything thus not generating an event in the Change Stream.\n\nNow that we are done with the basic example, let's explore some features of the Change Streams.\n\nTerminate the Change Stream program we started earlier and let's move on.\n\n### A simple Change Stream filtering on the operation type\n\nNow let's do the same thing but let's imagine that we are only interested by insert and delete operations.\n\n``` java\nList<Bson> pipeline = singletonList(match(in(\"operationType\", asList(\"insert\", \"delete\"))));\ngrades.watch(pipeline).forEach(printEvent());\n```\n\nAs you can see here, I'm using the aggregation pipeline feature of Change Streams to filter down the change events I want to process.\n\nUncomment the example 2 in `ChangeStreams.java` and execute the program followed by `MappingPOJO.java`, just like we did earlier.\n\nHere are the change events I'm receiving.\n\n``` json\nChangeStreamDocument {operationType=OperationType {value= 'insert'},\n  resumeToken= {\"_data\": \"825E2F4983000000012B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E2F4983CC1D2842BFF555640004\"},\n  namespace=sample_training.grades,\n  destinationNamespace=null,\n  fullDocument=Grade\n  {\n    id=5e2f4983cc1d2842bff55564,\n    student_id=10003.0,\n    class_id=10.0,\n    scores= [ Score {type= 'homework', score=50.0}]\n  },\n  documentKey= {\"_id\": {\"$oid\": \"5e2f4983cc1d2842bff55564\" }},\n  clusterTime=Timestamp {value=6786723990460170241, seconds=1580157315, inc=1 },\n  updateDescription=null,\n  txnNumber=null,\n  lsid=null\n}\n\nChangeStreamDocument { operationType=OperationType {value= 'delete'},\n  resumeToken= {\"_data\": \"825E2F4983000000042B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E2F4983CC1D2842BFF555640004\"},\n  namespace=sample_training.grades,\n  destinationNamespace=null,\n  fullDocument=null,\n  documentKey= {\"_id\": {\"$oid\": \"5e2f4983cc1d2842bff55564\"}},\n  clusterTime=Timestamp {value=6786723990460170244, seconds=1580157315, inc=4},\n  updateDescription=null,\n  txnNumber=null,\n  lsid=null\n  }\n]\n```\n\nThis time, I'm only getting 2 events `insert` and `delete`. The `replace` event has been filtered out compared to the first example.\n\n### Change Stream default behavior with update operations\n\nSame as earlier, I'm filtering my change stream to keep only the update operations this time.\n\n``` java\nList<Bson> pipeline = singletonList(match(eq(\"operationType\", \"update\")));\ngrades.watch(pipeline).forEach(printEvent());\n```\n\nThis time, follow these steps.\n\n-   uncomment the example 3 in `ChangeStreams.java`,\n-   if you never ran `Create.java`, run it. We are going to use these new documents in the next step.\n-   start `Update.java` in another console.\n\nIn your change stream console, you should see 13 update events. Here is the first one:\n\n``` json\nChangeStreamDocument {operationType=OperationType {value= 'update'},\n  resumeToken= {\"_data\": \"825E2FB83E000000012B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCCE74AA51A0486763FE0004\"},\n  namespace=sample_training.grades,\n  destinationNamespace=null,\n  fullDocument=null,\n  documentKey= {\"_id\": {\"$oid\": \"5e27bcce74aa51a0486763fe\"}},\n  clusterTime=Timestamp {value=6786845739898109953, seconds=1580185662, inc=1},\n  updateDescription=UpdateDescription {removedFields= [], updatedFields= {\"comments.10\": \"You will learn a lot if you read the MongoDB blog!\"}},\n  txnNumber=null,\n  lsid=null\n}\n```\n\nAs you can see, we are retrieving our update operation in the `updateDescription` field, but we are only getting the difference with the previous version of this document.\n\nThe `fullDocument` field is `null` because, by default, MongoDB only sends the difference to avoid overloading the change stream with potentially useless information.\n\nLet's see how we can change this behavior in the next example.\n\n### Change Stream with \"Update Lookup\"\n\nFor this part, uncomment the example 4 from `ChangeStreams.java` and execute the programs as above.\n\n``` java\nList<Bson> pipeline = singletonList(match(eq(\"operationType\", \"update\")));\ngrades.watch(pipeline).fullDocument(UPDATE_LOOKUP).forEach(printEvent());\n```\n\nI added the option `UPDATE_LOOKUP` this time so we can also retrieve the entire document during an update operation.\n\nLet's see again the first update in my change stream:\n\n``` json\nChangeStreamDocument {operationType=OperationType {value= 'update'},\n  resumeToken= {\"_data\": \"825E2FBBC1000000012B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCCE74AA51A0486763FE0004\"},\n  namespace=sample_training.grades,\n  destinationNamespace=null,\n  fullDocument=Grade\n    {\n      id=5e27bcce74aa51a0486763fe,\n      student_id=10002.0,\n      class_id=10.0,\n      scores=null\n    },\n  documentKey= {\"_id\": {\"$oid\": \"5e27bcce74aa51a0486763fe\" }},\n  clusterTime=Timestamp {value=6786849601073709057, seconds=1580186561, inc=1 },\n  updateDescription=UpdateDescription {removedFields= [], updatedFields= {\"comments.11\": \"You will learn a lot if you read the MongoDB blog!\"}},\n  txnNumber=null,\n  lsid=null\n}\n```\n\n>Note: The `Update.java` program updates a made-up field \"comments\" that doesn't exist in my POJO `Grade` which represents the original schema for this collection. Thus the field doesn't appear in the output as it's not mapped.\n\nIf I want to see this `comments` field, I can use a `MongoCollection` not mapped automatically to my `Grade.java` POJO.\n\n``` java\nMongoCollection<Document> grades = db.getCollection(\"grades\");\nList<Bson> pipeline = singletonList(match(eq(\"operationType\", \"update\")));\ngrades.watch(pipeline).fullDocument(UPDATE_LOOKUP).forEach((Consumer<ChangeStreamDocument<Document>>) System.out::println);\n```\n\nThen this is what I get in my change stream:\n\n``` json\nChangeStreamDocument {operationType=OperationType {value= 'update'},\n  resumeToken= {\"_data\": \"825E2FBD89000000012B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCCE74AA51A0486763FE0004\"},\n  namespace=sample_training.grades,\n  destinationNamespace=null,\n  fullDocument=Document {\n    {\n      _id=5e27bcce74aa51a0486763fe,\n      class_id=10.0,\n      student_id=10002.0,\n      comments= [ You will learn a lot if you read the MongoDB blog!, [...], You will learn a lot if you read the MongoDB blog!]\n    }\n  },\n  documentKey= {\"_id\": {\"$oid\": \"5e27bcce74aa51a0486763fe\"}},\n  clusterTime=Timestamp {value=6786851559578796033, seconds=1580187017, inc=1},\n  updateDescription=UpdateDescription {removedFields= [], updatedFields= {\"comments.13\": \"You will learn a lot if you read the MongoDB blog!\"}},\n  txnNumber=null,\n  lsid=null\n}\n```\n\nI have shortened the `comments` field to keep it readable but it contains 14 times the same comment in my case.\n\nThe full document we are retrieving here during our update operation is the document **after** the update has occurred. Read more about this in [our documentation](https://docs.mongodb.com/manual/changeStreams/#lookup-full-document-for-update-operations).\n\n### Change Streams are resumable\n\nIn this final example 5, I have simulated an error and I'm restarting my Change Stream from a `resumeToken` I got from a previous operation in my Change Stream.\n\n>It's important to note that a change stream will resume itself automatically in the face of an \"incident\". Generally, the only reason that an application needs to restart the change stream manually from a resume token is if there is an incident in the application itself rather than the change stream (e.g. an operator has decided that the application needs to be restarted).\n\n``` java\nprivate static void exampleWithResumeToken(MongoCollection<Grade> grades) {\n    List<Bson> pipeline = singletonList(match(eq(\"operationType\", \"update\")));\n    ChangeStreamIterable<Grade> changeStream = grades.watch(pipeline);\n    MongoChangeStreamCursor<ChangeStreamDocument<Grade>> cursor = changeStream.cursor();\n    System.out.println(\"==> Going through the stream a first time & record a resumeToken\");\n    int indexOfOperationToRestartFrom = 5;\n    int indexOfIncident = 8;\n    int counter = 0;\n    BsonDocument resumeToken = null;\n    while (cursor.hasNext() && counter != indexOfIncident) {\n        ChangeStreamDocument<Grade> event = cursor.next();\n        if (indexOfOperationToRestartFrom == counter) {\n            resumeToken = event.getResumeToken();\n        }\n        System.out.println(event);\n        counter++;\n    }\n    System.out.println(\"==> Let's imagine something wrong happened and I need to restart my Change Stream.\");\n    System.out.println(\"==> Starting from resumeToken=\" + resumeToken);\n    assert resumeToken != null;\n    grades.watch(pipeline).resumeAfter(resumeToken).forEach(printEvent());\n}\n```\n\nFor this final example, the same as earlier. Uncomment the part 5 (which is just calling the method above) and start `ChangeStreams.java` then `Update.java`.\n\nThis is the output you should get:\n\n``` json\n==> Going through the stream a first time & record a resumeToken\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC276000000012B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCCE74AA51A0486763FE0004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcce74aa51a0486763fe\"}}, clusterTime=Timestamp{value=6786856975532556289, seconds=1580188278, inc=1}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC276000000022B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBBA0004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbba\"}}, clusterTime=Timestamp{value=6786856975532556290, seconds=1580188278, inc=2}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.15\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC276000000032B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBBB0004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbbb\"}}, clusterTime=Timestamp{value=6786856975532556291, seconds=1580188278, inc=3}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC276000000042B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBBC0004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbbc\"}}, clusterTime=Timestamp{value=6786856975532556292, seconds=1580188278, inc=4}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC276000000052B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBBD0004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbbd\"}}, clusterTime=Timestamp{value=6786856975532556293, seconds=1580188278, inc=5}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC276000000062B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBBE0004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbbe\"}}, clusterTime=Timestamp{value=6786856975532556294, seconds=1580188278, inc=6}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC276000000072B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBBF0004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbbf\"}}, clusterTime=Timestamp{value=6786856975532556295, seconds=1580188278, inc=7}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC276000000082B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBC00004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbc0\"}}, clusterTime=Timestamp{value=6786856975532556296, seconds=1580188278, inc=8}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\n==> Let's imagine something wrong happened and I need to restart my Change Stream.\n==> Starting from resumeToken={\"_data\": \"825E2FC276000000062B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBBE0004\"}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC276000000072B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBBF0004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbbf\"}}, clusterTime=Timestamp{value=6786856975532556295, seconds=1580188278, inc=7}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC276000000082B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBC00004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbc0\"}}, clusterTime=Timestamp{value=6786856975532556296, seconds=1580188278, inc=8}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC276000000092B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBC10004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbc1\"}}, clusterTime=Timestamp{value=6786856975532556297, seconds=1580188278, inc=9}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC2760000000A2B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBC20004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbc2\"}}, clusterTime=Timestamp{value=6786856975532556298, seconds=1580188278, inc=10}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC2760000000B2B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBC30004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbc3\"}}, clusterTime=Timestamp{value=6786856975532556299, seconds=1580188278, inc=11}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"comments.14\": \"You will learn a lot if you read the MongoDB blog!\"}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC2760000000D2B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC8F94B5117D894CBB90004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc8f94b5117d894cbb9\"}}, clusterTime=Timestamp{value=6786856975532556301, seconds=1580188278, inc=13}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"scores.0.score\": 904745.0267635228, \"x\": 150}}, txnNumber=null, lsid=null}\nChangeStreamDocument{ operationType=OperationType{value='update'}, resumeToken={\"_data\": \"825E2FC2760000000F2B022C0100296E5A100496C525567BB74BD28BFD504F987082C046645F696400645E27BCC9F94B5117D894CBBA0004\"}, namespace=sample_training.grades, destinationNamespace=null, fullDocument=null, documentKey={\"_id\": {\"$oid\": \"5e27bcc9f94b5117d894cbba\"}}, clusterTime=Timestamp{value=6786856975532556303, seconds=1580188278, inc=15}, updateDescription=UpdateDescription{removedFields=[], updatedFields={\"scores.0.score\": 2126144.0353088505, \"x\": 150}}, txnNumber=null, lsid=null}\n```\n\nAs you can see here, I was able to stop reading my Change Stream and, from the `resumeToken` I collected earlier, I can start a new Change Stream from this point in time.\n\n## Final Code\n\n`ChangeStreams.java` ([code](https://github.com/mongodb-developer/java-quick-start/blob/master/src/main/java/com/mongodb/quickstart/ChangeStreams.java)):\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.ConnectionString;\nimport com.mongodb.MongoClientSettings;\nimport com.mongodb.client.*;\nimport com.mongodb.client.model.changestream.ChangeStreamDocument;\nimport com.mongodb.quickstart.models.Grade;\nimport org.bson.BsonDocument;\nimport org.bson.Document;\nimport org.bson.codecs.configuration.CodecRegistry;\nimport org.bson.codecs.pojo.PojoCodecProvider;\nimport org.bson.conversions.Bson;\n\nimport java.util.List;\nimport java.util.function.Consumer;\n\nimport static com.mongodb.client.model.Aggregates.match;\nimport static com.mongodb.client.model.Filters.eq;\nimport static com.mongodb.client.model.Filters.in;\nimport static com.mongodb.client.model.changestream.FullDocument.UPDATE_LOOKUP;\nimport static java.util.Arrays.asList;\nimport static java.util.Collections.singletonList;\nimport static org.bson.codecs.configuration.CodecRegistries.fromProviders;\nimport static org.bson.codecs.configuration.CodecRegistries.fromRegistries;\n\npublic class ChangeStreams {\n\n    public static void main(String[] args) {\n        ConnectionString connectionString = new ConnectionString(System.getProperty(\"mongodb.uri\"));\n        CodecRegistry pojoCodecRegistry = fromProviders(PojoCodecProvider.builder().automatic(true).build());\n        CodecRegistry codecRegistry = fromRegistries(MongoClientSettings.getDefaultCodecRegistry(), pojoCodecRegistry);\n        MongoClientSettings clientSettings = MongoClientSettings.builder()\n                                                                .applyConnectionString(connectionString)\n                                                                .codecRegistry(codecRegistry)\n                                                                .build();\n\n        try (MongoClient mongoClient = MongoClients.create(clientSettings)) {\n            MongoDatabase db = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Grade> grades = db.getCollection(\"grades\", Grade.class);\n            List<Bson> pipeline;\n\n            // Only uncomment one example at a time. Follow instructions for each individually then kill all remaining processes.\n\n            /** => Example 1: print all the write operations.\n             *  => Start \"ChangeStreams\" then \"MappingPOJOs\" to see some change events.\n             */\n            grades.watch().forEach(printEvent());\n\n            /** => Example 2: print only insert and delete operations.\n             *  => Start \"ChangeStreams\" then \"MappingPOJOs\" to see some change events.\n             */\n            // pipeline = singletonList(match(in(\"operationType\", asList(\"insert\", \"delete\"))));\n            // grades.watch(pipeline).forEach(printEvent());\n\n            /** => Example 3: print only updates without fullDocument.\n             *  => Start \"ChangeStreams\" then \"Update\" to see some change events (start \"Create\" before if not done earlier).\n             */\n            // pipeline = singletonList(match(eq(\"operationType\", \"update\")));\n            // grades.watch(pipeline).forEach(printEvent());\n\n            /** => Example 4: print only updates with fullDocument.\n             *  => Start \"ChangeStreams\" then \"Update\" to see some change events.\n             */\n            // pipeline = singletonList(match(eq(\"operationType\", \"update\")));\n            // grades.watch(pipeline).fullDocument(UPDATE_LOOKUP).forEach(printEvent());\n\n            /**\n             * => Example 5: iterating using a cursor and a while loop + remembering a resumeToken then restart the Change Streams.\n             * => Start \"ChangeStreams\" then \"Update\" to see some change events.\n             */\n            // exampleWithResumeToken(grades);\n        }\n    }\n\n    private static void exampleWithResumeToken(MongoCollection<Grade> grades) {\n        List<Bson> pipeline = singletonList(match(eq(\"operationType\", \"update\")));\n        ChangeStreamIterable<Grade> changeStream = grades.watch(pipeline);\n        MongoChangeStreamCursor<ChangeStreamDocument<Grade>> cursor = changeStream.cursor();\n        System.out.println(\"==> Going through the stream a first time & record a resumeToken\");\n        int indexOfOperationToRestartFrom = 5;\n        int indexOfIncident = 8;\n        int counter = 0;\n        BsonDocument resumeToken = null;\n        while (cursor.hasNext() && counter != indexOfIncident) {\n            ChangeStreamDocument<Grade> event = cursor.next();\n            if (indexOfOperationToRestartFrom == counter) {\n                resumeToken = event.getResumeToken();\n            }\n            System.out.println(event);\n            counter++;\n        }\n        System.out.println(\"==> Let's imagine something wrong happened and I need to restart my Change Stream.\");\n        System.out.println(\"==> Starting from resumeToken=\" + resumeToken);\n        assert resumeToken != null;\n        grades.watch(pipeline).resumeAfter(resumeToken).forEach(printEvent());\n    }\n\n    private static Consumer<ChangeStreamDocument<Grade>> printEvent() {\n        return System.out::println;\n    }\n}\n```\n\n>Remember to uncomment only one Change Stream example at a time.\n\n## Wrapping Up\n\nChange Streams are very easy to use and setup in MongoDB. They are the key to any real-time processing system.\n\nThe only remaining problem here is how to get this in production correctly. Change Streams are basically an infinite loop, processing an infinite stream of events. Multiprocessing is, of course, a must-have for this kind of setup, especially if your processing time is greater than the time separating 2 events.\n\nScaling up correctly a Change Stream data processing pipeline can be tricky. That's why you can implement this easily using [MongoDB Triggers in MongoDB Realm](https://docs.mongodb.com/realm/triggers/database-triggers/).\n\nYou can check out my [MongoDB Realm sample application](https://github.com/MaBeuLux88/mongodb-stitch-movie-collection) if you want to see a real example with several Change Streams in action.\n\n>If you want to learn more and deepen your knowledge faster, I recommend you check out the M220J: MongoDB for Java Developers training available for free on [MongoDB University](https://university.mongodb.com/).\n\nIn the next blog post, I will show you multi-document ACID transactions in Java.","description":"Learn how to use the Change Streams using the MongoDB Java Driver.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt8f7472526173bea9/644c4738b2699f1a38d036c3/JavaArtWork.png?branch=prod","description":null}}]},"slug":"/java-change-streams","title":"*Java - Change Streams","original_publish_date":"2022-02-01T18:10:45.394Z","strapi_updated_at":"2023-01-26T16:56:03.124Z","expiry_date":"2022-12-14T02:03:18.239Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Change Streams","calculated_slug":"/products/mongodb/change-streams"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to use the Change Streams using the MongoDB Java Driver.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blte4d352cbee89ad36/644c47391297ea7caed1f5df/JavaThumb.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@MBeugnet"},"system":{"updated_at":"2023-04-28T22:52:01.502Z","publish_details":{"time":"2023-04-28T22:53:01.518Z"}}},{"calculated_slug":"/languages/java/java-client-side-field-level-encryption","content":"## Updates\n\nThe MongoDB Java quickstart repository is [available on Github](https://github.com/mongodb-developer/java-quick-start).\n\n### March 25th, 2021\n\n-   Update Java Driver to 4.2.2.\n-   Added Client Side Field Level Encryption example.\n\n### October 21th, 2020\n\n-   Update Java Driver to 4.1.1.\n-   The Java Driver logging is now enabled via the popular [SLF4J](http://www.slf4j.org/) API so I added logback in the `pom.xml` and a configuration file `logback.xml`.\n\n## What's the Client Side Field Level Encryption?\n\n<div>\n    <img \n        style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-java.png\" alt=\"Java badge\" />\n\nThe [Client Side Field Level Encryption](https://docs.mongodb.com/manual/core/security-client-side-encryption/) (CSFLE for short) is a new feature added in MongoDB 4.2 that allows you to encrypt some fields of your MongoDB documents prior to transmitting them over the wire to the cluster for storage.\n</div>\n\nIt's the ultimate piece of security against any kind of intrusion or snooping around your MongoDB cluster. Only the application with the correct encryption keys can decrypt and read the protected data.\n\nLet's check out the Java CSFLE API with a simple example.\n\n## Getting Set Up\n\nI will use the same repository as usual in this series. If you don't have a copy of it yet, you can clone it or just update it if you already have it:\n\n``` sh\ngit clone https://github.com/mongodb-developer/java-quick-start\n```\n\n>\n>\n>If you didn't set up your free cluster on MongoDB Atlas, now is great time to do so. You have all the instructions in this [blog post](/quickstart/free-atlas-cluster/).\n>\n>\n\nFor this CSFLE quickstart blog post, I will only use the Community Edition of MongoDB. As a matter of fact, the only part of CSFLE that is an enterprise-only feature is the automatic [encryption of fields](https://docs.mongodb.com/manual/core/security-client-side-encryption/#supported-encryption-methods) which is enforced by [mongocryptd](https://docs.mongodb.com/manual/reference/security-client-side-encryption-appendix/#mongocryptd).\n\nIn this tutorial, I will be using the explicit (or manual) encryption of fields which doesn't require `mongocryptd` and the enterprise edition of MongoDB or Atlas. If you would like to explore the enterprise version of CSFLE with Java, you can find out more in [this documentation](https://mongodb.github.io/mongo-java-driver/4.2/driver/tutorials/client-side-encryption/#examples).\n\n>\n>\n>Do not confuse `mongocryptd` with the `libmongocrypt` library which is the companion C library used by the drivers to encrypt and decrypt your data. We *need* this library to run CSFLE. I added it in the `pom.xml` file of this project.\n>\n>\n\n``` xml\n<dependency>\n  <groupId>org.mongodb</groupId>\n  <artifactId>mongodb-crypt</artifactId>\n  <version>1.2.0</version>\n</dependency>\n```\n\nTo keep the code samples short and sweet in the examples below, I will only share the most relevant parts. If you want to see the code working with all its context, please check the source code in the github repository in the [csfle package](https://github.com/mongodb-developer/java-quick-start/tree/master/src/main/java/com/mongodb/quickstart/csfle) directly.\n\n## Run the Quickstart Code\n\nIn this quickstart tutorial, I will show you the CSFLE API using the MongoDB Java Driver. I will show you how to:\n\n-   create and configure the MongoDB connections we need.\n-   create a master key.\n-   create Data Encryption Keys (DEK).\n-   create and read encrypted documents.\n\nTo run my code from the above repository, check out the [README](https://github.com/mongodb-developer/java-quick-start/blob/master/README.md).\n\nBut for short, the following command should get you up and running in no time:\n\n``` shell\nmvn compile exec:java -Dexec.mainClass=\"com.mongodb.quickstart.csfle.ClientSideFieldLevelEncryption\" -Dmongodb.uri=\"mongodb+srv://USERNAME:PASSWORD@cluster0-abcde.mongodb.net/test?w=majority\"\n```\n\nThis is the output you should get:\n\n``` none\n**************\n* MASTER KEY *\n**************\nA new Master Key has been generated and saved to file \"master_key.txt\".\nMaster Key: [100, 82, 127, -61, -92, -93, 0, -11, 41, -96, 89, -39, -26, -25, -33, 37, 85, -50, 64, 70, -91, 99, -44, -57, 18, 105, -101, -111, -67, -81, -19, 56, -112, 62, 11, 106, -6, 85, -125, 49, -7, -49, 38, 81, 24, -48, -6, -15, 21, -120, -37, -5, 65, 82, 74, -84, -74, -65, -43, -15, 40, 80, -23, -52, -114, -18, -78, -64, -37, -3, -23, -33, 102, -44, 32, 65, 70, -123, -97, -49, -13, 126, 33, -63, -75, -52, 78, -5, -107, 91, 126, 103, 118, 104, 86, -79]\n\n******************\n* INITIALIZATION *\n******************\n=> Creating local Key Management System using the master key.\n=> Creating encryption client.\n=> Creating MongoDB client with automatic decryption.\n=> Cleaning entire cluster.\n\n*************************************\n* CREATE KEY ALT NAMES UNIQUE INDEX *\n*************************************\n\n*******************************\n* CREATE DATA ENCRYPTION KEYS *\n*******************************\nCreated Bobby's data key ID: 668a35af-df8f-4c41-9493-8d09d3d46d3b\nCreated Alice's data key ID: 003024b3-a3b6-490a-9f31-7abb7bcc334d\n\n************************************************\n* INSERT ENCRYPTED DOCUMENTS FOR BOBBY & ALICE *\n************************************************\n2 docs have been inserted.\n\n**********************************\n* FIND BOBBY'S DOCUMENT BY PHONE *\n**********************************\nBobby document found by phone number:\n{\n  \"_id\": {\n    \"$oid\": \"60551bc8dd8b737958e3733f\"\n  },\n  \"name\": \"Bobby\",\n  \"age\": 33,\n  \"phone\": \"01 23 45 67 89\",\n  \"blood_type\": \"A+\",\n  \"medical_record\": [\n    {\n      \"test\": \"heart\",\n      \"result\": \"bad\"\n    }\n  ]\n}\n\n****************************\n* READING ALICE'S DOCUMENT *\n****************************\nBefore we remove Alice's key, we can read her document.\n{\n  \"_id\": {\n    \"$oid\": \"60551bc8dd8b737958e37340\"\n  },\n  \"name\": \"Alice\",\n  \"age\": 28,\n  \"phone\": \"09 87 65 43 21\",\n  \"blood_type\": \"O+\"\n}\n\n***************************************************************\n* REMOVE ALICE's KEY + RESET THE CONNECTION (reset DEK cache) *\n***************************************************************\nAlice key is now removed: 1 key removed.\n=> Creating MongoDB client with automatic decryption.\n\n****************************************\n* TRY TO READ ALICE DOC AGAIN BUT FAIL *\n****************************************\nWe get a MongoException because 'libmongocrypt' can't decrypt these fields anymore.\n```\n\nLet's have a look in depth to understand what is happening.\n\n## How it Works\n\n![CSFLE diagram with master key and DEK vault](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/quickstart/java-csfle/csfle-diagram.png)\n\nCSFLE looks complicated, like any security and encryption feature, I guess. Let's try to make it simple in a few words.\n\n1.  We need a [master key](https://docs.mongodb.com/drivers/security/client-side-field-level-encryption-guide/#std-label-fle-create-a-master-key) which unlocks all the [Data Encryption Keys](https://docs.mongodb.com/drivers/security/client-side-field-level-encryption-guide/#b.-create-a-data-encryption-key) (DEK for short) that we can use to encrypt one or more fields in our documents.\n2.  You can use one DEK for our entire cluster or a different DEK for each field of each document in your cluster. It's up to you.\n3.  The DEKs are stored in a collection in a MongoDB cluster which does **not** have to be the same that contains the encrypted data. The DEKs are stored **encrypted**. They are useless without the master key which needs to be protected.\n4.  You can use the manual (community edition) or the automated (enterprise advanced or Atlas) encryption of fields.\n5.  The decryption can be manual or automated. Both are part of the community edition of MongoDB. In this blog post, I will use manual encryption and automated decryption to stick with the community edition of MongoDB.\n\n## GDPR Compliance\n\n<div>\n<img style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 40%;\" alt=\"GDPR logo\" src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/quickstart/java-csfle/GDPR.jpg\" />\n\nEuropean laws enforce data protection and privacy. Any oversight can result in massive fines.\n\nCSFLE is a great way to save [millions of dollars/euros](https://en.wikipedia.org/wiki/GDPR_fines_and_notices).\n\nFor example, CSFLE could be a great way to enforce the [\"right-to-be-forgotten\" policy](https://gdpr-info.eu/art-17-gdpr/) of GDPR. If a user asks to be removed from your systems, the data must be erased from your production cluster, of course, but also the logs, the dev environment, and the backups... And let's face it: Nobody will ever remove this user's data from the backups. And if you ever restore or use these backups, this can cost you millions of dollars/euros.\n</div>\n\nBut now... encrypt each user's data with a unique Data Encryption Key (DEK) and to \"forget\" a user forever, all you have to do is lose the key. So, saving the DEKs on a separated cluster and enforcing a low retention policy on this cluster will ensure that a user is truly forgotten forever once the key is deleted.\n\n[Kenneth White](https://www.linkedin.com/in/biotech/), Security Principal at MongoDB who worked on CSFLE, explains this perfectly in [this answer](https://developer.mongodb.com/community/forums/t/client-side-field-level-encryption-deks-and-backups/13577/2) in the [MongoDB Community Forum](https://developer.mongodb.com/community/forums/).\n\n>\n>\n>If the primary motivation is just to provably ensure that deleted plaintext user records remain deleted no matter what, then it becomes a simple timing and separation of concerns strategy, and the most straight-forward solution is to move the keyvault collection to a different database or cluster completely, configured with a much shorter backup retention; FLE does not assume your encrypted keyvault collection is co-resident with your active cluster or has the same access controls and backup history, just that the client can, when needed, make an authenticated connection to that keyvault database. Important to note though that with a shorter backup cycle, in the event of some catastrophic data corruption (malicious, intentional, or accidental), all keys for that db (and therefore all encrypted data) are only as recoverable to the point in time as the shorter keyvault backup would restore.\n>\n>\n\nMore trivial, but in the event of an intrusion, any stolen data will be completely worthless without the master key and would not result in a ruinous fine.\n\n## The Master Key\n\nThe master key is an array of 96 bytes. It can be stored in a Key Management Service in a cloud provider or can be locally managed ([documentation](https://docs.mongodb.com/drivers/security/client-side-field-level-encryption-local-key-to-kms/)).  One way or another, you must secure it from any threat.\n\nIt's as simple as that to generate a new one:\n\n``` java\nfinal byte[] masterKey = new byte[96];\nnew SecureRandom().nextBytes(masterKey);\n```\n\nBut you most probably just want to do this once and then reuse the same one each time you restart your application.\n\nHere is my implementation to store it in a local file the first time and then reuse it for each restart.\n\n``` java\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.security.SecureRandom;\nimport java.util.Arrays;\n\npublic class MasterKey {\n\n    private static final int SIZE_MASTER_KEY = 96;\n    private static final String MASTER_KEY_FILENAME = \"master_key.txt\";\n\n    public static void main(String[] args) {\n        new MasterKey().tutorial();\n    }\n\n    private void tutorial() {\n        final byte[] masterKey = generateNewOrRetrieveMasterKeyFromFile(MASTER_KEY_FILENAME);\n        System.out.println(\"Master Key: \" + Arrays.toString(masterKey));\n    }\n\n    private byte[] generateNewOrRetrieveMasterKeyFromFile(String filename) {\n        byte[] masterKey = new byte[SIZE_MASTER_KEY];\n        try {\n            retrieveMasterKeyFromFile(filename, masterKey);\n            System.out.println(\"An existing Master Key was found in file \\\"\" + filename + \"\\\".\");\n        } catch (IOException e) {\n            masterKey = generateMasterKey();\n            saveMasterKeyToFile(filename, masterKey);\n            System.out.println(\"A new Master Key has been generated and saved to file \\\"\" + filename + \"\\\".\");\n        }\n        return masterKey;\n    }\n\n    private void retrieveMasterKeyFromFile(String filename, byte[] masterKey) throws IOException {\n        try (FileInputStream fis = new FileInputStream(filename)) {\n            fis.read(masterKey, 0, SIZE_MASTER_KEY);\n        }\n    }\n\n    private byte[] generateMasterKey() {\n        byte[] masterKey = new byte[SIZE_MASTER_KEY];\n        new SecureRandom().nextBytes(masterKey);\n        return masterKey;\n    }\n\n    private void saveMasterKeyToFile(String filename, byte[] masterKey) {\n        try (FileOutputStream fos = new FileOutputStream(filename)) {\n            fos.write(masterKey);\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n>\n>\n>This is nowhere near safe for a production environment because leaving the `master_key.txt` directly in the application folder on your production server is like leaving the vault combination on a sticky note. Secure that file or please consider using a [KMS](https://docs.mongodb.com/drivers/security/client-side-field-level-encryption-local-key-to-kms/) in production.\n>\n>\n\nIn this simple quickstart, I will only use a single master key, but it's totally possible to use multiple master keys.\n\n## The Key Management Service (KMS) Provider\n\nWhichever solution you choose for the master key, you need a [KMS provider](https://docs.mongodb.com/drivers/security/client-side-field-level-encryption-local-key-to-kms/) to set up the `ClientEncryptionSettings` and the `AutoEncryptionSettings`.\n\nHere is the configuration for a local KMS:\n\n``` java\nMap<String, Map<String, Object>> kmsProviders = new HashMap<String, Map<String, Object>>() {{\n    put(\"local\", new HashMap<String, Object>() {{\n        put(\"key\", localMasterKey);\n    }});\n}};\n```\n\n## The Clients\n\nWe will need to set up two different clients:\n\n-   The first one ─ `ClientEncryption` ─ will be used to create our Data Encryption Keys (DEK) and encrypt our fields manually.\n-   The second one ─ `MongoClient` ─ will be the more conventional MongoDB connection that we will use to read and write our documents, with the difference that it will be configured to automatically decrypt the encrypted fields.\n\n### ClientEncryption\n\n``` java\nConnectionString connection_string = new ConnectionString(\"mongodb://localhost\");\nMongoClientSettings kvmcs = MongoClientSettings.builder().applyConnectionString(connection_string).build();\n\nClientEncryptionSettings ces = ClientEncryptionSettings.builder()\n                                                       .keyVaultMongoClientSettings(kvmcs)\n                                                       .keyVaultNamespace(\"csfle.vault\")\n                                                       .kmsProviders(kmsProviders)\n                                                       .build();\n\nClientEncryption encryption = ClientEncryptions.create(ces);\n```\n\n### MongoClient\n\n``` java\nAutoEncryptionSettings aes = AutoEncryptionSettings.builder()\n                                                   .keyVaultNamespace(\"csfle.vault\")\n                                                   .kmsProviders(kmsProviders)\n                                                   .bypassAutoEncryption(true)\n                                                   .build();\n\nMongoClientSettings mcs = MongoClientSettings.builder()\n                                             .applyConnectionString(connection_string)\n                                             .autoEncryptionSettings(aes)\n                                             .build();\n\nMongoClient client = MongoClients.create(mcs);\n```\n\n>\n>\n>`bypassAutoEncryption(true)` is the ticket for the Community Edition.  Without it, `mongocryptd` would rely on the JSON schema that you would have to provide to encrypt automatically the documents. See this [example in the documentation](https://mongodb.github.io/mongo-java-driver/4.2/driver/tutorials/client-side-encryption/#examples).\n>\n>\n\n>\n>\n>You don't have to reuse the same connection string for both connections.  It would actually be a lot more \"GDPR-friendly\" to use separated clusters so you can enforce a low retention policy on the Data Encryption Keys.\n>\n>\n\n## Unique Index on Key Alternate Names\n\nThe first thing you should do before you create your first Data Encryption Key is to create a unique index on the key alternate names to make sure that you can't reuse the same alternate name on two different DEKs.\n\nThese names will help you \"label\" your keys to know what each one is used for ─ which is still totally up to you.\n\n``` java\nMongoCollection<Document> vaultColl = client.getDatabase(\"csfle\").getCollection(\"vault\");\nvaultColl.createIndex(ascending(\"keyAltNames\"),\n                      new IndexOptions().unique(true).partialFilterExpression(exists(\"keyAltNames\")));\n```\n\nIn my example, I choose to use one DEK per user. I will encrypt all the fields I want to secure in each user document with the same key. If I want to \"forget\" a user, I just need to drop that key. In my example, the names are unique so I'm using this for my `keyAltNames`. It's a great way to enforce GDPR compliance.\n\n## Create Data Encryption Keys\n\nLet's create two Data Encryption Keys: one for Bobby and one for Alice.  Each will be used to encrypt all the fields I want to keep safe in my respective user documents.\n\n``` java\nBsonBinary bobbyKeyId = encryption.createDataKey(\"local\", keyAltName(\"Bobby\"));\nBsonBinary aliceKeyId = encryption.createDataKey(\"local\", keyAltName(\"Alice\"));\n```\n\nWe get a little help from this private method to make my code easier to read:\n\n``` java\nprivate DataKeyOptions keyAltName(String altName) {\n    return new DataKeyOptions().keyAltNames(singletonList(altName));\n}\n```\n\nHere is what Bobby's DEK looks like in my `csfle.vault` collection:\n\n``` json\n{\n  \"_id\" : UUID(\"aaa2e53d-875e-49d8-9ce0-dec9a9658571\"),\n  \"keyAltNames\" : [ \"Bobby\" ],\n  \"keyMaterial\" : BinData(0,\"/ozPZBMNUJU9udZyTYe1hX/KHqJJPrjdPads8UNjHX+cZVkIXnweZe5pGPpzcVcGmYctTAdxB3b+lmY5ONTzEZkqMg8JIWenIWQVY5fogIpfHDJQylQoEjXV3+e3ZY1WmWJR8mOp7pMoTyoGlZU2TwyqT9fcN7E5pNRh0uL3kCPk0sOOxLT/ejQISoY/wxq2uvyIK/C6/LrD1ymIC9w6YA==\"),\n  \"creationDate\" : ISODate(\"2021-03-19T16:16:09.800Z\"),\n  \"updateDate\" : ISODate(\"2021-03-19T16:16:09.800Z\"),\n  \"status\" : 0,\n  \"masterKey\" : {\n    \"provider\" : \"local\"\n  }\n}\n```\n\nAs you can see above, the `keyMaterial` (the DEK itself) is encrypted by the master key. Without the master key to decrypt it, it's useless.  Also, you can identify that it's Bobby's key in the `keyAltNames` field.\n\n## Create Encrypted Documents\n\nNow that we have an encryption key for Bobby and Alice, I can create their respective documents and insert them into MongoDB like so:\n\n``` java\nprivate static final String DETERMINISTIC = \"AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic\";\nprivate static final String RANDOM = \"AEAD_AES_256_CBC_HMAC_SHA_512-Random\";\n\nprivate Document createBobbyDoc(ClientEncryption encryption) {\n    BsonBinary phone = encryption.encrypt(new BsonString(\"01 23 45 67 89\"), deterministic(BOBBY));\n    BsonBinary bloodType = encryption.encrypt(new BsonString(\"A+\"), random(BOBBY));\n    BsonDocument medicalEntry = new BsonDocument(\"test\", new BsonString(\"heart\")).append(\"result\", new BsonString(\"bad\"));\n    BsonBinary medicalRecord = encryption.encrypt(new BsonArray(singletonList(medicalEntry)), random(BOBBY));\n    return new Document(\"name\", BOBBY).append(\"age\", 33)\n                                      .append(\"phone\", phone)\n                                      .append(\"blood_type\", bloodType)\n                                      .append(\"medical_record\", medicalRecord);\n}\n\nprivate Document createAliceDoc(ClientEncryption encryption) {\n    BsonBinary phone = encryption.encrypt(new BsonString(\"09 87 65 43 21\"), deterministic(ALICE));\n    BsonBinary bloodType = encryption.encrypt(new BsonString(\"O+\"), random(ALICE));\n    return new Document(\"name\", ALICE).append(\"age\", 28).append(\"phone\", phone).append(\"blood_type\", bloodType);\n}\n\nprivate EncryptOptions deterministic(String keyAltName) {\n    return new EncryptOptions(DETERMINISTIC).keyAltName(keyAltName);\n}\n\nprivate EncryptOptions random(String keyAltName) {\n    return new EncryptOptions(RANDOM).keyAltName(keyAltName);\n}\n\nprivate void createAndInsertBobbyAndAlice(ClientEncryption encryption, MongoCollection<Document> usersColl) {\n    Document bobby = createBobbyDoc(encryption);\n    Document alice = createAliceDoc(encryption);\n    int nbInsertedDocs = usersColl.insertMany(asList(bobby, alice)).getInsertedIds().size();\n    System.out.println(nbInsertedDocs + \" docs have been inserted.\");\n}\n```\n\nHere is what Bobby and Alice documents look like in my `encrypted.users` collection:\n\n**Bobby**\n\n``` json\n{\n  \"_id\" : ObjectId(\"6054d91c26a275034fe53300\"),\n  \"name\" : \"Bobby\",\n  \"age\" : 33,\n  \"phone\" : BinData(6,\"ATKkRdZWR0+HpqNyYA7zgIUCgeBE4SvLRwaXz/rFl8NPZsirWdHRE51pPa/2W9xgZ13lnHd56J1PLu9uv/hSkBgajE+MJLwQvJUkXatOJGbZd56BizxyKKTH+iy+8vV7CmY=\"),\n  \"blood_type\" : BinData(6,\"AjKkRdZWR0+HpqNyYA7zgIUCUdc30A8lTi2i1pWn7CRpz60yrDps7A8gUJhJdj+BEqIIx9xSUQ7xpnc/6ri2/+ostFtxIq/b6IQArGi+8ZBISw==\"),\n  \"medical_record\" : BinData(6,\"AjKkRdZWR0+HpqNyYA7zgIUESl5s4tPPvzqwe788XF8o91+JNqOUgo5kiZDKZ8qudloPutr6S5cE8iHAJ0AsbZDYq7XCqbqiXvjQobObvslR90xJvVMQidHzWtqWMlzig6ejdZQswz2/WT78RrON8awO\")\n}\n```\n\n**Alice**\n\n``` json\n{\n  \"_id\" : ObjectId(\"6054d91c26a275034fe53301\"),\n  \"name\" : \"Alice\",\n  \"age\" : 28,\n  \"phone\" : BinData(6,\"AX7Xd65LHUcWgYj+KbUT++sCC6xaCZ1zaMtzabawAgB79quwKvld8fpA+0m+CtGevGyIgVRjtj2jAHAOvREsoy3oq9p5mbJvnBqi8NttHUJpqooUn22Wx7o+nlo633QO8+c=\"),\n  \"blood_type\" : BinData(6,\"An7Xd65LHUcWgYj+KbUT++sCTyp+PJXudAKM5HcdX21vB0VBHqEXYSplHdZR0sCOxzBMPanVsTRrOSdAK5yHThP3Vitsu9jlbNo+lz5f3L7KYQ==\")\n}\n```\n\nClient Side Field Level Encryption currently provides [two different algorithms](https://docs.mongodb.com/manual/core/security-client-side-encryption/#encryption-algorithms) to encrypt the data you want to secure.\n\n### AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic\n\nWith this algorithm, the result of the encryption ─ given the same inputs (value and DEK) ─ is [deterministic](https://en.wikipedia.org/wiki/Deterministic_encryption).  This means that we have a greater support for read operations, but encrypted data with low [cardinality](https://en.wikipedia.org/wiki/Cardinality) is susceptible to [frequency analysis attacks](https://en.wikipedia.org/wiki/Frequency_analysis).\n\nIn my example, if I want to be able to retrieve users by phone numbers, I must use the deterministic algorithm. As a phone number is likely to be unique in my collection of users, it's safe to use this algorithm here.\n\n### AEAD_AES_256_CBC_HMAC_SHA_512-Random\n\nWith this algorithm, the result of the encryption is *always* [different](https://en.wikipedia.org/wiki/Probabilistic_encryption).  That means that it provides the strongest guarantees of data confidentiality, even when the cardinality is low, but prevents read operations based on these fields.\n\nIn my example, the blood type has a low cardinality and it doesn't make sense to search in my user collection by blood type anyway, so it's safe to use this algorithm for this field.\n\nAlso, Bobby's medical record must be very safe. So, the entire subdocument containing all his medical records is encrypted with the random algorithm as well and won't be used to search Bobby in my collection anyway.\n\n## Read Bobby's Document\n\nAs mentioned in the previous section, it's possible to search documents by fields encrypted with the deterministic algorithm.\n\nHere is how:\n\n``` java\nBsonBinary phone = encryption.encrypt(new BsonString(\"01 23 45 67 89\"), deterministic(BOBBY));\nString doc = usersColl.find(eq(\"phone\", phone)).first().toJson();\n```\n\nI simply encrypt again, with the same key, the phone number I'm looking for, and I can use this `BsonBinary` in my query to find Bobby.\n\nIf I output the `doc` string, I get:\n\n``` none\n{\n  \"_id\": {\n    \"$oid\": \"6054d91c26a275034fe53300\"\n  },\n  \"name\": \"Bobby\",\n  \"age\": 33,\n  \"phone\": \"01 23 45 67 89\",\n  \"blood_type\": \"A+\",\n  \"medical_record\": [\n    {\n      \"test\": \"heart\",\n      \"result\": \"bad\"\n    }\n  ]\n}\n```\n\nAs you can see, the automatic decryption worked as expected, I can see my document in clear text. To find this document, I could use the `_id`, the `name`, the `age`, or the phone number, but not the `blood_type` or the `medical_record`.\n\n## Read Alice's Document\n\nNow let's put CSFLE to the test. I want to be sure that if Alice's DEK is destroyed, Alice's document is lost forever and can never be restored, even from a backup that could be restored. That's why it's important to keep the DEKs and the encrypted documents in two different clusters that don't have the same backup retention policy.\n\nLet's retrieve Alice's document by name, but let's protect my code in case something \"bad\" has happened to her key...\n\n``` java\nprivate void readAliceIfPossible(MongoCollection<Document> usersColl) {\n    try {\n        String aliceDoc = usersColl.find(eq(\"name\", ALICE)).first().toJson();\n        System.out.println(\"Before we remove Alice's key, we can read her document.\");\n        System.out.println(aliceDoc);\n    } catch (MongoException e) {\n        System.err.println(\"We get a MongoException because 'libmongocrypt' can't decrypt these fields anymore.\");\n    }\n}\n```\n\nIf her key still exists in the database, then I can decrypt her document:\n\n``` none\n{\n  \"_id\": {\n    \"$oid\": \"6054d91c26a275034fe53301\"\n  },\n  \"name\": \"Alice\",\n  \"age\": 28,\n  \"phone\": \"09 87 65 43 21\",\n  \"blood_type\": \"O+\"\n}\n```\n\nNow, let's remove her key from the database:\n\n``` java\nvaultColl.deleteOne(eq(\"keyAltNames\", ALICE));\n```\n\nIn a real-life production environment, it wouldn't make sense to read her document again; and because we are all professional and organised developers who like to keep things tidy, we would also delete Alice's document along with her DEK, as this document is now completely worthless for us anyway.\n\nIn my example, I want to try to read this document anyway. But if I try to read it immediately after deleting her document, there is a great chance that I will still able to do so because of the [60 seconds Data Encryption Key Cache](https://github.com/mongodb/specifications/blob/master/source/client-side-encryption/client-side-encryption.rst#libmongocrypt-data-key-caching) that is managed by `libmongocrypt`.\n\nThis cache is very important because, without it, multiple back-and-forth would be necessary to decrypt my document. It's critical to prevent CSFLE from killing the performances of your MongoDB cluster.\n\nSo, to make sure I'm not using this cache anymore, I'm creating a brand new `MongoClient` (still with auto decryption settings) for the sake of this example. But of course, in production, it wouldn't make sense to do so.\n\nNow if I try to access Alice's document again, I get the following `MongoException`, as expected:\n\n``` none\ncom.mongodb.MongoException: not all keys requested were satisfied\n  at com.mongodb.MongoException.fromThrowableNonNull(MongoException.java:83)\n  at com.mongodb.client.internal.Crypt.fetchKeys(Crypt.java:286)\n  at com.mongodb.client.internal.Crypt.executeStateMachine(Crypt.java:244)\n  at com.mongodb.client.internal.Crypt.decrypt(Crypt.java:128)\n  at com.mongodb.client.internal.CryptConnection.command(CryptConnection.java:121)\n  at com.mongodb.client.internal.CryptConnection.command(CryptConnection.java:131)\n  at com.mongodb.internal.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:345)\n  at com.mongodb.internal.operation.CommandOperationHelper.executeCommand(CommandOperationHelper.java:336)\n  at com.mongodb.internal.operation.CommandOperationHelper.executeCommandWithConnection(CommandOperationHelper.java:222)\n  at com.mongodb.internal.operation.FindOperation$1.call(FindOperation.java:658)\n  at com.mongodb.internal.operation.FindOperation$1.call(FindOperation.java:652)\n  at com.mongodb.internal.operation.OperationHelper.withReadConnectionSource(OperationHelper.java:583)\n  at com.mongodb.internal.operation.FindOperation.execute(FindOperation.java:652)\n  at com.mongodb.internal.operation.FindOperation.execute(FindOperation.java:80)\n  at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:170)\n  at com.mongodb.client.internal.FindIterableImpl.first(FindIterableImpl.java:200)\n  at com.mongodb.quickstart.csfle.ClientSideFieldLevelEncryption.readAliceIfPossible(ClientSideFieldLevelEncryption.java:91)\n  at com.mongodb.quickstart.csfle.ClientSideFieldLevelEncryption.demo(ClientSideFieldLevelEncryption.java:79)\n  at com.mongodb.quickstart.csfle.ClientSideFieldLevelEncryption.main(ClientSideFieldLevelEncryption.java:41)\nCaused by: com.mongodb.crypt.capi.MongoCryptException: not all keys requested were satisfied\n  at com.mongodb.crypt.capi.MongoCryptContextImpl.throwExceptionFromStatus(MongoCryptContextImpl.java:145)\n  at com.mongodb.crypt.capi.MongoCryptContextImpl.throwExceptionFromStatus(MongoCryptContextImpl.java:151)\n  at com.mongodb.crypt.capi.MongoCryptContextImpl.completeMongoOperation(MongoCryptContextImpl.java:93)\n  at com.mongodb.client.internal.Crypt.fetchKeys(Crypt.java:284)\n  ... 17 more\n```\n\n## Wrapping Up\n\nIn this quickstart tutorial, we have discovered how to use Client Side Field Level Encryption using the MongoDB Java Driver, using only the community edition of MongoDB. You can learn more about the [automated encryption](https://docs.mongodb.com/manual/core/security-automatic-client-side-encryption/) in our documentation.\n\nCSFLE is the ultimate security feature to ensure the maximal level of security for your cluster. Not even your admins will be able to access the data in production if they don't have access to the master keys.\n\nBut it's not the only security measure you should use to protect your cluster. Preventing access to your cluster is, of course, the first security measure that you should enforce by [enabling the authentication](https://docs.mongodb.com/manual/tutorial/enable-authentication/) and [limit network exposure](https://docs.mongodb.com/manual/administration/security-checklist/#arrow-limit-network-exposure).\n\nIn doubt, check out the [security checklist](https://docs.mongodb.com/manual/administration/security-checklist/) before launching a cluster in production to make sure that you didn't overlook any of the security options MongoDB has to offer to protect your data.\n\nThere is a lot of flexibility in the implementation of CSFLE: You can choose to use one or multiple master keys, same for the Data Encryption Keys. You can also choose to encrypt all your phone numbers in your collection with the same DEK or use a different one for each user. It's really up to you how you will organise your encryption strategy but, of course, make sure it fulfills all your legal obligations. There are multiple right ways to implement CSFLE, so make sure to find the most suitable one for your use case.\n\n>\n>\n>If you have questions, please head to our [developer community website](https://community.mongodb.com/) where the MongoDB engineers and the MongoDB community will help you build your next big idea with MongoDB.\n>\n>\n\n### Documentation\n\n-   [GitHub repository with all the Java Quickstart examples of this series](https://github.com/mongodb-developer/java-quick-start)\n-   [MongoDB CSFLE Doc](https://docs.mongodb.com/manual/core/security-client-side-encryption/)\n-   [MongoDB Java Driver CSFLE Doc](https://docs.mongodb.com/drivers/security/client-side-field-level-encryption-guide/)\n-   [MongoDB University CSFLE implementation example](https://github.com/mongodb-university/csfle-guides/tree/master/java/)\n","description":"Learn how to use the client side field level encryption using the MongoDB Java Driver.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blte0424980e374823d/644c4736efd5716730fe6f7b/Java_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/java-client-side-field-level-encryption","title":"*Java - Client Side Field Level Encryption","original_publish_date":"2022-02-01T19:14:36.227Z","strapi_updated_at":"2023-01-26T16:56:03.124Z","expiry_date":"2022-12-14T02:03:26.118Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Advanced","calculated_slug":"/expertise-levels/advanced"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[{"node":{"title":"*Security","calculated_slug":"/products/mongodb/security"}}]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to use the client side field level encryption using the MongoDB Java Driver.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltea9590d8bc695e7a/644c473775b185097146a90d/Java_Tile_360x360.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@MBeugnet"},"system":{"updated_at":"2023-04-28T22:52:01.099Z","publish_details":{"time":"2023-04-28T22:53:01.559Z"}}},{"calculated_slug":"/languages/java/java-mapping-pojos","content":"## Updates\n\nThe MongoDB Java quickstart repository is [available on Github](https://github.com/mongodb-developer/java-quick-start).\n\n### March 25th, 2021\n\n-   Update Java Driver to 4.2.2.\n-   Added Client Side Field Level Encryption example.\n\n### October 21th, 2020\n\n-   Update Java Driver to 4.1.1.\n-   The Java Driver logging is now enabled via the popular [SLF4J](http://www.slf4j.org/) API so I added logback in the `pom.xml` and a configuration file `logback.xml`.\n\n## Introduction\n\n<div>\n<img \n        style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-java.png\" class=\"align-right\" alt=\"Java badge\" />\n\nJava is an object-oriented programming language and MongoDB stores documents, which look a lot like objects. Indeed, this is not a coincidence because that's the core idea behind the MongoDB database.\n</div>\n\nIn this blog post, as promised in the [first blog post](/quickstart/java-setup-crud-operations/) of this series, I will show you how to automatically map MongoDB documents to Plain Old Java Objects (POJOs) using only the MongoDB driver.\n\n## Getting Set Up\n\nI will use the same repository as usual in this series. If you don't have a copy of it yet, you can clone it or just update it if you already have it:\n\n``` sh\ngit clone https://github.com/mongodb-developer/java-quick-start\n```\n\nIf you haven't yet set up your free cluster on MongoDB Atlas, now is a great time to do so. You have all the instructions in this [blog post](/quickstart/free-atlas-cluster/).\n\n## The Grades Collection\n\nIf you followed this series, you know that we have been working with the `grades` collection in the `sample_training` database. You can import it easily by [loading the sample dataset in MongoDB Atlas](https://docs.atlas.mongodb.com/sample-data/load-sample-data/).\n\nHere is what a MongoDB document looks like in extended JSON format. I'm using the extended JSON because it's easier to identify the field types and we will need them to build the POJOs.\n\n``` json\n{\n    \"_id\": {\n        \"$oid\": \"56d5f7eb604eb380b0d8d8ce\"\n    },\n    \"student_id\": {\n        \"$numberDouble\": \"0\"\n    },\n    \"scores\": [{\n        \"type\": \"exam\",\n        \"score\": {\n            \"$numberDouble\": \"78.40446309504266\"\n        }\n    }, {\n        \"type\": \"quiz\",\n        \"score\": {\n            \"$numberDouble\": \"73.36224783231339\"\n        }\n    }, {\n        \"type\": \"homework\",\n        \"score\": {\n            \"$numberDouble\": \"46.980982486720535\"\n        }\n    }, {\n        \"type\": \"homework\",\n        \"score\": {\n            \"$numberDouble\": \"76.67556138656222\"\n        }\n    }],\n    \"class_id\": {\n        \"$numberDouble\": \"339\"\n    }\n}\n```\n\n## POJOs\n\nThe first thing we need is a representation of this document in Java.  For each document or subdocument, I need a corresponding POJO class.\n\nAs you can see in the document above, I have the main document itself and I have an array of subdocuments in the `scores` field. Thus, we will need 2 POJOs to represent this document in Java:\n\n-   One for the grade,\n-   One for the scores.\n\nIn the package `com.mongodb.quickstart.models`, I created two new POJOs: `Grade.java` and `Score.java`.\n\n[Grade.java](https://github.com/mongodb-developer/java-quick-start/blob/master/src/main/java/com/mongodb/quickstart/models/Grade.java):\n\n``` java\npackage com.mongodb.quickstart.models;\n\n// imports\n\npublic class Grade {\n\n    private ObjectId id;\n    @BsonProperty(value = \"student_id\")\n    private Double studentId;\n    @BsonProperty(value = \"class_id\")\n    private Double classId;\n    private List<Score> scores;\n\n    // getters and setters with builder pattern\n    // toString()\n    // equals()\n    // hashCode()\n}\n```\n\n>In the Grade class above, I'm using `@BsonProperty` to avoid violating Java naming conventions for variables, getters, and setters. This allows me to indicate to the mapper that I want the `\"student_id\"` field in JSON to be mapped to the `\"studentId\"` field in Java.\n\n[Score.java](https://github.com/mongodb-developer/java-quick-start/blob/master/src/main/java/com/mongodb/quickstart/models/Score.java):\n\n``` java\npackage com.mongodb.quickstart.models;\n\nimport java.util.Objects;\n\npublic class Score {\n\n    private String type;\n    private Double score;\n\n    // getters and setters with builder pattern\n    // toString()\n    // equals()\n    // hashCode()\n}\n```\n\nAs you can see, we took care of matching the Java types with the JSON value types to follow the same data model. You can read more about [types and documents in the documentation](https://mongodb.github.io/mongo-java-driver/3.12/bson/documents/).\n\n## Mapping POJOs\n\nNow that we have everything we need, we can start the MongoDB driver code.\n\nI created a new class `MappingPOJO` in the `com.mongodb.quickstart` package and here are the key lines of code:\n\n-   I need a `ConnectionString` instance instead of the usual `String` I have used so far in this series. I'm still retrieving my MongoDB Atlas URI from the system properties. See my [starting and setup blog post](/quickstart/java-setup-crud-operations/) if you need a reminder.\n\n``` java\nConnectionString connectionString = new ConnectionString(System.getProperty(\"mongodb.uri\"));\n```\n\n-   I need to configure the CodecRegistry to include a codec to handle the translation to and from BSON for our POJOs.\n\n``` java\nCodecRegistry pojoCodecRegistry = fromProviders(PojoCodecProvider.builder().automatic(true).build());\n```\n\n-   And I need to add the default codec registry, which contains all the default codecs. They can handle all the major types in Java-like `Boolean`, `Double`, `String`, `BigDecimal`, etc.\n\n``` java\nCodecRegistry codecRegistry = fromRegistries(MongoClientSettings.getDefaultCodecRegistry(),\n                                             pojoCodecRegistry);\n```\n\n-   I can now wrap all my settings together using `MongoClientSettings`.\n\n``` java\nMongoClientSettings clientSettings = MongoClientSettings.builder()\n                                                        .applyConnectionString(connectionString)\n                                                        .codecRegistry(codecRegistry)\n                                                        .build();\n```\n\n-   I can finally initialise my connection with MongoDB.\n\n``` java\ntry (MongoClient mongoClient = MongoClients.create(clientSettings)) {\n    MongoDatabase db = mongoClient.getDatabase(\"sample_training\");\n    MongoCollection<Grade> grades = db.getCollection(\"grades\", Grade.class);\n    [...]\n}\n```\n\nAs you can see in this last line of Java, all the magic is happening here. The `MongoCollection<Grade>` I'm retrieving is typed by `Grade` and not by `Document` as usual.\n\nIn the previous blog posts in this series, I showed you how to use CRUD operations by manipulating `MongoCollection<Document>`. Let's review all the CRUD operations using POJOs now.\n\n-   Here is an insert (create).\n\n``` java\nGrade newGrade = new Grade().setStudent_id(10003d)\n                            .setClass_id(10d)\n                            .setScores(singletonList(new Score().setType(\"homework\").setScore(50d)));\ngrades.insertOne(newGrade);\n```\n\n-   Here is a find (read).\n\n``` java\nGrade grade = grades.find(eq(\"student_id\", 10003d)).first();\nSystem.out.println(\"Grade found:\\t\" + grade);\n```\n\n-   Here is an update with a `findOneAndReplace` returning the newest version of the document.\n\n``` java\nList<Score> newScores = new ArrayList<>(grade.getScores());\nnewScores.add(new Score().setType(\"exam\").setScore(42d));\ngrade.setScores(newScores);\nDocument filterByGradeId = new Document(\"_id\", grade.getId());\nFindOneAndReplaceOptions returnDocAfterReplace = new FindOneAndReplaceOptions()\n                                                     .returnDocument(ReturnDocument.AFTER);\nGrade updatedGrade = grades.findOneAndReplace(filterByGradeId, grade, returnDocAfterReplace);\nSystem.out.println(\"Grade replaced:\\t\" + updatedGrade);\n```\n\n-   And finally here is a delete.\n\n``` java\nSystem.out.println(grades.deleteOne(filterByGradeId));\n```\n\n## Final Code\n\n`MappingPojo.java` ([code](https://github.com/mongodb-developer/java-quick-start/blob/master/src/main/java/com/mongodb/quickstart/MappingPOJO.java)):\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.ConnectionString;\nimport com.mongodb.MongoClientSettings;\nimport com.mongodb.client.MongoClient;\nimport com.mongodb.client.MongoClients;\nimport com.mongodb.client.MongoCollection;\nimport com.mongodb.client.MongoDatabase;\nimport com.mongodb.client.model.FindOneAndReplaceOptions;\nimport com.mongodb.client.model.ReturnDocument;\nimport com.mongodb.quickstart.models.Grade;\nimport com.mongodb.quickstart.models.Score;\nimport org.bson.Document;\nimport org.bson.codecs.configuration.CodecRegistry;\nimport org.bson.codecs.pojo.PojoCodecProvider;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport static com.mongodb.client.model.Filters.eq;\nimport static java.util.Collections.singletonList;\nimport static org.bson.codecs.configuration.CodecRegistries.fromProviders;\nimport static org.bson.codecs.configuration.CodecRegistries.fromRegistries;\n\npublic class MappingPOJO {\n\n    public static void main(String[] args) {\n        ConnectionString connectionString = new ConnectionString(System.getProperty(\"mongodb.uri\"));\n        CodecRegistry pojoCodecRegistry = fromProviders(PojoCodecProvider.builder().automatic(true).build());\n        CodecRegistry codecRegistry = fromRegistries(MongoClientSettings.getDefaultCodecRegistry(), pojoCodecRegistry);\n        MongoClientSettings clientSettings = MongoClientSettings.builder()\n                                                                .applyConnectionString(connectionString)\n                                                                .codecRegistry(codecRegistry)\n                                                                .build();\n        try (MongoClient mongoClient = MongoClients.create(clientSettings)) {\n            MongoDatabase db = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Grade> grades = db.getCollection(\"grades\", Grade.class);\n\n            // create a new grade.\n            Grade newGrade = new Grade().setStudent_id(10003d)\n                                        .setClass_id(10d)\n                                        .setScores(singletonList(new Score().setType(\"homework\").setScore(50d)));\n            grades.insertOne(newGrade);\n\n            // find this grade.\n            Grade grade = grades.find(eq(\"student_id\", 10003d)).first();\n            System.out.println(\"Grade found:\\t\" + grade);\n\n            // update this grade: adding an exam grade\n            List<Score> newScores = new ArrayList<>(grade.getScores());\n            newScores.add(new Score().setType(\"exam\").setScore(42d));\n            grade.setScores(newScores);\n            Document filterByGradeId = new Document(\"_id\", grade.getId());\n            FindOneAndReplaceOptions returnDocAfterReplace = new FindOneAndReplaceOptions().returnDocument(ReturnDocument.AFTER);\n            Grade updatedGrade = grades.findOneAndReplace(filterByGradeId, grade, returnDocAfterReplace);\n            System.out.println(\"Grade replaced:\\t\" + updatedGrade);\n\n            // delete this grade\n            System.out.println(grades.deleteOne(filterByGradeId));\n        }\n    }\n}\n```\n\nTo start this program, you can use this maven command line in your root project (where the `src` folder is) or your favorite IDE.\n\n``` bash\nmvn compile exec:java -Dexec.mainClass=\"com.mongodb.quickstart.MappingPOJO\" -Dmongodb.uri=\"mongodb+srv://USERNAME:PASSWORD@cluster0-abcde.mongodb.net/test?w=majority\"\n```\n\n## Wrapping Up\n\nMapping POJOs and your MongoDB documents simplifies your life a lot when you are solving real-world problems with Java, but you can certainly be successful without using POJOs.\n\nMongoDB is a dynamic schema database which means your documents can have different schemas within a single collection. Mapping all the documents from such a collection can be a challenge. So, sometimes, using the \"old school\" method and the `Document` class will be easier.\n\n>If you want to learn more and deepen your knowledge faster, I recommend you check out the M220J: MongoDB for Java Developers training available for free on [MongoDB University](https://university.mongodb.com/).\n\nIn the next blog post, I will show you the aggregation framework in Java.","description":"Learn how to use the native mapping of POJOs using the MongoDB Java Driver.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blte0424980e374823d/644c4736efd5716730fe6f7b/Java_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/java-mapping-pojos","title":"*Java - Mapping POJOs","original_publish_date":"2022-02-01T19:18:33.052Z","strapi_updated_at":"2023-01-26T16:56:03.124Z","expiry_date":"2022-12-14T02:03:35.330Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to use the native mapping of POJOs using the MongoDB Java Driver.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltea9590d8bc695e7a/644c473775b185097146a90d/Java_Tile_360x360.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@MBeugnet"},"system":{"updated_at":"2023-04-28T22:52:00.725Z","publish_details":{"time":"2023-04-28T22:53:01.595Z"}}},{"calculated_slug":"/languages/java/java-setup-crud-operations","content":"## Updates\n\nThe MongoDB Java quickstart repository is [available on Github](https://github.com/mongodb-developer/java-quick-start).\n\n### March 25th, 2021\n\n-   Update Java Driver to 4.2.2.\n-   Added Client Side Field Level Encryption example.\n\n### October 21th, 2020\n\n-   Update Java Driver to 4.1.1.\n-   The MongoDB Java Driver logging is now enabled via the popular [SLF4J](http://www.slf4j.org/) API so I added logback in the `pom.xml` and a configuration file `logback.xml`.\n\n## Introduction\n\n<div>\n    <img\n        style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-java.png\" alt=\"Java badge\" />\n\nIn this very first blog post of the Java Quick Start series, I will show you how to set up your Java project with Maven and execute a MongoDB command in Java. Then we will explore the most common operations — such as create, read, update, and delete — using the [MongoDB Java driver](https://www.mongodb.com/docs/drivers/java-drivers/). I will also show you some of the more powerful options and features available as part of the MongoDB [Java driver](https://www.mongodb.com/docs/drivers/java/sync/current/compatibility/) for each of these operations, giving you a really great foundation of knowledge to build upon as we go through the series.\n</div>\n\nIn future blog posts, we will move on and work through:\n\n-   [Mapping MongoDB BSON documents directly to Plain Old Java Object (POJO)](/developer/languages/java/java-mapping-pojos/).\n-   [The MongoDB Aggregation Framework](/developer/languages/java/java-aggregation-pipeline/).\n-   [Change Streams](/developer/languages/java/java-change-streams/).\n-   Multi-document ACID transactions.\n-   The MongoDB Java reactive streams driver.\n\n### Why MongoDB and Java? \n\nJava is the [most popular language in the IT industry](https://stackify.com/popular-programming-languages-2018/) at the date of this blog post, and [developers voted MongoDB as their most wanted database four years in a row](https://www.mongodb.com/blog/post/mongodb-the-most-wanted-database-by-developers-for-the-4th-consecutive-year).  In this series of blog posts, I will be demonstrating how powerful these two great pieces of technology are when combined and how you can access that power.\n\n### Prerequisites\n\nTo follow along, you can use any environment you like and the integrated development environment of your choice. I'll use [Maven](http://maven.apache.org/install.html) 3.6.2 and the Java OpenJDK 13, but all the code will be compatible with Java versions 8 to 13, so feel free to use the JDK of your choice and update the Java version accordingly in the pom.xml file we are about to set up.\n\nFor the MongoDB cluster, we will be using a M0 Free Tier MongoDB Cluster from [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/). If you don't have one already, check out my [Get Started with an M0 Cluster](/developer/products/atlas/free-atlas-cluster/) blog post.\n\n>Get your free M0 cluster on [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/) today. It's free forever, and you'll be able to use it to work with the examples in this blog series.\n\nLet's jump in and take a look at how well Java and MongoDB work together.\n\n## Getting set up\n\nTo begin with, we will need to set up a new Maven project. You have two options at this point. You can either clone this series' git repository or you can create and set up the Maven project.\n\n### Using the git repository\n\nIf you choose to use git, you will get all the code immediately. I still recommend you read through the manual set-up.\n\nYou can clone the repository if you like with the following command.\n\n``` bash\ngit clone https://github.com/mongodb-developer/java-quick-start\n```\n\nOr you can [download the repository as a zip file](https://github.com/mongodb-developer/java-quick-start/archive/master.zip).\n\n### Setting up manually\n\nYou can either use your favorite IDE to create a new Maven project for you or you can create the Maven project manually. Either way, you should get the following folder architecture:\n\n``` none\njava-quick-start/\n├── pom.xml\n└── src\n    └── main\n        └── java\n            └── com\n                └── mongodb\n                    └── quickstart\n```\n\nThe pom.xml file should contain the following code:\n\n``` xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.mongodb</groupId>\n    <artifactId>java-quick-start</artifactId>\n    <version>1.0-SNAPSHOT</version>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <maven-compiler-plugin.source>8</maven-compiler-plugin.source>\n        <maven-compiler-plugin.target>8</maven-compiler-plugin.target>\n        <maven-compiler-plugin.version>3.10.1</maven-compiler-plugin.version>\n        <mongodb-driver-sync.version>4.6.1</mongodb-driver-sync.version>\n        <mongodb-crypt.version>1.4.1</mongodb-crypt.version>\n        <logback-classic.version>1.2.11</logback-classic.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.mongodb</groupId>\n            <artifactId>mongodb-driver-sync</artifactId>\n            <version>${mongodb-driver-sync.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.mongodb</groupId>\n            <artifactId>mongodb-crypt</artifactId>\n            <version>${mongodb-crypt.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>ch.qos.logback</groupId>\n            <artifactId>logback-classic</artifactId>\n            <version>${logback-classic.version}</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>${maven-compiler-plugin.version}</version>\n                <configuration>\n                    <source>${maven-compiler-plugin.source}</source>\n                    <target>${maven-compiler-plugin.target}</target>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n```\n\nTo verify that everything works correctly, you should be able to create and run a simple \"Hello MongoDB!\" program. In `src/main/java/com/mongodb/quickstart`, create the `HelloMongoDB.java` file:\n\n``` java\npackage com.mongodb.quickstart;\n\npublic class HelloMongoDB {\n\n    public static void main(String[] args) {\n        System.out.println(\"Hello MongoDB!\");\n    }\n}\n```\n\nThen compile and execute it with your IDE or use the command line in the root directory (where the `src` folder is):\n\n``` bash\nmvn compile exec:java -Dexec.mainClass=\"com.mongodb.quickstart.HelloMongoDB\"\n```\n\nThe result should look like this:\n\n``` none\n[...]\n[INFO] Scanning for projects...\n[INFO]\n[INFO] --------------------< com.mongodb:java-quick-start >--------------------\n[INFO] Building java-quick-start 1.0-SNAPSHOT\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO]\n[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ java-quick-start ---\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 1 resource\n[INFO]\n[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ java-quick-start ---\n[INFO] Nothing to compile - all classes are up to date\n[INFO]\n[INFO] --- exec-maven-plugin:1.4.0:java (default-cli) @ java-quick-start ---\nHello MongoDB!\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  0.786 s\n[INFO] Finished at: 2019-10-02T20:19:36+02:00\n[INFO] ------------------------------------------------------------------------\n```\n\n>Note: If you see some warnings about an illegal reflective access from `guice.java`, it's safe to ignore them. Guice is used by Maven and needs an update. You can read more about it [in this GitHub issue](https://github.com/google/guice/issues/1133). These warnings will disappear in a future release of Guice and Maven.\n\n## Connecting with Java\n\nNow that our Maven project works and we have resolved our dependencies, we can start using MongoDB Atlas with Java.\n\nIf you have imported the [sample dataset](/developer/products/atlas/atlas-sample-datasets/) as suggested in the [Quick Start Atlas blog post](/developer/products/atlas/free-atlas-cluster/), then with the Java code we are about to create, you will be able to see a list of the databases in the sample dataset.\n\nThe first step is to instantiate a `MongoClient` by passing a MongoDB Atlas connection string into the `MongoClients.create()` static method.  This will establish a connection to [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/) using the connection string. Then we can retrieve the list of databases on this cluster and print them out to test the connection with MongoDB.\n\nIn `src/main/java/com/mongodb`, create the `Connection.java` file:\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.client.MongoClient;\nimport com.mongodb.client.MongoClients;\nimport org.bson.Document;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class Connection {\n\n    public static void main(String[] args) {\n        String connectionString = System.getProperty(\"mongodb.uri\");\n        try (MongoClient mongoClient = MongoClients.create(connectionString)) {\n            List<Document> databases = mongoClient.listDatabases().into(new ArrayList<>());\n            databases.forEach(db -> System.out.println(db.toJson()));\n        }\n    }\n}\n```\n\nAs you can see, the MongoDB connection string is retrieved from the *System Properties*, so we need to set this up. Once you have retrieved your [MongoDB Atlas connection string](https://docs.mongodb.com/guides/cloud/connectionstring/), you can add the `mongodb.uri` system property into your IDE. Here is my configuration with IntelliJ for example.\n\n![IntelliJ Configuration](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/intellij-configuration.png \"IntelliJ Configuration\")\n\nOr if you prefer to use Maven in command line, here is the equivalent command line you can run in the root directory:\n\n``` bash\nmvn compile exec:java -Dexec.mainClass=\"com.mongodb.quickstart.Connection\" -Dmongodb.uri=\"mongodb+srv://username:password@cluster0-abcde.mongodb.net/test?w=majority\"\n```\n\n>Note: Don't forget the double quotes around the MongoDB URI to avoid surprises from your shell.\n\nThe standard output should look like this:\n\n``` none\n{\"name\": \"admin\", \"sizeOnDisk\": 303104.0, \"empty\": false}\n{\"name\": \"config\", \"sizeOnDisk\": 147456.0, \"empty\": false}\n{\"name\": \"local\", \"sizeOnDisk\": 5.44731136E8, \"empty\": false}\n{\"name\": \"sample_airbnb\", \"sizeOnDisk\": 5.761024E7, \"empty\": false}\n{\"name\": \"sample_geospatial\", \"sizeOnDisk\": 1384448.0, \"empty\": false}\n{\"name\": \"sample_mflix\", \"sizeOnDisk\": 4.583424E7, \"empty\": false}\n{\"name\": \"sample_supplies\", \"sizeOnDisk\": 1339392.0, \"empty\": false}\n{\"name\": \"sample_training\", \"sizeOnDisk\": 7.4801152E7, \"empty\": false}\n{\"name\": \"sample_weatherdata\", \"sizeOnDisk\": 5103616.0, \"empty\": false}\n```\n\n## Insert operations\n\n### Getting set up\n\nIn the Connecting with Java section, we created the classes `HelloMongoDB` and `Connection`. Now we will work on the `Create` class.\n\nIf you didn't set up your free cluster on MongoDB Atlas, now is great time to do so. Get the directions for [creating your cluster](/developer/products/atlas/free-atlas-cluster/).\n\n### Checking the collection and data model\n\nIn the sample dataset, you can find the database `sample_training`, which contains a collection `grades`. Each document in this collection represents a student's grades for a particular class.\n\nHere is the JSON representation of a document in the [mongo shell](https://docs.mongodb.com/manual/mongo/).\n\n``` bash\nMongoDB Enterprise Cluster0-shard-0:PRIMARY> db.grades.findOne({student_id: 0, class_id: 339})\n{\n    \"_id\" : ObjectId(\"56d5f7eb604eb380b0d8d8ce\"),\n    \"student_id\" : 0,\n    \"scores\" : [\n        {\n            \"type\" : \"exam\",\n            \"score\" : 78.40446309504266\n        },\n        {\n            \"type\" : \"quiz\",\n            \"score\" : 73.36224783231339\n        },\n        {\n            \"type\" : \"homework\",\n            \"score\" : 46.980982486720535\n        },\n        {\n            \"type\" : \"homework\",\n            \"score\" : 76.67556138656222\n        }\n    ],\n    \"class_id\" : 339\n}\n```\n\nAnd here is the [extended JSON](https://docs.mongodb.com/manual/reference/mongodb-extended-json/) representation of the same student. You can retrieve it in [MongoDB Compass](https://www.mongodb.com/products/compass), our free GUI tool, if you want.\n\nExtended JSON is the human readable version of a BSON document without loss of type information. You can read more about the Java driver and BSON [in the MongoDB Java driver documentation](https://mongodb.github.io/mongo-java-driver/3.11/bson/extended-json/).\n\n``` json\n{\n    \"_id\": {\n        \"$oid\": \"56d5f7eb604eb380b0d8d8ce\"\n    },\n    \"student_id\": {\n        \"$numberDouble\": \"0\"\n    },\n    \"scores\": [{\n        \"type\": \"exam\",\n        \"score\": {\n            \"$numberDouble\": \"78.40446309504266\"\n        }\n    }, {\n        \"type\": \"quiz\",\n        \"score\": {\n            \"$numberDouble\": \"73.36224783231339\"\n        }\n    }, {\n        \"type\": \"homework\",\n        \"score\": {\n            \"$numberDouble\": \"46.980982486720535\"\n        }\n    }, {\n        \"type\": \"homework\",\n        \"score\": {\n            \"$numberDouble\": \"76.67556138656222\"\n        }\n    }],\n    \"class_id\": {\n        \"$numberDouble\": \"339\"\n    }\n}\n```\n\nAs you can see, MongoDB stores BSON documents and for each key-value pair, the BSON contains the key and the value along with its type. This is how MongoDB knows that `class_id` is actually a double and not an integer, which is not explicit in the mongo shell representation of this document.\n\nWe have 10,000 students (`student_id` from 0 to 9999) already in this collection and each of them took 10 different classes, which adds up to 100,000 documents in this collection. Let's say a new student (`student_id` 10,000) just arrived in this university and received a bunch of (random) grades in his first class. Let's insert this new student document using Java and the MongoDB Java driver.\n\nIn this university, the `class_id` varies from 0 to 500, so I can use any random value between 0 and 500.\n\n### Selecting databases and collections\n\nFirstly, we need to set up our `Create` class and access this `sample_training.grades` collection.\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.client.MongoClient;\nimport com.mongodb.client.MongoClients;\nimport com.mongodb.client.MongoCollection;\nimport com.mongodb.client.MongoDatabase;\nimport org.bson.Document;\n\npublic class Create {\n\n    public static void main(String[] args) {\n        try (MongoClient mongoClient = MongoClients.create(System.getProperty(\"mongodb.uri\"))) {\n\n            MongoDatabase sampleTrainingDB = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Document> gradesCollection = sampleTrainingDB.getCollection(\"grades\");\n\n        }\n    }\n}\n```\n\n### Create a BSON document\n\nSecondly, we need to represent this new student in Java using the `Document` class.\n\n``` java\nRandom rand = new Random();\nDocument student = new Document(\"_id\", new ObjectId());\nstudent.append(\"student_id\", 10000d)\n       .append(\"class_id\", 1d)\n       .append(\"scores\", asList(new Document(\"type\", \"exam\").append(\"score\", rand.nextDouble() * 100),\n                                new Document(\"type\", \"quiz\").append(\"score\", rand.nextDouble() * 100),\n                                new Document(\"type\", \"homework\").append(\"score\", rand.nextDouble() * 100),\n                                new Document(\"type\", \"homework\").append(\"score\", rand.nextDouble() * 100)));\n```\n\nAs you can see, we reproduced the same data model from the existing documents in this collection as we made sure that `student_id`, `class_id`, and `score` are all doubles.\n\nAlso, the Java driver would have generated the `_id` field with an ObjectId for us if we didn't explicitly create one here, but it's good practice to set the `_id` ourselves. This won't change our life right now, but it makes more sense when we directly manipulate POJOs and we want to create a clean REST API. I'm doing this in my [mapping POJOs post](/developer/languages/java/java-mapping-pojos/).\n\nNote as well that we are inserting a document into an existing collection and database, but if these didn’t already exist, MongoDB would automatically create them the first time you to go insert a document into the collection.\n\n### Insert document\n\nFinally, we can insert this document.\n\n``` java\ngradesCollection.insertOne(student);\n```\n\n### Final code to insert one document\n\nHere is the final `Create` class to insert one document in MongoDB with all the details I mentioned above.\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.client.MongoClient;\nimport com.mongodb.client.MongoClients;\nimport com.mongodb.client.MongoCollection;\nimport com.mongodb.client.MongoDatabase;\nimport org.bson.Document;\nimport org.bson.types.ObjectId;\n\nimport java.util.Random;\n\nimport static java.util.Arrays.asList;\n\npublic class Create {\n\n    public static void main(String[] args) {\n        try (MongoClient mongoClient = MongoClients.create(System.getProperty(\"mongodb.uri\"))) {\n\n            MongoDatabase sampleTrainingDB = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Document> gradesCollection = sampleTrainingDB.getCollection(\"grades\");\n\n            Random rand = new Random();\n            Document student = new Document(\"_id\", new ObjectId());\n            student.append(\"student_id\", 10000d)\n                   .append(\"class_id\", 1d)\n                   .append(\"scores\", asList(new Document(\"type\", \"exam\").append(\"score\", rand.nextDouble() * 100),\n                                            new Document(\"type\", \"quiz\").append(\"score\", rand.nextDouble() * 100),\n                                            new Document(\"type\", \"homework\").append(\"score\", rand.nextDouble() * 100),\n                                            new Document(\"type\", \"homework\").append(\"score\", rand.nextDouble() * 100)));\n\n            gradesCollection.insertOne(student);\n        }\n    }\n}\n```\n\nYou can execute this class with the following Maven command line in the root directory or using your IDE (see above for more details). Don't forget the double quotes around the MongoDB URI to avoid surprises.\n\n``` bash\nmvn compile exec:java -Dexec.mainClass=\"com.mongodb.quickstart.Create\" -Dmongodb.uri=\"mongodb+srv://USERNAME:PASSWORD@cluster0-abcde.mongodb.net/test?w=majority\"\n```\n\nAnd here is the document I extracted from [MongoDB\nCompass](https://www.mongodb.com/products/compass).\n\n``` json\n{\n    \"_id\": {\n        \"$oid\": \"5d97c375ded5651ea3462d0f\"\n    },\n    \"student_id\": {\n        \"$numberDouble\": \"10000\"\n    },\n    \"class_id\": {\n        \"$numberDouble\": \"1\"\n    },\n    \"scores\": [{\n        \"type\": \"exam\",\n        \"score\": {\n            \"$numberDouble\": \"4.615256396625178\"\n        }\n    }, {\n        \"type\": \"quiz\",\n        \"score\": {\n            \"$numberDouble\": \"73.06173415145801\"\n        }\n    }, {\n        \"type\": \"homework\",\n        \"score\": {\n            \"$numberDouble\": \"19.378205578990727\"\n        }\n    }, {\n        \"type\": \"homework\",\n        \"score\": {\n            \"$numberDouble\": \"82.3089189278531\"\n        }\n    }]\n}\n```\n\nNote that the order of the fields is different from the initial document with `\"student_id\": 0`.\n\nWe could get exactly the same order if we wanted to by creating the document like this.\n\n``` java\nRandom rand = new Random();\nDocument student = new Document(\"_id\", new ObjectId());\nstudent.append(\"student_id\", 10000d)\n       .append(\"scores\", asList(new Document(\"type\", \"exam\").append(\"score\", rand.nextDouble() * 100),\n                                new Document(\"type\", \"quiz\").append(\"score\", rand.nextDouble() * 100),\n                                new Document(\"type\", \"homework\").append(\"score\", rand.nextDouble() * 100),\n                                new Document(\"type\", \"homework\").append(\"score\", rand.nextDouble() * 100)))\n       .append(\"class_id\", 1d);\n```\n\nBut if you do things correctly, this should not have any impact on your code and logic as fields in JSON documents are not ordered.\n\nI'm quoting [json.org](http://json.org/) for this:\n\n>An object is an unordered set of name/value pairs.\n\n### Insert multiple documents\n\nNow that we know how to create one document, let's learn how to insert many documents.\n\nOf course, we could just wrap the previous `insert` operation into a `for` loop. Indeed, if we loop 10 times on this method, we would send 10 insert commands to the cluster and expect 10 insert acknowledgments. As you can imagine, this would not be very efficient as it would generate a lot more TCP communications than necessary.\n\nInstead, we want to wrap our 10 documents and send them in one call to the cluster and we want to receive only one insert acknowledgement for the entire list.\n\nLet's refactor the code. First, let's make the random generator a `private static final` field.\n\n``` java\nprivate static final Random rand = new Random();\n```\n\nLet's make a grade factory method.\n\n``` java\nprivate static Document generateNewGrade(double studentId, double classId) {\n    List<Document> scores = asList(new Document(\"type\", \"exam\").append(\"score\", rand.nextDouble() * 100),\n                                   new Document(\"type\", \"quiz\").append(\"score\", rand.nextDouble() * 100),\n                                   new Document(\"type\", \"homework\").append(\"score\", rand.nextDouble() * 100),\n                                   new Document(\"type\", \"homework\").append(\"score\", rand.nextDouble() * 100));\n    return new Document(\"_id\", new ObjectId()).append(\"student_id\", studentId)\n                                              .append(\"class_id\", classId)\n                                              .append(\"scores\", scores);\n}\n```\n\nAnd now we can use this to insert 10 documents all at once.\n\n``` java\nList<Document> grades = new ArrayList<>();\nfor (double classId = 1d; classId <= 10d; classId++) {\n    grades.add(generateNewGrade(10001d, classId));\n}\n\ngradesCollection.insertMany(grades, new InsertManyOptions().ordered(false));\n```\n\nAs you can see, we are now wrapping our grade documents into a list and we are sending this list in a single call with the `insertMany` method.\n\nBy default, the `insertMany` method will insert the documents in order and stop if an error occurs during the process. For example, if you try to insert a new document with the same `_id` as an existing document, you would get a `DuplicateKeyException`.\n\nTherefore, with an ordered `insertMany`, the last documents of the list would not be inserted and the insertion process would stop and return the appropriate exception as soon as the error occurs.\n\nAs you can see here, this is not the behaviour we want because all the grades are completely independent from one to another. So, if one of them fails, we want to process all the grades and then eventually fall back to an exception for the ones that failed.\n\nThis is why you see the second parameter `new InsertManyOptions().ordered(false)` which is true by default.\n\n### The final code to insert multiple documents\n\nLet's refactor the code a bit and here is the final `Create` class.\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.client.MongoClient;\nimport com.mongodb.client.MongoClients;\nimport com.mongodb.client.MongoCollection;\nimport com.mongodb.client.MongoDatabase;\nimport com.mongodb.client.model.InsertManyOptions;\nimport org.bson.Document;\nimport org.bson.types.ObjectId;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Random;\n\nimport static java.util.Arrays.asList;\n\npublic class Create {\n\n    private static final Random rand = new Random();\n\n    public static void main(String[] args) {\n        try (MongoClient mongoClient = MongoClients.create(System.getProperty(\"mongodb.uri\"))) {\n\n            MongoDatabase sampleTrainingDB = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Document> gradesCollection = sampleTrainingDB.getCollection(\"grades\");\n\n            insertOneDocument(gradesCollection);\n            insertManyDocuments(gradesCollection);\n        }\n    }\n\n    private static void insertOneDocument(MongoCollection<Document> gradesCollection) {\n        gradesCollection.insertOne(generateNewGrade(10000d, 1d));\n        System.out.println(\"One grade inserted for studentId 10000.\");\n    }\n\n    private static void insertManyDocuments(MongoCollection<Document> gradesCollection) {\n        List<Document> grades = new ArrayList<>();\n        for (double classId = 1d; classId <= 10d; classId++) {\n            grades.add(generateNewGrade(10001d, classId));\n        }\n\n        gradesCollection.insertMany(grades, new InsertManyOptions().ordered(false));\n        System.out.println(\"Ten grades inserted for studentId 10001.\");\n    }\n\n    private static Document generateNewGrade(double studentId, double classId) {\n        List<Document> scores = asList(new Document(\"type\", \"exam\").append(\"score\", rand.nextDouble() * 100),\n                                       new Document(\"type\", \"quiz\").append(\"score\", rand.nextDouble() * 100),\n                                       new Document(\"type\", \"homework\").append(\"score\", rand.nextDouble() * 100),\n                                       new Document(\"type\", \"homework\").append(\"score\", rand.nextDouble() * 100));\n        return new Document(\"_id\", new ObjectId()).append(\"student_id\", studentId)\n                                                  .append(\"class_id\", classId)\n                                                  .append(\"scores\", scores);\n    }\n}\n```\n\nAs a reminder, every write operation (create, replace, update, delete) performed on a **single** document is [ACID](https://en.wikipedia.org/wiki/ACID) in MongoDB. Which means `insertMany` is not ACID by default but, good news, since MongoDB 4.0, we can wrap this call in a multi-document ACID transaction to make it fully ACID. I explain this in more detail in my blog about [multi-document ACID transactions](https://www.mongodb.com/blog/post/java-and-mongodb-40-support-for-multidocument-acid-transactions).\n\n## Read documents\n\n### Create data\n\nWe created the class `Create`. Now we will work in the `Read` class.\n\nWe wrote 11 new grades, one for the student with `{\"student_id\": 10000}` and 10 for the student with `{\"student_id\": 10001}` in the `sample_training.grades` collection.\n\nAs a reminder, here are the grades of the `{\"student_id\": 10000}`.\n\n``` javascript\nMongoDB Enterprise Cluster0-shard-0:PRIMARY> db.grades.findOne({\"student_id\":10000})\n{\n    \"_id\" : ObjectId(\"5daa0e274f52b44cfea94652\"),\n    \"student_id\" : 10000,\n    \"class_id\" : 1,\n    \"scores\" : [\n        {\n            \"type\" : \"exam\",\n            \"score\" : 39.25175977753478\n        },\n        {\n            \"type\" : \"quiz\",\n            \"score\" : 80.2908713167313\n        },\n        {\n            \"type\" : \"homework\",\n            \"score\" : 63.5444978481843\n        },\n        {\n            \"type\" : \"homework\",\n            \"score\" : 82.35202261582563\n        }\n    ]\n}\n```\n\nWe also discussed BSON types and we noted that `student_id` and `class_id` are doubles.\n\nMongoDB treats some types as equivalent for comparison purposes. For instance, numeric types undergo conversion before comparison.\n\nSo, don't be surprised if I filter with an integer number and match a document which contains a double number for example. If you want to filter documents by value types, you can use the [$type operator](https://docs.mongodb.com/manual/reference/operator/query/type/).\n\nYou can read more about [type bracketing](https://docs.mongodb.com/manual/reference/method/db.collection.find/#type-bracketing) and [comparison and sort order](https://docs.mongodb.com/manual/reference/bson-type-comparison-order/) in our documentation.\n\n### Read a specific document\n\nLet's read the document above. To achieve this, we will use the method `find`, passing it a filter to help identify the document we want to find.\n\nPlease create a class `Read` in the `com.mongodb.quickstart` package with this code:\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.client.*;\nimport org.bson.Document;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport static com.mongodb.client.model.Filters.*;\nimport static com.mongodb.client.model.Projections.*;\nimport static com.mongodb.client.model.Sorts.descending;\n\npublic class Read {\n\n    public static void main(String[] args) {\n        try (MongoClient mongoClient = MongoClients.create(System.getProperty(\"mongodb.uri\"))) {\n            MongoDatabase sampleTrainingDB = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Document> gradesCollection = sampleTrainingDB.getCollection(\"grades\");\n\n            // find one document with new Document\n            Document student1 = gradesCollection.find(new Document(\"student_id\", 10000)).first();\n            System.out.println(\"Student 1: \" + student1.toJson());\n        }\n    }\n}\n```\n\nAlso, make sure you set up your `mongodb.uri` in your system properties using your IDE if you want to run this code in your favorite IDE.\n\nAlternatively, you can use this Maven command line in your root project (where the `src` folder is):\n\n``` bash\nmvn compile exec:java -Dexec.mainClass=\"com.mongodb.quickstart.Read\" -Dmongodb.uri=\"mongodb+srv://USERNAME:PASSWORD@cluster0-abcde.mongodb.net/test?w=majority\"\n```\n\nThe standard output should be:\n\n``` javascript\nStudent 1: {\"_id\": {\"$oid\": \"5daa0e274f52b44cfea94652\"},\n    \"student_id\": 10000.0,\n    \"class_id\": 1.0,\n    \"scores\": [\n        {\"type\": \"exam\", \"score\": 39.25175977753478},\n        {\"type\": \"quiz\", \"score\": 80.2908713167313},\n        {\"type\": \"homework\", \"score\": 63.5444978481843},\n        {\"type\": \"homework\", \"score\": 82.35202261582563}\n    ]\n}\n```\n\nThe MongoDB driver comes with a few helpers to ease the writing of these queries. Here's an equivalent query using the `Filters.eq()` method.\n\n``` java\ngradesCollection.find(eq(\"student_id\", 10000)).first();\n```\n\nOf course, I used a static import to make the code as compact and easy to read as possible.\n\n``` java\nimport static com.mongodb.client.model.Filters.eq;\n```\n\n### Read a range of documents\n\nIn the previous example, the benefit of these helpers is not obvious, but let me show you another example where I'm searching all the grades with a *student_id* greater than or equal to 10,000.\n\n``` java\n// without helpers\ngradesCollection.find(new Document(\"student_id\", new Document(\"$gte\", 10000)));\n// with the Filters.gte() helper\ngradesCollection.find(gte(\"student_id\", 10000));\n```\n\nAs you can see, I'm using the `$gte` operator to write this query. You can learn about all the different [query operators](https://docs.mongodb.com/manual/reference/operator/query/) in the MongoDB documentation.\n\n### Iterators\n\nThe `find` method returns an object that implements the interface `FindIterable`, which ultimately extends the `Iterable` interface so we can use an iterator to go through the list of documents we are receiving from MongoDB:\n\n``` java\nFindIterable<Document> iterable = gradesCollection.find(gte(\"student_id\", 10000));\nMongoCursor<Document> cursor = iterable.iterator();\nSystem.out.println(\"Student list with cursor: \");\nwhile (cursor.hasNext()) {\n    System.out.println(cursor.next().toJson());\n}\n```\n\n### Lists\n\nLists are usually easier to manipulate than iterators, so we can also do this to retrieve directly an `ArrayList<Document>`:\n\n``` java\nList<Document> studentList = gradesCollection.find(gte(\"student_id\", 10000)).into(new ArrayList<>());\nSystem.out.println(\"Student list with an ArrayList:\");\nfor (Document student : studentList) {\n    System.out.println(student.toJson());\n}\n```\n\n### Consumers\n\nWe could also use a `Consumer` which is a functional interface:\n\n``` java\nConsumer<Document> printConsumer = document -> System.out.println(document.toJson());\ngradesCollection.find(gte(\"student_id\", 10000)).forEach(printConsumer);\n```\n\n### Cursors, sort, skip, limit, and projections\n\nAs we saw above with the `Iterator` example, MongoDB leverages [cursors](https://docs.mongodb.com/manual/reference/method/js-cursor/) to iterate through your result set.\n\nIf you are already familiar with the cursors in the [mongo shell](https://www.mongodb.com/docs/v4.4/mongo/), you know that transformations can be applied to it. A cursor can be [sorted](https://docs.mongodb.com/manual/reference/method/cursor.sort/) and the documents it contains can be transformed using a [projection](https://docs.mongodb.com/manual/tutorial/project-fields-from-query-results/).  Also, once the cursor is sorted, we can choose to skip a few documents and limit the number of documents in the output. This is very useful to implement pagination in your frontend for example.\n\nLet's combine everything we have learnt in one query:\n\n``` java\nList<Document> docs = gradesCollection.find(and(eq(\"student_id\", 10001), lte(\"class_id\", 5)))\n                                                  .projection(fields(excludeId(),\n                                                                     include(\"class_id\",\n                                                                             \"student_id\")))\n                                                  .sort(descending(\"class_id\"))\n                                                  .skip(2)\n                                                  .limit(2)\n                                                  .into(new ArrayList<>());\n\nSystem.out.println(\"Student sorted, skipped, limited and projected: \");\nfor (Document student : docs) {\n    System.out.println(student.toJson());\n}\n```\n\nHere is the output we get:\n\n``` javascript\n{\"student_id\": 10001.0, \"class_id\": 3.0}\n{\"student_id\": 10001.0, \"class_id\": 2.0}\n```\n\nRemember that documents are returned in the [natural order](https://docs.mongodb.com/manual/reference/glossary/#term-natural-order), so if you want your output ordered, you need to sort your cursors to make sure there is no randomness in your algorithm.\n\n### Indexes\n\nIf you want to make these queries (with or without sort) efficient, **you need** [indexes](https://docs.mongodb.com/manual/indexes/)!\n\nTo make my last query efficient, I should create this index:\n\n``` javascript\ndb.grades.createIndex({\"student_id\": 1, \"class_id\": -1})\n```\n\nWhen I run an [explain](https://docs.mongodb.com/manual/reference/method/cursor.explain/) on this query, this is the winning plan I get:\n\n``` javascript\n\"winningPlan\" : {\n            \"stage\" : \"LIMIT\",\n            \"limitAmount\" : 2,\n            \"inputStage\" : {\n                \"stage\" : \"PROJECTION_COVERED\",\n                \"transformBy\" : {\n                    \"_id\" : 0,\n                    \"class_id\" : 1,\n                    \"student_id\" : 1\n                },\n                \"inputStage\" : {\n                    \"stage\" : \"SKIP\",\n                    \"skipAmount\" : 2,\n                    \"inputStage\" : {\n                        \"stage\" : \"IXSCAN\",\n                        \"keyPattern\" : {\n                            \"student_id\" : 1,\n                            \"class_id\" : -1\n                        },\n                        \"indexName\" : \"student_id_1_class_id_-1\",\n                        \"isMultiKey\" : false,\n                        \"multiKeyPaths\" : {\n                            \"student_id\" : [ ],\n                            \"class_id\" : [ ]\n                        },\n                        \"isUnique\" : false,\n                        \"isSparse\" : false,\n                        \"isPartial\" : false,\n                        \"indexVersion\" : 2,\n                        \"direction\" : \"forward\",\n                        \"indexBounds\" : {\n                            \"student_id\" : [\n                                \"[10001.0, 10001.0]\"\n                            ],\n                            \"class_id\" : [\n                                \"[5.0, -inf.0]\"\n                            ]\n                        }\n                    }\n                }\n            }\n        }\n```\n\nWith this index, we can see that we have no *SORT* stage, so we are not doing a sort in memory as the documents are already sorted \"for free\" and returned in the order of the index.\n\nAlso, we can see that we don't have any *FETCH* stage, so this is a [covered query](https://docs.mongodb.com/manual/core/query-optimization/#covered-query), the most efficient type of query you can run in MongoDB. Indeed, all the information we are returning at the end is already in the index, so the index itself contains everything we need to answer this query.\n\n### The final code to read documents\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.client.*;\nimport org.bson.Document;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.function.Consumer;\n\nimport static com.mongodb.client.model.Filters.*;\nimport static com.mongodb.client.model.Projections.*;\nimport static com.mongodb.client.model.Sorts.descending;\n\npublic class Read {\n\n    public static void main(String[] args) {\n        try (MongoClient mongoClient = MongoClients.create(System.getProperty(\"mongodb.uri\"))) {\n            MongoDatabase sampleTrainingDB = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Document> gradesCollection = sampleTrainingDB.getCollection(\"grades\");\n\n            // find one document with new Document\n            Document student1 = gradesCollection.find(new Document(\"student_id\", 10000)).first();\n            System.out.println(\"Student 1: \" + student1.toJson());\n\n            // find one document with Filters.eq()\n            Document student2 = gradesCollection.find(eq(\"student_id\", 10000)).first();\n            System.out.println(\"Student 2: \" + student2.toJson());\n\n            // find a list of documents and iterate throw it using an iterator.\n            FindIterable<Document> iterable = gradesCollection.find(gte(\"student_id\", 10000));\n            MongoCursor<Document> cursor = iterable.iterator();\n            System.out.println(\"Student list with a cursor: \");\n            while (cursor.hasNext()) {\n                System.out.println(cursor.next().toJson());\n            }\n\n            // find a list of documents and use a List object instead of an iterator\n            List<Document> studentList = gradesCollection.find(gte(\"student_id\", 10000)).into(new ArrayList<>());\n            System.out.println(\"Student list with an ArrayList:\");\n            for (Document student : studentList) {\n                System.out.println(student.toJson());\n            }\n\n            // find a list of documents and print using a consumer\n            System.out.println(\"Student list using a Consumer:\");\n            Consumer<Document> printConsumer = document -> System.out.println(document.toJson());\n            gradesCollection.find(gte(\"student_id\", 10000)).forEach(printConsumer);\n\n            // find a list of documents with sort, skip, limit and projection\n            List<Document> docs = gradesCollection.find(and(eq(\"student_id\", 10001), lte(\"class_id\", 5)))\n                                                  .projection(fields(excludeId(), include(\"class_id\", \"student_id\")))\n                                                  .sort(descending(\"class_id\"))\n                                                  .skip(2)\n                                                  .limit(2)\n                                                  .into(new ArrayList<>());\n\n            System.out.println(\"Student sorted, skipped, limited and projected: \");\n            for (Document student : docs) {\n                System.out.println(student.toJson());\n            }\n        }\n    }\n}\n```\n\n## Update documents\n\n### Update one document\n\nLet's edit the document with `{student_id: 10000}`. To achieve this, we will use the method `updateOne`.\n\nPlease create a class `Update` in the `com.mongodb.quickstart` package with this code:\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.client.MongoClient;\nimport com.mongodb.client.MongoClients;\nimport com.mongodb.client.MongoCollection;\nimport com.mongodb.client.MongoDatabase;\nimport com.mongodb.client.model.FindOneAndUpdateOptions;\nimport com.mongodb.client.model.ReturnDocument;\nimport com.mongodb.client.model.UpdateOptions;\nimport com.mongodb.client.result.UpdateResult;\nimport org.bson.Document;\nimport org.bson.conversions.Bson;\nimport org.bson.json.JsonWriterSettings;\n\nimport static com.mongodb.client.model.Filters.and;\nimport static com.mongodb.client.model.Filters.eq;\nimport static com.mongodb.client.model.Updates.*;\n\npublic class Update {\n\n    public static void main(String[] args) {\n        JsonWriterSettings prettyPrint = JsonWriterSettings.builder().indent(true).build();\n\n        try (MongoClient mongoClient = MongoClients.create(System.getProperty(\"mongodb.uri\"))) {\n            MongoDatabase sampleTrainingDB = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Document> gradesCollection = sampleTrainingDB.getCollection(\"grades\");\n\n            // update one document\n            Bson filter = eq(\"student_id\", 10000);\n            Bson updateOperation = set(\"comment\", \"You should learn MongoDB!\");\n            UpdateResult updateResult = gradesCollection.updateOne(filter, updateOperation);\n            System.out.println(\"=> Updating the doc with {\\\"student_id\\\":10000}. Adding comment.\");\n            System.out.println(gradesCollection.find(filter).first().toJson(prettyPrint));\n            System.out.println(updateResult);\n        }\n    }\n}\n```\n\nAs you can see in this example, the method `updateOne` takes two parameters:\n\n-   The first one is the filter that identifies the document we want to update.\n-   The second one is the update operation. Here, we are setting a new field `comment` with the value `\"You should learn MongoDB!\"`.\n\nIn order to run this program, make sure you set up your `mongodb.uri` in your system properties using your IDE if you want to run this code in your favorite IDE (see above for more details).\n\nAlternatively, you can use this Maven command line in your root project (where the `src` folder is):\n\n``` bash\nmvn compile exec:java -Dexec.mainClass=\"com.mongodb.quickstart.Update\" -Dmongodb.uri=\"mongodb+srv://USERNAME:PASSWORD@cluster0-abcde.mongodb.net/test?w=majority\"\n```\n\nThe standard output should look like this:\n\n``` javascript\n=> Updating the doc with {\"student_id\":10000}. Adding comment.\n{\n  \"_id\": {\n    \"$oid\": \"5dd5c1f351f97d4a034109ed\"\n  },\n  \"student_id\": 10000.0,\n  \"class_id\": 1.0,\n  \"scores\": [\n    {\n      \"type\": \"exam\",\n      \"score\": 21.580800815091415\n    },\n    {\n      \"type\": \"quiz\",\n      \"score\": 87.66967927111044\n    },\n    {\n      \"type\": \"homework\",\n      \"score\": 96.4060480668003\n    },\n    {\n      \"type\": \"homework\",\n      \"score\": 75.44966835508427\n    }\n  ],\n  \"comment\": \"You should learn MongoDB!\"\n}\nAcknowledgedUpdateResult{matchedCount=1, modifiedCount=1, upsertedId=null}\n```\n\n### Upsert a document\n\nAn upsert is a mix between an insert operation and an update one. It happens when you want to update a document, assuming it exists, but it actually doesn't exist yet in your database.\n\nIn MongoDB, you can set an option to create this document on the fly and carry on with your update operation. This is an upsert operation.\n\nIn this example, I want to add a comment to the grades of my student 10002 for the class 10 but this document doesn't exist yet.\n\n``` java\nfilter = and(eq(\"student_id\", 10002d), eq(\"class_id\", 10d));\nupdateOperation = push(\"comments\", \"You will learn a lot if you read the MongoDB blog!\");\nUpdateOptions options = new UpdateOptions().upsert(true);\nupdateResult = gradesCollection.updateOne(filter, updateOperation, options);\nSystem.out.println(\"\\n=> Upsert document with {\\\"student_id\\\":10002.0, \\\"class_id\\\": 10.0} because it doesn't exist yet.\");\nSystem.out.println(updateResult);\nSystem.out.println(gradesCollection.find(filter).first().toJson(prettyPrint));\n```\n\nAs you can see, I'm using the third parameter of the update operation to set the option upsert to true.\n\nI'm also using the static method `Updates.push()` to push a new value in my array `comments` which does not exist yet, so I'm creating an array of one element in this case.\n\nThis is the output we get:\n\n``` javascript\n=> Upsert document with {\"student_id\":10002.0, \"class_id\": 10.0} because it doesn't exist yet.\nAcknowledgedUpdateResult{matchedCount=0, modifiedCount=0, upsertedId=BsonObjectId{value=5ddeb7b7224ad1d5cfab3733}}\n{\n  \"_id\": {\n    \"$oid\": \"5ddeb7b7224ad1d5cfab3733\"\n  },\n  \"class_id\": 10.0,\n  \"student_id\": 10002.0,\n  \"comments\": [\n    \"You will learn a lot if you read the MongoDB blog!\"\n  ]\n}\n```\n\n### Update many documents\n\nThe same way I was able to update one document with `updateOne()`, I can update multiple documents with `updateMany()`.\n\n``` java\nfilter = eq(\"student_id\", 10001);\nupdateResult = gradesCollection.updateMany(filter, updateOperation);\nSystem.out.println(\"\\n=> Updating all the documents with {\\\"student_id\\\":10001}.\");\nSystem.out.println(updateResult);\n```\n\nIn this example, I'm using the same `updateOperation` as earlier, so I'm creating a new one element array `comments` in these 10 documents.\n\nHere is the output:\n\n``` javascript\n=> Updating all the documents with {\"student_id\":10001}.\nAcknowledgedUpdateResult{matchedCount=10, modifiedCount=10, upsertedId=null}\n```\n\n### The findOneAndUpdate method\n\nFinally, we have one last very useful method available in the MongoDB Java Driver: `findOneAndUpdate()`.\n\nIn most web applications, when a user updates something, they want to see this update reflected on their web page. Without the `findOneAndUpdate()` method, you would have to run an update operation and then fetch the document with a find operation to make sure you are printing the latest version of this object in the web page.\n\nThe `findOneAndUpdate()` method allows you to combine these two operations in one.\n\n``` java\n// findOneAndUpdate\nfilter = eq(\"student_id\", 10000);\nBson update1 = inc(\"x\", 10); // increment x by 10. As x doesn't exist yet, x=10.\nBson update2 = rename(\"class_id\", \"new_class_id\"); // rename variable \"class_id\" in \"new_class_id\".\nBson update3 = mul(\"scores.0.score\", 2); // multiply the first score in the array by 2.\nBson update4 = addToSet(\"comments\", \"This comment is uniq\"); // creating an array with a comment.\nBson update5 = addToSet(\"comments\", \"This comment is uniq\"); // using addToSet so no effect.\nBson updates = combine(update1, update2, update3, update4, update5);\n// returns the old version of the document before the update.\nDocument oldVersion = gradesCollection.findOneAndUpdate(filter, updates);\nSystem.out.println(\"\\n=> FindOneAndUpdate operation. Printing the old version by default:\");\nSystem.out.println(oldVersion.toJson(prettyPrint));\n\n// but I can also request the new version\nfilter = eq(\"student_id\", 10001);\nFindOneAndUpdateOptions optionAfter = new FindOneAndUpdateOptions().returnDocument(ReturnDocument.AFTER);\nDocument newVersion = gradesCollection.findOneAndUpdate(filter, updates, optionAfter);\nSystem.out.println(\"\\n=> FindOneAndUpdate operation. But we can also ask for the new version of the doc:\");\nSystem.out.println(newVersion.toJson(prettyPrint));\n```\n\nHere is the output:\n\n``` javascript\n=> FindOneAndUpdate operation. Printing the old version by default:\n{\n  \"_id\": {\n    \"$oid\": \"5dd5d46544fdc35505a8271b\"\n  },\n  \"student_id\": 10000.0,\n  \"class_id\": 1.0,\n  \"scores\": [\n    {\n      \"type\": \"exam\",\n      \"score\": 69.52994626959251\n    },\n    {\n      \"type\": \"quiz\",\n      \"score\": 87.27457417188077\n    },\n    {\n      \"type\": \"homework\",\n      \"score\": 83.40970667948744\n    },\n    {\n      \"type\": \"homework\",\n      \"score\": 40.43663797673247\n    }\n  ],\n  \"comment\": \"You should learn MongoDB!\"\n}\n\n=> FindOneAndUpdate operation. But we can also ask for the new version of the doc:\n{\n  \"_id\": {\n    \"$oid\": \"5dd5d46544fdc35505a82725\"\n  },\n  \"student_id\": 10001.0,\n  \"scores\": [\n    {\n      \"type\": \"exam\",\n      \"score\": 138.42535412437857\n    },\n    {\n      \"type\": \"quiz\",\n      \"score\": 84.66740178906916\n    },\n    {\n      \"type\": \"homework\",\n      \"score\": 36.773091359279675\n    },\n    {\n      \"type\": \"homework\",\n      \"score\": 14.90842128691825\n    }\n  ],\n  \"comments\": [\n    \"You will learn a lot if you read the MongoDB blog!\",\n    \"This comment is uniq\"\n  ],\n  \"new_class_id\": 10.0,\n  \"x\": 10\n}\n```\n\nAs you can see in this example, you can choose which version of the document you want to return using the appropriate option.\n\nI also used this example to show you a bunch of update operators:\n\n-   `set` will set a value.\n-   `inc` will increment a value.\n-   `rename` will rename a field.\n-   `mul` will multiply the value by the given number.\n-   `addToSet` is similar to push but will only push the value in the array if the value doesn't exist already.\n\nThere are a few other update operators. You can consult the entire list in our [documentation](https://docs.mongodb.com/manual/reference/operator/update/).\n\n### The final code for updates\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.client.MongoClient;\nimport com.mongodb.client.MongoClients;\nimport com.mongodb.client.MongoCollection;\nimport com.mongodb.client.MongoDatabase;\nimport com.mongodb.client.model.FindOneAndUpdateOptions;\nimport com.mongodb.client.model.ReturnDocument;\nimport com.mongodb.client.model.UpdateOptions;\nimport com.mongodb.client.result.UpdateResult;\nimport org.bson.Document;\nimport org.bson.conversions.Bson;\nimport org.bson.json.JsonWriterSettings;\n\nimport static com.mongodb.client.model.Filters.and;\nimport static com.mongodb.client.model.Filters.eq;\nimport static com.mongodb.client.model.Updates.*;\n\npublic class Update {\n\n    public static void main(String[] args) {\n        JsonWriterSettings prettyPrint = JsonWriterSettings.builder().indent(true).build();\n\n        try (MongoClient mongoClient = MongoClients.create(System.getProperty(\"mongodb.uri\"))) {\n            MongoDatabase sampleTrainingDB = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Document> gradesCollection = sampleTrainingDB.getCollection(\"grades\");\n\n            // update one document\n            Bson filter = eq(\"student_id\", 10000);\n            Bson updateOperation = set(\"comment\", \"You should learn MongoDB!\");\n            UpdateResult updateResult = gradesCollection.updateOne(filter, updateOperation);\n            System.out.println(\"=> Updating the doc with {\\\"student_id\\\":10000}. Adding comment.\");\n            System.out.println(gradesCollection.find(filter).first().toJson(prettyPrint));\n            System.out.println(updateResult);\n\n            // upsert\n            filter = and(eq(\"student_id\", 10002d), eq(\"class_id\", 10d));\n            updateOperation = push(\"comments\", \"You will learn a lot if you read the MongoDB blog!\");\n            UpdateOptions options = new UpdateOptions().upsert(true);\n            updateResult = gradesCollection.updateOne(filter, updateOperation, options);\n            System.out.println(\"\\n=> Upsert document with {\\\"student_id\\\":10002.0, \\\"class_id\\\": 10.0} because it doesn't exist yet.\");\n            System.out.println(updateResult);\n            System.out.println(gradesCollection.find(filter).first().toJson(prettyPrint));\n\n            // update many documents\n            filter = eq(\"student_id\", 10001);\n            updateResult = gradesCollection.updateMany(filter, updateOperation);\n            System.out.println(\"\\n=> Updating all the documents with {\\\"student_id\\\":10001}.\");\n            System.out.println(updateResult);\n\n            // findOneAndUpdate\n            filter = eq(\"student_id\", 10000);\n            Bson update1 = inc(\"x\", 10); // increment x by 10. As x doesn't exist yet, x=10.\n            Bson update2 = rename(\"class_id\", \"new_class_id\"); // rename variable \"class_id\" in \"new_class_id\".\n            Bson update3 = mul(\"scores.0.score\", 2); // multiply the first score in the array by 2.\n            Bson update4 = addToSet(\"comments\", \"This comment is uniq\"); // creating an array with a comment.\n            Bson update5 = addToSet(\"comments\", \"This comment is uniq\"); // using addToSet so no effect.\n            Bson updates = combine(update1, update2, update3, update4, update5);\n            // returns the old version of the document before the update.\n            Document oldVersion = gradesCollection.findOneAndUpdate(filter, updates);\n            System.out.println(\"\\n=> FindOneAndUpdate operation. Printing the old version by default:\");\n            System.out.println(oldVersion.toJson(prettyPrint));\n\n            // but I can also request the new version\n            filter = eq(\"student_id\", 10001);\n            FindOneAndUpdateOptions optionAfter = new FindOneAndUpdateOptions().returnDocument(ReturnDocument.AFTER);\n            Document newVersion = gradesCollection.findOneAndUpdate(filter, updates, optionAfter);\n            System.out.println(\"\\n=> FindOneAndUpdate operation. But we can also ask for the new version of the doc:\");\n            System.out.println(newVersion.toJson(prettyPrint));\n        }\n    }\n}\n```\n\n## Delete documents\n\n### Delete one document\n\nLet's delete the document above. To achieve this, we will use the method `deleteOne`.\n\nPlease create a class `Delete` in the `com.mongodb.quickstart` package with this code:\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.client.MongoClient;\nimport com.mongodb.client.MongoClients;\nimport com.mongodb.client.MongoCollection;\nimport com.mongodb.client.MongoDatabase;\nimport com.mongodb.client.result.DeleteResult;\nimport org.bson.Document;\nimport org.bson.conversions.Bson;\n\nimport static com.mongodb.client.model.Filters.eq;\nimport static com.mongodb.client.model.Filters.gte;\n\npublic class Delete {\n\n    public static void main(String[] args) {\n\n        try (MongoClient mongoClient = MongoClients.create(System.getProperty(\"mongodb.uri\"))) {\n            MongoDatabase sampleTrainingDB = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Document> gradesCollection = sampleTrainingDB.getCollection(\"grades\");\n\n            // delete one document\n            Bson filter = eq(\"student_id\", 10000);\n            DeleteResult result = gradesCollection.deleteOne(filter);\n            System.out.println(result);\n        }\n    }\n}\n```\n\nAs you can see in this example, the method `deleteOne` only takes one parameter: a filter, just like the `find()` operation.\n\nIn order to run this program, make sure you set up your `mongodb.uri` in your system properties using your IDE if you want to run this code in your favorite IDE (see above for more details).\n\nAlternatively, you can use this Maven command line in your root project (where the `src` folder is):\n\n``` bash\nmvn compile exec:java -Dexec.mainClass=\"com.mongodb.quickstart.Delete\" -Dmongodb.uri=\"mongodb+srv://USERNAME:PASSWORD@cluster0-abcde.mongodb.net/test?w=majority\"\n```\n\nThe standard output should look like this:\n\n``` javascript\nAcknowledgedDeleteResult{deletedCount=1}\n```\n\n### FindOneAndDelete()\n\nAre you emotionally attached to your document and want a chance to see it one last time before it's too late? We have what you need.\n\nThe method `findOneAndDelete()` allows you to retrieve a document and delete it in a single atomic operation.\n\nHere is how it works:\n\n``` java\nBson filter = eq(\"student_id\", 10002);\nDocument doc = gradesCollection.findOneAndDelete(filter);\nSystem.out.println(doc.toJson(JsonWriterSettings.builder().indent(true).build()));\n```\n\nHere is the output we get:\n\n``` javascript\n{\n  \"_id\": {\n    \"$oid\": \"5ddec378224ad1d5cfac02b8\"\n  },\n  \"class_id\": 10.0,\n  \"student_id\": 10002.0,\n  \"comments\": [\n    \"You will learn a lot if you read the MongoDB blog!\"\n  ]\n}\n```\n\n### Delete many documents\n\nThis time we will use `deleteMany()` instead of `deleteOne()` and we will use a different filter to match more documents.\n\n``` java\nBson filter = gte(\"student_id\", 10000);\nDeleteResult result = gradesCollection.deleteMany(filter);\nSystem.out.println(result);\n```\n\nAs a reminder, you can learn more about all the query selectors [in our\ndocumentation](https://docs.mongodb.com/manual/reference/operator/query/#query-selectors).\n\nThis is the output we get:\n\n``` javascript\nAcknowledgedDeleteResult{deletedCount=10}\n```\n\n### Delete a collection\n\nDeleting all the documents from a collection will not delete the collection itself because a collection also contains metadata like the index definitions or the chunk distribution if your collection is sharded for example.\n\nIf you want to remove the entire collection **and** all the metadata associated with it, then you need to use the `drop()` method.\n\n``` java\ngradesCollection.drop();\n```\n\n### The final code for delete operations\n\n``` java\npackage com.mongodb.quickstart;\n\nimport com.mongodb.client.MongoClient;\nimport com.mongodb.client.MongoClients;\nimport com.mongodb.client.MongoCollection;\nimport com.mongodb.client.MongoDatabase;\nimport com.mongodb.client.result.DeleteResult;\nimport org.bson.Document;\nimport org.bson.conversions.Bson;\nimport org.bson.json.JsonWriterSettings;\n\nimport static com.mongodb.client.model.Filters.eq;\nimport static com.mongodb.client.model.Filters.gte;\n\npublic class Delete {\n\n    public static void main(String[] args) {\n        try (MongoClient mongoClient = MongoClients.create(System.getProperty(\"mongodb.uri\"))) {\n            MongoDatabase sampleTrainingDB = mongoClient.getDatabase(\"sample_training\");\n            MongoCollection<Document> gradesCollection = sampleTrainingDB.getCollection(\"grades\");\n\n            // delete one document\n            Bson filter = eq(\"student_id\", 10000);\n            DeleteResult result = gradesCollection.deleteOne(filter);\n            System.out.println(result);\n\n            // findOneAndDelete operation\n            filter = eq(\"student_id\", 10002);\n            Document doc = gradesCollection.findOneAndDelete(filter);\n            System.out.println(doc.toJson(JsonWriterSettings.builder().indent(true).build()));\n\n            // delete many documents\n            filter = gte(\"student_id\", 10000);\n            result = gradesCollection.deleteMany(filter);\n            System.out.println(result);\n\n            // delete the entire collection and its metadata (indexes, chunk metadata, etc).\n            gradesCollection.drop();\n        }\n    }\n}\n```\n\n## Wrapping up\n\nWith this blog post, we have covered all the basic operations, such as create and read, and have also seen how we can easily use powerful functions available in the Java driver for MongoDB. You can find the links to the other blog posts of this series just below.\n\n>If you want to learn more and deepen your knowledge faster, I recommend you check out the M220J: MongoDB for Java Developers training available for free on [MongoDB University](https://university.mongodb.com/courses/M220J/about).","description":"Learn how to use MongoDB with Java in this tutorial on CRUD operations with example code and walkthrough.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blte0424980e374823d/644c4736efd5716730fe6f7b/Java_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/java-setup-crud-operations","title":"*Getting Started with MongoDB and Java - CRUD Operations Tutorial","original_publish_date":"2022-02-01T19:28:57.565Z","strapi_updated_at":"2023-01-26T16:56:03.124Z","expiry_date":"2022-12-14T02:03:44.090Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*Java","calculated_slug":"/languages/java"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to use MongoDB with Java in this tutorial on CRUD operations with example code and walkthrough.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltea9590d8bc695e7a/644c473775b185097146a90d/Java_Tile_360x360.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@MBeugnet"},"system":{"updated_at":"2023-04-28T22:52:00.316Z","publish_details":{"time":"2023-04-28T22:53:02.675Z"}}},{"calculated_slug":"/languages/javascript/node-aggregation-framework-3-3-2","content":"When you want to analyze data stored in MongoDB, you can use MongoDB's powerful aggregation framework to do so. Today, I'll give you a high-level overview of the aggregation framework and show you how to use it.\n\n>This post uses MongoDB 4.0, MongoDB Node.js Driver 3.3.2, and Node.js 10.16.3.\n>\n>Click [here](/developer/code-examples/javascript/node-aggregation-framework) to see a newer version of this post that uses MongoDB 4.4, MongoDB Node.js Driver 3.6.4, and Node.js 14.15.4.\n\n<img\n    style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n    src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-nodejs.png\"\n    alt=\"QuickStart Node.js Logo\" />\n\nIf you're just joining us in this Quick Start with MongoDB and Node.js series, welcome! So far, we've covered how to [connect to MongoDB](/developer/code-examples/javascript/node-connect-mongodb-3-3-2/) and perform each of the [CRUD (Create, Read, Update, and Delete) operations](/developer/languages/javascript/node-crud-tutorial-3-3-2/).  The code we write today will use the same structure as the code we built in the first post in the series; so, if you have any questions about how to get started or how the code is structured, [head back to that first post](/developer/code-examples/javascript/node-connect-mongodb-3-3-2/).\n\nAnd, with that, let's dive into the aggregation framework!\n\n>If you are more of a video person than an article person, fear not. I've made a video just for you! The video below covers the same content as this article.\n>\n>:youtube[]{vid=iz37fDe1XoM}\n>\n>Get started with an M0 cluster on [Atlas](https://www.mongodb.com/cloud/atlas) today. It's free forever, and it's the easiest way to try out the steps in this blog series.\n\n## What is the Aggregation Framework?\n\nThe aggregation framework allows you to analyze your data in real time.  Using the framework, you can create an aggregation pipeline that consists of one or more [stages](https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/#aggregation-pipeline-operator-reference).  Each stage transforms the documents and passes the output to the next stage.\n\nIf you're familiar with the Linux pipe ( `|` ), you can think of the aggregation pipeline as a very similar concept. Just as output from one command is passed as input to the next command when you use piping, output from one stage is passed as input to the next stage when you use the aggregation pipeline.\n\nThe aggregation framework has a variety of [stages](https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/) available for you to use. Today, we'll discuss the basics of how to use [$match](https://docs.mongodb.com/manual/reference/operator/aggregation/match/), [$group](https://docs.mongodb.com/manual/reference/operator/aggregation/group/), [$sort](https://docs.mongodb.com/manual/reference/operator/aggregation/sort/), and [$limit](https://docs.mongodb.com/manual/reference/operator/aggregation/limit/).  Note that the aggregation framework has many other powerful stages including [$count](https://docs.mongodb.com/manual/reference/operator/aggregation/count/), [$geoNear](https://docs.mongodb.com/manual/reference/operator/aggregation/geoNear/), [$graphLookup](https://docs.mongodb.com/manual/reference/operator/aggregation/graphLookup/), [$project](https://docs.mongodb.com/manual/reference/operator/aggregation/project/), [$unwind](https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/), and others.\n\n## How Do You Use the Aggregation Framework?\n\nI'm hoping to visit the beautiful city of Sydney, Australia soon. Sydney is a huge city with many suburbs, and I'm not sure where to start looking for a cheap rental. I want to know which Sydney suburbs have, on average, the cheapest one-bedroom Airbnb listings.\n\nI could write a query to pull all of the one-bedroom listings in the Sydney area and then write a script to group the listings by suburb and calculate the average price per suburb. Or, I could write a single command using the aggregation pipeline. Let's use the aggregation pipeline.\n\nThere is a variety of ways you can create aggregation pipelines. You can write them manually in a code editor or create them visually inside of [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) or [MongoDB Compass](https://www.mongodb.com/products/compass). In general, I don't recommend writing pipelines manually as it's much easier to understand what your pipeline is doing and spot errors when you use a visual editor. Since you're already setup to use MongoDB Atlas for this blog series, we'll create our aggregation pipeline in Atlas.\n\n### Navigate to the Aggregation Pipeline Builder in Atlas\n\nThe first thing we need to do is navigate to the Aggregation Pipeline Builder in Atlas.\n\n1.  Navigate to [Atlas](https://www.mongodb.com/cloud/atlas) and authenticate if you're not already authenticated.\n2.  In the **Organizations** menu in the upper-left corner, select the organization you are using for this Quick Start series.\n3.  In the **Projects** menu (located beneath the Organizations menu), select the project you are using for this Quick Start series.\n4.  In the right pane for your cluster, click **COLLECTIONS**.\n5.  In the list of databases and collections that appears, select **listingsAndReviews**.\n6.  In the right pane, select the **Aggregation** view to open the Aggregation Pipeline Builder.\n\n![The Aggregation Pipeline Builder in Atlas](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/AggregationView-mdb40.png)\n\nThe Aggregation Pipeline Builder provides you with a visual representation of your aggregation pipeline. Each stage is represented by a new row. You can put the code for each stage on the left side of a row, and the Aggregation Pipeline Builder will automatically provide a live sample of results for that stage on the right side of the row.\n\n## Build an Aggregation Pipeline\n\nNow we are ready to build an aggregation pipeline.\n\n### Add a $match Stage\n\nLet's begin by narrowing down the documents in our pipeline to one-bedroom listings in the Sydney, Australia market where the room type is \"Entire home/apt.\" We can do so by using the [$match](https://docs.mongodb.com/manual/reference/operator/aggregation/match/) stage.\n\n1.  On the row representing the first stage of the pipeline, choose **$match** in the **Select**... box. The Aggregation Pipeline Builder automatically provides sample code for how to use the `$match` operator in the code box for the stage.\n\n    ![The $match stage](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/match-query.png)\n\n2.  Now we can input a query in the code box. The query syntax for `$match` is the same as the `findOne()` syntax that we used in a [previous post](/quickstart/node-crud-tutorial-3-3-2/#read-one-document). Replace the code in the `$match` stage's code box with the following:\n\n``` json\n{\n  bedrooms: 1,\n  \"address.country\": \"Australia\",\n  \"address.market\": \"Sydney\",\n  \"address.suburb\": { $exists: 1, $ne: \"\" },\n  room_type: \"Entire home/apt\"\n}\n```\n\nNote that we will be using the `address.suburb` field later in the pipeline, so we are filtering out documents where `address.suburb` does not exist or is represented by an empty string.\n\nThe Aggregation Pipeline Builder automatically updates the output on the right side of the row to show a sample of 20 documents that will be included in the results after the `$match` stage is executed.\n\n![$match stage output](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/match-output.png)\n\n### Add a $group Stage\n\nNow that we have narrowed our documents down to one-bedroom listings in the Sydney, Australia market, we are ready to group them by suburb. We can do so by using the [$group](https://docs.mongodb.com/manual/reference/operator/aggregation/group/) stage.\n\n1.  Click **ADD STAGE**. A new stage appears in the pipeline.\n2.  On the row representing the new stage of the pipeline, choose **$group** in the **Select**... box. The Aggregation Pipeline Builder automatically provides sample code for how to use the `$group` operator in the code box for the stage.\n\n    ![The $group stage](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/group-code.png)\n\n3.  Now we can input code for the `$group` stage. We will provide an `_id`, which is the field that the Aggregation Framework will use to create our groups. In this case, we will use `$address.suburb` as our `_id`. Inside of the $group stage, we will also create a new field named `averagePrice`. We can use the [$avg](https://docs.mongodb.com/manual/reference/operator/aggregation/avg/index.html) aggregation pipeline operator to calculate the average price for each suburb. Replace the code in the $group stage's code box with the following:\n\n``` json\n{\n  _id: \"$address.suburb\",\n  averagePrice: {\n    \"$avg\": \"$price\"\n  }\n}\n```\n\nThe Aggregation Pipeline Builder automatically updates the output on the right side of the row to show a sample of 20 documents that will be included in the results after the `$group` stage is executed. Note that the documents have been transformed. Instead of having a document for each listing, we now have a document for each suburb. The suburb documents have only two fields: `_id` (the name of the suburb) and `averagePrice`.\n\n![$group stage output](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/group-output.png)\n\n### Add a $sort Stage\n\nNow that we have the average prices for suburbs in the Sydney, Australia market, we are ready to sort them to discover which are the least expensive. We can do so by using the [$sort](https://docs.mongodb.com/manual/reference/operator/aggregation/sort/) stage.\n\n1.  Click **ADD STAGE**. A new stage appears in the pipeline.\n2.  On the row representing the new stage of the pipeline, choose **$sort** in the **Select**... box. The Aggregation Pipeline Builder automatically provides sample code for how to use the `$sort` operator in the code box for the stage.\n\n    ![The $sort stage](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/sort-code.png)\n\n3.  Now we are ready to input code for the `$sort` stage. We will sort on the `$averagePrice` field we created in the previous stage. We will indicate we want to sort in ascending order by passing `1`. Replace the code in the `$sort` stage's code box with the following:\n\n``` json\n{\n  \"averagePrice\": 1\n}\n```\n\nThe Aggregation Pipeline Builder automatically updates the output on the right side of the row to show a sample of 20 documents that will be included in the results after the `$sort` stage is executed. Note that the documents have the same shape as the documents in the previous stage; the documents are simply sorted from least to most expensive.\n\n![$sort stage output](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/sort-output.png)\n\n### Add a $limit Stage\n\nNow we have the average prices for suburbs in the Sydney, Australia market sorted from least to most expensive. We may not want to work with all of the suburb documents in our application. Instead, we may want to limit our results to the 10 least expensive suburbs. We can do so by using the [$limit](https://docs.mongodb.com/manual/reference/operator/aggregation/limit/) stage.\n\n1.  Click **ADD STAGE**. A new stage appears in the pipeline.\n2.  On the row representing the new stage of the pipeline, choose **$limit** in the **Select**... box. The Aggregation Pipeline Builder automatically provides sample code for how to use the `$limit` operator in the code box for the stage.\n\n    ![The $limit stage](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/limit-code.png)\n\n3.  Now we are ready to input code for the `$limit` stage. Let's limit our results to 10 documents. Replace the code in the $limit stage's code box with the following:\n\n``` json\n10\n```\n\nThe Aggregation Pipeline Builder automatically updates the output on the right side of the row to show a sample of 10 documents that will be included in the results after the `$limit` stage is executed. Note that the documents have the same shape as the documents in the previous stage; we've simply limited the number of results to 10.\n\n![$limit stage output](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/limit-output.png)\n\n## Execute an Aggregation Pipeline in Node.js\n\nNow that we have built an aggregation pipeline, let's execute it from inside of a Node.js script.\n\n### Get a Copy of the Node.js Template\n\nTo make following along with this blog post easier, I've created a starter template for a Node.js script that accesses an Atlas cluster.\n\n1.  Download a copy of [template.js](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/template.js).\n2.  Open `template.js` in your favorite code editor.\n3.  Update the Connection URI to point to your Atlas cluster. If you're not sure how to do that, refer back to the [first post in this series](/quickstart/node-connect-mongodb-3-3-2/).\n4.  Save the file as `aggregation.js`.\n\nYou can run this file by executing `node aggregation.js` in your shell. At this point, the file simply opens and closes a connection to your Atlas cluster, so no output is expected. If you see DeprecationWarnings, you can ignore them for the purposes of this post.\n\n### Create a Function\n\nLet's create a function whose job it is to print the cheapest suburbs for a given market.\n\n1.  Continuing to work in `aggregation.js`, create an asynchronous function named `printCheapestSuburbs` that accepts a connected MongoClient, a country, a market, and the maximum number of results to print as parameters.\n\n    ``` js\n    async function printCheapestSuburbs(client, country, market, maxNumberToPrint) {\n    }\n    ```\n\n2.  We can execute a pipeline in Node.js by calling\n    [Collection](https://mongodb.github.io/node-mongodb-native/3.3/api/Collection.html)'s\n    [aggregate()](https://mongodb.github.io/node-mongodb-native/3.3/api/Collection.html#aggregate).\n    Paste the following in your new function:\n\n    ``` js\n    const pipeline = [];\n\n    const aggCursor = client.db(\"sample_airbnb\")\n                            .collection(\"listingsAndReviews\")\n                            .aggregate(pipeline);\n    ```\n\n3.  The first param for `aggregate()` is a pipeline of type object. We could manually create the pipeline here. Since we've already created a pipeline inside of Atlas, let's export the pipeline from there. Return to the Aggregation Pipeline Builder in Atlas. Click the **Export pipeline code to language** button.\n\n    ![Export pipeline in Atlas](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/export-pipeline-mdb40.png)\n\n4.  The **Export Pipeline To Language** dialog appears. In the **Export Pipleine To** selection box, choose **NODE**.\n5.  In the Node pane on the right side of the dialog, click the **copy** button.\n6.  Return to your code editor and paste the `pipeline` in place of the empty object currently assigned to the pipeline constant.\n\n    ``` js\n    const pipeline = [\n    {\n        '$match': {\n        'bedrooms': 1,\n        'address.country': 'Australia', \n        'address.market': 'Sydney', \n        'address.suburb': {\n            '$exists': 1, \n            '$ne': ''\n        }, \n        'room_type': 'Entire home/apt'\n        }\n    }, {\n        '$group': {\n        '_id': '$address.suburb', \n        'averagePrice': {\n            '$avg': '$price'\n        }\n        }\n    }, {\n        '$sort': {\n        'averagePrice': 1\n        }\n    }, {\n        '$limit': 10\n    }\n    ];\n    ```\n\n7.  This pipeline would work fine as written. However, it is hardcoded to search for 10 results in the Sydney, Australia market. We should update this pipeline to be more generic. Make the following replacements in the pipeline definition:\n    1.  Replace `'Australia'` with `country`\n    2.  Replace `'Sydney'` with `market`\n    3.  Replace `10` with `maxNumberToPrint`\n\n8.  `aggregate()` will return an [AggregationCursor](https://mongodb.github.io/node-mongodb-native/3.3/api/AggregationCursor.html), which we are storing in the `aggCursor` constant. An AggregationCursor allows traversal over the aggregation pipeline results. We can use AggregationCursor's [forEach()](https://mongodb.github.io/node-mongodb-native/3.3/api/AggregationCursor.html#forEach) to iterate over the results. Paste the following inside `printCheapestSuburbs()` below the definition of `aggCursor`.\n\n``` js\nawait aggCursor.forEach(airbnbListing => {\n  console.log(`${airbnbListing._id}: ${airbnbListing.averagePrice}`);\n});\n```\n\n### Call the Function\n\nNow we are ready to call our function to print the 10 cheapest suburbs in the Sydney, Australia market. Add the following call in the `main()` function beneath the comment that says `Make the appropriate DB calls`.\n\n``` js\nawait printCheapestSuburbs(client, \"Australia\", \"Sydney\", 10);\n```\n\nRunning aggregation.js results in the following output:\n\n``` json\nBalgowlah: 45.00\nWilloughby: 80.00\nMarrickville: 94.50\nSt Peters: 100.00\nRedfern: 101.00\nCronulla: 109.00\nBellevue Hill: 109.50\nKingsgrove: 112.00\nCoogee: 115.00\nNeutral Bay: 119.00\n```\n\nNow I know what suburbs to begin searching as I prepare for my trip to Sydney, Australia.\n\n## Wrapping Up\n\nThe aggregation framework is an incredibly powerful way to analyze your data. Learning to create pipelines may seem a little intimidating at first, but it's worth the investment. The aggregation framework can get results to your end-users faster and save you from a lot of scripting.\n\nToday, we only scratched the surface of the aggregation framework. I highly recommend MongoDB University's free course specifically on the aggregation framework: [M121: The MongoDB Aggregation Framework](https://university.mongodb.com/courses/M121/about). The course has a more thorough explanation of how the aggregation framework works and provides detail on how to use the various pipeline stages.\n\nThis post included many code snippets that built on code written in the [first](/developer/code-examples/javascript/node-connect-mongodb-3-3-2/) post of this MongoDB and Node.js Quick Start series. To get a full copy of the code used in today's post, visit the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/aggregation.js).\n\nNow you're ready to move on to the [next post in this series](/developer/code-examples/javascript/nodejs-change-streams-triggers-3-3-2) all about change streams and triggers. In that post, you'll learn how to automatically react to changes in your database.\n\nQuestions? Comments? We'd love to connect with you. Join the conversation on the [MongoDB Community Forums](https://developer.mongodb.com/community/forums/).","description":"Discover how to analyze your data using MongoDB's Aggregation Framework and Node.js.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt1033dd97d4751b12/644c473be8cf4ed75aeec867/NodeJSArtwork.png?branch=prod","description":null}}]},"slug":"/node-aggregation-framework-3-3-2","title":"*Aggregation Framework with Node.js 3.3.2 Tutorial","original_publish_date":"2022-02-02T13:33:44.379Z","strapi_updated_at":"2023-04-20T15:55:18.027Z","expiry_date":"2022-12-14T02:03:53.250Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Discover how to analyze your data using MongoDB's Aggregation Framework and Node.js.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt9afca9fff8e16c04/644c473c118da15fc4486387/NodejsThumb.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@Lauren_Schaefer"},"system":{"updated_at":"2023-04-28T22:51:59.919Z","publish_details":{"time":"2023-04-28T22:53:02.713Z"}}},{"calculated_slug":"/languages/javascript/node-aggregation-framework","content":"When you want to analyze data stored in MongoDB, you can use MongoDB's powerful aggregation framework to do so. Today, I'll give you a high-level overview of the aggregation framework and show you how to use it.\n\n>This post uses MongoDB 4.0, MongoDB Node.js Driver 3.3.2, and Node.js 10.16.3.\n>\n>Click [here](/developer/languages/javascript/node-aggregation-framework) to see a newer version of this post that uses MongoDB 4.4, MongoDB Node.js Driver 3.6.4, and Node.js 14.15.4.\n\n<img\n    style=\"float: right;\n        border-radius: 10px;\n        margin-bottom: 30px;\n        vertical-align: bottom;\n        width: 30%;\"\n    src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-nodejs.png\"\n    alt=\"QuickStart Node.js Logo\" />\n\nIf you're just joining us in this Quick Start with MongoDB and Node.js series, welcome! So far, we've covered how to [connect to MongoDB](/developer/languages/javascript/node-connect-mongodb/) and perform each of the [CRUD (Create, Read, Update, and Delete) operations](/developer/languages/javascript/node-crud-tutorial/).  The code we write today will use the same structure as the code we built in the first post in the series; so, if you have any questions about how to get started or how the code is structured, [head back to that first post](/developer/languages/javascript/node-connect-mongodb/).\n\nAnd, with that, let's dive into the aggregation framework!\n\n>If you are more of a video person than an article person, fear not. I've made a video just for you! The video below covers the same content as this article.\n>\n>:youtube[]{vid=iz37fDe1XoM}\n>\n>Get started with an M0 cluster on [Atlas](https://www.mongodb.com/cloud/atlas) today. It's free forever, and it's the easiest way to try out the steps in this blog series.\n\n## What is the Aggregation Framework?\n\nThe aggregation framework allows you to analyze your data in real time.  Using the framework, you can create an aggregation pipeline that consists of one or more [stages](https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/#aggregation-pipeline-operator-reference).  Each stage transforms the documents and passes the output to the next stage.\n\nIf you're familiar with the Linux pipe ( `|` ), you can think of the aggregation pipeline as a very similar concept. Just as output from one command is passed as input to the next command when you use piping, output from one stage is passed as input to the next stage when you use the aggregation pipeline.\n\nThe aggregation framework has a variety of [stages](https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/) available for you to use. Today, we'll discuss the basics of how to use [$match](https://docs.mongodb.com/manual/reference/operator/aggregation/match/), [$group](https://docs.mongodb.com/manual/reference/operator/aggregation/group/), [$sort](https://docs.mongodb.com/manual/reference/operator/aggregation/sort/), and [$limit](https://docs.mongodb.com/manual/reference/operator/aggregation/limit/).  Note that the aggregation framework has many other powerful stages including [$count](https://docs.mongodb.com/manual/reference/operator/aggregation/count/), [$geoNear](https://docs.mongodb.com/manual/reference/operator/aggregation/geoNear/), [$graphLookup](https://docs.mongodb.com/manual/reference/operator/aggregation/graphLookup/), [$project](https://docs.mongodb.com/manual/reference/operator/aggregation/project/), [$unwind](https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/), and others.\n\n## How Do You Use the Aggregation Framework?\n\nI'm hoping to visit the beautiful city of Sydney, Australia soon. Sydney is a huge city with many suburbs, and I'm not sure where to start looking for a cheap rental. I want to know which Sydney suburbs have, on average, the cheapest one-bedroom Airbnb listings.\n\nI could write a query to pull all of the one-bedroom listings in the Sydney area and then write a script to group the listings by suburb and calculate the average price per suburb. Or, I could write a single command using the aggregation pipeline. Let's use the aggregation pipeline.\n\nThere is a variety of ways you can create aggregation pipelines. You can write them manually in a code editor or create them visually inside of [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) or [MongoDB Compass](https://www.mongodb.com/products/compass). In general, I don't recommend writing pipelines manually as it's much easier to understand what your pipeline is doing and spot errors when you use a visual editor. Since you're already setup to use MongoDB Atlas for this blog series, we'll create our aggregation pipeline in Atlas.\n\n### Navigate to the Aggregation Pipeline Builder in Atlas\n\nThe first thing we need to do is navigate to the Aggregation Pipeline Builder in Atlas.\n\n1.  Navigate to [Atlas](https://www.mongodb.com/cloud/atlas) and authenticate if you're not already authenticated.\n2.  In the **Organizations** menu in the upper-left corner, select the organization you are using for this Quick Start series.\n3.  In the **Projects** menu (located beneath the Organizations menu), select the project you are using for this Quick Start series.\n4.  In the right pane for your cluster, click **COLLECTIONS**.\n5.  In the list of databases and collections that appears, select **listingsAndReviews**.\n6.  In the right pane, select the **Aggregation** view to open the Aggregation Pipeline Builder.\n\n![The Aggregation Pipeline Builder in Atlas](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/AggregationView-mdb40.png)\n\nThe Aggregation Pipeline Builder provides you with a visual representation of your aggregation pipeline. Each stage is represented by a new row. You can put the code for each stage on the left side of a row, and the Aggregation Pipeline Builder will automatically provide a live sample of results for that stage on the right side of the row.\n\n## Build an Aggregation Pipeline\n\nNow we are ready to build an aggregation pipeline.\n\n### Add a $match Stage\n\nLet's begin by narrowing down the documents in our pipeline to one-bedroom listings in the Sydney, Australia market where the room type is \"Entire home/apt.\" We can do so by using the [$match](https://docs.mongodb.com/manual/reference/operator/aggregation/match/) stage.\n\n1.  On the row representing the first stage of the pipeline, choose **$match** in the **Select**... box. The Aggregation Pipeline Builder automatically provides sample code for how to use the `$match` operator in the code box for the stage.\n\n    ![The $match stage](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/match-query.png)\n\n2.  Now we can input a query in the code box. The query syntax for `$match` is the same as the `findOne()` syntax that we used in a [previous post](/quickstart/node-crud-tutorial-3-3-2/#read-one-document). Replace the code in the `$match` stage's code box with the following:\n\n``` json\n{\n  bedrooms: 1,\n  \"address.country\": \"Australia\",\n  \"address.market\": \"Sydney\",\n  \"address.suburb\": { $exists: 1, $ne: \"\" },\n  room_type: \"Entire home/apt\"\n}\n```\n\nNote that we will be using the `address.suburb` field later in the pipeline, so we are filtering out documents where `address.suburb` does not exist or is represented by an empty string.\n\nThe Aggregation Pipeline Builder automatically updates the output on the right side of the row to show a sample of 20 documents that will be included in the results after the `$match` stage is executed.\n\n![$match stage output](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/match-output.png)\n\n### Add a $group Stage\n\nNow that we have narrowed our documents down to one-bedroom listings in the Sydney, Australia market, we are ready to group them by suburb. We can do so by using the [$group](https://docs.mongodb.com/manual/reference/operator/aggregation/group/) stage.\n\n1.  Click **ADD STAGE**. A new stage appears in the pipeline.\n2.  On the row representing the new stage of the pipeline, choose **$group** in the **Select**... box. The Aggregation Pipeline Builder automatically provides sample code for how to use the `$group` operator in the code box for the stage.\n\n    ![The $group stage](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/group-code.png)\n\n3.  Now we can input code for the `$group` stage. We will provide an `_id`, which is the field that the Aggregation Framework will use to create our groups. In this case, we will use `$address.suburb` as our `_id`. Inside of the $group stage, we will also create a new field named `averagePrice`. We can use the [$avg](https://docs.mongodb.com/manual/reference/operator/aggregation/avg/index.html) aggregation pipeline operator to calculate the average price for each suburb. Replace the code in the $group stage's code box with the following:\n\n``` json\n{\n  _id: \"$address.suburb\",\n  averagePrice: {\n    \"$avg\": \"$price\"\n  }\n}\n```\n\nThe Aggregation Pipeline Builder automatically updates the output on the right side of the row to show a sample of 20 documents that will be included in the results after the `$group` stage is executed. Note that the documents have been transformed. Instead of having a document for each listing, we now have a document for each suburb. The suburb documents have only two fields: `_id` (the name of the suburb) and `averagePrice`.\n\n![$group stage output](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/group-output.png)\n\n### Add a $sort Stage\n\nNow that we have the average prices for suburbs in the Sydney, Australia market, we are ready to sort them to discover which are the least expensive. We can do so by using the [$sort](https://docs.mongodb.com/manual/reference/operator/aggregation/sort/) stage.\n\n1.  Click **ADD STAGE**. A new stage appears in the pipeline.\n2.  On the row representing the new stage of the pipeline, choose **$sort** in the **Select**... box. The Aggregation Pipeline Builder automatically provides sample code for how to use the `$sort` operator in the code box for the stage.\n\n    ![The $sort stage](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/sort-code.png)\n\n3.  Now we are ready to input code for the `$sort` stage. We will sort on the `$averagePrice` field we created in the previous stage. We will indicate we want to sort in ascending order by passing `1`. Replace the code in the `$sort` stage's code box with the following:\n\n``` json\n{\n  \"averagePrice\": 1\n}\n```\n\nThe Aggregation Pipeline Builder automatically updates the output on the right side of the row to show a sample of 20 documents that will be included in the results after the `$sort` stage is executed. Note that the documents have the same shape as the documents in the previous stage; the documents are simply sorted from least to most expensive.\n\n![$sort stage output](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/sort-output.png)\n\n### Add a $limit Stage\n\nNow we have the average prices for suburbs in the Sydney, Australia market sorted from least to most expensive. We may not want to work with all of the suburb documents in our application. Instead, we may want to limit our results to the 10 least expensive suburbs. We can do so by using the [$limit](https://docs.mongodb.com/manual/reference/operator/aggregation/limit/) stage.\n\n1.  Click **ADD STAGE**. A new stage appears in the pipeline.\n2.  On the row representing the new stage of the pipeline, choose **$limit** in the **Select**... box. The Aggregation Pipeline Builder automatically provides sample code for how to use the `$limit` operator in the code box for the stage.\n\n    ![The $limit stage](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/limit-code.png)\n\n3.  Now we are ready to input code for the `$limit` stage. Let's limit our results to 10 documents. Replace the code in the $limit stage's code box with the following:\n\n``` json\n10\n```\n\nThe Aggregation Pipeline Builder automatically updates the output on the right side of the row to show a sample of 10 documents that will be included in the results after the `$limit` stage is executed. Note that the documents have the same shape as the documents in the previous stage; we've simply limited the number of results to 10.\n\n![$limit stage output](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/limit-output.png)\n\n## Execute an Aggregation Pipeline in Node.js\n\nNow that we have built an aggregation pipeline, let's execute it from inside of a Node.js script.\n\n### Get a Copy of the Node.js Template\n\nTo make following along with this blog post easier, I've created a starter template for a Node.js script that accesses an Atlas cluster.\n\n1.  Download a copy of [template.js](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/template.js).\n2.  Open `template.js` in your favorite code editor.\n3.  Update the Connection URI to point to your Atlas cluster. If you're not sure how to do that, refer back to the [first post in this series](/quickstart/node-connect-mongodb-3-3-2/).\n4.  Save the file as `aggregation.js`.\n\nYou can run this file by executing `node aggregation.js` in your shell. At this point, the file simply opens and closes a connection to your Atlas cluster, so no output is expected. If you see DeprecationWarnings, you can ignore them for the purposes of this post.\n\n### Create a Function\n\nLet's create a function whose job it is to print the cheapest suburbs for a given market.\n\n1.  Continuing to work in `aggregation.js`, create an asynchronous function named `printCheapestSuburbs` that accepts a connected MongoClient, a country, a market, and the maximum number of results to print as parameters.\n\n    ``` js\n    async function printCheapestSuburbs(client, country, market, maxNumberToPrint) {\n    }\n    ```\n\n2.  We can execute a pipeline in Node.js by calling\n    [Collection](https://mongodb.github.io/node-mongodb-native/3.3/api/Collection.html)'s\n    [aggregate()](https://mongodb.github.io/node-mongodb-native/3.3/api/Collection.html#aggregate).\n    Paste the following in your new function:\n\n    ``` js\n    const pipeline = [];\n\n    const aggCursor = client.db(\"sample_airbnb\")\n                            .collection(\"listingsAndReviews\")\n                            .aggregate(pipeline);\n    ```\n\n3.  The first param for `aggregate()` is a pipeline of type object. We could manually create the pipeline here. Since we've already created a pipeline inside of Atlas, let's export the pipeline from there. Return to the Aggregation Pipeline Builder in Atlas. Click the **Export pipeline code to language** button.\n\n    ![Export pipeline in Atlas](https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/node-quickstart/export-pipeline-mdb40.png)\n\n4.  The **Export Pipeline To Language** dialog appears. In the **Export Pipleine To** selection box, choose **NODE**.\n5.  In the Node pane on the right side of the dialog, click the **copy** button.\n6.  Return to your code editor and paste the `pipeline` in place of the empty object currently assigned to the pipeline constant.\n\n    ``` js\n    const pipeline = [\n    {\n        '$match': {\n        'bedrooms': 1,\n        'address.country': 'Australia', \n        'address.market': 'Sydney', \n        'address.suburb': {\n            '$exists': 1, \n            '$ne': ''\n        }, \n        'room_type': 'Entire home/apt'\n        }\n    }, {\n        '$group': {\n        '_id': '$address.suburb', \n        'averagePrice': {\n            '$avg': '$price'\n        }\n        }\n    }, {\n        '$sort': {\n        'averagePrice': 1\n        }\n    }, {\n        '$limit': 10\n    }\n    ];\n    ```\n\n7.  This pipeline would work fine as written. However, it is hardcoded to search for 10 results in the Sydney, Australia market. We should update this pipeline to be more generic. Make the following replacements in the pipeline definition:\n    1.  Replace `'Australia'` with `country`\n    2.  Replace `'Sydney'` with `market`\n    3.  Replace `10` with `maxNumberToPrint`\n\n8.  `aggregate()` will return an [AggregationCursor](https://mongodb.github.io/node-mongodb-native/3.3/api/AggregationCursor.html), which we are storing in the `aggCursor` constant. An AggregationCursor allows traversal over the aggregation pipeline results. We can use AggregationCursor's [forEach()](https://mongodb.github.io/node-mongodb-native/3.3/api/AggregationCursor.html#forEach) to iterate over the results. Paste the following inside `printCheapestSuburbs()` below the definition of `aggCursor`.\n\n``` js\nawait aggCursor.forEach(airbnbListing => {\n  console.log(`${airbnbListing._id}: ${airbnbListing.averagePrice}`);\n});\n```\n\n### Call the Function\n\nNow we are ready to call our function to print the 10 cheapest suburbs in the Sydney, Australia market. Add the following call in the `main()` function beneath the comment that says `Make the appropriate DB calls`.\n\n``` js\nawait printCheapestSuburbs(client, \"Australia\", \"Sydney\", 10);\n```\n\nRunning aggregation.js results in the following output:\n\n``` json\nBalgowlah: 45.00\nWilloughby: 80.00\nMarrickville: 94.50\nSt Peters: 100.00\nRedfern: 101.00\nCronulla: 109.00\nBellevue Hill: 109.50\nKingsgrove: 112.00\nCoogee: 115.00\nNeutral Bay: 119.00\n```\n\nNow I know what suburbs to begin searching as I prepare for my trip to Sydney, Australia.\n\n## Wrapping Up\n\nThe aggregation framework is an incredibly powerful way to analyze your data. Learning to create pipelines may seem a little intimidating at first, but it's worth the investment. The aggregation framework can get results to your end-users faster and save you from a lot of scripting.\n\nToday, we only scratched the surface of the aggregation framework. I highly recommend MongoDB University's free course specifically on the aggregation framework: [M121: The MongoDB Aggregation Framework](https://university.mongodb.com/courses/M121/about). The course has a more thorough explanation of how the aggregation framework works and provides detail on how to use the various pipeline stages.\n\nThis post included many code snippets that built on code written in the [first](/developer/languages/javascript/node-connect-mongodb) post of this MongoDB and Node.js Quick Start series. To get a full copy of the code used in today's post, visit the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/aggregation.js).\n\nNow you're ready to move on to the [next post in this series](/developer/languages/javascript/nodejs-change-streams-triggers) all about change streams and triggers. In that post, you'll learn how to automatically react to changes in your database.\n\nQuestions? Comments? We'd love to connect with you. Join the conversation on the [MongoDB Community Forums](https://developer.mongodb.com/community/forums/).","description":"Discover how to analyze your data using MongoDB's Aggregation Framework and Node.js.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt63f3d0a396ee8471/644c46ed2c3b2b582beba83f/NodeJS_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/node-aggregation-framework","title":"*Aggregation Framework with Node.js Tutorial","original_publish_date":"2022-02-02T14:23:53.006Z","strapi_updated_at":"2022-10-21T16:43:54.775Z","expiry_date":"2022-12-14T02:04:02.364Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*Nodejs","calculated_slug":"/technologies/nodejs"}}]}},"seo":{"canonical_url":"","meta_description":"Discover how to analyze your data using MongoDB's Aggregation Framework and Node.js.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt5d55a3b73299d926/644c46ee28676e98e5b6ccc4/NodeJS_Tile_360x360.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@Lauren_Schaefer"},"system":{"updated_at":"2023-04-28T22:51:59.534Z","publish_details":{"time":"2023-04-28T22:53:02.750Z"}}},{"calculated_slug":"/code-examples/javascript/node-connect-mongodb-3-3-2","content":"<div>\n    <img\n        style=\"float: right; width: 15%\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-nodejs.png\"\n        alt=\"QuickStart Node.js Logo\" />\n\nUse Node.js? Want to learn MongoDB? This is the blog series for you!\n\nIn this Quick Start series, I'll walk you through the basics of how to get started using MongoDB with Node.js. In today's post, we'll work through connecting to a MongoDB database from a Node.js script, retrieving a list of databases, and printing the results to your console.\n</div>\n\n>\n>\n>This post uses MongoDB 4.0, MongoDB Node.js Driver 3.3.2, and Node.js 10.16.3.\n>\n>Click [here](/developer/languages/javascript/node-connect-mongodb) to see a newer version of this post that uses MongoDB 4.4, MongoDB Node.js Driver 3.6.4, and Node.js 14.15.4.\n>\n>\n\n>\n>\n>Prefer to learn by video? I've got ya covered. Check out the video below that covers how to get connected as well as how to perform the CRUD operations.\n>\n>:youtube[]{vid=fbYExfeFsI0}\n>\n>\n\n## Set Up\n\nBefore we begin, we need to ensure you've completed a few prerequisite steps.\n\n### Install Node.js\n\nFirst, make sure you have a supported version of Node.js installed (the MongoDB Node.js Driver requires Node 4.x or greater, and, for these examples, I've used Node.js 10.16.3).\n\n### Install the MongoDB Node.js Driver\n\nThe MongoDB Node.js Driver allows you to easily interact with MongoDB databases from within Node.js applications. You'll need the driver in order to connect to your database and execute the queries described in this Quick Start series.\n\nIf you don't have the MongoDB Node.js Driver installed, you can install it with the following command.\n\n``` bash\nnpm install mongodb\n```\n\nAt the time of writing, this installed version 3.3.2 of the driver.  Running `npm list mongodb` will display the currently installed driver version number. For more details on the driver and installation, see the [official documentation](https://docs.mongodb.com/drivers/node/).\n\n### Create a Free MongoDB Atlas Cluster and Load the Sample Data\n\nNext, you'll need a MongoDB database. The easiest way to get started with MongoDB is to use Atlas, MongoDB's fully-managed database-as-a-service.\n\n[Head over to Atlas](https://www.mongodb.com/cloud/atlas) and create a new cluster in the free tier. At a high level, a cluster is a set of nodes where copies of your database will be stored. Once your tier is created, [load the sample data](https://docs.atlas.mongodb.com/sample-data/). If you're not familiar with how to create a new cluster and load the sample data, check out [this video tutorial](https://developer.mongodb.com/quickstart/free-atlas-cluster) from MongoDB Developer Advocate [Maxime Beugnet](https://www.linkedin.com/in/maximebeugnet).\n\n>\n>\n>Get started with an M0 cluster on [Atlas](https://www.mongodb.com/cloud/atlas) today. It's free forever, and it's the easiest way to try out the steps in this blog series.\n>\n>\n\n### Get Your Cluster's Connection Info\n\nThe final step is to prep your cluster for connection.\n\nIn [Atlas](https://www.mongodb.com/cloud/atlas), navigate to your cluster and click **CONNECT**. The Cluster Connection Wizard will appear.\n\nThe Wizard will prompt you to add your current IP address to the IP Access List and create a MongoDB user if you haven't already done so. Be sure to note the username and password you use for the new MongoDB user as you'll need them in a later step.\n\nNext, the Wizard will prompt you to choose a connection method. Select **Connect Your Application**. When the Wizard prompts you to select your driver version, select **Node.js** and **3.0 or later**. Copy the provided connection string.\n\nFor more details on how to access the Connection Wizard and complete the steps described above, see the [official documentation](https://docs.atlas.mongodb.com/connect-to-cluster/).\n\n## Connect to Your Database From a Node.js Application\n\nNow that everything is set up, it's time to code! Let's write a Node.js script that connects to your database and lists the databases in your cluster.\n\n### Import MongoClient\n\nThe MongoDB module exports `MongoClient`, and that's what we'll use to connect to a MongoDB database. We can use an instance of MongoClient to connect to a cluster, access the database in that cluster, and close the connection to that cluster.\n\n``` js\nconst { MongoClient } = require('mongodb');\n```\n\n### Create Our Main Function\n\nLet's create an asynchronous function named `main()` where we will connect to our MongoDB cluster, call functions that query our database, and disconnect from our cluster.\n\nThe first thing we need to do inside of `main()` is create a constant for our connection URI. The connection URI is the connection string you copied in Atlas in the previous section. When you paste the connection string, don't forget to update `<username>` and `<password>` to be the credentials for the user you created in the previous section. The connection string includes a `<dbname>` placeholder. For these examples, we'll be using the `sample_airbnb` database, so replace `<dbname>` with `sample_airbnb`.\n\n**Note**: The username and password you provide in the connection string are NOT the same as your Atlas credentials.\n\n``` js\n/**\n* Connection URI. Update <username>, <password>, and <your-cluster-url> to reflect your cluster.\n* See https://docs.mongodb.com/ecosystem/drivers/node/ for more details\n*/\nconst uri = \"mongodb+srv://<username>:<password>@<your-cluster-url>/sample_airbnb?retryWrites=true&w=majority\";       \n```\n\nNow that we have our URI, we can create an instance of MongoClient.\n\n``` js\nconst client = new MongoClient(uri);\n```\n\n**Note**: When you run this code, you may see DeprecationWarnings around the URL string `parser` and the Server Discover and Monitoring engine.  If you see these warnings, you can remove them by passing options to the MongoClient. For example, you could instantiate MongoClient by calling `new MongoClient(uri, { useNewUrlParser: true, useUnifiedTopology: true })`.  See the [Node.js MongoDB Driver API documentation](https://mongodb.github.io/node-mongodb-native/3.3/api/MongoClient.html) for more information on these options.\n\nNow we're ready to use MongoClient to connect to our cluster.  `client.connect()` will return a [promise](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise).  We will use the [await](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/await) keyword when we call `client.connect()` to indicate that we should block further execution until that operation has completed.\n\n``` js\nawait client.connect();\n```\n\nWe can now interact with our database. Let's build a function that prints the names of the databases in this cluster. It's often useful to contain this logic in well-named functions in order to improve the readability of your codebase. Throughout this series, we'll create new functions similar to the function we're creating here as we learn how to write different types of queries. For now, let's call a function named `listDatabases()`.\n\n``` js\nawait listDatabases(client);\n```\n\nLet's wrap our calls to functions that interact with the database in a `try/catch` statement so that we handle any unexpected errors.\n\n``` js\ntry {\n    await client.connect();\n\n    await listDatabases(client);\n\n} catch (e) {\n    console.error(e);\n}\n```\n\nWe want to be sure we close the connection to our cluster, so we'll end our `try/catch` with a finally statement.\n\n``` js\nfinally {\n    await client.close();\n}\n```\n\nOnce we have our `main()` function written, we need to call it. Let's send the errors to the console.\n\n``` js\nmain().catch(console.error);\n```\n\nPutting it all together, our `main()` function and our call to it will look something like the following.\n\n``` js\nasync function main(){\n    /**\n     * Connection URI. Update <username>, <password>, and <your-cluster-url> to reflect your cluster.\n     * See https://docs.mongodb.com/ecosystem/drivers/node/ for more details\n     */\n    const uri = \"mongodb+srv://<username>:<password>@<your-cluster-url>/test?retryWrites=true&w=majority\";\n\n\n    const client = new MongoClient(uri);\n\n    try {\n        // Connect to the MongoDB cluster\n        await client.connect();\n\n        // Make the appropriate DB calls\n        await  listDatabases(client);\n\n    } catch (e) {\n        console.error(e);\n    } finally {\n        await client.close();\n    }\n}\n\nmain().catch(console.error);\n```\n\n### List the Databases in Our Cluster\n\nIn the previous section, we referenced the `listDatabases()` function.  Let's implement it!\n\nThis function will retrieve a list of databases in our cluster and print the results in the console.\n\n``` js\nasync function listDatabases(client){\n    databasesList = await client.db().admin().listDatabases();\n\n    console.log(\"Databases:\");\n    databasesList.databases.forEach(db => console.log(` - ${db.name}`));\n};\n```\n\n### Save Your File\n\nYou've been implementing a lot of code. Save your changes, and name your file something like `connection.js`. To see a copy of the complete file, visit the [nodejs-quickstart](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/connection.js) GitHub repo.\n\n### Execute Your Node.js Script\n\nNow you're ready to test your code! Execute your script by running a command like the following in your terminal: `node connection.js`.\n\nYou will see output like the following:\n\n``` js\nDatabases:\n - sample_airbnb\n - sample_geospatial\n - sample_mflix\n - sample_supplies\n - sample_training\n - sample_weatherdata\n - admin\n - local\n```\n\n## What's Next?\n\nToday, you were able to connect to a MongoDB database from a Node.js script, retrieve a list of databases in your cluster, and view the results in your console. Nice!\n\nNow that you're connected to your database, continue on to the [next post in this series](/developer/languages/javascript/node-crud-tutorial-3-3-2), where you'll learn to execute each of the CRUD (create, read, update, and delete) operations.\n\nIn the meantime, check out the following resources:\n\n-   [MongoDB Node.js Driver](https://mongodb.github.io/node-mongodb-native/index.html)\n-   [Official MongoDB Documentation on the MongoDB Node.js Driver](https://docs.mongodb.com/drivers/node/)\n-   [MongoDB University Free Course: M220JS: MongoDB for Javascript Developers](https://university.mongodb.com/courses/M220JS/about)\n\nQuestions? Comments? We'd love to connect with you. Join the conversation on the [MongoDB Community Forums](https://developer.mongodb.com/community/forums/).\n","description":"Node.js and MongoDB is a powerful pairing and in this code example project we show you how.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blte98406f0cedb9af7/644c473e70063ecd9607cd4a/nodejs.png?branch=prod","description":null}}]},"slug":"/node-connect-mongodb-3-3-2","title":"*Connect to a MongoDB Database Using Node.js 3.3.2","original_publish_date":"2022-02-04T19:02:29.891Z","strapi_updated_at":"2023-04-20T15:56:08.336Z","expiry_date":"2022-12-14T02:04:12.150Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":"Snippet","content_typeConnection":{"edges":[{"node":{"title":"*Code Example","calculated_slug":"/code-examples"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":"https://github.com/mongodb-developer/nodejs-quickstart","l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Nodejs","calculated_slug":"/technologies/nodejs"}}]}},"seo":{"canonical_url":"","meta_description":"Node.js and MongoDB is a powerful pairing and in this Quick Start series we show you how.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt975a508362036b3a/644c4741bae606e0d4bc1d23/og-pink-pattern.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@Lauren_Schaefer"},"system":{"updated_at":"2023-04-28T22:51:59.142Z","publish_details":{"time":"2023-04-28T22:53:02.781Z"}}},{"calculated_slug":"/languages/javascript/node-connect-mongodb","content":"<div>\n    <img\n        style=\"float: right; width: 15%\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-nodejs.png\"\n        alt=\"QuickStart Node.js Logo\" />\n\nUse Node.js? Want to learn MongoDB? This is the blog series for you!\n\nIn this Quick Start series, I'll walk you through the basics of how to get started using MongoDB with Node.js. In today's post, we'll work through connecting to a MongoDB database from a Node.js script, retrieving a list of databases, and printing the results to your console.\n</div>\n\n>\n>\n>This post uses MongoDB 4.4, MongoDB Node.js Driver 3.6.4, and Node.js 14.15.4.\n>\n>Click [here](/developer/code-examples/javascript/node-connect-mongodb-3-3-2) to see a previous version of this post that uses MongoDB 4.0, MongoDB Node.js Driver 3.3.2, and Node.js 10.16.3.\n>\n>\n\n>\n>\n>Prefer to learn by video? I've got ya covered. Check out the video below that covers how to get connected as well as how to perform the CRUD operations.\n>\n>:youtube[]{vid=fbYExfeFsI0}\n>\n>\n\n## Set Up\n\nBefore we begin, we need to ensure you've completed a few prerequisite steps.\n\n### Install Node.js\n\nFirst, make sure you have a supported version of Node.js installed. The current version of MongoDB Node.js Driver requires Node 4.x or greater.  For these examples, I've used Node.js 14.15.4. See the [MongoDB Compatability](https://docs.mongodb.com/drivers/node/current/compatibility/) docs for more information on which version of Node.js is required for each version of the Node.js driver.\n\n### Install the MongoDB Node.js Driver\n\nThe MongoDB Node.js Driver allows you to easily interact with MongoDB databases from within Node.js applications. You'll need the driver in order to connect to your database and execute the queries described in this Quick Start series.\n\nIf you don't have the MongoDB Node.js Driver installed, you can install it with the following command.\n\n``` bash\nnpm install mongodb\n```\n\nAt the time of writing, this installed version 3.6.4 of the driver.  Running `npm list mongodb` will display the currently installed driver version number. For more details on the driver and installation, see the [official documentation](https://docs.mongodb.com/drivers/node/).\n\n### Create a Free MongoDB Atlas Cluster and Load the Sample Data\n\nNext, you'll need a MongoDB database. The easiest way to get started with MongoDB is to use Atlas, MongoDB's fully-managed database-as-a-service.\n\n[Head over to Atlas](https://www.mongodb.com/cloud/atlas) and create a new cluster in the free tier. At a high level, a cluster is a set of nodes where copies of your database will be stored. Once your tier is created, [load the sample data](https://docs.atlas.mongodb.com/sample-data/). If you're not familiar with how to create a new cluster and load the sample data, check out [this video tutorial](https://developer.mongodb.com/quickstart/free-atlas-cluster) from MongoDB Developer Advocate [Maxime Beugnet](https://www.linkedin.com/in/maximebeugnet).\n\n>\n>\n>Get started with an M0 cluster on [Atlas](https://www.mongodb.com/cloud/atlas) today. It's free forever, and it's the easiest way to try out the steps in this blog series.\n>\n>\n\n### Get Your Cluster's Connection Info\n\nThe final step is to prep your cluster for connection.\n\nIn [Atlas](https://www.mongodb.com/cloud/atlas), navigate to your cluster and click **CONNECT**. The Cluster Connection Wizard will appear.\n\nThe Wizard will prompt you to add your current IP address to the IP Access List and create a MongoDB user if you haven't already done so. Be sure to note the username and password you use for the new MongoDB user as you'll need them in a later step.\n\nNext, the Wizard will prompt you to choose a connection method. Select **Connect Your Application**. When the Wizard prompts you to select your driver version, select **Node.js** and **3.6 or later**. Copy the provided connection string.\n\nFor more details on how to access the Connection Wizard and complete the steps described above, see the [official documentation](https://docs.atlas.mongodb.com/connect-to-cluster/).\n\n## Connect to Your Database from a Node.js Application\n\nNow that everything is set up, it's time to code! Let's write a Node.js script that connects to your database and lists the databases in your cluster.\n\n### Import MongoClient\n\nThe MongoDB module exports `MongoClient`, and that's what we'll use to connect to a MongoDB database. We can use an instance of MongoClient to connect to a cluster, access the database in that cluster, and close the connection to that cluster.\n\n``` js\nconst { MongoClient } = require('mongodb');\n```\n\n### Create our Main Function\n\nLet's create an asynchronous function named `main()` where we will connect to our MongoDB cluster, call functions that query our database, and disconnect from our cluster.\n\n``` js\nasync function main() {\n   // we'll add code here soon\n}\n```\n\nThe first thing we need to do inside of `main()` is create a constant for our connection URI. The connection URI is the connection string you copied in Atlas in the previous section. When you paste the connection string, don't forget to update `<username>` and `<password>` to be the credentials for the user you created in the previous section. The connection string includes a `<dbname>` placeholder. For these examples, we'll be using the `sample_airbnb` database, so replace `<dbname>` with `sample_airbnb`.\n\n**Note**: The username and password you provide in the connection string are NOT the same as your Atlas credentials.\n\n``` js\n/**\n* Connection URI. Update <username>, <password>, and <your-cluster-url> to reflect your cluster.\n* See https://docs.mongodb.com/ecosystem/drivers/node/ for more details\n*/\nconst uri = \"mongodb+srv://<username>:<password>@<your-cluster-url>/sample_airbnb?retryWrites=true&w=majority\";       \n```\n\nNow that we have our URI, we can create an instance of MongoClient.\n\n``` js\nconst client = new MongoClient(uri);\n```\n\n**Note**: When you run this code, you may see DeprecationWarnings around the URL string `parser` and the Server Discover and Monitoring engine.  If you see these warnings, you can remove them by passing options to the MongoClient. For example, you could instantiate MongoClient by calling `new MongoClient(uri, { useNewUrlParser: true, useUnifiedTopology: true })`.  See the [Node.js MongoDB Driver API documentation](https://mongodb.github.io/node-mongodb-native/3.6/api/MongoClient.html) for more information on these options.\n\nNow we're ready to use MongoClient to connect to our cluster.  `client.connect()` will return a [promise](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise).  We will use the [await](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/await) keyword when we call `client.connect()` to indicate that we should block further execution until that operation has completed.\n\n``` js\nawait client.connect();\n```\n\nWe can now interact with our database. Let's build a function that prints the names of the databases in this cluster. It's often useful to contain this logic in well named functions in order to improve the readability of your codebase. Throughout this series, we'll create new functions similar to the function we're creating here as we learn how to write different types of queries. For now, let's call a function named `listDatabases()`.\n\n``` js\nawait listDatabases(client);\n```\n\nLet's wrap our calls to functions that interact with the database in a `try/catch` statement so that we handle any unexpected errors.\n\n``` js\ntry {\n    await client.connect();\n\n    await listDatabases(client);\n\n} catch (e) {\n    console.error(e);\n}\n```\n\nWe want to be sure we close the connection to our cluster, so we'll end our `try/catch` with a finally statement.\n\n``` js\nfinally {\n    await client.close();\n}\n```\n\nOnce we have our `main()` function written, we need to call it. Let's send the errors to the console.\n\n``` js\nmain().catch(console.error);\n```\n\nPutting it all together, our `main()` function and our call to it will look something like the following.\n\n``` js\nasync function main(){\n    /**\n     * Connection URI. Update <username>, <password>, and <your-cluster-url> to reflect your cluster.\n     * See https://docs.mongodb.com/ecosystem/drivers/node/ for more details\n     */\n    const uri = \"mongodb+srv://<username>:<password>@<your-cluster-url>/sample_airbnb?retryWrites=true&w=majority\";\n\n\n    const client = new MongoClient(uri);\n\n    try {\n        // Connect to the MongoDB cluster\n        await client.connect();\n\n        // Make the appropriate DB calls\n        await  listDatabases(client);\n\n    } catch (e) {\n        console.error(e);\n    } finally {\n        await client.close();\n    }\n}\n\nmain().catch(console.error);\n```\n\n### List the Databases in Our Cluster\n\nIn the previous section, we referenced the `listDatabases()` function.  Let's implement it!\n\nThis function will retrieve a list of databases in our cluster and print the results in the console.\n\n``` js\nasync function listDatabases(client){\n    databasesList = await client.db().admin().listDatabases();\n\n    console.log(\"Databases:\");\n    databasesList.databases.forEach(db => console.log(` - ${db.name}`));\n};\n```\n\n### Save Your File\n\nYou've been implementing a lot of code. Save your changes, and name your file something like `connection.js`. To see a copy of the complete file, visit the [nodejs-quickstart](https://github.com/mongodb-developer/nodejs-quickstart/blob/master/connection.js) GitHub repo.\n\n### Execute Your Node.js Script\n\nNow you're ready to test your code! Execute your script by running a command like the following in your terminal: `node connection.js`\n\nYou will see output like the following:\n\n``` js\nDatabases:\n - sample_airbnb\n - sample_geospatial\n - sample_mflix\n - sample_supplies\n - sample_training\n - sample_weatherdata\n - admin\n - local\n```\n\n## What's Next?\n\nToday, you were able to connect to a MongoDB database from a Node.js script, retrieve a list of databases in your cluster, and view the results in your console. Nice!\n\nNow that you're connected to your database, continue on to the [next post in this series](/developer/languages/javascript/node-crud-tutorial) where you'll learn to execute each of the CRUD (create, read, update, and delete) operations.\n\nIn the meantime, check out the following resources:\n\n-   [Official MongoDB Documentation on the MongoDB Node.js Driver](https://docs.mongodb.com/drivers/node/)\n-   [MongoDB University Free Course: M220JS: MongoDB for Javascript Developers](https://university.mongodb.com/courses/M220JS/about)\n\nQuestions? Comments? We'd love to connect with you. Join the conversation on the [MongoDB Community Forums](https://developer.mongodb.com/community/forums/).\n","description":"Node.js and MongoDB is a powerful pairing and in this Quick Start series we show you how.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt63f3d0a396ee8471/644c46ed2c3b2b582beba83f/NodeJS_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/node-connect-mongodb","title":"*Connect to a MongoDB Database Using Node.js","original_publish_date":"2022-02-04T19:19:34.216Z","strapi_updated_at":"2022-10-21T16:46:25.872Z","expiry_date":"2022-12-14T02:04:22.595Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[{"node":{"title":"*Nodejs","calculated_slug":"/technologies/nodejs"}}]}},"seo":{"canonical_url":"","meta_description":"Node.js and MongoDB is a powerful pairing and in this Quick Start series we show you how.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt5d55a3b73299d926/644c46ee28676e98e5b6ccc4/NodeJS_Tile_360x360.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@Lauren_Schaefer"},"system":{"updated_at":"2023-04-28T22:51:58.747Z","publish_details":{"time":"2023-04-28T22:53:02.808Z"}}},{"calculated_slug":"/languages/javascript/node-crud-tutorial-3-3-2","content":"<div>\n    <img\n        style=\"float: right; width: 15%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-nodejs.png\" alt=\"QuickStart Node.js Logo\" />\n\n[In the first post in this series](/developer/code-examples/javascript/node-connect-mongodb-3-3-2), I walked you through how to connect to a MongoDB database from a Node.js script, retrieve a list of databases, and print the results to your console. If you haven't read that post yet, I recommend you do so and then return here.\n\n</div>\n\n>This post uses MongoDB 4.0, MongoDB Node.js Driver 3.3.2, and Node.js 10.16.3.\n>\n>Click [here](/developer/languages/javascript/node-crud-tutorial) to see a newer version of this post that uses MongoDB 4.4, MongoDB Node.js Driver 3.6.4, and Node.js 14.15.4.\n\nNow that we have connected to a database, let's kick things off with the CRUD (create, read, update, and delete) operations.\n\nIf you prefer video over text, I've got you covered. Check out the video in the [section below](#learn-by-video). :-)\n\n>Get started with an M0 cluster on [Atlas](https://www.mongodb.com/cloud/atlas/signup) today. It's free forever, and it's the easiest way to try out the steps in this blog series.\n\nHere is a summary of what we'll cover in this post:\n\n- [Learn by Video](#learn-by-video)\n- [How MongoDB Stores Data](#how-mongodb-stores-data)\n- [Create](#create)\n- [Read](#read)\n- [Update](#update)\n- [Delete](#delete)\n- [Wrapping Up](#wrapping-up)\n\n## Learn by Video\n\nI created the video below for those who prefer to learn by video instead\nof text. You might also find this video helpful if you get stuck while\ntrying the steps in the text-based instructions below.\n\nHere is a summary of what the video covers:\n\n-   How to connect to a MongoDB database hosted on [MongoDB Atlas](https://bit.ly/MDB_Atlas) from inside of a Node.js script [(00:40)](https://youtu.be/ayNI9Q84v8g?t=40)\n-   How MongoDB stores data in documents and collections (instead of rows and tables) [(08:51)](https://youtu.be/ayNI9Q84v8g?t=531)\n-   How to create documents using `insertOne()` and `insertMany()` [(11:01)](https://youtu.be/ayNI9Q84v8g?t=661)\n-   How to read documents using `findOne()` and `find()` [(20:04)](https://youtu.be/ayNI9Q84v8g?t=1204)\n-   How to update documents using `updateOne()` with and without `upsert` as well as `updateMany()` [(31:13)](https://youtu.be/ayNI9Q84v8g?t=1873)\n-   How to delete documents using `deleteOne()` and `deleteMany()` [(46:07)](https://youtu.be/ayNI9Q84v8g?t=2767)\n\n:youtube[]{vid=ayNI9Q84v8g}\n\nNote: In the video, I type `main().catch(console.err);`, which is incorrect. Instead, I should have typed `main().catch(console.error);`.\n\nBelow are the links I mentioned in the video.\n\n-   [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/signup)\n-   [How to create a free cluster on Atlas](https://youtu.be/rPqRyYJmx2g)\n-   [MongoDB University's Data Modeling Course](http://bit.ly/M320_DataModeling)\n-   [MongoDB University's JavaScript Course](https://bit.ly/M220JS)\n\n## How MongoDB Stores Data\n\nBefore we go any further, let's take a moment to understand how data is stored in MongoDB.\n\nMongoDB stores data in [BSON documents](https://bit.ly/Docs_Documents).  BSON is a binary representation of JSON (JavaScript Object Notation) documents. When you read MongoDB documentation, you'll frequently see the term \"[document](https://bit.ly/Glossary_Document),\" but you can think of a document as simply a JavaScript object. For those coming from the SQL world, you can think of a document as being roughly equivalent to a row.\n\nMongoDB stores groups of documents in [collections](https://bit.ly/Glossary_Collection). For those with a SQL background, you can think of a collection as being roughly equivalent to a table.\n\nEvery document is required to have a field named `_id`. The value of `_id` must be unique for each document in a collection, is immutable, and can be of any type other than an array. MongoDB will automatically create an index on `_id`. You can choose to make the value of `_id` meaningful (rather than a somewhat random [ObjectId](https://bit.ly/Docs_ObjectIds)) if you have a unique value for each document that you'd like to be able to quickly search.\n\nIn this blog series, we'll use the [sample Airbnb listings dataset](https://docs.atlas.mongodb.com/sample-data/sample-airbnb/#sample-airbnb).  The `sample_airbnb` database contains one collection: `listingsAndReviews`. This collection contains documents about Airbnb listings and their reviews.\n\nLet's take a look at a document in the `listingsAndReviews` collection.  Below is part of an [Extended JSON](https://bit.ly/Docs_EJSON) representation of a BSON document:\n\n``` json\n{\n     \"_id\":\"10057447\",\n     \"listing_url\":\"https://www.airbnb.com/rooms/10057447\",\n     \"name\":\"Modern Spacious 1 Bedroom Loft\",\n     \"summary\":\"Prime location, amazing lighting and no annoying neighbours.  Good place to rent if you want a relaxing time in Montreal.\",\n     \"property_type\":\"Apartment\",\n     \"bedrooms\":{\"$numberInt\":\"1\"},\n     \"bathrooms\":{\"$numberDecimal\":\"1.0\"},\n     \"amenities\":[\"Internet\",\"Wifi\",\"Kitchen\",\"Heating\",\"Family/kid friendly\",\"Washer\",\"Dryer\",\"Smoke detector\",\"First aid kit\",\"Safety card\",\"Fire extinguisher\",\"Essentials\",\"Shampoo\",\"24-hour check-in\",\"Hangers\",\"Iron\",\"Laptop friendly workspace\"],\n}\n```\n\nFor more information on how MongoDB stores data, see the [MongoDB Back to Basics Webinar](https://www.mongodb.com/presentations/back-to-basics-2020) that I co-hosted with [Ken Alger](https://www.linkedin.com/in/kenwalger/).\n\n## Create\n\nNow that we know how to connect to a MongoDB database and we understand how data is stored in a MongoDB database, let's create some data!\n\n### Create One Document\n\nLet's begin by creating a new Airbnb listing. We can do so by calling [Collection](https://bit.ly/Node_Collection)'s [insertOne()](https://bit.ly/Node_InsertOne). `insertOne()` will insert a single document into the collection. The only required parameter is the new document (of type object) that will be inserted. If our new document does not contain the `_id` field, the MongoDB driver will automatically create an id for the document.\n\nOur function to create a new listing will look something like the following:\n\n``` javascript\nasync function createListing(client, newListing){\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").insertOne(newListing);\n    console.log(`New listing created with the following id: ${result.insertedId}`);\n}\n```\n\nWe can call this function by passing a connected MongoClient as well as an object that contains information about a listing.\n\n``` javascript\nawait createListing(client,\n        {\n            name: \"Lovely Loft\",\n            summary: \"A charming loft in Paris\",\n            bedrooms: 1,\n            bathrooms: 1\n        }\n    );\n```\n\nThe output would be something like the following:\n\n``` none\nNew listing created with the following id: 5d9ddadee415264e135ccec8\n```\n\nNote that since we did not include a field named `_id` in the document, the MongoDB driver automatically created an `_id` for us. The `_id` of the document you create will be different from the one shown above. For more information on how MongoDB generates `_id`, see [Quick Start: BSON Data Types - ObjectId](https://www.mongodb.com/blog/post/quick-start-bson-data-types--objectid).\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/create.js).\n\n### Create Multiple Documents\n\nSometimes, you will want to insert more than one document at a time. You could choose to repeatedly call `insertOne()`. The problem is that, depending on how you've structured your code, you may end up waiting for each insert operation to return before beginning the next, resulting in slow code.\n\nInstead, you can choose to call [Collection](https://bit.ly/Node_Collection)'s [insertMany()](https://bit.ly/Node_InsertMany). `insertMany()` will insert an array of documents into your collection.\n\nOne important option to note for `insertMany()` is `ordered`. If `ordered` is set to `true`, the documents will be inserted in the order given in the array. If any of the inserts fail (for example, if you attempt to insert a document with an `_id` that is already being used by another document in the collection), the remaining documents will not be inserted. If ordered is set to `false`, the documents may not be inserted in the order given in the array. MongoDB will attempt to insert all of the documents in the given array—regardless of whether any of the other inserts fail. By default, `ordered` is set to `true`.\n\nLet's write a function to create multiple Airbnb listings.\n\n``` javascript\nasync function createMultipleListings(client, newListings){\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").insertMany(newListings);\n\n    console.log(`${result.insertedCount} new listing(s) created with the following id(s):`);\n    console.log(result.insertedIds);      \n}\n```\n\nWe can call this function by passing a connected MongoClient and an array of objects that contain information about listings.\n\n``` javascript\nawait createMultipleListings(client, [\n    {\n        name: \"Infinite Views\",\n        summary: \"Modern home with infinite views from the infinity pool\",\n        property_type: \"House\",\n        bedrooms: 5,\n        bathrooms: 4.5,\n        beds: 5\n    },\n    {\n        name: \"Private room in London\",\n        property_type: \"Apartment\",\n        bedrooms: 1,\n        bathroom: 1\n    },\n    {\n        name: \"Beautiful Beach House\",\n        summary: \"Enjoy relaxed beach living in this house with a private beach\",\n        bedrooms: 4,\n        bathrooms: 2.5,\n        beds: 7,\n        last_review: new Date()\n    }\n]);\n```\n\nNote that every document does not have the same fields, which is perfectly OK. (I'm guessing that those who come from the SQL world will find this incredibly uncomfortable, but it really will be OK 😊.) When you use MongoDB, you get a lot of flexibility in how to structure your documents. If you later decide you want to add [schema validation rules](https://www.mongodb.com/blog/post/json-schema-validation--locking-down-your-model-the-smart-way) so you can guarantee your documents have a particular structure, you can.\n\nThe output of calling `createMultipleListings()` would be something like the following:\n\n``` none\n3 new listing(s) created with the following id(s):\n{ \n  '0': 5d9ddadee415264e135ccec9,\n  '1': 5d9ddadee415264e135cceca,\n  '2': 5d9ddadee415264e135ccecb \n}\n```\n\nJust like the MongoDB Driver automatically created the `_id` field for us when we called `insertOne()`, the Driver has once again created the `_id` field for us when we called `insertMany()`.\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/create.js).\n\n## Read\n\nNow that we know how to **create** documents, let's **read** one!\n\n### Read One Document\n\nLet's begin by querying for an Airbnb listing in the [listingsAndReviews collection](https://docs.atlas.mongodb.com/sample-data/sample-airbnb/#sample-airbnb) by name.\n\nWe can query for a document by calling [Collection](https://bit.ly/Node_Collection)'s [findOne()](https://bit.ly/Node_findOne). `findOne()` will return the first document that matches the given query. Even if more than one document matches the query, only one document will be returned.\n\n`findOne()` has only one required parameter: a query of type object. The query object can contain zero or more properties that MongoDB will use to find a document in the collection. If you want to query all documents in a collection without narrowing your results in any way, you can simply send an empty object.\n\nSince we want to search for an Airbnb listing with a particular name, we will include the name field in the query object we pass to `findOne()`:\n\n``` javascript\nfindOne({ name: nameOfListing })\n```\n\nOur function to find a listing by querying the name field could look something like the following:\n\n``` javascript\nasync function findOneListingByName(client, nameOfListing) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").findOne({ name: nameOfListing });\n\n    if (result) {\n        console.log(`Found a listing in the collection with the name '${nameOfListing}':`);\n        console.log(result);\n    } else {\n        console.log(`No listings found with the name '${nameOfListing}'`);\n    }\n}\n```\n\nWe can call this function by passing a connected MongoClient as well as the name of a listing we want to find. Let's search for a listing named \"Infinite Views\" that we [created in an earlier section](#create-multiple-documents).\n\n``` javascript\nawait findOneListingByName(client, \"Infinite Views\");\n```\n\nThe output should be something like the following.\n\n``` none\nFound a listing in the collection with the name 'Infinite Views':\n{ \n  _id: 5da9b5983e104518671ae128,\n  name: 'Infinite Views',\n  summary: 'Modern home with infinite views from the infinity pool',\n  property_type: 'House',\n  bedrooms: 5,\n  bathrooms: 4.5,\n  beds: 5 \n}\n```\n\nNote that the `_id` of the document in your database will not match the `_id` in the sample output above.\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/read.js).\n\n### Read Multiple Documents\n\nNow that you know how to query for one document, let's discuss how to query for multiple documents at a time. We can do so by calling [Collection](https://bit.ly/Node_Collection)'s [find()](https://bit.ly/Node_find).\n\nSimilar to `findOne()`, the first parameter for `find()` is the query object. You can include zero to many properties in the query object.\n\nLet's say we want to search for all Airbnb listings that have minimum numbers of bedrooms and bathrooms. We could do so by making a call like the following:\n\n``` javascript\nconst cursor = client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").find(\n                        {\n                            bedrooms: { $gte: minimumNumberOfBedrooms },\n                            bathrooms: { $gte: minimumNumberOfBathrooms }\n                        }\n                    );\n```\n\nAs you can see above, we have two properties in our query object: one for bedrooms and one for bathrooms. We can leverage the [$gte](https://bit.ly/Docs_gte) comparison query operator to search for documents that have bedrooms greater than or equal to a given number. We can do the same to satisfy our minimum number of bathrooms requirement.  MongoDB provides a variety of other comparison query operators that you can utilize in your queries. See the [official documentation](https://bit.ly/Docs_QueryOperators) for more details.\n\nThe query above will return a [Cursor](https://bit.ly/Node_Cursor). A Cursor allows traversal over the result set of a query.\n\nYou can also use Cursor's functions to modify what documents are included in the results. For example, let's say we want to sort our results so that those with the most recent reviews are returned first.  We could use Cursor's [sort()](https://bit.ly/Node_sort) function to sort the results using the `last_review` field. We could sort the results in descending order (indicated by passing -1 to `sort()`) so that listings with the most recent reviews will be returned first. We can now update our existing query to look like the following.\n\n``` javascript\nconst cursor = client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").find(\n                        {\n                            bedrooms: { $gte: minimumNumberOfBedrooms },\n                            bathrooms: { $gte: minimumNumberOfBathrooms }\n                        }\n                    ).sort({ last_review: -1 });\n```\n\nThe above query matches 192 documents in our collection. Let's say we don't want to process that many results inside of our script. Instead, we want to limit our results to a smaller number of documents. We can chain another of `sort()`'s functions to our existing query: [limit()](https://bit.ly/Node_limit). As the name implies, `limit()` will set the limit for the cursor. We can now update our query to only return a certain number of results.\n\n``` javascript\nconst cursor = client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").find(\n                        {\n                            bedrooms: { $gte: minimumNumberOfBedrooms },\n                            bathrooms: { $gte: minimumNumberOfBathrooms }\n                        }\n                    ).sort({ last_review: -1 })\n                    .limit(maximumNumberOfResults);\n```\n\nWe could choose to iterate over the cursor to get the results one by one. Instead, if we want to retrieve all of our results in an array, we can call Cursor's [toArray()](https://bit.ly/Node_toArray) function. Now our code looks like the following:\n\n``` javascript\nconst cursor = client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").find(\n                        {\n                            bedrooms: { $gte: minimumNumberOfBedrooms },\n                            bathrooms: { $gte: minimumNumberOfBathrooms }\n                        }\n                    ).sort({ last_review: -1 })\n                    .limit(maximumNumberOfResults);\nconst results = await cursor.toArray();\n```\n\nNow that we have our query ready to go, let's put it inside an asynchronous function and add functionality to print the results.\n\n``` javascript\nasync function findListingsWithMinimumBedroomsBathroomsAndMostRecentReviews(client, {\n    minimumNumberOfBedrooms = 0,\n    minimumNumberOfBathrooms = 0,\n    maximumNumberOfResults = Number.MAX_SAFE_INTEGER\n} = {}) {\n    const cursor = client.db(\"sample_airbnb\").collection(\"listingsAndReviews\")\n        .find({\n            bedrooms: { $gte: minimumNumberOfBedrooms },\n            bathrooms: { $gte: minimumNumberOfBathrooms }\n        }\n        )\n        .sort({ last_review: -1 })\n        .limit(maximumNumberOfResults);\n\n    const results = await cursor.toArray();\n\n    if (results.length > 0) {\n        console.log(`Found listing(s) with at least ${minimumNumberOfBedrooms} bedrooms and ${minimumNumberOfBathrooms} bathrooms:`);\n        results.forEach((result, i) => {\n            date = new Date(result.last_review).toDateString();\n\n            console.log();\n            console.log(`${i + 1}. name: ${result.name}`);\n            console.log(`   _id: ${result._id}`);\n            console.log(`   bedrooms: ${result.bedrooms}`);\n            console.log(`   bathrooms: ${result.bathrooms}`);\n            console.log(`   most recent review date: ${new Date(result.last_review).toDateString()}`);\n        });\n    } else {\n        console.log(`No listings found with at least ${minimumNumberOfBedrooms} bedrooms and ${minimumNumberOfBathrooms} bathrooms`);\n    }\n}\n```\n\nWe can call this function by passing a connected MongoClient as well as an object with properties indicating the minimum number of bedrooms, the minimum number of bathrooms, and the maximum number of results.\n\n``` javascript\nawait findListingsWithMinimumBedroomsBathroomsAndMostRecentReviews(client, {\n    minimumNumberOfBedrooms: 4,\n    minimumNumberOfBathrooms: 2,\n    maximumNumberOfResults: 5\n});\n```\n\nIf you've created the documents as described in the [earlier section](#create-multiple-documents), the output would be something like the following:\n\n``` none\nFound listing(s) with at least 4 bedrooms and 2 bathrooms:\n\n1. name: Beautiful Beach House\n    _id: 5db6ed14f2e0a60683d8fe44\n    bedrooms: 4\n    bathrooms: 2.5\n    most recent review date: Mon Oct 28 2019\n\n2. name: Spectacular Modern Uptown Duplex\n    _id: 582364\n    bedrooms: 4\n    bathrooms: 2.5\n    most recent review date: Wed Mar 06 2019\n\n3. name: Grace 1 - Habitat Apartments\n    _id: 29407312\n    bedrooms: 4\n    bathrooms: 2.0\n    most recent review date: Tue Mar 05 2019\n\n4. name: 6 bd country living near beach\n    _id: 2741869\n    bedrooms: 6\n    bathrooms: 3.0\n    most recent review date: Mon Mar 04 2019\n\n5. name: Awesome 2-storey home Bronte Beach next to Bondi!\n    _id: 20206764\n    bedrooms: 4\n    bathrooms: 2.0\n    most recent review date: Sun Mar 03 2019\n```\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/read.js).\n\n## Update\n\nWe're halfway through the CRUD operations. Now that we know how to **create** and **read** documents, let's discover how to **update** them.\n\n### Update One Document\n\nLet's begin by updating a single Airbnb listing in the [listingsAndReviews collection](https://docs.atlas.mongodb.com/sample-data/sample-airbnb/#sample-airbnb).\n\nWe can update a single document by calling [Collection](http://bit.ly/Node_Collection)'s [updateOne()](http://bit.ly/Node_updateOne). `updateOne()` has two required parameters:\n\n1.  `filter` (object): the Filter used to select the document to update.  You can think of the filter as essentially the same as the query param we used in [findOne()](https://mongodb.github.io/node-mongodb-native/3.3/api/Collection.html#findOne) to search for a particular document. You can include zero properties in the filter to search for all documents in the collection, or you can include one or more properties to narrow your search.\n2.  `update` (object): the update operations to be applied to the document. MongoDB has a variety of update operators you can use such as `$inc`, `$currentDate`, `$set`, and `$unset`, among others. See the [official documentation](https://docs.mongodb.com/manual/reference/operator/update/) for a complete list of update operators and their descriptions.\n\n`updateOne()` also has an optional `options` param. See the [updateOne()](https://bit.ly/Node_updateOne) docs for more information on these options.\n\n`updateOne()` will update the first document that matches the given query. Even if more than one document matches the query, only one document will be updated.\n\nLet's say we want to update an Airbnb listing with a particular name. We can use `updateOne()` to achieve this. We'll include the name of the listing in the filter param. We'll use the [$set](https://bit.ly/docs_set) update operator to set new values for new or existing fields in the document we are updating. When we use `$set`, we pass a document that contains fields and values that should be updated or created. The document that we pass to `$set` will not replace the existing document; any fields that are part of the original document but not part of the document we pass to `$set` will remain as they are.\n\nOur function to update a listing with a particular name would look like the following:\n\n``` javascript\nasync function updateListingByName(client, nameOfListing, updatedListing) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\")\n                        .updateOne({ name: nameOfListing }, { $set: updatedListing });\n\n    console.log(`${result.matchedCount} document(s) matched the query criteria.`);\n    console.log(`${result.modifiedCount} document(s) was/were updated.`);\n}\n```\n\nLet's say we want to update our Airbnb listing that has the name \"Infinite Views.\" We created this listing in [an earlier section](#create-multiple-documents).\n\n``` javascript\n{ \n    _id: 5db6ed14f2e0a60683d8fe42,\n    name: 'Infinite Views',\n    summary: 'Modern home with infinite views from the infinity pool',\n    property_type: 'House',\n    bedrooms: 5,\n    bathrooms: 4.5,\n    beds: 5 \n}\n```\n\nWe can call `updateListingByName()` by passing a connected MongoClient, the name of the listing, and an object containing the fields we want to update and/or create.\n\n``` javascript\nawait updateListingByName(client, \"Infinite Views\", { bedrooms: 6, beds: 8 });\n```\n\nExecuting this command results in the following output.\n\n``` none\n1 document(s) matched the query criteria.\n1 document(s) was/were updated.\n```\n\nNow our listing has an updated number of bedrooms and beds.\n\n``` json\n{ \n    _id: 5db6ed14f2e0a60683d8fe42,\n    name: 'Infinite Views',\n    summary: 'Modern home with infinite views from the infinity pool',\n    property_type: 'House',\n    bedrooms: 6,\n    bathrooms: 4.5,\n    beds: 8 \n}\n```\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/update.js).\n\n### Upsert One Document\n\nOne of the options you can choose to pass to `updateOne()` is upsert.  Upsert is a handy feature that allows you to update a document if it exists or insert a document if it does not.\n\nFor example, let's say you wanted to ensure that an Airbnb listing with a particular name had a certain number of bedrooms and bathrooms.  Without upsert, you'd first use `findOne()` to check if the document existed. If the document existed, you'd use `updateOne()` to update the document. If the document did not exist, you'd use `insertOne()` to create the document. When you use upsert, you can combine all of that functionality into a single command.\n\nOur function to upsert a listing with a particular name can be basically identical to the function we wrote above with one key difference: We'll pass `{upsert: true}` in the `options` param for `updateOne()`.\n\n``` javascript\nasync function upsertListingByName(client, nameOfListing, updatedListing) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\")\n                        .updateOne({ name: nameOfListing }, \n                                   { $set: updatedListing }, \n                                   { upsert: true });\n    console.log(`${result.matchedCount} document(s) matched the query criteria.`);\n\n    if (result.upsertedCount > 0) {\n        console.log(`One document was inserted with the id ${result.upsertedId._id}`);\n    } else {\n        console.log(`${result.modifiedCount} document(s) was/were updated.`);\n    }\n}\n```\n\nLet's say we aren't sure if a listing named \"Cozy Cottage\" is in our collection or, if it does exist, if it holds old data. Either way, we want to ensure the listing that exists in our collection has the most up-to-date data. We can call `upsertListingByName()` with a connected MongoClient, the name of the listing, and an object containing the up-to-date data that should be in the listing.\n\n``` javascript\nawait upsertListingByName(client, \"Cozy Cottage\", { name: \"Cozy Cottage\", bedrooms: 2, bathrooms: 1 });\n```\n\nIf the document did not previously exist, the output of the function would be something like the following:\n\n``` none\n0 document(s) matched the query criteria.\nOne document was inserted with the id 5db9d9286c503eb624d036a1\n```\n\nWe have a new document in the listingsAndReviews collection:\n\n``` json\n{ \n    _id: 5db9d9286c503eb624d036a1,\n    name: 'Cozy Cottage',\n    bathrooms: 1,\n    bedrooms: 2 \n}\n```\n\nIf we discover more information about the \"Cozy Cottage\" listing, we can use `upsertListingByName()` again.\n\n``` javascript\nawait upsertListingByName(client, \"Cozy Cottage\", { beds: 2 });\n```\n\nAnd we would see the following output.\n\n``` none\n1 document(s) matched the query criteria.\n1 document(s) was/were updated.\n```\n\nNow our document has a new field named \"beds.\"\n\n``` json\n{ \n    _id: 5db9d9286c503eb624d036a1,\n    name: 'Cozy Cottage',\n    bathrooms: 1,\n    bedrooms: 2,\n    beds: 2 \n}\n```\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/update.js).\n\n### Update Multiple Documents\n\nSometimes, you'll want to update more than one document at a time. In this case, you can use [Collection](http://bit.ly/Node_Collection)'s [updateMany()](https://bit.ly/Node_updateMany). Like `updateOne()`, `updateMany()` requires that you pass a filter of type object and an update of type object. You can choose to include options of type object as well.\n\nLet's say we want to ensure that every document has a field named `property_type`. We can use the [$exists](https://bit.ly/docs_exists) query operator to search for documents where the `property_type` field does not exist. Then we can use the [$set](https://bit.ly/docs_set) update operator to set the `property_type` to \"Unknown\" for those documents. Our function will look like the following.\n\n``` javascript\nasync function updateAllListingsToHavePropertyType(client) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\")\n                        .updateMany({ property_type: { $exists: false } }, \n                                    { $set: { property_type: \"Unknown\" } });\n    console.log(`${result.matchedCount} document(s) matched the query criteria.`);\n    console.log(`${result.modifiedCount} document(s) was/were updated.`);\n}\n```\n\nWe can call this function with a connected MongoClient.\n\n``` javascript\nawait updateAllListingsToHavePropertyType(client);\n```\n\nBelow is the output from executing the previous command.\n\n``` none\n3 document(s) matched the query criteria.\n3 document(s) was/were updated.\n```\n\nNow our \"Cozy Cottage\" document and all of the other documents in the Airbnb collection have the `property_type` field.\n\n``` json\n{ \n    _id: 5db9d9286c503eb624d036a1,\n    name: 'Cozy Cottage',\n    bathrooms: 1,\n    bedrooms: 2,\n    beds: 2,\n    property_type: 'Unknown' \n}\n```\n\nListings that contained a `property_type` before we called `updateMany()` remain as they were. For example, the \"Spectacular Modern Uptown Duplex\" listing still has `property_type` set to `Apartment`.\n\n``` json\n{ \n    _id: '582364',\n    listing_url: 'https://www.airbnb.com/rooms/582364',\n    name: 'Spectacular Modern Uptown Duplex',\n    property_type: 'Apartment',\n    room_type: 'Entire home/apt',\n    bedrooms: 4,\n    beds: 7\n    ...\n}\n```\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/update.js).\n\n## Delete\n\nNow that we know how to **create**, **read**, and **update** documents, let's tackle the final CRUD operation: **delete**.\n\n### Delete One Document\n\nLet's begin by deleting a single Airbnb listing in the [listingsAndReviews collection](https://docs.atlas.mongodb.com/sample-data/sample-airbnb/#sample-airbnb).\n\nWe can delete a single document by calling [Collection](http://bit.ly/Node_Collection)'s [deleteOne()](https://bit.ly/Node_deleteOne). `deleteOne()` has one required parameter: a filter of type object. The filter is used to select the document to delete. You can think of the filter as essentially the same as the query param we used in [findOne()](https://bit.ly/Node_findOne) and the filter param we used in [updateOne()](https://bit.ly/Node_updateOne). You can include zero properties in the filter to search for all documents in the collection, or you can include one or more properties to narrow your search.\n\n`deleteOne()` also has an optional `options` param. See the [deleteOne()](https://bit.ly/Node_deleteOne) docs for more information on these options.\n\n`deleteOne()` will delete the first document that matches the given query. Even if more than one document matches the query, only one document will be deleted. If you do not specify a filter, the first document found in [natural order](https://docs.mongodb.com/manual/reference/glossary/#term-natural-order) will be deleted.\n\nLet's say we want to delete an Airbnb listing with a particular name. We can use `deleteOne()` to achieve this. We'll include the name of the listing in the filter param. We can create a function to delete a listing with a particular name.\n\n``` javascript\nasync function deleteListingByName(client, nameOfListing) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\")\n            .deleteOne({ name: nameOfListing });\n    console.log(`${result.deletedCount} document(s) was/were deleted.`);\n}\n```\n\nLet's say we want to delete the Airbnb listing we created in an [earlier section](#upsert-one-document) that has the name \"Cozy Cottage.\" We can call `deleteListingsByName()` by passing a connected MongoClient and the name \"Cozy Cottage.\"\n\n``` javascript\nawait deleteListingByName(client, \"Cozy Cottage\");\n```\n\nExecuting the command above results in the following output.\n\n``` none\n1 document(s) was/were deleted.\n```\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/delete.js).\n\n### Deleting Multiple Documents\n\nSometimes, you'll want to delete more than one document at a time. In this case, you can use [Collection](http://bit.ly/Node_Collection)'s [deleteMany()](https://bit.ly/Node_deleteMany). Like `deleteOne()`, `deleteMany()` requires that you pass a filter of type object. You can choose to include options of type object as well.\n\nLet's say we want to remove documents that have not been updated recently. We can call `deleteMany()` with a filter that searches for documents that were scraped prior to a particular date. Our function will look like the following.\n\n``` javascript\nasync function deleteListingsScrapedBeforeDate(client, date) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\")\n        .deleteMany({ \"last_scraped\": { $lt: date } });\n    console.log(`${result.deletedCount} document(s) was/were deleted.`);\n}\n```\n\nTo delete listings that were scraped prior to February 15, 2019, we can call `deleteListingsScrapedBeforeDate()` with a connected MongoClient and a Date instance that represents February 15.\n\n``` javascript\nawait deleteListingsScrapedBeforeDate(client, new Date(\"2019-02-15\"));\n```\n\nExecuting the command above will result in the following output.\n\n``` none\n606 document(s) was/were deleted.\n```\n\nNow, only recently scraped documents are in our collection.\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/3.3.2/delete.js).\n\n## Wrapping Up\n\nWe covered a lot today! Let's recap.\n\nWe began by exploring [how MongoDB stores data in documents and collections](#how-mongodb-stores-data). Then we learned the basics of [creating](#create), [reading](#read), [updating](#update), and [deleting](#delete) data.\n\nContinue on to the [next post in this series](https://developer.mongodb.com/developer/languages/javascript/node-aggregation-framework-3-3-2/), where we'll discuss how you can analyze and manipulate data using the aggregation pipeline.\n\nComments? Questions? We'd love to chat with you in the [MongoDB Community](https://community.mongodb.com).\n","description":"Learn how to execute the CRUD (create, read, update, and delete) operations in MongoDB using Node.js in this step-by-step tutorial.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt63f3d0a396ee8471/644c46ed2c3b2b582beba83f/NodeJS_Banner_1280x720.png?branch=prod","description":null}}]},"slug":"/node-crud-tutorial-3-3-2","title":"*MongoDB and Node.js 3.3.2 Tutorial - CRUD Operations","original_publish_date":"2022-02-04T12:03:09.973Z","strapi_updated_at":"2023-04-20T15:56:39.802Z","expiry_date":"2022-12-14T02:04:31.730Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[{"node":{"title":"*English","calculated_slug":"/spoken-languages/english"}}]},"technologiesConnection":{"edges":[{"node":{"title":"*Nodejs","calculated_slug":"/technologies/nodejs"}}]}},"seo":{"canonical_url":"","meta_description":"Learn how to execute the CRUD (create, read, update, and delete) operations in MongoDB using Node.js in this step-by-step tutorial.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt5d55a3b73299d926/644c46ee28676e98e5b6ccc4/NodeJS_Tile_360x360.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@Lauren_Schaefer"},"system":{"updated_at":"2023-04-28T22:51:58.354Z","publish_details":{"time":"2023-04-28T22:53:02.837Z"}}},{"calculated_slug":"/languages/javascript/node-crud-tutorial","content":"<div>\n    <img\n        style=\"float: right; width: 15%;\"\n        src=\"https://mongodb-devhub-cms.s3.us-west-1.amazonaws.com/old_images/qs-badges/qs-badge-nodejs.png\" alt=\"QuickStart Node.js Logo\" />\n\n[In the first post in this series](/developer/languages/javascript/node-connect-mongodb), I walked you through how to connect to a MongoDB database from a Node.js script, retrieve a list of databases, and print the results to your console. If you haven't read that post yet, I recommend you do so and then return here.\n\n</div>\n\n>\n>\n>This post uses MongoDB 4.4, MongoDB Node.js Driver 3.6.4, and Node.js 14.15.4.\n>\n>Click [here](/developer/code-examples/javascript/node-crud-tutorial-3-3-2) to see a previous version of this post that uses MongoDB 4.0, MongoDB Node.js Driver 3.3.2, and Node.js 10.16.3.\n>\n>\n\nNow that we have connected to a database, let's kick things off with the CRUD (create, read, update, and delete) operations.\n\nIf you prefer video over text, I've got you covered. Check out the video\nin the [section below](#learn-by-video). :-)\n\n>\n>\n>Get started with an M0 cluster on [Atlas](https://www.mongodb.com/cloud/atlas/signup) today. It's free forever, and it's the easiest way to try out the steps in this blog series.\n>\n>\n\nHere is a summary of what we'll cover in this post:\n\n- [Learn by Video](#learn-by-video)\n- [How MongoDB Stores Data](#how-mongodb-stores-data)\n- [Setup](#setup)\n- [Create](#create)\n- [Read](#read)\n- [Update](#update)\n- [Delete](#delete)\n- [Wrapping Up](#wrapping-up)\n\n\n## Learn by Video\n\nI created the video below for those who prefer to learn by video instead of text. You might also find this video helpful if you get stuck while trying the steps in the text-based instructions below.\n\nHere is a summary of what the video covers:\n\n-   How to connect to a MongoDB database hosted on [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) from inside of a Node.js script [(01:00)](https://youtu.be/fbYExfeFsI0?t=60)\n-   How MongoDB stores data in documents and collections (instead of rows and tables) [(08:22)](https://youtu.be/fbYExfeFsI0?t=502)\n-   How to create documents using `insertOne()` and `insertMany()` [(11:47)](https://youtu.be/fbYExfeFsI0?t=707)\n-   How to read documents using `findOne()` and `find()` [(17:16)](https://youtu.be/fbYExfeFsI0?t=1036)\n-   How to update documents using `updateOne()` with and without `upsert` as well as `updateMany()` [(24:46​)](https://youtu.be/fbYExfeFsI0?t=1486)\n-   How to delete documents using `deleteOne()` and `deleteMany()` [(35:58)](https://youtu.be/fbYExfeFsI0?t=2158)\n\n:youtube[]{vid=fbYExfeFsI0}\n\nBelow are the links I mentioned in the video.\n\n-   [GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart)\n-   [Back to Basics Webinar Recording](https://www.mongodb.com/presentations/back-to-basics-webinar-2021)\n\n## How MongoDB Stores Data\n\nBefore we go any further, let's take a moment to understand how data is stored in MongoDB.\n\nMongoDB stores data in [BSON documents](https://docs.mongodb.com/manual/core/document/). BSON is a binary representation of JSON (JavaScript Object Notation) documents.  When you read MongoDB documentation, you'll frequently see the term \"[document](https://docs.mongodb.com/manual/reference/glossary/#term-document),\" but you can think of a document as simply a JavaScript object. For those coming from the SQL world, you can think of a document as being roughly equivalent to a row.\n\nMongoDB stores groups of documents in [collections](https://docs.mongodb.com/manual/reference/glossary/#term-collection).  For those with a SQL background, you can think of a collection as being roughly equivalent to a table.\n\nEvery document is required to have a field named `_id`. The value of `_id` must be unique for each document in a collection, is immutable, and can be of any type other than an array. MongoDB will automatically create an index on `_id`. You can choose to make the value of `_id` meaningful (rather than a somewhat random [ObjectId](https://docs.mongodb.com/manual/reference/bson-types/#objectid)) if you have a unique value for each document that you'd like to be able to quickly search.\n\nIn this blog series, we'll use the [sample Airbnb listings dataset](https://docs.atlas.mongodb.com/sample-data/sample-airbnb/#sample-airbnb).  The `sample_airbnb` database contains one collection: `listingsAndReviews`. This collection contains documents about Airbnb listings and their reviews.\n\nLet's take a look at a document in the `listingsAndReviews` collection.  Below is part of an [Extended JSON](https://docs.mongodb.com/manual/reference/mongodb-extended-json/index.html#bson-data-types-and-associated-representations) representation of a BSON document:\n\n``` json\n{\n   \"_id\": \"10057447\",\n   \"listing_url\": \"https://www.airbnb.com/rooms/10057447\",\n   \"name\": \"Modern Spacious 1 Bedroom Loft\",\n   \"summary\": \"Prime location, amazing lighting and no annoying neighbours.  Good place to rent if you want a relaxing time in Montreal.\",\n   \"property_type\": \"Apartment\",\n   \"bedrooms\": {\"$numberInt\":\"1\"},\n   \"bathrooms\": {\"$numberDecimal\":\"1.0\"},\n   \"amenities\": [\"Internet\",\"Wifi\",\"Kitchen\",\"Heating\",\"Family/kid friendly\",\"Washer\",\"Dryer\",\"Smoke detector\",\"First aid kit\",\"Safety card\",\"Fire extinguisher\",\"Essentials\",\"Shampoo\",\"24-hour check-in\",\"Hangers\",\"Iron\",\"Laptop friendly workspace\"],\n}\n```\n\nFor more information on how MongoDB stores data, see the [MongoDB Back to Basics Webinar](https://www.mongodb.com/presentations/back-to-basics-webinar-2021) that I co-hosted with [Ken Alger](https://www.linkedin.com/in/kenwalger/).\n\n## Setup\n\nTo make following along with this blog post easier, I've created a starter template for a Node.js script that accesses an Atlas cluster.\n\n1.  Download a copy of [template.js](https://github.com/mongodb-developer/nodejs-quickstart/blob/master/template.js).\n2.  Open `template.js` in your favorite code editor.\n3.  Update the Connection URI to point to your Atlas cluster. If you're not sure how to do that, refer back to the [first post in this series](/quickstart/node-connect-mongodb).\n4.  Save the file as `crud.js`.\n\nYou can run this file by executing `node crud.js` in your shell. At this point, the file simply opens and closes a connection to your Atlas cluster, so no output is expected. If you see DeprecationWarnings, you can ignore them for the purposes of this post.\n\n## Create\n\nNow that we know how to connect to a MongoDB database and we understand how data is stored in a MongoDB database, let's create some data!\n\n### Create One Document\n\nLet's begin by creating a new Airbnb listing. We can do so by calling [Collection](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html)'s [insertOne()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#insertOne).  `insertOne()` will insert a single document into the collection. The only required parameter is the new document (of type object) that will be inserted. If our new document does not contain the `_id` field, the MongoDB driver will automatically create an `_id` for the document.\n\nOur function to create a new listing will look something like the following:\n\n``` javascript\nasync function createListing(client, newListing){\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").insertOne(newListing);\n    console.log(`New listing created with the following id: ${result.insertedId}`);\n}\n```\n\nWe can call this function by passing a connected MongoClient as well as an object that contains information about a listing.\n\n``` javascript\nawait createListing(client,\n        {\n            name: \"Lovely Loft\",\n            summary: \"A charming loft in Paris\",\n            bedrooms: 1,\n            bathrooms: 1\n        }\n    );\n```\n\nThe output would be something like the following:\n\n``` none\nNew listing created with the following id: 5d9ddadee415264e135ccec8\n```\n\nNote that since we did not include a field named `_id` in the document, the MongoDB driver automatically created an `_id` for us. The `_id` of the document you create will be different from the one shown above. For more information on how MongoDB generates `_id`, see [Quick Start: BSON Data Types - ObjectId](https://developer.mongodb.com/quickstart/bson-data-types-objectid/).\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/master/create.js).\n\n### Create Multiple Documents\n\nSometimes you will want to insert more than one document at a time. You could choose to repeatedly call `insertOne()`. The problem is that, depending on how you've structured your code, you may end up waiting for each insert operation to return before beginning the next, resulting in slow code.\n\nInstead, you can choose to call [Collection](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html)'s [insertMany()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#insertMany).  `insertMany()` will insert an array of documents into your collection.\n\nOne important option to note for `insertMany()` is `ordered`. If `ordered` is set to `true`, the documents will be inserted in the order given in the array. If any of the inserts fail (for example, if you attempt to insert a document with an `_id` that is already being used by another document in the collection), the remaining documents will not be inserted. If ordered is set to `false`, the documents may not be inserted in the order given in the array. MongoDB will attempt to insert all of the documents in the given array—regardless of whether any of the other inserts fail. By default, `ordered` is set to `true`.\n\nLet's write a function to create multiple Airbnb listings.\n\n``` javascript\nasync function createMultipleListings(client, newListings){\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").insertMany(newListings);\n\n    console.log(`${result.insertedCount} new listing(s) created with the following id(s):`);\n    console.log(result.insertedIds);       \n}\n```\n\nWe can call this function by passing a connected MongoClient and an array of objects that contain information about listings.\n\n``` javascript\nawait createMultipleListings(client, [\n    {\n        name: \"Infinite Views\",\n        summary: \"Modern home with infinite views from the infinity pool\",\n        property_type: \"House\",\n        bedrooms: 5,\n        bathrooms: 4.5,\n        beds: 5\n    },\n    {\n        name: \"Private room in London\",\n        property_type: \"Apartment\",\n        bedrooms: 1,\n        bathroom: 1\n    },\n    {\n        name: \"Beautiful Beach House\",\n        summary: \"Enjoy relaxed beach living in this house with a private beach\",\n        bedrooms: 4,\n        bathrooms: 2.5,\n        beds: 7,\n        last_review: new Date()\n    }\n]);\n```\n\nNote that every document does not have the same fields, which is perfectly OK. (I'm guessing that those who come from the SQL world will find this incredibly uncomfortable, but it really will be OK 😊.) When you use MongoDB, you get a lot of flexibility in how to structure your documents. If you later decide you want to add [schema validation rules](https://www.mongodb.com/blog/post/json-schema-validation--locking-down-your-model-the-smart-way) so you can guarantee your documents have a particular structure, you can.\n\nThe output of calling `createMultipleListings()` would be something like the following:\n\n``` none\n3 new listing(s) created with the following id(s):\n{ \n  '0': 5d9ddadee415264e135ccec9,\n  '1': 5d9ddadee415264e135cceca,\n  '2': 5d9ddadee415264e135ccecb \n}\n```\n\nJust like the MongoDB Driver automatically created the `_id` field for us when we called `insertOne()`, the Driver has once again created the `_id` field for us when we called `insertMany()`.\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/master/create.js).\n\n## Read\n\nNow that we know how to **create** documents, let's **read** one!\n\n### Read One Document\n\nLet's begin by querying for an Airbnb listing in the [listingsAndReviews collection](https://docs.atlas.mongodb.com/sample-data/sample-airbnb/#sample-airbnb)\nby name.\n\nWe can query for a document by calling [Collection](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html)'s [findOne()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#findOne).  `findOne()` will return the first document that matches the given query.  Even if more than one document matches the query, only one document will be returned.\n\n`findOne()` has only one required parameter: a query of type object. The query object can contain zero or more properties that MongoDB will use to find a document in the collection. If you want to query all documents in a collection without narrowing your results in any way, you can simply send an empty object.\n\nSince we want to search for an Airbnb listing with a particular name, we will include the name field in the query object we pass to `findOne()`:\n\n``` javascript\nfindOne({ name: nameOfListing })\n```\n\nOur function to find a listing by querying the name field could look something like the following:\n\n``` javascript\nasync function findOneListingByName(client, nameOfListing) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").findOne({ name: nameOfListing });\n\n    if (result) {\n        console.log(`Found a listing in the collection with the name '${nameOfListing}':`);\n        console.log(result);\n    } else {\n        console.log(`No listings found with the name '${nameOfListing}'`);\n    }\n}\n```\n\nWe can call this function by passing a connected MongoClient as well as the name of a listing we want to find. Let's search for a listing named \"Infinite Views\" that we [created in an earlier section](#create-multiple-documents).\n\n``` javascript\nawait findOneListingByName(client, \"Infinite Views\");\n```\n\nThe output should be something like the following.\n\n``` none\nFound a listing in the collection with the name 'Infinite Views':\n{ \n  _id: 5da9b5983e104518671ae128,\n  name: 'Infinite Views',\n  summary: 'Modern home with infinite views from the infinity pool',\n  property_type: 'House',\n  bedrooms: 5,\n  bathrooms: 4.5,\n  beds: 5 \n}\n```\n\nNote that the `_id` of the document in your database will not match the `_id` in the sample output above.\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/master/read.js).\n\n### Read Multiple Documents\n\nNow that you know how to query for one document, let's discuss how to query for multiple documents at a time. We can do so by calling [Collection](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html)'s [find()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#find).\n\nSimilar to `findOne()`, the first parameter for `find()` is the query object. You can include zero to many properties in the query object.\n\nLet's say we want to search for all Airbnb listings that have minimum numbers of bedrooms and bathrooms. We could do so by making a call like the following:\n\n``` javascript\nclient.db(\"sample_airbnb\").collection(\"listingsAndReviews\").find(\n        {\n            bedrooms: { $gte: minimumNumberOfBedrooms },\n            bathrooms: { $gte: minimumNumberOfBathrooms }\n        }\n    );\n```\n\nAs you can see above, we have two properties in our query object: one for bedrooms and one for bathrooms. We can leverage the [$gte](https://docs.mongodb.com/manual/reference/operator/query/gte/) comparison query operator to search for documents that have bedrooms greater than or equal to a given number. We can do the same to satisfy our minimum number of bathrooms requirement. MongoDB provides a variety of other comparison query operators that you can utilize in your queries. See the [official documentation](https://docs.mongodb.com/manual/reference/operator/query-comparison/) for more details.\n\nThe query above will return a [Cursor](https://mongodb.github.io/node-mongodb-native/3.6/api/Cursor.html).  A Cursor allows traversal over the result set of a query.\n\nYou can also use Cursor's functions to modify what documents are included in the results. For example, let's say we want to sort our results so that those with the most recent reviews are returned first.  We could use Cursor's [sort()](https://mongodb.github.io/node-mongodb-native/3.6/api/Cursor.html#sort) function to sort the results using the `last_review` field. We could sort the results in descending order (indicated by passing -1 to `sort()`) so that listings with the most recent reviews will be returned first. We can now update our existing query to look like the following.\n\n``` javascript\nconst cursor = client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").find(\n                        {\n                            bedrooms: { $gte: minimumNumberOfBedrooms },\n                            bathrooms: { $gte: minimumNumberOfBathrooms }\n                        }\n                    ).sort({ last_review: -1 });\n```\n\nThe above query matches 192 documents in our collection. Let's say we don't want to process that many results inside of our script. Instead, we want to limit our results to a smaller number of documents. We can chain another of `sort()`'s functions to our existing query: [limit()](https://mongodb.github.io/node-mongodb-native/3.6/api/Cursor.html#limit).  As the name implies, `limit()` will set the limit for the cursor. We can now update our query to only return a certain number of results.\n\n``` javascript\nconst cursor = client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").find(\n                        {\n                            bedrooms: { $gte: minimumNumberOfBedrooms },\n                            bathrooms: { $gte: minimumNumberOfBathrooms }\n                        }\n                    ).sort({ last_review: -1 })\n                    .limit(maximumNumberOfResults);\n```\n\nWe could choose to iterate over the cursor to get the results one by one. Instead, if we want to retrieve all of our results in an array, we can call Cursor's [toArray()](https://mongodb.github.io/node-mongodb-native/3.6/api/Cursor.html#toArray) function. Now our code looks like the following:\n\n``` javascript\nconst cursor = client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").find(\n                        {\n                            bedrooms: { $gte: minimumNumberOfBedrooms },\n                            bathrooms: { $gte: minimumNumberOfBathrooms }\n                        }\n                    ).sort({ last_review: -1 })\n                    .limit(maximumNumberOfResults);\nconst results = await cursor.toArray();\n```\n\nNow that we have our query ready to go, let's put it inside an asynchronous function and add functionality to print the results.\n\n``` javascript\nasync function findListingsWithMinimumBedroomsBathroomsAndMostRecentReviews(client, {\n    minimumNumberOfBedrooms = 0,\n    minimumNumberOfBathrooms = 0,\n    maximumNumberOfResults = Number.MAX_SAFE_INTEGER\n} = {}) {\n    const cursor = client.db(\"sample_airbnb\").collection(\"listingsAndReviews\").find(\n                            {\n                                bedrooms: { $gte: minimumNumberOfBedrooms },\n                                bathrooms: { $gte: minimumNumberOfBathrooms }\n                            }\n                            ).sort({ last_review: -1 })\n                            .limit(maximumNumberOfResults);\n\n    const results = await cursor.toArray();\n\n    if (results.length > 0) {\n        console.log(`Found listing(s) with at least ${minimumNumberOfBedrooms} bedrooms and ${minimumNumberOfBathrooms} bathrooms:`);\n        results.forEach((result, i) => {\n            date = new Date(result.last_review).toDateString();\n\n            console.log();\n            console.log(`${i + 1}. name: ${result.name}`);\n            console.log(`   _id: ${result._id}`);\n            console.log(`   bedrooms: ${result.bedrooms}`);\n            console.log(`   bathrooms: ${result.bathrooms}`);\n            console.log(`   most recent review date: ${new Date(result.last_review).toDateString()}`);\n        });\n    } else {\n        console.log(`No listings found with at least ${minimumNumberOfBedrooms} bedrooms and ${minimumNumberOfBathrooms} bathrooms`);\n    }\n}\n```\n\nWe can call this function by passing a connected MongoClient as well as an object with properties indicating the minimum number of bedrooms, the minimum number of bathrooms, and the maximum number of results.\n\n``` javascript\nawait findListingsWithMinimumBedroomsBathroomsAndMostRecentReviews(client, {\n    minimumNumberOfBedrooms: 4,\n    minimumNumberOfBathrooms: 2,\n    maximumNumberOfResults: 5\n});\n```\n\nIf you've created the documents as described in the [earlier section](#create-multiple-documents), the output would be something like the following:\n\n``` none\nFound listing(s) with at least 4 bedrooms and 2 bathrooms:\n\n1. name: Beautiful Beach House\n    _id: 5db6ed14f2e0a60683d8fe44\n    bedrooms: 4\n    bathrooms: 2.5\n    most recent review date: Mon Oct 28 2019\n\n2. name: Spectacular Modern Uptown Duplex\n    _id: 582364\n    bedrooms: 4\n    bathrooms: 2.5\n    most recent review date: Wed Mar 06 2019\n\n3. name: Grace 1 - Habitat Apartments\n    _id: 29407312\n    bedrooms: 4\n    bathrooms: 2.0\n    most recent review date: Tue Mar 05 2019\n\n4. name: 6 bd country living near beach\n    _id: 2741869\n    bedrooms: 6\n    bathrooms: 3.0\n    most recent review date: Mon Mar 04 2019\n\n5. name: Awesome 2-storey home Bronte Beach next to Bondi!\n    _id: 20206764\n    bedrooms: 4\n    bathrooms: 2.0\n    most recent review date: Sun Mar 03 2019\n```\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/master/read.js).\n\n## Update\n\nWe're halfway through the CRUD operations. Now that we know how to **create** and **read** documents, let's discover how to **update** them.\n\n### Update One Document\n\nLet's begin by updating a single Airbnb listing in the [listingsAndReviews collection](https://docs.atlas.mongodb.com/sample-data/sample-airbnb/#sample-airbnb).\n\nWe can update a single document by calling [Collection](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html)'s [updateOne()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#updateOne).  `updateOne()` has two required parameters:\n\n1.  `filter` (object): the Filter used to select the document to update.  You can think of the filter as essentially the same as the query param we used in [findOne()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#findOne) to search for a particular document. You can include zero properties in the filter to search for all documents in the collection, or you can include one or more properties to narrow your search.\n2.  `update` (object): the update operations to be applied to the document. MongoDB has a variety of update operators you can use such as `$inc`, `$currentDate`, `$set`, and `$unset` among others. See the [official documentation](https://docs.mongodb.com/manual/reference/operator/update/) for a complete list of update operators and their descriptions.\n\n`updateOne()` also has an optional `options` param. See the [updateOne()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#updateOne) docs for more information on these options.\n\n`updateOne()` will update the first document that matches the given query. Even if more than one document matches the query, only one document will be updated.\n\nLet's say we want to update an Airbnb listing with a particular name. We can use `updateOne()` to achieve this. We'll include the name of the listing in the filter param. We'll use the [$set](https://docs.mongodb.com/manual/reference/operator/update/set/index.html) update operator to set new values for new or existing fields in the document we are updating. When we use `$set`, we pass a document that contains fields and values that should be updated or created. The document that we pass to `$set` will not replace the existing document; any fields that are part of the original document but not part of the document we pass to `$set` will remain as they are.\n\nOur function to update a listing with a particular name would look like the following:\n\n``` javascript\nasync function updateListingByName(client, nameOfListing, updatedListing) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\")\n                        .updateOne({ name: nameOfListing }, { $set: updatedListing });\n\n    console.log(`${result.matchedCount} document(s) matched the query criteria.`);\n    console.log(`${result.modifiedCount} document(s) was/were updated.`);\n}\n```\n\nLet's say we want to update our Airbnb listing that has the name \"Infinite Views.\" We created this listing in [an earlier section](#create-multiple-documents).\n\n``` javascript\n{ \n    _id: 5db6ed14f2e0a60683d8fe42,\n    name: 'Infinite Views',\n    summary: 'Modern home with infinite views from the infinity pool',\n    property_type: 'House',\n    bedrooms: 5,\n    bathrooms: 4.5,\n    beds: 5 \n}\n```\n\nWe can call `updateListingByName()` by passing a connected MongoClient, the name of the listing, and an object containing the fields we want to update and/or create.\n\n``` javascript\nawait updateListingByName(client, \"Infinite Views\", { bedrooms: 6, beds: 8 });\n```\n\nExecuting this command results in the following output.\n\n``` none\n1 document(s) matched the query criteria.\n1 document(s) was/were updated.\n```\n\nNow our listing has an updated number of bedrooms and beds.\n\n``` json\n{ \n    _id: 5db6ed14f2e0a60683d8fe42,\n    name: 'Infinite Views',\n    summary: 'Modern home with infinite views from the infinity pool',\n    property_type: 'House',\n    bedrooms: 6,\n    bathrooms: 4.5,\n    beds: 8 \n}\n```\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/master/update.js).\n\n### Upsert One Document\n\nOne of the options you can choose to pass to `updateOne()` is upsert.  Upsert is a handy feature that allows you to update a document if it exists or insert a document if it does not.\n\nFor example, let's say you wanted to ensure that an Airbnb listing with a particular name had a certain number of bedrooms and bathrooms.  Without upsert, you'd first use `findOne()` to check if the document existed. If the document existed, you'd use `updateOne()` to update the document. If the document did not exist, you'd use `insertOne()` to create the document. When you use upsert, you can combine all of that functionality into a single command.\n\nOur function to upsert a listing with a particular name can be basically identical to the function we wrote above with one key difference: We'll pass `{upsert: true}` in the `options` param for `updateOne()`.\n\n``` javascript\nasync function upsertListingByName(client, nameOfListing, updatedListing) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\")\n                        .updateOne({ name: nameOfListing }, \n                                   { $set: updatedListing }, \n                                   { upsert: true });\n    console.log(`${result.matchedCount} document(s) matched the query criteria.`);\n\n    if (result.upsertedCount > 0) {\n        console.log(`One document was inserted with the id ${result.upsertedId._id}`);\n    } else {\n        console.log(`${result.modifiedCount} document(s) was/were updated.`);\n    }\n}\n```\n\nLet's say we aren't sure if a listing named \"Cozy Cottage\" is in our collection or, if it does exist, if it holds old data. Either way, we want to ensure the listing that exists in our collection has the most up-to-date data. We can call `upsertListingByName()` with a connected MongoClient, the name of the listing, and an object containing the up-to-date data that should be in the listing.\n\n``` javascript\nawait upsertListingByName(client, \"Cozy Cottage\", { name: \"Cozy Cottage\", bedrooms: 2, bathrooms: 1 });\n```\n\nIf the document did not previously exist, the output of the function would be something like the following:\n\n``` none\n0 document(s) matched the query criteria.\nOne document was inserted with the id 5db9d9286c503eb624d036a1\n```\n\nWe have a new document in the listingsAndReviews collection:\n\n``` json\n{ \n    _id: 5db9d9286c503eb624d036a1,\n    name: 'Cozy Cottage',\n    bathrooms: 1,\n    bedrooms: 2 \n}\n```\n\nIf we discover more information about the \"Cozy Cottage\" listing, we can use `upsertListingByName()` again.\n\n``` javascript\nawait upsertListingByName(client, \"Cozy Cottage\", { beds: 2 });\n```\n\nAnd we would see the following output.\n\n``` none\n1 document(s) matched the query criteria.\n1 document(s) was/were updated.\n```\n\nNow our document has a new field named \"beds.\"\n\n``` json\n{ \n    _id: 5db9d9286c503eb624d036a1,\n    name: 'Cozy Cottage',\n    bathrooms: 1,\n    bedrooms: 2,\n    beds: 2 \n}\n```\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/master/update.js).\n\n### Update Multiple Documents\n\nSometimes you'll want to update more than one document at a time. In this case, you can use [Collection](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html)'s [updateMany()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#updateMany).  Like `updateOne()`, `updateMany()` requires that you pass a filter of type object and an update of type object. You can choose to include options of type object as well.\n\nLet's say we want to ensure that every document has a field named `property_type`. We can use the [$exists](https://docs.mongodb.com/manual/reference/operator/query/exists/#op._S_exists) query operator to search for documents where the `property_type` field does not exist. Then we can use the [$set](https://docs.mongodb.com/manual/reference/operator/update/set/index.html) update operator to set the `property_type` to \"Unknown\" for those documents. Our function will look like the following.\n\n``` javascript\nasync function updateAllListingsToHavePropertyType(client) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\")\n                        .updateMany({ property_type: { $exists: false } }, \n                                    { $set: { property_type: \"Unknown\" } });\n    console.log(`${result.matchedCount} document(s) matched the query criteria.`);\n    console.log(`${result.modifiedCount} document(s) was/were updated.`);\n}\n```\n\nWe can call this function with a connected MongoClient.\n\n``` javascript\nawait updateAllListingsToHavePropertyType(client);\n```\n\nBelow is the output from executing the previous command.\n\n``` none\n3 document(s) matched the query criteria.\n3 document(s) was/were updated.\n```\n\nNow our \"Cozy Cottage\" document and all of the other documents in the Airbnb collection have the `property_type` field.\n\n``` json\n{ \n    _id: 5db9d9286c503eb624d036a1,\n    name: 'Cozy Cottage',\n    bathrooms: 1,\n    bedrooms: 2,\n    beds: 2,\n    property_type: 'Unknown' \n}\n```\n\nListings that contained a `property_type` before we called `updateMany()` remain as they were. For example, the \"Spectacular Modern Uptown Duplex\" listing still has `property_type` set to `Apartment`.\n\n``` json\n{ \n    _id: '582364',\n    listing_url: 'https://www.airbnb.com/rooms/582364',\n    name: 'Spectacular Modern Uptown Duplex',\n    property_type: 'Apartment',\n    room_type: 'Entire home/apt',\n    bedrooms: 4,\n    beds: 7\n    ...\n}\n```\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/master/update.js).\n\n## Delete\n\nNow that we know how to **create**, **read**, and **update** documents, let's tackle the final CRUD operation: **delete**.\n\n### Delete One Document\n\nLet's begin by deleting a single Airbnb listing in the [listingsAndReviews collection](https://docs.atlas.mongodb.com/sample-data/sample-airbnb/#sample-airbnb).\n\nWe can delete a single document by calling [Collection](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html)'s [deleteOne()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#deleteOne).  `deleteOne()` has one required parameter: a filter of type object. The filter is used to select the document to delete. You can think of the filter as essentially the same as the query param we used in [findOne()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#findOne) and the filter param we used in [updateOne()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#updateOne).  You can include zero properties in the filter to search for all documents in the collection, or you can include one or more properties to narrow your search.\n\n`deleteOne()` also has an optional `options` param. See the [deleteOne()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#deleteOne) docs for more information on these options.\n\n`deleteOne()` will delete the first document that matches the given query. Even if more than one document matches the query, only one document will be deleted. If you do not specify a filter, the first document found in [natural order](https://docs.mongodb.com/manual/reference/glossary/#term-natural-order) will be deleted.\n\nLet's say we want to delete an Airbnb listing with a particular name. We can use `deleteOne()` to achieve this. We'll include the name of the listing in the filter param. We can create a function to delete a listing with a particular name.\n\n``` javascript\nasync function deleteListingByName(client, nameOfListing) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\")\n            .deleteOne({ name: nameOfListing });\n    console.log(`${result.deletedCount} document(s) was/were deleted.`);\n}\n```\n\nLet's say we want to delete the Airbnb listing we created in an [earlier section](#upsert-one-document) that has the name \"Cozy Cottage.\" We can call `deleteListingsByName()` by passing a connected MongoClient and the name \"Cozy Cottage.\"\n\n``` javascript\nawait deleteListingByName(client, \"Cozy Cottage\");\n```\n\nExecuting the command above results in the following output.\n\n``` none\n1 document(s) was/were deleted.\n```\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/master/delete.js).\n\n### Deleting Multiple Documents\n\nSometimes you'll want to delete more than one document at a time. In this case, you can use [Collection](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html)'s [deleteMany()](https://mongodb.github.io/node-mongodb-native/3.6/api/Collection.html#deleteMany).  Like `deleteOne()`, `deleteMany()` requires that you pass a filter of type object. You can choose to include options of type object as well.\n\nLet's say we want to remove documents that have not been updated recently. We can call `deleteMany()` with a filter that searches for documents that were scraped prior to a particular date. Our function will look like the following.\n\n``` javascript\nasync function deleteListingsScrapedBeforeDate(client, date) {\n    const result = await client.db(\"sample_airbnb\").collection(\"listingsAndReviews\")\n        .deleteMany({ \"last_scraped\": { $lt: date } });\n    console.log(`${result.deletedCount} document(s) was/were deleted.`);\n}\n```\n\nTo delete listings that were scraped prior to February 15, 2019, we can call `deleteListingsScrapedBeforeDate()` with a connected MongoClient and a Date instance that represents February 15.\n\n``` javascript\nawait deleteListingsScrapedBeforeDate(client, new Date(\"2019-02-15\"));\n```\n\nExecuting the command above will result in the following output.\n\n``` none\n606 document(s) was/were deleted.\n```\n\nNow only recently scraped documents are in our collection.\n\nIf you're not a fan of copying and pasting, you can get a full copy of the code above in the [Node.js Quick Start GitHub Repo](https://github.com/mongodb-developer/nodejs-quickstart/blob/master/delete.js).\n\n## Wrapping Up\n\nWe covered a lot today! Let's recap.\n\n\nWe began by exploring [how MongoDB stores data in documents and collections](#how-mongodb-stores-data). Then we learned the basics of [creating](#create), [reading](#read), [updating](#update), and [deleting](#delete) data.\n\nContinue on to the [next post in this series](https://mongodb.com/developer/languages/javascript/node-aggregation-framework/), where we'll discuss how you can analyze and manipulate data using the aggregation pipeline.\n\nComments? Questions? We'd love to chat with you in the [MongoDB Community](https://community.mongodb.com).\n","description":"Learn how to execute the CRUD (create, read, update, and delete) operations in MongoDB using Node.js in this step-by-step tutorial.","imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt62338cd518132b84/644c4744c6590904f0300942/nodejs.png?branch=prod","description":null}}]},"slug":"/node-crud-tutorial","title":"*MongoDB and Node.js Tutorial - CRUD Operations","original_publish_date":"2022-02-04T12:44:55.053Z","strapi_updated_at":"2022-10-21T16:48:36.871Z","expiry_date":"2022-12-14T02:04:41.232Z","authorsConnection":{"edges":[]},"primary_tag":{"tagConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]}},"other_tags":{"author_typeConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/author-types/mongodb"}}]},"code_type":null,"content_typeConnection":{"edges":[{"node":{"title":"*Quickstart","calculated_slug":"/quickstarts"}}]},"expertise_levelConnection":{"edges":[{"node":{"title":"*Introductory","calculated_slug":"/expertise-levels/introductory"}}]},"github_url":null,"l1_productConnection":{"edges":[{"node":{"title":"*MongoDB","calculated_slug":"/products/mongodb"}}]},"l2_productConnection":{"edges":[]},"livesite_url":null,"programming_languagesConnection":{"edges":[{"node":{"title":"*JavaScript","calculated_slug":"/languages/javascript"}}]},"spoken_languageConnection":{"edges":[]},"technologiesConnection":{"edges":[]}},"seo":{"canonical_url":"","meta_description":"Learn how to execute the CRUD (create, read, update, and delete) operations in MongoDB using Node.js in this step-by-step tutorial.","og_description":"","og_imageConnection":{"edges":[{"node":{"url":"https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltcdbc83c4640e3f23/644c47461297ea7b90d1f5e5/og-pink-pattern.png?branch=prod"}}]},"og_type":"","og_url":"","twitter_creator":"@Lauren_Schaefer"},"system":{"updated_at":"2023-04-28T22:51:57.936Z","publish_details":{"time":"2023-04-28T22:53:02.873Z"}}}]}}